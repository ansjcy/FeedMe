{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "自回归语义视觉重建有助于VLM更好地理解 (原标题: Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better)",
      "link": "https://arxiv.org/abs/2506.09040",
      "pubDate": "Tue, 10 Jun 2025 13:57:50 GMT",
      "isoDate": "2025-06-10T13:57:50.000Z",
      "creator": "Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, Jiaqi Wang",
      "summary": "### 当前大型视觉-语言模型 (LVLMs) 的局限性\n\n*   **过度依赖文本监督：** 典型的LVLMs主要对文本序列应用自回归监督，未能充分将视觉模态整合到学习过程中。\n*   **由此产生的限制：**\n    1.  **无法利用无说明图像：** 模型难以处理没有附带文本说明的图像。\n    2.  **说明信息不完整：** 图像说明可能遗漏关键的视觉细节。\n    3.  **文本表达局限性：** 某些以视觉为中心的内容无法通过文本充分传达。\n*   **视觉信息处理不足：** 结果是，现有LVLMs通常优先考虑视觉到语言的对齐，可能忽视细粒度的视觉信息。\n\n### 提出的解决方案：自回归语义视觉重建 (ASVR)\n\n*   **核心思想：** ASVR 引入了一种新方法，旨在在一个统一的自回归框架内实现视觉和文本模态的联合学习。\n*   **关键发现：**\n    *   研究表明，自回归重建图像的**原始视觉外观**并不能增强甚至可能损害多模态理解。\n    *   相反，自回归重建图像的**语义表示**能持续改善理解能力。\n*   **机制：** 即使模型输入是连续的图像特征，它们也能有效地重建离散的语义标记，从而带来稳定且一致的性能提升。\n\n### ASVR 的性能提升\n\n*   **广泛适用性：** ASVR 在不同数据规模（556k-2M）和不同LLM骨干网络上都实现了显著的性能提升。\n*   **具体成果：** ASVR 使 LLaVA-1.5 在14个多模态基准测试中的平均得分提高了5%。\n\n### 代码可用性\n\n*   相关代码已公开提供。",
      "shortSummary": "当前大型视觉-语言模型（LVLMs）因过度依赖文本而存在视觉理解局限。本文提出自回归语义视觉重建（ASVR），通过在统一自回归框架中联合学习视觉和文本模态。研究发现，重建图像的语义表示而非原始外观，能显著提升多模态理解能力。ASVR使模型即使从连续图像特征也能有效重建离散语义标记，并在多项基准测试中实现稳定提升，例如使LLaVA-1.5平均得分提高5%。",
      "translated_title": "自回归语义视觉重建有助于VLM更好地理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in three key limitations: (1) an inability to utilize images without accompanying captions, (2) the risk that captions omit critical visual details, and (3) the challenge that certain vision-centric content cannot be adequately conveyed through text. As a result, current LVLMs often prioritize vision-to-language alignment while potentially overlooking fine-grained visual information. While some prior works have explored autoregressive image generation, effectively leveraging autoregressive visual supervision to enhance image understanding remains an open challenge. In this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR), which enables joint learning of visual and textual modalities within a unified autoregressive framework. We show that autoregressively reconstructing the raw visual appearance of images does not enhance and may even impair multimodal understanding. In contrast, autoregressively reconstructing the semantic representation of images consistently improves comprehension. Notably, we find that even when models are given continuous image features as input, they can effectively reconstruct discrete semantic tokens, resulting in stable and consistent improvements across a wide range of multimodal understanding benchmarks. Our approach delivers significant performance gains across varying data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is available at https://github.com/AlenjandroWang/ASVR."
    },
    {
      "title": "DiscoVLA：用于参数高效视频文本检索的视觉、语言和对齐差异消减 (原标题: DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval)",
      "link": "https://arxiv.org/abs/2506.08887",
      "pubDate": "Tue, 10 Jun 2025 11:16:40 GMT",
      "isoDate": "2025-06-10T11:16:40.000Z",
      "creator": "Leqi Shen, Guoqiang Gong, Tianxiang Hao, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Jungong Han, Guiguang Ding",
      "summary": "# DiscoVLA：用于参数高效视频文本检索的视觉、语言和对齐差异消减\n\n## 引言与背景\n当前研究热点在于如何将预训练的图像-文本模型CLIP高效地应用于视频-文本检索。CLIP模型主要关注图像级别的视觉-语言匹配，但视频-文本检索则要求对视频内容有更全面的理解。在从图像级别向视频级别迁移的过程中，出现了三个关键的差异：**视觉差异**、**语言差异**和**对齐差异**。然而，现有的大多数方法主要集中于解决视觉差异，而忽视了语言和对齐方面的挑战。\n\n## DiscoVLA方法概述\n本文提出了一种名为 **DiscoVLA (Discrepancy Reduction in Vision, Language, and Alignment)** 的新方法，旨在同时缓解上述三种差异。\n\n### 核心策略\nDiscoVLA通过以下创新机制来解决这些差异：\n*   **图像-视频特征融合 (Image-Video Features Fusion)：** 该模块旨在整合图像级别的特征与视频级别的特征。通过这种融合，DiscoVLA能够有效地处理视觉差异和语言差异，确保模型对不同粒度的视觉和文本信息都有全面的理解。\n*   **生成伪图像字幕 (Pseudo Image Captions Generation)：** 为了学习更细粒度的图像级别对齐，DiscoVLA会生成伪图像字幕。这有助于模型在图像层面建立精确的视觉-语言对应关系。\n*   **图像到视频对齐蒸馏 (Image-to-Video Alignment Distillation)：** 为了缓解视频级别的对齐差异，DiscoVLA利用了图像级别学习到的对齐知识。通过将图像级别的对齐信息蒸馏到视频级别，模型能够增强视频内容的视觉-语言对齐能力。\n\n## 实验结果与性能\n广泛的实验结果表明了DiscoVLA方法的优越性。特别是在MSRVTT数据集上，当使用CLIP (ViT-B/16) 作为基础模型时，DiscoVLA在R@1指标上取得了显著提升，超越了现有方法1.5%，最终达到了50.5%的R@1分数。这充分证明了DiscoVLA在解决视频-文本检索中视觉、语言和对齐差异方面的有效性。\n\n## 代码可用性\n论文指出，DiscoVLA的代码已公开。",
      "shortSummary": "“DiscoVLA”旨在解决CLIP模型在视频-文本检索中面临的视觉、语言和对齐三大差异。该方法通过图像-视频特征融合来处理视觉和语言差异，并利用生成伪图像字幕和图像到视频对齐蒸馏来增强对齐。实验证明，DiscoVLA性能优越，在MSRVTT数据集上，R@1指标达到50.5%，超越现有方法1.5%，有效提升了参数高效视频文本检索的准确性。",
      "translated_title": "DiscoVLA：用于参数高效视频文本检索的视觉、语言和对齐差异消减",
      "images": [],
      "contentSource": "完整文章",
      "content": "The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA."
    },
    {
      "title": "RuleReasoner：通过领域感知动态采样强化基于规则的推理 (原标题: RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling)",
      "link": "https://arxiv.org/abs/2506.08672",
      "pubDate": "Tue, 10 Jun 2025 06:31:21 GMT",
      "isoDate": "2025-06-10T06:31:21.000Z",
      "creator": "Yang Liu, Jiaqi Li, Zilong Zheng",
      "summary": "### RuleReasoner：通过领域感知动态采样强化基于规则的推理\n\n**1. 背景与挑战**\n\n*   **基于规则的推理**：被认为是推理领域的基础问题之一。\n*   **现实世界挑战**：规则格式、类型和复杂性的差异带来了严峻挑战。\n*   **大型推理模型 (LRMs)**：近期研究表明，LRMs 具有卓越的推理能力，并通过强化学习 (RL) 显著增强了性能。\n*   **未解决问题**：小型推理模型 (SRMs) 是否能有效学习基于规则的推理，并在不同任务和领域中展现出强大的泛化能力，仍是一个开放性问题。\n\n**2. RuleReasoner 方法介绍**\n\n*   **目标**：解决SRMs在基于规则推理中的泛化能力问题。\n*   **方法名称**：RuleReasoner，即“强化基于规则的推理”。\n*   **核心机制**：通过精心策划的大量任务和一种新颖的“领域感知动态采样”方法进行基于规则的推理。\n*   **具体实现**：\n    *   RuleReasoner 根据历史奖励更新不同领域的采样权重，从而重新采样每个训练批次。\n    *   这种方法促进了领域增强和RL的灵活在线学习调度。\n    *   它避免了现有方法中需要预先进行人工设计的混合训练方案。\n\n**3. 实验评估与结果**\n\n*   **评估基准**：在分布内 (ID) 和分布外 (OOD) 基准上进行了实证评估。\n*   **性能表现**：\n    *   RuleReasoner 显著优于前沿的 LRMs。\n    *   在八个 ID 任务上平均提升 4.1% 的点数（相对于 OpenAI-o1）。\n    *   在三个 OOD 任务上平均提升 10.4% 的点数（相对于 OpenAI-o1）。\n*   **计算效率**：与现有用于 RL 的动态采样方法相比，RuleReasoner 也展现出更高的计算效率。\n\n**4. 总结**\n\nRuleReasoner 是一种简单而有效的方法，通过领域感知动态采样，显著提升了小型推理模型在基于规则推理任务上的性能和泛化能力，并在计算效率上优于现有方法。",
      "shortSummary": "RuleReasoner 是一种通过领域感知动态采样强化基于规则推理的新方法。它解决了小型推理模型（SRMs）的泛化挑战，通过根据历史奖励动态调整领域采样权重，避免了复杂的人工训练配置。实验表明，RuleReasoner 在分布内和分布外基准测试中均显著优于大型推理模型（LRMs），并展现出更高的计算效率，为基于规则的推理提供了有效且高效的解决方案。",
      "translated_title": "RuleReasoner：通过领域感知动态采样强化基于规则的推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rule-based reasoning has been acknowledged as one of the fundamental problems in reasoning, while deviations in rule formats, types, and complexity in real-world applications pose severe challenges. Recent studies have shown that large reasoning models (LRMs) have remarkable reasoning capabilities, and their performance is substantially enhanced by reinforcement learning (RL). However, it remains an open question whether small reasoning models (SRMs) can learn rule-based reasoning effectively with robust generalization across diverse tasks and domains. To address this, we introduce Reinforced Rule-based Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct rule-based reasoning via a wide collection of curated tasks and a novel domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples each training batch by updating the sampling weights of different domains based on historical rewards. This facilitates domain augmentation and flexible online learning schedules for RL, obviating the need for pre-hoc human-engineered mix-training recipes used in existing methods. Empirical evaluations on in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that RuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1% average points on eight ID tasks and Delta10.4% average points on three OOD tasks over OpenAI-o1). Notably, our approach also exhibits higher computational efficiency compared to prior dynamic sampling methods for RL."
    },
    {
      "title": "GUI-Reflection：赋予多模态GUI模型自反思能力 (原标题: GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior)",
      "link": "https://arxiv.org/abs/2506.08012",
      "pubDate": "Mon, 09 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-09T13:59:57.000Z",
      "creator": "Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, Ziwei Liu",
      "summary": "### GUI-Reflection：赋予多模态GUI模型自反思能力\n\n本文提出了一个名为GUI-Reflection的新框架，旨在解决现有多模态大语言模型（MLLMs）在图形用户界面（GUI）自动化中普遍存在的缺乏反思和错误恢复能力的问题。现有GUI模型主要依赖于从几乎无错误的离线轨迹中学习，因此在面对错误时表现不足。\n\n**核心问题与解决方案：**\n\n*   **问题：** 现有GUI模型缺乏自反思和错误恢复能力，因为它们主要从无错误的离线数据中学习。\n*   **解决方案：** GUI-Reflection框架通过专门的训练阶段，明确地将自反思和错误纠正能力整合到端到端的多模态GUI模型中。\n\n**GUI-Reflection框架的关键组成部分和创新点：**\n\n1.  **专用训练阶段：**\n    *   **GUI特定预训练：** 奠定基础。\n    *   **离线监督微调（SFT）：** 利用现有数据进行初步学习。\n    *   **在线反思调优：** 核心阶段，使模型能够在线学习反思和纠错。\n\n2.  **全自动化数据生成与学习：**\n    *   GUI-Reflection实现了完全自动化的数据生成和学习过程，无需任何人工标注，从而促进了自反思行为的出现。\n    *   **可扩展数据管道：** 提出可扩展的数据管道，能够从现有成功的轨迹中自动构建反思和错误纠正数据。\n\n3.  **GUI-Reflection任务套件：**\n    *   与现有GUI模型主要关注接地和UI理解能力不同，GUI-Reflection提出了一个专门的任务套件，用于明确学习和评估面向反思的能力。\n\n4.  **多样化高效的在线环境：**\n    *   构建了一个多样化且高效的环境，用于在移动设备上进行GUI模型的在线训练和数据收集。\n\n5.  **迭代在线反思调优算法：**\n    *   利用上述环境，提出了一种迭代的在线反思调优算法，使模型能够持续增强其反思和错误纠正能力。\n\n**预期成果与发布：**\n\n该框架旨在赋予GUI智能体自反思和纠正能力，为实现更鲁棒、适应性更强、更智能的GUI自动化铺平道路。所有相关数据、模型、环境和工具都将公开发布。",
      "shortSummary": "GUI-Reflection是一个新颖的框架，旨在通过整合自反思和错误纠正能力，革新多模态GUI模型。它通过GUI特定预训练、离线监督微调和在线反思调优等专用训练阶段实现这一目标。该框架采用全自动化数据生成和学习过程，无需人工标注，并引入了GUI-Reflection任务套件和迭代在线反思调优算法。最终目标是使GUI智能体更鲁棒、适应性更强，并实现更智能的GUI自动化。",
      "translated_title": "GUI-Reflection：赋予多模态GUI模型自反思能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation. However, existing GUI models mostly rely on learning from nearly error-free offline trajectories, thus lacking reflection and error recovery capabilities. To bridge this gap, we propose GUI-Reflection, a novel framework that explicitly integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. GUI-reflection enables self-reflection behavior emergence with fully automated data generation and learning processes without requiring any human annotation. Specifically, 1) we first propose scalable data pipelines to automatically construct reflection and error correction data from existing successful trajectories. While existing GUI models mainly focus on grounding and UI understanding ability, we propose the GUI-Reflection Task Suite to learn and evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a diverse and efficient environment for online training and data collection of GUI models on mobile devices. 3) We also present an iterative online reflection tuning algorithm leveraging the proposed environment, enabling the model to continuously enhance its reflection and error correction abilities. Our framework equips GUI agents with self-reflection and correction capabilities, paving the way for more robust, adaptable, and intelligent GUI automation, with all data, models, environments, and tools to be released publicly."
    },
    {
      "title": "视觉Transformer不需要训练过的寄存器 (原标题: Vision Transformers Don't Need Trained Registers)",
      "link": "https://arxiv.org/abs/2506.08010",
      "pubDate": "Mon, 09 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-09T13:59:57.000Z",
      "creator": "Nick Jiang, Amil Dravid, Alexei Efros, Yossi Gandelsman",
      "summary": "本文研究了视觉Transformer中先前发现的一种现象的潜在机制——高范数token的出现，导致噪声注意力图。研究表明，在多个模型（例如，CLIP，DINOv2）中，一组稀疏的神经元负责将高范数激活集中在异常值token上，从而导致不规则的注意力模式并降低下游视觉处理的性能。现有的解决方案需要从头开始重新训练模型，并添加额外的学习到的寄存器token，但本文提出了一种无需训练的方法来缓解这些伪影。通过将高范数激活从发现的寄存器神经元转移到额外的未经训练的token中，可以模拟寄存器token对已经训练过的模型的影响。实验证明，该方法可以产生更清晰的注意力和特征图，提高多个下游视觉任务中优于基础模型的性能，并达到与使用寄存器token显式训练的模型相当的结果。此外，本文还将测试时寄存器扩展到现成的视觉语言模型，以提高其可解释性。结果表明，测试时寄存器有效地承担了测试时寄存器token的角色，为任何没有发布它们的预训练模型提供了一种无需训练的解决方案。",
      "shortSummary": "本文研究了视觉Transformer中高范数token导致噪声注意力图的现象，发现一组稀疏神经元负责将高范数激活集中在异常值token上。提出了一种无需训练的方法，通过将高范数激活转移到未经训练的token中，模拟寄存器token的效果，从而改善注意力和特征图，提高下游视觉任务的性能，并提高视觉语言模型的可解释性。该方法为没有寄存器token的预训练模型提供了一种无需训练的解决方案。",
      "translated_title": "视觉Transformer不需要训练过的寄存器",
      "images": [],
      "contentSource": "完整文章",
      "content": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned register tokens, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered register neurons into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models to improve their interpretability. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them."
    },
    {
      "title": "自强制：弥合自回归视频扩散模型中的训练-测试鸿沟 (原标题: Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion)",
      "link": "https://arxiv.org/abs/2506.08009",
      "pubDate": "Mon, 09 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-09T13:59:55.000Z",
      "creator": "Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Eli Shechtman",
      "summary": "### 自强制：弥合自回归视频扩散模型中的训练-测试鸿沟\n\n本文介绍了一种名为“自强制”（Self Forcing）的新型训练范式，专为自回归视频扩散模型设计。它旨在解决长期存在的“曝光偏差”（exposure bias）问题，即模型在训练时依赖真实上下文，但在推理时却必须基于自身不完美的输出进行生成。\n\n#### 核心机制\n\n*   **条件生成：** 与以往基于真实上下文帧去噪未来帧的方法不同，“自强制”在训练期间通过键值（KV）缓存进行自回归展开（autoregressive rollout），从而使每一帧的生成都以先前自身生成的输出为条件。\n*   **整体损失：** 这种策略通过视频层面的整体损失（holistic loss）实现监督，直接评估整个生成序列的质量，而非仅仅依赖传统的逐帧目标。\n\n#### 效率与性能优化\n\n*   **训练效率：** 为确保训练效率，该方法采用了少步扩散模型（few-step diffusion model）和随机梯度截断策略（stochastic gradient truncation strategy），有效平衡了计算成本和性能。\n*   **视频外推：** 进一步引入了滚动KV缓存机制（rolling KV cache mechanism），以实现高效的自回归视频外推。\n\n#### 实验结果\n\n*   广泛的实验证明，该方法实现了实时流式视频生成。\n*   在单个GPU上实现了亚秒级（sub-second）的低延迟。\n*   生成质量与显著更慢且非因果的扩散模型相当甚至超越。",
      "shortSummary": "自强制（Self Forcing）是一种新型训练范式，旨在解决自回归视频扩散模型中的曝光偏差问题。它通过在训练时让模型基于自身先前生成的输出进行条件生成，并采用视频层面的整体损失进行监督。该方法结合了少步扩散和滚动KV缓存等优化策略，实现了在单个GPU上实时、亚秒级延迟的流式视频生成，其质量可与更慢的非因果模型媲美甚至超越。",
      "translated_title": "自强制：弥合自回归视频扩散模型中的训练-测试鸿沟",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/"
    },
    {
      "title": "强化预训练 (原标题: Reinforcement Pre-Training)",
      "link": "https://arxiv.org/abs/2506.08007",
      "pubDate": "Mon, 09 Jun 2025 13:59:53 GMT",
      "isoDate": "2025-06-09T13:59:53.000Z",
      "creator": "Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, Furu Wei",
      "summary": "## 强化预训练 (RPT) 概述\n\n本文介绍了一种名为“强化预训练 (Reinforcement Pre-Training, RPT)”的新型扩展范式，旨在应用于大型语言模型 (LLM) 和强化学习 (RL) 领域。\n\n### RPT 的核心理念\n\n*   **任务重构**：RPT 将传统的“下一词元预测”任务重新定义为一项推理任务。\n*   **RL 训练**：该推理任务通过强化学习进行训练，模型因正确预测下一词元而获得可验证的奖励。\n\n### RPT 的优势与特点\n\n*   **数据利用效率**：RPT 提供了一种可扩展的方法，能够有效利用海量的文本数据进行通用目的的强化学习，从而避免了对特定领域标注答案的依赖。\n*   **能力提升**：通过激励模型进行下一词元推理的能力，RPT 显著提高了语言模型在预测下一词元方面的准确性。\n*   **基础奠定**：RPT 为后续的强化微调提供了强大的预训练基础。\n\n### 实验结果与展望\n\n*   **扩展性表现**：扩展曲线（scaling curves）表明，增加训练计算量能够持续提升下一词元预测的准确性。\n*   **前景**：这些结果表明，RPT 是一种有效且有前景的扩展范式，有望推动语言模型预训练的进一步发展。",
      "shortSummary": "本文提出“强化预训练 (RPT)”作为大型语言模型和强化学习的新范式。RPT 将下一词元预测重构为RL推理任务，通过可验证奖励进行训练。它能有效利用海量文本数据进行通用RL，显著提升语言模型准确性，并为后续微调奠定基础。实验显示，增加计算资源可持续提高预测准确率，表明RPT是推进语言模型预训练的有效方法。",
      "translated_title": "强化预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training."
    },
    {
      "title": "Dreamland：利用模拟器和生成模型实现可控世界创建 (原标题: Dreamland: Controllable World Creation with Simulator and Generative Models)",
      "link": "https://arxiv.org/abs/2506.08006",
      "pubDate": "Mon, 09 Jun 2025 13:59:52 GMT",
      "isoDate": "2025-06-09T13:59:52.000Z",
      "creator": "Sicheng Mo, Ziyang Leng, Leon Liu, Weizhen Wang, Honglin He, Bolei Zhou",
      "summary": "## Dreamland：可控世界创建的混合框架\n\n### 挑战\n大型视频生成模型虽然能合成多样化且逼真的动态世界内容，但它们通常缺乏对元素级别的精细控制能力。这限制了它们在场景编辑和训练具身AI智能体方面的应用。\n\n### 解决方案：Dreamland 框架\nDreamland 是一种混合世界生成框架，旨在结合物理模拟器的精细控制能力与大规模预训练生成模型的光照真实感内容输出。其核心设计包括：\n\n*   **分层世界抽象**：设计了一种中间表示，编码像素级和对象级的语义与几何信息，以桥接模拟器和生成模型。\n\n### 核心优势\nDreamland 方法带来了多重优势：\n\n*   **增强可控性**：通过结合模拟器，实现了对世界元素的更精细控制。\n*   **最小化适应成本**：通过早期与真实世界分布对齐，减少了模型适应的成本。\n*   **支持即插即用**：能够直接利用现有及未来的预训练生成模型。\n\n### 数据集\n为了促进混合生成管道的训练和评估，研究团队构建了 **D3Sim 数据集**。\n\n### 实验结果\n实验证明，Dreamland 在性能上超越了现有基线：\n\n*   **图像质量提升**：图像质量提高了50.8%。\n*   **可控性增强**：可控性增强了17.9%。\n*   **具身智能体训练潜力**：在增强具身智能体训练方面展现出巨大潜力。\n\n### 可用性\n项目代码和数据将对外开放。",
      "shortSummary": "Dreamland是一个混合世界生成框架，旨在解决现有大型视频生成模型缺乏精细控制的问题。它结合了物理模拟器的精确控制和生成模型的逼真内容输出，通过分层世界抽象作为中间表示。该框架显著提升了图像质量和可控性（分别提高50.8%和17.9%），并有望增强具身AI智能体的训练。研究团队还构建了D3Sim数据集，代码和数据将公开。",
      "translated_title": "Dreamland：利用模拟器和生成模型实现可控世界创建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale video generative models can synthesize diverse and realistic visual content for dynamic world creation, but they often lack element-wise controllability, hindering their use in editing scenes and training embodied AI agents. We propose Dreamland, a hybrid world generation framework combining the granular control of a physics-based simulator and the photorealistic content output of large-scale pretrained generative models. In particular, we design a layered world abstraction that encodes both pixel-level and object-level semantics and geometry as an intermediate representation to bridge the simulator and the generative model. This approach enhances controllability, minimizes adaptation cost through early alignment with real-world distributions, and supports off-the-shelf use of existing and future pretrained generative models. We further construct a D3Sim dataset to facilitate the training and evaluation of hybrid generation pipelines. Experiments demonstrate that Dreamland outperforms existing baselines with 50.8% improved image quality, 17.9% stronger controllability, and has great potential to enhance embodied agent training. Code and data will be made available."
    },
    {
      "title": "逐Token对齐文本、图像和3D结构 (原标题: Aligning Text, Images, and 3D Structure Token-by-Token)",
      "link": "https://arxiv.org/abs/2506.08002",
      "pubDate": "Mon, 09 Jun 2025 13:59:37 GMT",
      "isoDate": "2025-06-09T13:59:37.000Z",
      "creator": "Aadarsh Sahoo, Vansh Tibrewal, Georgia Gkioxari",
      "summary": "### 逐Token对齐文本、图像和3D结构\n\n**引言**\n文章指出，创建能够理解3D世界的机器对于辅助3D环境设计者、编辑者以及在三维空间中导航和交互的机器人至关重要。受语言和图像建模领域进展的启发，作者团队探索了自回归模型在处理一种新模态——结构化3D场景——方面的潜力。\n\n**核心贡献**\n为此，研究人员提出了一个统一的LLM（大型语言模型）框架，旨在对齐语言、图像和3D场景。该框架还附带了一份详细的“操作指南”（cookbook），其中概述了实现最佳训练和性能的关键设计选择，解决了数据表示、模态特定目标等核心问题。\n\n**方法与评估**\n该方法在四项核心3D任务上进行了性能评估，包括：\n*   渲染 (Rendering)\n*   识别 (Recognition)\n*   指令遵循 (Instruction-following)\n*   问答 (Question-answering)\n\n评估使用了四种3D数据集，涵盖了合成数据和真实世界数据。\n\n**扩展应用**\n研究进一步将该方法扩展到重建复杂的3D物体形状。通过使用量化形状编码来丰富3D模态，模型在真实世界的3D物体识别任务中展现了其有效性。",
      "shortSummary": "该研究提出一个统一的LLM框架，旨在逐Token对齐语言、图像和结构化3D场景，以实现机器对3D世界的理解。文章提供了一份详细的“操作指南”，涵盖数据表示和训练优化。模型在渲染、识别、指令遵循和问答等四项核心3D任务及合成与真实世界数据上进行了评估，并扩展到通过量化形状编码重建复杂3D物体，在真实世界3D物体识别中表现出有效性。",
      "translated_title": "逐Token对齐文本、图像和3D结构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/"
    },
    {
      "title": "重新思考多模态扩散Transformer中的跨模态交互 (原标题: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers)",
      "link": "https://arxiv.org/abs/2506.07986",
      "pubDate": "Mon, 09 Jun 2025 13:54:04 GMT",
      "isoDate": "2025-06-09T13:54:04.000Z",
      "creator": "Zhengyao Lv, Tianlin Pan, Chenyang Si, Zhaoxi Chen, Wangmeng Zuo, Ziwei Liu, Kwan-Yee K. Wong",
      "summary": "### 背景与问题\n\n*   多模态扩散Transformer (MM-DiTs) 在文本驱动的视觉生成方面取得了显著进展，但即使是FLUX等最先进的模型，在实现文本提示与生成内容之间的精确对齐方面仍面临挑战。\n*   研究发现MM-DiT的注意力机制存在两个关键问题，阻碍了对齐：\n    1.  由于视觉和文本模态之间的token不平衡，导致跨模态注意力受到抑制。\n    2.  缺乏时间步感知的注意力加权。\n\n### 提出的方法：温度调整跨模态注意力 (TACA)\n\n*   为解决上述问题，研究者提出了**温度调整跨模态注意力 (TACA)**。\n*   TACA是一种参数高效的方法，通过温度缩放和时间步依赖的调整，动态地重新平衡多模态交互。\n\n### 实验结果与优势\n\n*   当与LoRA微调结合时，TACA显著增强了T2I-CompBench基准上的文本-图像对齐，且计算开销极小。\n*   TACA已在FLUX和SD3.5等最先进模型上进行了测试，证明其能够改善图像-文本对齐，具体体现在：\n    *   物体外观\n    *   属性绑定\n    *   空间关系\n\n### 结论\n\n*   研究结果强调了平衡跨模态注意力在提高文本到图像扩散模型语义保真度方面的重要性。\n\n### 代码可用性\n\n*   相关代码已公开。",
      "shortSummary": "多模态扩散Transformer (MM-DiTs) 在文本-图像对齐方面存在挑战，主要源于注意力机制中的token不平衡和缺乏时间步感知加权。为解决此问题，研究提出了一种参数高效的**温度调整跨模态注意力 (TACA)** 方法。TACA通过动态调整跨模态交互，显著提升了T2I-CompBench基准上的文本-图像对齐精度，尤其在物体外观、属性绑定和空间关系方面。该方法在FLUX和SD3.5等模型上表现出良好效果，且计算开销极小，强调了平衡跨模态注意力对语义保真度的重要性。",
      "translated_title": "重新思考多模态扩散Transformer中的跨模态交互",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress in text-driven visual generation. However, even state-of-the-art MM-DiT models like FLUX struggle with achieving precise alignment between text prompts and generated content. We identify two key issues in the attention mechanism of MM-DiT, namely 1) the suppression of cross-modal attention due to token imbalance between visual and textual modalities and 2) the lack of timestep-aware attention weighting, which hinder the alignment. To address these issues, we propose Temperature-Adjusted Cross-modal Attention (TACA), a parameter-efficient method that dynamically rebalances multimodal interactions through temperature scaling and timestep-dependent adjustment. When combined with LoRA fine-tuning, TACA significantly enhances text-image alignment on the T2I-CompBench benchmark with minimal computational overhead. We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating its ability to improve image-text alignment in terms of object appearance, attribute binding, and spatial relationships. Our findings highlight the importance of balancing cross-modal attention in improving semantic fidelity in text-to-image diffusion models. Our codes are publicly available at https://github.com/Vchitect/TACA"
    },
    {
      "title": "OneIG-Bench: 面向图像生成的全方位细致评估 (原标题: OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation)",
      "link": "https://arxiv.org/abs/2506.07977",
      "pubDate": "Mon, 09 Jun 2025 13:50:21 GMT",
      "isoDate": "2025-06-09T13:50:21.000Z",
      "creator": "Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, Hai-Bao Chen",
      "summary": "本文介绍了一个名为OneIG-Bench的综合性基准框架，用于对文本到图像（T2I）模型进行细粒度的评估。该框架旨在解决现有基准测试的局限性，这些局限性包括缺乏对推理、文本渲染和风格等方面的全面评估。OneIG-Bench通过在多个维度上评估T2I模型，包括提示-图像对齐、文本渲染精度、推理生成内容、风格化和多样性，从而实现对模型性能的深入分析。该基准测试允许用户灵活地关注特定的评估子集，从而能够有针对性地进行评估。作者公开了代码库和数据集，以促进可重复的评估研究和T2I研究社区内的跨模型比较。",
      "shortSummary": "本文提出了OneIG-Bench，一个用于全面评估文本到图像(T2I)模型的基准框架。该框架旨在解决现有基准测试在推理、文本渲染和风格等方面评估不足的问题。OneIG-Bench通过评估提示-图像对齐、文本渲染精度、推理生成内容、风格化和多样性等维度，实现对模型性能的深入分析。代码库和数据集已公开，以促进可重复的评估研究。",
      "translated_title": "OneIG-Bench: 面向图像生成的全方位细致评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-to-image (T2I) models have garnered significant attention for generating high-quality images aligned with text prompts. However, rapid T2I model advancements reveal limitations in early benchmarks, lacking comprehensive evaluations, for example, the evaluation on reasoning, text rendering and style. Notably, recent state-of-the-art models, with their rich knowledge modeling capabilities, show promising results on the image generation problems requiring strong reasoning ability, yet existing evaluation systems have not adequately addressed this frontier. To systematically address these gaps, we introduce OneIG-Bench, a meticulously designed comprehensive benchmark framework for fine-grained evaluation of T2I models across multiple dimensions, including prompt-image alignment, text rendering precision, reasoning-generated content, stylization, and diversity. By structuring the evaluation, this benchmark enables in-depth analysis of model performance, helping researchers and practitioners pinpoint strengths and bottlenecks in the full pipeline of image generation. Specifically, OneIG-Bench enables flexible evaluation by allowing users to focus on a particular evaluation subset. Instead of generating images for the entire set of prompts, users can generate images only for the prompts associated with the selected dimension and complete the corresponding evaluation accordingly. Our codebase and dataset are now publicly available to facilitate reproducible evaluation studies and cross-model comparisons within the T2I research community."
    },
    {
      "title": "Squeeze3D：您的3D生成模型是秘密的极致神经压缩器 (原标题: Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor)",
      "link": "https://arxiv.org/abs/2506.07932",
      "pubDate": "Mon, 09 Jun 2025 12:52:10 GMT",
      "isoDate": "2025-06-09T12:52:10.000Z",
      "creator": "Rishit Dagli, Yushi Guan, Sankeerth Durvasula, Mohammadreza Mofayezi, Nandita Vijaykumar",
      "summary": "Squeeze3D是一种新颖的框架，它利用现有预训练3D生成模型学习到的隐式先验知识，以极高的压缩比压缩3D数据。\n\n### 核心理念\n\nSqueeze3D将预训练编码器和预训练生成模型之间的潜在空间通过可训练的映射网络连接起来，从而实现高效的3D数据压缩和解压缩。\n\n### 工作原理\n\n1.  **压缩阶段**：\n    *   任何表示为网格、点云或辐射场的3D模型，首先由预训练编码器进行编码。\n    *   编码后的数据被转换成一个高度紧凑的潜在代码，作为网格或点云的极致压缩表示。\n2.  **解压缩阶段**：\n    *   一个映射网络将这个压缩的潜在代码转换到强大生成模型的潜在空间。\n    *   生成模型被条件化以重建原始3D模型。\n\n### 训练与灵活性\n\n*   Squeeze3D完全在生成的合成数据上进行训练，无需任何3D数据集。\n*   该架构具有高度灵活性，可与现有的预训练3D编码器和生成模型配合使用。\n*   它能够灵活支持不同的3D数据格式，包括网格、点云和辐射场。\n\n### 性能表现\n\n实验结果表明，Squeeze3D在保持与许多现有方法相当的视觉质量的同时，实现了显著的压缩比：\n\n*   **带纹理网格**：最高可达2187倍压缩比。\n*   **点云**：最高可达55倍压缩比。\n*   **辐射场**：最高可达619倍压缩比。\n\n此外，Squeeze3D的压缩和解压缩延迟很小，因为它不涉及为每个对象训练特定的网络。",
      "shortSummary": "Squeeze3D是一个利用预训练3D生成模型隐式知识的框架，旨在实现极致的3D数据压缩。它通过映射网络连接编码器和生成模型的潜在空间，将3D模型压缩成紧凑的潜在代码，并能高效重建。该模型在合成数据上训练，支持多种3D格式，并实现了高达2187倍的网格压缩比，同时保持视觉质量和低延迟。",
      "translated_title": "Squeeze3D：您的3D生成模型是秘密的极致神经压缩器",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose Squeeze3D, a novel framework that leverages implicit prior knowledge learnt by existing pre-trained 3D generative models to compress 3D data at extremely high compression ratios. Our approach bridges the latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. Any 3D model represented as a mesh, point cloud, or a radiance field is first encoded by the pre-trained encoder and then transformed (i.e. compressed) into a highly compact latent code. This latent code can effectively be used as an extremely compressed representation of the mesh or point cloud. A mapping network transforms the compressed latent code into the latent space of a powerful generative model, which is then conditioned to recreate the original 3D model (i.e. decompression). Squeeze3D is trained entirely on generated synthetic data and does not require any 3D datasets. The Squeeze3D architecture can be flexibly used with existing pre-trained 3D encoders and existing generative models. It can flexibly support different formats, including meshes, point clouds, and radiance fields. Our experiments demonstrate that Squeeze3D achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields while maintaining visual quality comparable to many existing methods. Squeeze3D only incurs a small compression and decompression latency since it does not involve training object-specific networks to compress an object."
    },
    {
      "title": "使用大型语言模型解决不等式证明 (原标题: Solving Inequality Proofs with Large Language Models)",
      "link": "https://arxiv.org/abs/2506.07927",
      "pubDate": "Mon, 09 Jun 2025 12:43:38 GMT",
      "isoDate": "2025-06-09T12:43:38.000Z",
      "creator": "Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu",
      "summary": "# 使用大型语言模型解决不等式证明\n\n## 摘要\n\n本文探讨了大型语言模型（LLMs）在不等式证明这一复杂数学推理任务中的表现。不等式证明在多个科学和数学领域至关重要，它要求高级推理能力，例如发现紧密界限和策略性地应用定理，这使其成为LLMs的一个独特且具有挑战性的前沿领域。\n\n## 现有挑战与解决方案\n\n*   **现有数据集的局限性：** 当前用于不等式证明的数据集通常稀缺、合成或过于形式化，阻碍了LLMs在该领域的进展。\n*   **提出的任务形式化：** 为解决这一问题，研究者提出了一种非正式但可验证的任务形式化方法，将不等式证明重新定义为两个可自动检查的子任务：\n    *   **界限估计 (Bound Estimation)**\n    *   **关系预测 (Relation Prediction)**\n\n## IneqMath 数据集\n\n*   **数据集发布：** 基于上述形式化，研究者发布了 **IneqMath** 数据集。\n*   **数据集特点：**\n    *   由专家策划，包含奥林匹克竞赛级别的不等式。\n    *   包括一个测试集和一个训练语料库。\n    *   训练语料库富含分步解决方案和定理注释。\n*   **数据可用性：** 代码和数据已公开提供。\n\n## LLM-as-Judge 评估框架\n\n*   **评估方法：** 研究者开发了一种新颖的“LLM即评判者”评估框架。\n*   **评判者组成：**\n    *   **最终答案评判者 (Final-Answer Judge)：** 评估最终答案的正确性。\n    *   **四种分步评判者 (Four Step-wise Judges)：** 旨在检测常见的推理缺陷，对证明过程的每一步进行细致审查。\n\n## 评估结果与发现\n\n*   **模型表现：** 对29个领先的LLMs在IneqMath数据集上进行了系统评估。\n*   **惊人现实：** 即使是顶尖模型（如o1），在分步审查下的整体准确率也低于10%。\n*   **准确率显著下降：** 与仅考虑最终答案等效性时的准确率相比，分步审查导致准确率下降高达65.5%。\n*   **推理缺陷：** 这种巨大差异揭示了当前LLMs演绎链的脆弱性，以及它们在“仅仅找到一个答案”和“构建一个严谨证明”之间存在的关键差距。\n*   **规模与计算的局限：** 扩大模型规模和增加测试时计算量对整体证明正确性的提升有限。\n\n## 未来研究方向\n\n*   研究结果强调了以下有前景的研究方向：\n    *   **定理引导推理 (Theorem-guided Reasoning)**\n    *   **自我完善 (Self-refinement)**",
      "shortSummary": "大型语言模型在不等式证明中面临严峻挑战。研究者提出了一种新的任务形式化和IneqMath数据集，并开发了“LLM即评判者”评估框架。评估显示，即使是顶尖LLMs，在分步审查下其证明准确率也极低（低于10%），与最终答案准确率存在巨大差距，暴露出其推理链的脆弱性。模型规模和计算量的增加效果有限。未来研究应侧重于定理引导推理和自我完善。",
      "translated_title": "使用大型语言模型解决不等式证明",
      "images": [],
      "contentSource": "完整文章",
      "content": "Inequality proving, crucial across diverse scientific and mathematical fields, tests advanced reasoning skills such as discovering tight bounds and strategic theorem application. This makes it a distinct, demanding frontier for large language models (LLMs), offering insights beyond general mathematical problem-solving. Progress in this area is hampered by existing datasets that are often scarce, synthetic, or rigidly formal. We address this by proposing an informal yet verifiable task formulation, recasting inequality proving into two automatically checkable subtasks: bound estimation and relation prediction. Building on this, we release IneqMath, an expert-curated dataset of Olympiad-level inequalities, including a test set and training corpus enriched with step-wise solutions and theorem annotations. We also develop a novel LLM-as-judge evaluation framework, combining a final-answer judge with four step-wise judges designed to detect common reasoning flaws. A systematic evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even top models like o1 achieve less than 10% overall accuracy under step-wise scrutiny; this is a drop of up to 65.5% from their accuracy considering only final answer equivalence. This discrepancy exposes fragile deductive chains and a critical gap for current LLMs between merely finding an answer and constructing a rigorous proof. Scaling model size and increasing test-time computation yield limited gains in overall proof correctness. Instead, our findings highlight promising research directions such as theorem-guided reasoning and self-refinement. Code and data are available at https://ineqmath.github.io/."
    },
    {
      "title": "MiniCPM4：面向终端设备的超高效大语言模型 (原标题: MiniCPM4: Ultra-Efficient LLMs on End Devices)",
      "link": "https://arxiv.org/abs/2506.07900",
      "pubDate": "Mon, 09 Jun 2025 12:16:50 GMT",
      "isoDate": "2025-06-09T12:16:50.000Z",
      "creator": "MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengdan Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Yukun Yan, Jiarui Yuan, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun",
      "summary": "# MiniCPM4：面向终端设备的超高效大语言模型\n\n本文介绍了MiniCPM4，这是一种专为终端设备设计的高度高效大语言模型（LLM）。MiniCPM4通过在四个关键维度进行系统性创新，实现了其卓越的效率：模型架构、训练数据、训练算法和推理系统。\n\n## 关键创新维度\n\n1.  **模型架构**\n    *   提出了 **InfLLM v2**，这是一种可训练的稀疏注意力机制。\n    *   该机制能够加速长上下文处理中的预填充（prefilling）和解码（decoding）阶段。\n\n2.  **训练数据**\n    *   提出了 **UltraClean**，一种高效且准确的预训练数据过滤和生成策略。\n    *   提出了 **UltraChat v2**，一个全面的监督微调数据集。\n    *   通过使用这些数据集，MiniCPM4仅需8万亿训练tokens即可达到令人满意的模型性能。\n\n3.  **训练算法**\n    *   提出了 **ModelTunnel v2**，用于高效的预训练策略搜索。\n    *   通过引入用于负载均衡强化学习的**分块式展开（chunk-wise rollout）**，改进了现有的后训练方法。\n    *   引入了数据高效的三元大语言模型 **BitCPM**。\n\n4.  **推理系统**\n    *   提出了一个集成了稀疏注意力、模型量化和推测采样（speculative sampling）的系统（原文中为“this http URL”，此处不具体指代）。\n    *   该系统旨在实现高效的预填充和解码。\n\n## 模型版本与性能评估\n\nMiniCPM4提供两个版本，分别具有0.5B和8B参数，以满足多样化的设备端需求。\n\n充分的评估结果表明，MiniCPM4在多个基准测试中均优于同等规模的开源模型，凸显了其效率和有效性。值得注意的是，MiniCPM4-8B在处理长序列时，相比Qwen3-8B展现出显著的速度提升。\n\n## 应用场景\n\n通过进一步的适配，MiniCPM4已成功应用于多种场景，包括：\n*   可信赖的调查问卷生成。\n*   结合模型上下文协议的工具使用。\n\n这清晰地展示了MiniCPM4广泛的可用性。",
      "shortSummary": "MiniCPM4是一款专为终端设备设计的超高效大语言模型。它通过在模型架构（InfLLM v2稀疏注意力）、训练数据（UltraClean、UltraChat v2）、训练算法（ModelTunnel v2、BitCPM）和推理系统等四个维度进行创新，实现了卓越性能。MiniCPM4提供0.5B和8B版本，在多个基准测试中表现优异，尤其MiniCPM4-8B在长序列处理上显著快于Qwen3-8B。它已成功应用于调查生成和工具使用等场景，展现了广泛的实用性。",
      "translated_title": "MiniCPM4：面向终端设备的超高效大语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Sufficient evaluation results show that MiniCPM4 outperforms open-source models of similar size across multiple benchmarks, highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B demonstrates significant speed improvements over Qwen3-8B when processing long sequences. Through further adaptation, MiniCPM4 successfully powers diverse applications, including trustworthy survey generation and tool use with model context protocol, clearly showcasing its broad usability."
    },
    {
      "title": "PolyVivid：具有跨模态交互和增强的生动多主体视频生成 (原标题: PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement)",
      "link": "https://arxiv.org/abs/2506.07848",
      "pubDate": "Mon, 09 Jun 2025 11:11:09 GMT",
      "isoDate": "2025-06-09T11:11:09.000Z",
      "creator": "Teng Hu, Zhentao Yu, Zhengguang Zhou, Jiangning Zhang, Yuan Zhou, Qinglin Lu, Ran Yi",
      "summary": "### PolyVivid：用于多主体视频生成的创新框架\n\n**研究背景与挑战**\n\n尽管视频生成技术取得了显著进展，但现有模型在细粒度控制方面仍存在不足，尤其是在涉及多主体定制时，难以保持一致的身份和主体间的自然交互。\n\n**PolyVivid 框架介绍**\n\n本文提出了一种名为 PolyVivid 的多主体视频定制框架。该框架旨在实现灵活且身份一致的视频生成，以解决当前模型面临的挑战。\n\n**核心模块与技术**\n\nPolyVivid 框架集成了多个创新模块，以确保高质量的多主体视频生成：\n\n*   **基于 VLLM 的文本-图像融合模块：**\n    *   **目的：** 建立主体图像与文本实体之间的精确对应关系。\n    *   **方法：** 将视觉身份信息嵌入到文本空间中，从而实现精确的身份接地（grounding）。\n\n*   **基于 3D-RoPE 的增强模块：**\n    *   **目的：** 进一步增强身份保留和主体间的交互。\n    *   **方法：** 实现文本和图像嵌入之间结构化的双向融合，促进信息流通。\n\n*   **注意力继承的身份注入模块：**\n    *   **目的：** 有效地将融合后的身份特征注入到视频生成过程中。\n    *   **作用：** 显著减轻了在视频生成过程中可能出现的身份漂移问题，确保主体身份的稳定性。\n\n*   **基于 MLLM 的数据管道：**\n    *   **目的：** 生成高质量的多主体数据，为视频生成提供坚实基础。\n    *   **方法：** 结合了基于 MLLM 的接地、分割以及基于团（clique-based）的主体整合策略。\n    *   **作用：** 有效增强了主体之间的区分度，并减少了下游视频生成中的歧义。\n\n**实验结果**\n\n广泛的实验验证表明，PolyVivid 在多个关键性能指标上均表现出色。它在身份保真度、视频真实感和主体对齐方面均取得了卓越的性能，并优于现有的开源和商业基线模型。\n\n**研究领域**\n\n该研究主要属于以下领域：\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "PolyVivid 是一种创新的多主体视频生成框架，旨在解决现有模型在多主体身份一致性和交互控制方面的不足。它通过引入基于 VLLM 的文本-图像融合、基于 3D-RoPE 的增强、注意力继承的身份注入模块，以及基于 MLLM 的数据管道，实现了精确的身份接地、增强的交互和有效的身份保持。实验证明，PolyVivid 在身份保真度、视频真实感和主体对齐方面均优于现有基线。",
      "translated_title": "PolyVivid：具有跨模态交互和增强的生动多主体视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines."
    },
    {
      "title": "图像重建作为特征分析的工具 (原标题: Image Reconstruction as a Tool for Feature Analysis)",
      "link": "https://arxiv.org/abs/2506.07803",
      "pubDate": "Mon, 09 Jun 2025 10:32:18 GMT",
      "isoDate": "2025-06-09T10:32:18.000Z",
      "creator": "Eduard Allakhverdov, Dmitrii Tarasov, Elizaveta Goncharova, Andrey Kuznetsov",
      "summary": "本文提出了一种通过图像重建来解释视觉编码器内部特征表示的新方法。这项研究旨在揭示视觉编码器（包括纯视觉模型和多模态系统，如视觉-语言模型）如何在其内部表示特征，尽管它们在现代应用中取得了显著成功，但其内部机制仍不明确。\n\n**主要发现与应用：**\n\n*   **模型家族比较**：研究比较了SigLIP和SigLIP2两个模型家族，它们仅在训练目标上有所不同。结果显示，在基于图像的任务上预训练的编码器比在非图像任务（例如对比学习）上训练的编码器保留了显著更多的图像信息。\n*   **视觉编码器排名**：该方法被应用于一系列视觉编码器，并根据其特征表示的信息量对它们进行了排名。\n*   **特征空间操作**：研究表明，对特征空间进行操作会在重建图像中产生可预测的变化。具体而言，这揭示了正交旋转（而非空间变换）控制着颜色编码。\n\n**方法普适性与资源：**\n\n*   该方法具有普适性，可以应用于任何视觉编码器，从而有助于阐明其特征空间的内部结构。\n*   重现实验的代码和模型权重已在GitHub上提供。",
      "shortSummary": "本文提出一种通过图像重建解释视觉编码器特征的新方法。研究发现，在图像任务上训练的编码器比非图像任务保留更多图像信息，且通过操纵特征空间，揭示了正交旋转控制颜色编码。该方法普适于任何视觉编码器，有助于理解其内部特征结构。",
      "translated_title": "图像重建作为特征分析的工具",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision encoders are increasingly used in modern applications, from vision-only models to multimodal systems such as vision-language models. Despite their remarkable success, it remains unclear how these architectures represent features internally. Here, we propose a novel approach for interpreting vision features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ only in their training objective, and show that encoders pre-trained on image-based tasks retain significantly more image information than those trained on non-image tasks such as contrastive learning. We further apply our method to a range of vision encoders, ranking them by the informativeness of their feature representations. Finally, we demonstrate that manipulating the feature space yields predictable changes in reconstructed images, revealing that orthogonal rotations (rather than spatial transformations) control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner structure of its feature space. The code and model weights to reproduce the experiments are available in GitHub."
    },
    {
      "title": "穿越峡谷：小语言模型有效长链思维训练的路径 (原标题: Through the Valley: Path to Effective Long CoT Training for Small Language Models)",
      "link": "https://arxiv.org/abs/2506.07712",
      "pubDate": "Mon, 09 Jun 2025 08:56:41 GMT",
      "isoDate": "2025-06-09T08:56:41.000Z",
      "creator": "Renjie Luo, Jiaxi Li, Chen Huang, Wei Lu",
      "summary": "## 穿越峡谷：小语言模型有效长链思维训练的路径\n\n本文研究了长链思维（CoT）监督在增强语言模型推理能力方面的应用。研究发现，对于小型语言模型（SLMs，参数≤3B），在有限的长CoT数据上训练会导致一种称为“长CoT退化”的现象，即性能显著下降。\n\n**主要发现：**\n\n*   **普遍性：** 在Qwen2.5、LLaMA3和Gemma3系列模型上的实验表明，这种退化现象在SLMs中普遍存在。\n*   **性能损失：** 在某些情况下，仅使用8k个长CoT示例训练的模型，其性能会损失高达75%。即使使用220k个长CoT示例训练，一些特别小的模型也无法恢复或超过其微调前的原始性能。\n*   **原因分析：** 这种效应归因于错误累积。虽然更长的响应增加了多步推理的能力，但也放大了累积错误的风险。\n*   **对下游强化学习的影响：** 长CoT退化可能会对下游强化学习（RL）产生负面影响，但可以通过充分扩展的监督微调（SFT）来缓解。\n\n**结论：**\n\n本文的研究结果挑战了关于长CoT训练对SLMs益处的常见假设，并为构建更有效的小型推理模型提供了实践指导。",
      "shortSummary": "本文研究了小型语言模型（SLMs）在长链思维（CoT）训练中出现的“长CoT退化”现象。研究发现，在有限的长CoT数据上训练SLMs会导致性能显著下降，即使使用大量CoT数据也难以恢复。这种退化归因于错误累积，并可能对下游强化学习产生负面影响。研究结果挑战了长CoT训练对SLMs益处的常见假设，并为构建更有效的小型推理模型提供了实践指导。",
      "translated_title": "穿越峡谷：小语言模型有效长链思维训练的路径",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long chain-of-thought (CoT) supervision has become a common strategy to enhance reasoning in language models. While effective for large models, we identify a phenomenon we call Long CoT Degradation, in which small language models (SLMs; &lt;=3B parameters) trained on limited long CoT data experience significant performance deterioration. Through extensive experiments on the Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is widespread across SLMs. In some settings, models trained on only 8k long CoT examples lose up to 75% of their original performance before fine-tuning. Strikingly, we further observe that for some particularly small models, even training on 220k long CoT examples fails to recover or surpass their original performance prior to fine-tuning. Our analysis attributes this effect to error accumulation: while longer responses increase the capacity for multi-step reasoning, they also amplify the risk of compounding mistakes. Furthermore, we find that Long CoT Degradation may negatively impacts downstream reinforcement learning (RL), although this can be alleviated by sufficiently scaled supervised fine-tuning (SFT). Our findings challenge common assumptions about the benefits of long CoT training for SLMs and offer practical guidance for building more effective small-scale reasoning models."
    },
    {
      "title": "使用代理模型评估大型语言模型在低资源语言中的鲁棒性 (原标题: Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models)",
      "link": "https://arxiv.org/abs/2506.07645",
      "pubDate": "Mon, 09 Jun 2025 07:09:39 GMT",
      "isoDate": "2025-06-09T07:09:39.000Z",
      "creator": "Maciej Chrabąszcz, Katarzyna Lorenc, Karolina Seweryn",
      "summary": "# 使用代理模型评估大型语言模型在低资源语言中的鲁棒性\n\n## 摘要\n\n近年来，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展现出令人印象深刻的能力。然而，它们对“越狱”和扰动的敏感性，使得对其鲁棒性进行额外评估变得必要。\n\n## 问题背景\n\n*   **LLMs的脆弱性：** 尽管LLMs功能强大，但它们容易受到越狱攻击和各种扰动的影响。\n*   **语言资源不平衡：** 许多LLMs是多语言的，但其安全相关的训练数据主要包含高资源语言，例如英语。这使得LLMs在低资源语言（如波兰语）中可能更容易受到攻击。\n\n## 研究方法\n\n本研究展示了如何以低成本创建出乎意料的强大攻击：\n*   **少量字符修改：** 仅通过改变少量字符。\n*   **代理模型辅助：** 利用小型代理模型来计算词语的重要性。\n\n## 主要发现与结果\n\n*   **预测显著改变：** 研究发现，这些字符级和词语级的攻击能够显著改变不同LLMs的预测结果。\n*   **潜在安全漏洞：** 这表明LLMs可能存在潜在的漏洞，可以被用来规避其内部安全机制。\n*   **在低资源语言中的验证：** 研究团队在波兰语（一种低资源语言）上验证了其攻击构建方法，并成功发现了LLMs在该语言中的潜在漏洞。\n*   **方法可扩展性：** 此外，研究还表明该攻击方法可以扩展到其他语言。\n\n## 贡献\n\n*   为了促进进一步的研究，本研究发布了所创建的数据集和相关代码。",
      "shortSummary": "本研究评估了大型语言模型（LLMs）在低资源语言中的鲁棒性。发现LLMs因安全训练数据主要集中在高资源语言而易受攻击。研究提出一种低成本攻击方法，通过改变少量字符并利用代理模型，能显著改变LLMs预测，绕过其安全机制。该方法在波兰语中得到验证，并可扩展到其他语言。研究发布了数据集和代码以供进一步研究。",
      "translated_title": "使用代理模型评估大型语言模型在低资源语言中的鲁棒性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research."
    },
    {
      "title": "GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别 (原标题: GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition)",
      "link": "https://arxiv.org/abs/2506.07553",
      "pubDate": "Mon, 09 Jun 2025 04:47:10 GMT",
      "isoDate": "2025-06-09T04:47:10.000Z",
      "creator": "Jingchao Wang, Haote Yang, Jiang Wu, Yifan He, Xingjian Wei, Yinfan Wang, Chengjin Liu, Lingli Ge, Lijun Wu, Bin Wang, Dahua Lin, Conghui He",
      "summary": "# GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别\n\n本文提出了一种名为GTR-Mol-VLM的新框架，用于解决光学化学结构识别（OCSR）中的挑战。该框架包含两个关键创新：\n\n*   **图遍历作为视觉链式思考（Graph Traversal as Visual Chain of Thought）机制**：通过顺序原子-键预测，逐步解析分子图，模拟人类推理过程。\n*   **忠实识别所见（Faithfully Recognize What You've Seen）的数据中心原则**：解决图像中缩写结构与其扩展注释之间的不匹配问题。\n\n为了支持模型开发，作者构建了一个大规模指令调优数据集GTR-CoT-1.3M，并引入了MolRec-Bench，这是第一个用于细粒度评估OCSR中图解析准确性的基准。实验结果表明，GTR-Mol-VLM在分子图像中涉及官能团缩写的情况下，优于其他模型，在基于SMILES和基于图的指标上，比第二好的基线高出约14个百分点。作者希望这项工作能够推动OCSR技术更有效地满足实际需求，从而推进化学信息学和人工智能科学领域的发展。GTR-CoT数据集将在https URL发布。",
      "shortSummary": "本文提出GTR-Mol-VLM框架，用于提升光学化学结构识别（OCSR）的准确性。该框架包含图遍历视觉链式思考机制，模拟人类推理，并采用“忠实识别所见”的数据原则，解决图像缩写结构与注释不匹配问题。作者构建了GTR-CoT-1.3M数据集和MolRec-Bench基准。实验表明，GTR-Mol-VLM在包含官能团缩写的分子图像识别中表现优异，显著优于其他模型。该研究旨在推动OCSR技术发展，服务于化学信息学和人工智能科学领域。",
      "translated_title": "GTR-CoT: 基于图遍历的视觉链式思考用于分子结构识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing chemical knowledge by converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown potential in this task, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To overcome these challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key innovations: (1) the Graph Traversal as Visual Chain of Thought mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric principle of Faithfully Recognize What You've Seen, which addresses the mismatch between abbreviated structures in images and their expanded annotations. To support model development, we constructed GTR-CoT-1.3M, a large-scale instruction-tuning dataset with meticulously corrected annotations, and introduced MolRec-Bench, the first benchmark designed for a fine-grained evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments demonstrate that GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in scenarios involving molecular images with functional group abbreviations, GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage points, both in SMILES-based and graph-based metrics. We hope that this work will drive OCSR technology to more effectively meet real-world needs, thereby advancing the fields of cheminformatics and AI for Science. We will release GTR-CoT at https://github.com/opendatalab/GTR-CoT."
    },
    {
      "title": "BitVLA：用于机器人操作的1-bit视觉-语言-动作模型 (原标题: BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation)",
      "link": "https://arxiv.org/abs/2506.07530",
      "pubDate": "Mon, 09 Jun 2025 04:15:11 GMT",
      "isoDate": "2025-06-09T04:15:11.000Z",
      "creator": "Hongyu Wang, Chuyan Xiong, Ruiping Wang, Xilin Chen",
      "summary": "## BitVLA：用于机器人操作的1-bit视觉-语言-动作模型\n\n本文介绍了一种名为BitVLA的1-bit视觉-语言-动作（VLA）模型，专门用于机器人操作。该模型旨在解决VLA模型日益增长的尺寸所带来的挑战，尤其是在资源受限的机器人系统上的部署问题。\n\n**主要观点：**\n\n*   **1-bit量化：** BitVLA的关键创新在于其所有参数都是三元的，即{-1, 0, 1}，从而显著降低了内存占用。\n*   **蒸馏感知训练：** 为了进一步减少视觉编码器的内存占用，作者提出了一种蒸馏感知训练策略，将全精度编码器压缩到1.58-bit权重。在此过程中，全精度编码器充当教师模型，以更好地对齐潜在表示。\n*   **性能表现：** 尽管缺乏大规模的机器人预训练，BitVLA在LIBERO基准测试中实现了与最先进模型OpenVLA-OFT（采用4-bit后训练量化）相当的性能，同时仅消耗29.8%的内存。\n*   **适用性：** 实验结果表明BitVLA在内存受限的边缘设备上具有部署潜力。\n\n**研究意义：**\n\nBitVLA的提出为在资源受限的机器人平台上部署高性能VLA模型开辟了新的可能性。通过采用1-bit量化和蒸馏感知训练等技术，该模型在保持竞争力的同时，显著降低了内存需求，使其更适合在边缘设备上运行。\n\n**代码和模型权重：**\n\n代码和模型权重已在以下URL中发布：https://example.com",
      "shortSummary": "本文提出了一种名为BitVLA的1-bit视觉-语言-动作模型，用于机器人操作。该模型通过将所有参数量化为三元值{-1, 0, 1}，并采用蒸馏感知训练策略压缩视觉编码器，显著降低了内存占用。实验结果表明，BitVLA在LIBERO基准测试中实现了与最先进模型相当的性能，同时仅消耗29.8%的内存，使其更适合在资源受限的边缘设备上部署。代码和模型权重已发布。",
      "translated_title": "BitVLA：用于机器人操作的1-bit视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA."
    }
  ],
  "lastUpdated": "2025-06-11T09:33:56.209Z"
}