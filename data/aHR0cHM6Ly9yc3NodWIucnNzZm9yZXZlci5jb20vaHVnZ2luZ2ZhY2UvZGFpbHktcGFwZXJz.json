{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处 (原标题: When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs)",
      "link": "https://arxiv.org/abs/2506.20544",
      "pubDate": "Wed, 25 Jun 2025 11:37:53 GMT",
      "isoDate": "2025-06-25T11:37:53.000Z",
      "creator": "Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker",
      "summary": "### 扩展多语言大型语言模型推理计算的益处\n\n**研究背景与问题**\n\n*   近期大型语言模型（LLM）的进展已将重点转向扩展推理时计算，旨在不重新训练模型的情况下提升性能。\n*   一种常见方法是并行采样多个输出，并从中选择一个作为最终输出。\n*   然而，现有工作主要集中于英语以及数学和代码等少数领域，未能推广到开放式任务、可形式验证任务和跨语言场景。\n*   本研究旨在解决如何在多语言、多任务环境下，为开放式生成任务稳健地扩展推理时计算的问题。\n\n**主要发现与挑战**\n\n*   研究发现，基于温度变化的采样策略和选择策略都必须进行调整，以适应多样化的领域和不同的语言设置。\n*   对现有选择方法的评估表明，在英语中有效的策略往往无法推广到其他语言。\n\n**提出的新策略与成果**\n\n*   研究提出了专门针对多语言和多任务推理场景的新型采样和选择策略。\n*   这些策略在不同语言和任务中均取得了显著的性能提升。\n*   **具体成果：**\n    *   对于8B模型，结合本研究的采样和选择方法，在m-ArenaHard-v2.0提示上，与Gemini等专有模型相比，胜率平均提升了+6.8。\n    *   在更大规模的模型上，配备本研究方法的Command-A（111B模型）在相同基准测试中，仅使用五个样本，相较于单样本解码，胜率提升了+9.0，以最小的成本实现了显著的性能提升。\n\n**研究意义**\n\n*   研究结果强调了在推理时计算中采用语言和任务感知方法的必要性。\n*   这有助于在代表性不足的语言中普及性能改进，促进LLM的民主化应用。",
      "shortSummary": "这项研究旨在通过扩展推理计算来提升多语言大型语言模型（LLM）的性能，而无需重新训练。针对现有方法在跨语言和多任务场景中的局限性，研究提出了新颖的采样和选择策略。实验证明，这些优化策略显著提升了模型在多语言基准测试上的表现，例如使8B模型胜率平均提升6.8%，111B模型提升9.0%。研究强调了开发语言和任务感知型推理方法的重要性，以促进LLM在更多语言中的广泛应用。",
      "translated_title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."
    },
    {
      "title": "OctoThinker: 中期训练激励强化学习扩展 (原标题: OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling)",
      "link": "https://arxiv.org/abs/2506.20512",
      "pubDate": "Wed, 25 Jun 2025 10:58:13 GMT",
      "isoDate": "2025-06-25T10:58:13.000Z",
      "creator": "Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu",
      "summary": "### OctoThinker: 中期训练激励强化学习扩展\n\n本研究深入探讨了不同基础语言模型家族（如Llama和Qwen）在通过强化学习（RL）进行后训练时表现出的差异行为，尤其是在推理密集型任务上。理解是什么让一个基础语言模型适合强化学习，对于开发下一代可扩展的RL基础模型至关重要。\n\n**研究焦点与发现：**\n\n本研究主要调查了中期训练策略如何塑造RL动态，重点关注Qwen和Llama这两个代表性模型家族。研究揭示了以下关键洞察：\n\n1.  **高质量数学语料库的重要性：** 高质量的数学语料库，例如MegaMath-Web-Pro，能够显著提升基础模型和RL的性能。相比之下，现有的一些替代方案（如FineMath-4plus）未能达到同样的效果。\n2.  **问答式数据和长思维链（CoT）推理：** 进一步添加问答式数据，特别是长思维链（CoT）推理示例，能够增强RL结果。指令数据（instruction data）则能进一步释放这种效果。\n3.  **长CoT的权衡：** 尽管长CoT能够提高推理深度，但它也可能导致模型响应的冗长以及RL训练的不稳定性，这凸显了数据格式化的重要性。\n4.  **中期训练的扩展效应：** 扩展中期训练（scaling mid-training）能够持续带来更强的下游RL性能。\n\n**OctoThinker模型与“稳定-衰减”策略：**\n\n基于这些洞察，研究团队提出了一种名为“稳定-衰减”（Stable-then-Decay）的两阶段中期训练策略：\n\n*   **第一阶段：** 基础模型首先使用恒定学习率训练2000亿个token。\n*   **第二阶段：** 接着在三个以CoT为重点的分支上训练200亿个token，并采用学习率衰减。\n\n通过这种策略，研究团队开发了OctoThinker模型家族。OctoThinker模型展现出强大的RL兼容性，并成功缩小了与更RL友好模型家族（如Qwen）之间的性能差距。\n\n**研究意义与资源发布：**\n\n本研究旨在帮助塑造RL时代基础模型的预训练策略。为了支持进一步的研究，研究团队发布了开源模型，以及一个精心策划的、包含超过700亿个token的数学推理密集型语料库（即MegaMath-Web-Pro-Max）。",
      "shortSummary": "本研究探讨了中期训练策略对语言模型强化学习（RL）性能的影响。发现高质量数学语料库和长思维链（CoT）推理数据能显著提升RL效果，但需注意数据格式。研究提出“稳定-衰减”两阶段中期训练策略，成功开发了OctoThinker模型家族。OctoThinker展现出强大的RL兼容性，缩小了与RL友好模型的性能差距，旨在为RL时代的基础模型预训练提供指导。研究同时发布了开源模型和大型数学推理语料库。",
      "translated_title": "OctoThinker: 中期训练激励强化学习扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max)."
    },
    {
      "title": "ReCode：使用强化学习更新代码API知识 (原标题: ReCode: Updating Code API Knowledge with Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.20495",
      "pubDate": "Wed, 25 Jun 2025 10:41:13 GMT",
      "isoDate": "2025-06-25T10:41:13.000Z",
      "creator": "Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "## ReCode：使用强化学习更新代码API知识\n\n### 引言\n大型语言模型（LLMs）在代码生成方面展现出卓越的能力，但当外部库API频繁更新时，它们难以适应这些变化。这种局限性源于LLMs依赖其训练数据中过时的API知识，即使能够访问最新的文档，也无法在动态环境中可靠地生成代码。\n\n### ReCode框架\n为了解决这一关键问题，我们提出了ReCode（rule-based Reinforcement learning for Code Update），一个新颖的框架，旨在模仿人类程序员适应API变化的方式。\n\n### 核心机制\n*   **数据集构建：** 我们构建了一个包含大约2,000个数据条目的数据集，用于训练LLMs根据更新的信息执行版本迁移。\n*   **强化学习奖励：** 我们引入了一种修改后的字符串相似度度量作为强化学习的奖励，用于代码评估。\n\n### 实验结果与优势\n*   **性能显著提升：** 实验表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未曾见过的CodeUpdateArena任务上。\n*   **对通用能力影响小：** 与监督微调相比，ReCode对LLMs的通用代码生成能力影响较小。\n*   **广泛适用性：** 我们将ReCode应用于各种LLMs和强化学习算法（GRPO和DAPO），所有这些都实现了一致的改进。\n*   **卓越表现：** 值得注意的是，经过训练后，Qwen2.5-Coder-7B 的性能超越了相同架构的32B参数代码指令微调模型和推理模型。\n\n### 其他信息\n*   **项目状态：** 该工作目前正在进行中。\n*   **代码可用性：** 相关代码已提供。\n*   **研究领域：** 本研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、信息检索（cs.IR）、机器学习（cs.LG）和软件工程（cs.SE）等领域。",
      "shortSummary": "ReCode是一个基于强化学习的框架，旨在解决大型语言模型（LLMs）因API频繁更新而导致的过时代码生成问题。它通过构建数据集训练LLMs进行版本迁移，并使用修改后的字符串相似度作为奖励。实验证明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，且对通用能力影响较小。训练后的7B参数模型甚至超越了32B参数模型，显示出其有效性和潜力。",
      "translated_title": "ReCode：使用强化学习更新代码API知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."
    },
    {
      "title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成 (原标题: HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling)",
      "link": "https://arxiv.org/abs/2506.20452",
      "pubDate": "Wed, 25 Jun 2025 09:58:37 GMT",
      "isoDate": "2025-06-25T09:58:37.000Z",
      "creator": "Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber",
      "summary": "## HiWave：基于小波的扩散采样实现免训练高分辨率图像生成\n\n### 引言\n扩散模型已成为图像合成领域的领先方法，展现出卓越的真实感和多样性。然而，在高分辨率下训练扩散模型计算成本极高，且现有零样本生成技术在超越训练分辨率合成图像时常产生伪影，例如物体重复和空间不连贯。\n\n### HiWave 方法概述\n本文引入了 HiWave，这是一种免训练、零样本的方法，旨在利用预训练的扩散模型，显著增强超高分辨率图像合成的视觉保真度和结构连贯性。\n\n### 核心流程\nHiWave 采用一个两阶段的管道：\n\n1.  **基础图像生成与反演：**\n    *   首先，利用预训练模型生成一个基础图像。\n    *   接着，通过一个分块（patch-wise）的 DDIM 反演步骤，从该基础图像中导出初始噪声向量，以确保全局的连贯性。\n\n2.  **小波域细节增强：**\n    *   在采样过程中，HiWave 引入了一个新颖的小波域细节增强模块。\n    *   该模块的关键在于：\n        *   保留基础图像中的低频分量，以确保图像的结构一致性。\n        *   选择性地引导高频分量，以丰富图像的精细细节和纹理。\n\n### 优势与评估\n*   **免训练与零样本：** HiWave 的核心优势在于其无需额外的训练或对现有模型架构进行修改，即可实现高质量的超高分辨率图像生成。\n*   **伪影缓解：** 广泛的评估（使用 Stable Diffusion XL）表明，HiWave 能够有效缓解先前方法中常见的视觉伪影，如物体重复和空间不连贯。\n*   **卓越感知质量：** 该方法在图像合成中实现了卓越的感知质量。\n*   **用户研究验证：** 一项用户研究进一步证实了 HiWave 的性能。在超过 80% 的比较中，用户更倾向于 HiWave 而非最先进的替代方法，这充分凸显了其在高质量、超高分辨率图像合成方面的有效性。",
      "shortSummary": "HiWave 是一种免训练、零样本的方法，用于利用预训练扩散模型生成超高分辨率图像。它通过两阶段管道工作：首先生成基础图像并进行 DDIM 反演，然后通过新颖的小波域细节增强模块，保留低频结构并丰富高频细节。该方法有效缓解了伪影，实现了卓越的感知质量，并在用户研究中表现出色，无需重新训练或修改模型架构。",
      "translated_title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications."
    },
    {
      "title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法 (原标题: Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content)",
      "link": "https://arxiv.org/abs/2506.20331",
      "pubDate": "Wed, 25 Jun 2025 07:30:25 GMT",
      "isoDate": "2025-06-25T07:30:25.000Z",
      "creator": "Rian Touchent, Nathan Godey, Eric de la Clergerie",
      "summary": "## Biomed-Enriched：一个用于预训练和提取稀有及隐藏内容的生物医学数据集\n\nBiomed-Enriched是一个新颖的生物医学文本数据集，它通过两阶段的标注过程从PubMed构建而成。该数据集旨在为生物医学和临床自然语言处理（NLP）提供一个宝贵的资源，特别是用于预训练和提取难以获取的稀有及隐藏内容。\n\n### 数据集构建过程：\n\n1.  **第一阶段：大型语言模型（LLM）标注**\n    *   一个大型语言模型对来自PubMed科学文章的40万个段落进行标注。\n    *   标注内容包括段落的类型（综述、研究、临床病例、其他）、领域（临床、生物医学、其他）以及教育质量评分。\n    *   教育质量评分范围为1到5，用于评估段落对大学水平学习的有用性。\n2.  **第二阶段：小型语言模型（SLM）传播标签**\n    *   利用第一阶段生成的标注数据对一个小型语言模型进行微调。\n    *   该微调后的SLM将这些标签传播到整个PMC-OA语料库中。\n\n### 数据集的成果与价值：\n\n*   **精炼子集提取：** 生成的元数据使得能够提取出精炼的子集，例如：\n    *   200万个临床病例段落。\n    *   其中包含超过45万个来自具有商业使用许可文章的高质量临床病例段落。\n*   **解决数据访问难题：** 临床文本通常由于隐私限制（医院记录无法公开共享）而难以获取。Biomed-Enriched提供了一个替代方案，即一个大规模、公开可用的PubMed临床病例集合。\n*   **对NLP的价值：** 它是生物医学和临床NLP领域的一个重要资源。\n\n### 初步预训练实验结果（使用OLMo2）：\n\n*   **有针对性的改进：** 经过精心策划的子集能够实现有针对性的性能提升。\n    *   **临床上采样（Clinical Upsampling）：** 在MMLU ProfMed基准测试中，性能提升了约5%。\n    *   **教育质量过滤（Educational Quality Filtering）：** 在MedQA和MedMCQA基准测试中，性能提升了约1%。\n*   **训练效率提升：** 结合这些技术可以实现更快的收敛速度，仅用三分之一的训练令牌就能达到相同的性能。\n*   **未来潜力：** 这表明Biomed-Enriched数据集有潜力支持更高效、更有效的生物医学预训练策略。\n\n### 数据可用性：\n\n数据集链接已提供。",
      "shortSummary": "Biomed-Enriched是一个新的生物医学文本数据集，通过LLM对PubMed文章进行两阶段标注构建。它提供了200万个临床病例段落，包括45万个高质量病例，解决了临床文本隐私限制导致的访问难题。该数据集是生物医学和临床NLP的重要资源。初步实验表明，其精选子集能显著提升预训练模型的性能，如临床上采样使MMLU ProfMed提升约5%，并能加速模型收敛，实现更高效的生物医学预训练。",
      "translated_title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."
    },
    {
      "title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画 (原标题: AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.19851",
      "pubDate": "Tue, 24 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-24T13:59:58.000Z",
      "creator": "Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng",
      "summary": "# AnimaX：一种创新的3D动画框架\n\nAnimaX是一个前馈式3D动画框架，旨在弥合视频扩散模型的运动先验与基于骨骼动画的可控结构之间的鸿沟。\n\n## 传统方法的局限性\n\n传统的运动合成方法存在以下限制：\n*   受限于固定的骨骼拓扑结构。\n*   需要在高维变形空间中进行昂贵的优化。\n\n## AnimaX 的核心优势与方法\n\nAnimaX通过以下创新方法克服了传统方法的局限性：\n\n*   **知识迁移：** 有效地将基于视频的运动知识迁移到3D领域。\n*   **广泛支持：** 能够支持具有任意骨骼的各种关节网格模型。\n*   **运动表示：** 将3D运动表示为多视角、多帧的2D姿态图。\n*   **联合扩散：** 实现联合视频-姿态扩散，其条件是模板渲染和文本运动提示。\n*   **对齐机制：** 引入共享位置编码和模态感知嵌入，以确保视频和姿态序列之间的时空对齐，从而将视频先验有效地转移到运动生成任务中。\n*   **3D重建：** 生成的多视角姿态序列被三角化为3D关节位置。\n*   **网格动画：** 通过逆运动学（inverse kinematics）将3D关节位置转换为最终的网格动画。\n\n## 训练与性能\n\n*   AnimaX 在一个新策展的、包含160,000个绑定序列的数据集上进行了训练。\n*   在VBench基准测试中，AnimaX在泛化能力、运动保真度和效率方面取得了最先进（state-of-the-art）的结果。\n\n## 应用前景\n\nAnimaX为类别无关的3D动画提供了一个可扩展的解决方案，具有广泛的应用潜力。\n\n**项目页面：** 提供了项目页面链接以获取更多信息。",
      "shortSummary": "AnimaX是一个创新的3D动画框架，它结合了视频扩散模型的运动先验与骨骼动画的可控性。该方法将3D运动表示为多视角2D姿态图，并通过联合视频-姿态扩散生成动画。AnimaX支持任意骨骼的网格模型，解决了传统方法的局限性。它在16万个绑定序列数据集上训练，并在泛化、运动保真度和效率方面达到了最先进水平，为类别无关的3D动画提供了可扩展的解决方案。",
      "translated_title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}."
    },
    {
      "title": "统一的视觉-语言-动作模型 (原标题: Unified Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2506.19850",
      "pubDate": "Tue, 24 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-24T13:59:57.000Z",
      "creator": "Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang",
      "summary": "## UniVLA：统一的视觉-语言-动作模型\n\n### 引言\n视觉-语言-动作（VLA）模型因其在推动机器人操作方面的巨大潜力而备受关注。然而，现有方法主要依赖视觉-语言模型（VLM）的通用理解能力来生成动作信号，这往往忽略了视觉观察中固有的丰富时序和因果结构。\n\n### UniVLA 的提出与核心机制\n本文提出了 **UniVLA**，一个统一的、原生的多模态VLA模型。UniVLA的核心创新在于将视觉、语言和动作信号自回归地建模为离散的token序列。这种独特的公式化方法带来了多项优势：\n\n*   **灵活的多模态任务学习**：它使得模型能够灵活地学习各种多模态任务，尤其擅长从大规模视频数据中进行学习。\n*   **融入世界建模**：通过在后训练阶段融入世界建模，UniVLA能够有效地从视频中捕捉因果动态。\n*   **高效的策略迁移**：捕捉到的因果动态有助于模型有效迁移到下游策略学习，这对于解决长时程任务尤为重要。\n\n### 实验结果与性能\nUniVLA在多个广泛使用的模拟基准测试中取得了新的最先进结果，显著超越了现有方法，这些基准包括：\n\n*   CALVIN\n*   LIBERO\n*   Simplenv-Bridge\n\n**具体示例**：在LIBERO基准测试中，UniVLA的平均成功率达到了95.5%，显著超越了先前方法pi0-FAST的85.5%。\n\n### 实际应用\n除了在模拟环境中的卓越表现，UniVLA的广泛适用性还在真实世界的应用中得到了验证，包括：\n\n*   ALOHA机器人操作\n*   自动驾驶",
      "shortSummary": "UniVLA是一个统一的视觉-语言-动作（VLA）模型，它将视觉、语言和动作信号自回归地建模为离散token序列。通过融入世界建模，UniVLA能从视频中捕捉因果动态，有效支持长时程任务的策略学习。该模型在CALVIN、LIBERO等多个模拟基准测试中取得了最先进结果，例如在LIBERO上成功率达95.5%。UniVLA还展示了在真实世界机器人操作和自动驾驶中的广泛适用性。",
      "translated_title": "统一的视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving."
    },
    {
      "title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成 (原标题: ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing)",
      "link": "https://arxiv.org/abs/2506.19848",
      "pubDate": "Tue, 24 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-24T13:59:55.000Z",
      "creator": "Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, Jiaqi Wang, Feng Wu, Dahua Lin",
      "summary": "## ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成\n\n### 引言\n本文介绍ScaleCap，一种推理时可伸缩的图像字幕生成策略，旨在生成全面且详细的图像字幕。高质量图像字幕生成的关键挑战在于大型视觉语言模型（LVLMs）固有的偏见：\n\n*   **多模态偏见**：导致描述粒度不平衡，对某些元素描述详细，而对另一些则仅作概述。\n*   **语言偏见**：导致对不存在对象的幻觉描述。\n\n### ScaleCap：可伸缩的去偏字幕策略\n为解决上述问题，我们提出一种可伸缩的去偏字幕生成策略，该策略通过增加推理预算，持续丰富和校准字幕。该策略包含两个新颖组件：\n\n*   **启发式问答（Heuristic Question Answering）**：根据图像生成内容特定问题并回答，逐步向字幕中注入相关信息。\n*   **对比句评分（Contrastive Sentence Rating）**：采用句子级离线对比解码，有效识别并消除由语言偏见引起的幻觉。\n\n### 推理时可伸缩性\n随着推理成本的增加，ScaleCap会提出更多启发式问题，逐步捕捉额外的视觉细节，从而生成更准确、平衡且信息丰富的字幕。\n\n### 实验结果与应用\n\n*   广泛的模态对齐实验证明了ScaleCap的有效性。\n*   使用ScaleCap标注45万张图像并用于LVLM预训练，在11个广泛使用的基准测试中持续获得性能提升。\n*   此外，ScaleCap在两个额外任务中展示了生成字幕的卓越丰富性和保真度：\n    *   在VQA（视觉问答）任务中用字幕替换图像。\n    *   从字幕重建图像以评估语义覆盖范围。",
      "shortSummary": "ScaleCap是一种推理时可伸缩的图像字幕生成策略，旨在解决大型视觉语言模型（LVLMs）中的多模态和语言偏见。它通过启发式问答逐步注入细节，并利用对比句评分消除幻觉。随着推理预算增加，ScaleCap能生成更准确、平衡、信息丰富的字幕。实验证明，其能显著提升LVLM性能，并在VQA和图像重建等任务中展现出卓越的字幕质量。",
      "translated_title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap."
    },
    {
      "title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线 (原标题: SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution)",
      "link": "https://arxiv.org/abs/2506.19838",
      "pubDate": "Tue, 24 Jun 2025 13:57:26 GMT",
      "isoDate": "2025-06-24T13:57:26.000Z",
      "creator": "Liangbin Xie, Yu Li, Shian Du, Menghan Xia, Xintao Wang, Fanghua Yu, Ziyan Chen, Pengfei Wan, Jiantao Zhou, Chao Dong",
      "summary": "# SimpleGVR：潜在级联视频超分辨率的简单基线\n\n## 核心问题与背景\n当前，潜在扩散模型已成为高效视频生成领域的主导范式。然而，随着用户对更高分辨率输出的期望不断提高，仅依赖潜在计算已无法满足需求。一个有前景的方法是将视频生成过程解耦为两个阶段：语义内容生成和细节合成。前者利用计算密集型的基础模型在较低分辨率下生成内容，而后者则借助轻量级的级联视频超分辨率（VSR）模型实现高分辨率输出。\n\n## 本文研究重点\n本研究专注于探讨级联VSR模型的关键设计原则，这在当前领域中尚未得到充分探索。\n\n## 主要贡献与设计原则\n\n### 1. 降级策略\n文章提出了两种降级策略，用于生成训练对。这些策略旨在更好地模拟基础模型的输出特性，从而确保VSR模型与其上游生成器之间的高度对齐。\n\n### 2. VSR模型行为分析\n通过系统分析，本文提供了关于VSR模型行为的关键见解，具体包括：\n*   **时间步采样策略**：研究了不同时间步采样策略对模型性能的影响。\n*   **低分辨率（LR）输入上的噪声增强效果**：分析了噪声增强对LR输入的影响。\n这些发现直接指导了模型架构和训练方法的创新。\n\n### 3. 效率创新\n为了实现高效的训练和推理，并大幅降低计算开销，SimpleGVR引入了：\n*   **交错时间单元（interleaving temporal unit）**\n*   **稀疏局部注意力（sparse local attention）**\n\n## 实验结果与结论\n广泛的实验证明，SimpleGVR框架优于现有方法。消融研究也证实了每个设计选择的有效性。这项工作为级联视频超分辨率生成建立了一个简单而有效的基线，为未来高效级联合成系统的发展提供了实用的见解。",
      "shortSummary": "SimpleGVR提出了一种用于潜在级联视频超分辨率（VSR）的简单有效基线。它通过将视频生成解耦为内容生成和细节合成来解决高分辨率输出问题。主要贡献包括：优化训练对的降级策略、深入分析VSR模型行为（时间步采样、噪声增强），以及引入交错时间单元和稀疏局部注意力以提高效率。实验证明其优越性，为未来高效级联合成系统提供了实用见解。",
      "translated_title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线",
      "images": [],
      "contentSource": "完整文章",
      "content": "Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems."
    },
    {
      "title": "KnowRL: 探索用于事实性的知识增强强化学习 (原标题: KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality)",
      "link": "https://arxiv.org/abs/2506.19807",
      "pubDate": "Tue, 24 Jun 2025 13:17:17 GMT",
      "isoDate": "2025-06-24T13:17:17.000Z",
      "creator": "Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "本文提出了一种名为KnowRL的知识增强强化学习方法，旨在解决大型语言模型（LLMs）在推理过程中由于无法准确识别知识边界而产生的严重幻觉问题，尤其是在慢思考模型中。\n\n**主要观点：**\n\n*   **问题：** 大型语言模型（LLMs）的慢思考模型常常出现幻觉，输出不正确的内容，因为它们无法准确识别知识边界。\n*   **方法：** KnowRL通过将基于知识验证的事实性奖励整合到强化学习（RL）训练过程中，引导模型执行基于事实的慢思考，帮助它们识别知识边界。\n*   **机制：** KnowRL在RL训练期间提供有针对性的事实输入，使模型能够学习和内化基于事实的推理策略。通过直接奖励推理步骤中对事实的遵守，KnowRL培养了更可靠的思考过程。\n*   **实验结果：** 在三个幻觉评估数据集和两个推理评估数据集上的实验结果表明，KnowRL有效地缓解了慢思考模型中的幻觉，同时保持了其原有的强大推理能力。\n*   **代码：** 论文代码已开源，地址为：[this https URL](this https URL)。\n\n**关键词：**\n\n*   大型语言模型 (LLMs)\n*   强化学习 (RL)\n*   幻觉\n*   知识验证\n*   事实性奖励\n*   慢思考模型\n*   知识边界\n\n**研究领域：**\n\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)\n*   机器学习 (cs.LG)\n*   多智能体系统 (cs.MA)",
      "shortSummary": "本文提出KnowRL，一种知识增强强化学习方法，旨在解决大型语言模型（LLMs）慢思考模型中普遍存在的幻觉问题。KnowRL通过将基于知识验证的事实性奖励整合到强化学习训练中，引导模型进行基于事实的慢思考，从而帮助模型识别知识边界。实验结果表明，KnowRL能有效缓解慢思考模型中的幻觉，同时保持其推理能力。论文代码已开源。",
      "translated_title": "KnowRL: 探索用于事实性的知识增强强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL."
    },
    {
      "title": "为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究 (原标题: Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study)",
      "link": "https://arxiv.org/abs/2506.19794",
      "pubDate": "Tue, 24 Jun 2025 13:04:23 GMT",
      "isoDate": "2025-06-24T13:04:23.000Z",
      "creator": "Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang",
      "summary": "### 为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究\n\n**摘要**\n\n大型语言模型（LLMs）在自动化数据分析任务方面具有潜力，但开源模型在此类推理密集型场景中面临显著限制。本研究旨在探讨增强开源LLMs数据分析能力的策略。\n\n**研究方法**\n\n*   **数据集构建**：策划了一个包含多样化、真实场景的种子数据集。\n*   **评估维度**：从三个维度评估模型：\n    1.  数据理解（Data Understanding）\n    2.  代码生成（Code Generation）\n    3.  战略规划（Strategic Planning）\n\n**主要发现**\n\n本研究的分析揭示了三个关键发现：\n\n1.  **战略规划质量是主要决定因素**：战略规划的质量是模型性能的主要决定因素。\n2.  **交互设计与任务复杂性影响推理能力**：交互设计和任务复杂性显著影响模型的推理能力。\n3.  **数据质量优于多样性**：在实现最佳性能方面，数据质量的影响大于数据多样性。\n\n**研究成果与展望**\n\n研究团队利用这些洞察开发了一种数据合成方法，并证明该方法显著提升了开源LLMs的分析推理能力。\n\n**其他信息**\n\n*   **状态**：该工作仍在进行中（Work in progress）。\n*   **主题**：计算与语言（cs.CL）、人工智能（cs.AI）、信息检索（cs.IR）、机器学习（cs.LG）、多智能体系统（cs.MA）。\n*   **引用**：arXiv:2506.19794 [cs.CL]。",
      "shortSummary": "本研究探讨了开源大型语言模型（LLMs）在数据分析方面的局限性及其增强策略。通过评估数据理解、代码生成和战略规划，研究发现战略规划质量是模型性能的关键决定因素，交互设计和任务复杂性会影响推理能力，且数据质量比多样性更重要。基于这些发现，研究开发了一种数据合成方法，显著提升了开源LLMs的分析推理能力。",
      "translated_title": "为什么开源大型语言模型在数据分析方面表现不佳？一项系统性实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities."
    },
    {
      "title": "SRFT：一种基于监督和强化微调的单阶段推理方法 (原标题: SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning)",
      "link": "https://arxiv.org/abs/2506.19767",
      "pubDate": "Tue, 24 Jun 2025 12:31:37 GMT",
      "isoDate": "2025-06-24T12:31:37.000Z",
      "creator": "Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao",
      "summary": "本文提出了一种名为监督强化微调（SRFT）的单阶段方法，旨在解决大型语言模型（LLM）在推理任务中监督微调（SFT）和强化学习（RL）的最佳集成问题。通过对token分布、学习动态和集成机制的全面分析，揭示了SFT和RL之间的关键差异：SFT对LLM策略分布进行粗粒度的全局更改，而RL执行细粒度的选择性优化，熵是训练有效性的关键指标。SRFT通过熵感知加权机制统一了这两种微调范式，同时应用SFT和RL，使用演示和自我探索rollout直接优化LLM，而不是通过两阶段顺序方法。实验结果表明，SRFT在五个数学推理基准测试中实现了59.1%的平均准确率，优于零RL方法9.0%，在三个分布外基准测试中优于零RL方法10.9%。",
      "shortSummary": "本文提出了一种名为SRFT的单阶段方法，用于优化大型语言模型（LLM）的推理能力。该方法通过熵感知加权机制统一了监督微调（SFT）和强化学习（RL），实现了比传统方法更高的准确率，尤其是在数学推理和分布外基准测试中。",
      "translated_title": "SRFT：一种基于监督和强化微调的单阶段推理方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks."
    },
    {
      "title": "频域引导在低CFG尺度下实现高保真采样 (原标题: Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low CFG Scales)",
      "link": "https://arxiv.org/abs/2506.19713",
      "pubDate": "Tue, 24 Jun 2025 11:19:42 GMT",
      "isoDate": "2025-06-24T11:19:42.000Z",
      "creator": "Seyedmorteza Sadat, Tobias Vontobel, Farnood Salehi, Romann M. Weber",
      "summary": "### 频域引导在扩散模型中的新视角：频率解耦引导（FDG）\n\n**1. 引言：分类器无关引导（CFG）的挑战**\n分类器无关引导（CFG）已成为现代条件扩散模型中不可或缺的组成部分，显著提升了生成质量、细节和提示对齐度。然而，CFG增强这些特性的深层机制尚未被完全理解。\n\n**2. 频域分析：CFG的不同影响**\n本文提出了一种通过分析CFG在频域中的效应来理解其作用的新视角。研究表明，低频和高频对生成质量有着截然不同的影响：\n*   **低频引导**：主要控制全局结构和条件对齐。\n*   **高频引导**：主要增强视觉保真度。\n\n**3. 标准CFG的局限性**\n标准CFG在所有频率上应用统一的引导强度，这导致了以下问题：\n*   **高尺度下**：可能导致图像过饱和并降低多样性。\n*   **低尺度下**：可能导致视觉质量下降。\n\n**4. 提出的方法：频率解耦引导（FDG）**\n基于上述洞察，研究人员提出了一种有效的频率解耦引导（FDG）方法。FDG将CFG分解为低频和高频分量，并对每个分量应用单独的引导强度。\n\n**5. FDG的优势与实验结果**\nFDG的设计旨在解决标准CFG的局限性，并带来了显著的改进：\n*   **图像质量提升**：在低引导尺度下显著改善图像质量。\n*   **避免高CFG尺度弊端**：通过设计避免了高CFG尺度带来的过饱和和多样性降低问题。\n*   **保真度与多样性**：FDG持续增强样本保真度，同时保持了多样性。\n*   **性能指标提升**：与标准CFG相比，FDG在FID（Fréchet Inception Distance）和召回率（recall）方面均有所提升。\n*   **即插即用**：该方法可作为标准分类器无关引导的即插即用替代方案。\n\n**6. 结论**\n通过在多个数据集和模型上进行的广泛实验，本文证明了FDG的有效性，为扩散模型中的引导机制提供了新的理解和改进方案。",
      "shortSummary": "本文提出“频域引导在低CFG尺度下实现高保真采样”的新方法。研究发现，分类器无关引导（CFG）在低频影响全局结构，高频影响视觉细节。标准CFG统一尺度导致高尺度过饱和、低尺度质量差。为此，提出频率解耦引导（FDG），分别控制低频和高频引导强度。FDG在低引导尺度下提升图像质量，避免高CFG弊端，同时增强样本保真度并保持多样性，显著改善FID和召回率，是CFG的即插即用替代方案。",
      "translated_title": "频域引导在低CFG尺度下实现高保真采样",
      "images": [],
      "contentSource": "完整文章",
      "content": "Classifier-free guidance (CFG) has become an essential component of modern conditional diffusion models. Although highly effective in practice, the underlying mechanisms by which CFG enhances quality, detail, and prompt alignment are not fully understood. We present a novel perspective on CFG by analyzing its effects in the frequency domain, showing that low and high frequencies have distinct impacts on generation quality. Specifically, low-frequency guidance governs global structure and condition alignment, while high-frequency guidance mainly enhances visual fidelity. However, applying a uniform scale across all frequencies -- as is done in standard CFG -- leads to oversaturation and reduced diversity at high scales and degraded visual quality at low scales. Based on these insights, we propose frequency-decoupled guidance (FDG), an effective approach that decomposes CFG into low- and high-frequency components and applies separate guidance strengths to each component. FDG improves image quality at low guidance scales and avoids the drawbacks of high CFG scales by design. Through extensive experiments across multiple datasets and models, we demonstrate that FDG consistently enhances sample fidelity while preserving diversity, leading to improved FID and recall compared to CFG, establishing our method as a plug-and-play alternative to standard classifier-free guidance."
    },
    {
      "title": "用于大型语言模型鲁棒4比特量化的离群值安全预训练 (原标题: Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models)",
      "link": "https://arxiv.org/abs/2506.19697",
      "pubDate": "Tue, 24 Jun 2025 11:03:57 GMT",
      "isoDate": "2025-06-24T11:03:57.000Z",
      "creator": "Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang",
      "summary": "大型语言模型（LLMs）中的极端激活离群值严重降低了量化性能，阻碍了其在设备上的高效部署。虽然通道级操作和自适应梯度缩放被认为是导致离群值的原因，但实际的缓解措施仍具挑战性。\n\n本文引入了**离群值安全预训练（Outlier-Safe Pre-Training, OSP）**，这是一套实用的指导方针，旨在主动预防离群值的形成，而非依赖事后缓解。OSP结合了三项关键创新：\n\n*   **Muon优化器**：消除特权基（privileged bases），同时保持训练效率。\n*   **单尺度RMSNorm（Single-Scale RMSNorm）**：防止通道级放大。\n*   **可学习的嵌入投影（learnable embedding projection）**：重新分配源自嵌入矩阵的激活幅度。\n\n**验证与结果**：\n\n*   研究人员通过训练一个1.4亿参数的模型，使用了1万亿个tokens，这是首个在没有此类离群值的情况下训练的生产规模LLM。\n*   在激进的4比特量化下，OSP模型在10个基准测试中取得了35.7的平均分数，而使用Adam训练的模型仅为26.5。\n*   OSP的训练开销仅为2%。\n*   OSP模型表现出接近零的超额峰度（0.04），而标准模型则显示出极高的值（1818.56），这从根本上改变了LLM的量化行为。\n\n**结论**：\n\n这项工作表明，离群值并非LLMs固有的特性，而是训练策略的后果，这为更高效的LLM部署铺平了道路。\n\n**资源可用性**：\n\n源代码和预训练检查点可在提供的URL（this https URL）获取。",
      "shortSummary": "大型语言模型（LLMs）中的极端激活离群值严重阻碍了高效量化。本文提出“离群值安全预训练”（OSP），通过Muon优化器、单尺度RMSNorm和可学习嵌入投影，主动预防离群值形成。实验证明，OSP模型在4比特量化下性能显著提升（基准分数35.7 vs 26.5），训练开销仅2%，且峰度接近零。这表明离群值是训练策略而非模型固有的问题，为LLM高效部署开辟了新途径。",
      "translated_title": "用于大型语言模型鲁棒4比特量化的离群值安全预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training."
    },
    {
      "title": "MATE：基于LLM的多智能体翻译环境，用于无障碍应用 (原标题: MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility Applications)",
      "link": "https://arxiv.org/abs/2506.19502",
      "pubDate": "Tue, 24 Jun 2025 06:40:23 GMT",
      "isoDate": "2025-06-24T06:40:23.000Z",
      "creator": "Aleksandr Algazinov, Matt Laing, Paul Laban",
      "summary": "## MATE：基于LLM的多智能体翻译环境\n\n### 概述\n\n当前社会，无障碍性仍是一个关键问题，许多技术未能充分支持各类用户需求。现有的多智能体系统（MAS）因其闭源设计，缺乏定制化能力，难以向有特殊需求的用户提供全面帮助。这导致残障人士在与数字环境交互时常遇到显著障碍。\n\n### MATE系统介绍\n\n本文引入了 **MATE**，一个多模态无障碍多智能体系统，旨在根据用户需求执行模态转换，从而帮助残障人士将数据转换为可理解的格式。例如，如果用户视力不佳并收到一张图片，MATE系统能将该图片转换为其音频描述。\n\n#### 核心功能与应用\n\n*   **模态转换**：根据用户具体需求，将信息从一种模态（如图像）转换为另一种模态（如音频描述）。\n*   **广泛应用**：MATE可应用于广泛的领域、行业和区域，例如医疗保健，并能成为各类用户群体的有用助手。\n\n#### 技术特点\n\n*   **模型灵活性**：系统支持多种类型的模型，从大型语言模型（LLM）API调用到使用自定义机器学习（ML）分类器。这种灵活性确保系统能够适应各种需求，并兼容多种硬件。\n*   **隐私与安全**：MATE设计为本地运行，从而确保敏感信息的隐私和安全。\n*   **集成能力**：该框架可以有效地与机构技术（例如，数字医疗服务）集成，以提供实时用户协助。\n\n### ModCon-Task-Identifier\n\n文章还介绍了 **ModCon-Task-Identifier**，这是一个能够从用户输入中精确提取模态转换任务的模型。大量实验表明，ModCon-Task-Identifier 在自定义数据上始终优于其他LLM和统计模型。\n\n### 可用性\n\nMATE的代码和数据已公开可用。",
      "shortSummary": "MATE是一个基于LLM的多模态无障碍多智能体系统，旨在解决现有技术对残障用户支持不足的问题。它能根据用户需求进行模态转换，确保信息可理解。MATE支持多种模型，可本地运行保障隐私，并能与机构技术集成。其核心组件ModCon-Task-Identifier在任务识别方面表现优异。该系统在医疗等领域具有广泛应用前景，代码和数据已公开。",
      "translated_title": "MATE：基于LLM的多智能体翻译环境，用于无障碍应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE."
    },
    {
      "title": "Skywork-SWE：揭示LLM中软件工程的数据扩展定律 (原标题: Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs)",
      "link": "https://arxiv.org/abs/2506.19290",
      "pubDate": "Mon, 23 Jun 2025 23:53:36 GMT",
      "isoDate": "2025-06-23T23:53:36.000Z",
      "creator": "Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, Yahui Zhou",
      "summary": "## Skywork-SWE：LLM软件工程数据扩展定律的揭示\n\n### 引言：LLM在软件工程中的挑战\n\n软件工程（SWE）已成为下一代大型语言模型（LLM）代理的关键测试平台。这要求LLM具备两项核心能力：\n\n*   **持续迭代问题解决**：例如，超过50轮的交互。\n*   **长上下文依赖解析**：例如，处理超过32k个token。\n\n然而，当前SWE领域的数据收集过程面临巨大挑战，因为它高度依赖人工标注进行代码文件过滤，并需要设置专门的运行时环境来执行和验证单元测试。这导致现有数据集规模普遍较小，通常只有数千个GitHub实例。\n\n### Skywork-SWE的解决方案：自动化数据收集与大规模数据集\n\n为了解决数据稀缺问题，研究人员提出了一个**增量式、自动化数据收集管道**，旨在系统性地扩展SWE数据集的规模和多样性。\n\n*   **数据集构成**：该数据集包含来自2,531个不同GitHub仓库的10,169个真实世界Python任务实例。\n*   **数据特性**：每个任务实例都附带自然语言的任务描述，以及一个专用的运行时环境镜像，用于自动化单元测试验证。\n*   **训练轨迹**：研究团队精心整理了超过8,000条成功通过运行时验证的训练轨迹，用于模型微调。\n\n### 关键发现：数据扩展定律\n\n通过在这些轨迹上对Skywork-SWE模型进行微调，研究人员发现了一个显著的**数据扩展现象**：\n\n*   随着数据量的增加，训练后的模型在LLM软件工程能力方面的表现持续提升，**没有出现饱和迹象**。这表明，在SWE领域，更多的数据能够持续带来性能增益。\n\n### 性能突破与模型发布\n\nSkywork-SWE模型在SWE-bench Verified基准测试上取得了显著的性能突破：\n\n*   **基准性能**：在不使用验证器或多次运行的情况下，Skywork-SWE模型实现了**38.0%的pass@1准确率**。\n*   **行业领先地位**：这使其在基于OpenHands代理框架构建的Qwen2.5-Coder-32B LLM中，确立了新的**最先进（SOTA）水平**。\n*   **测试时扩展技术**：通过结合测试时扩展技术，模型性能进一步提升至**47.0%的准确率**，超越了之前参数量小于32B的模型的最先进结果。\n\n为了加速未来的研究，研究团队已发布了**Skywork-SWE-32B模型检查点**。",
      "shortSummary": "Skywork-SWE项目提出了一种自动化数据收集管道，解决了LLM软件工程（SWE）领域数据稀缺的问题。他们构建了一个包含10,169个Python任务实例的大规模数据集，并发现LLM的SWE能力随数据量增加而持续提升，无饱和迹象。Skywork-SWE模型在SWE-bench Verified基准测试上取得了38.0%的pass@1准确率，结合测试时扩展技术可达47.0%，创下32B以下参数模型的SOTA。Skywork-SWE-32B模型检查点已发布，以促进未来研究。",
      "translated_title": "Skywork-SWE：揭示LLM中软件工程的数据扩展定律",
      "images": [],
      "contentSource": "完整文章",
      "content": "Software engineering (SWE) has recently emerged as a crucial testbed for next-generation LLM agents, demanding inherent capabilities in two critical dimensions: sustained iterative problem-solving (e.g., &gt;50 interaction rounds) and long-context dependency resolution (e.g., &gt;32k tokens). However, the data curation process in SWE remains notoriously time-consuming, as it heavily relies on manual annotation for code file filtering and the setup of dedicated runtime environments to execute and validate unit tests. Consequently, most existing datasets are limited to only a few thousand GitHub-sourced instances. To this end, we propose an incremental, automated data-curation pipeline that systematically scales both the volume and diversity of SWE datasets. Our dataset comprises 10,169 real-world Python task instances from 2,531 distinct GitHub repositories, each accompanied by a task specified in natural language and a dedicated runtime-environment image for automated unit-test validation. We have carefully curated over 8,000 successfully runtime-validated training trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE model on these trajectories, we uncover a striking data scaling phenomenon: the trained model's performance for software engineering capabilities in LLMs continues to improve as the data size increases, showing no signs of saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on the SWE-bench Verified benchmark without using verifiers or multiple rollouts, establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based LLMs built on the OpenHands agent framework. Furthermore, with the incorporation of test-time scaling techniques, the performance further improves to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter models. We release the Skywork-SWE-32B model checkpoint to accelerate future research."
    },
    {
      "title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成 (原标题: VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory)",
      "link": "https://arxiv.org/abs/2506.18903",
      "pubDate": "Mon, 23 Jun 2025 13:59:56 GMT",
      "isoDate": "2025-06-23T13:59:56.000Z",
      "creator": "Runjia Li, Philip Torr, Andrea Vedaldi, Tomas Jakab",
      "summary": "## VMem：基于Surfel索引视图记忆的一致交互式视频场景生成\n\n### 摘要\n\n本文提出了一种名为“Surfel索引视图记忆”（VMem）的新型记忆机制，旨在构建能够交互式探索环境的视频生成器。现有方法在处理长期场景连贯性和错误累积方面存在显著局限性，VMem旨在解决这些问题。\n\n### 现有方法的局限性\n\n*   **2D视图外绘与3D几何重建：** 这种方法虽然能实现交互式探索，但会迅速累积误差，导致场景不一致。\n*   **短上下文窗口视频生成器：** 这类生成器难以在长期内保持场景的连贯性，随着时间的推移，场景细节容易出现偏差。\n\n### VMem的创新机制\n\n*   **核心思想：** VMem通过几何索引过去视图来“记忆”它们，这种索引基于这些视图所观察到的3D表面元素（surfels）。\n*   **高效检索：** 这种索引机制使得在生成新视图时，能够高效地检索出最相关的历史视图。\n*   **聚焦相关视图：** 通过仅关注这些相关的历史视图，VMem能够以远低于将所有历史视图作为上下文的计算成本，生成一致的想象环境探索。\n\n### 性能评估\n\n*   **基准测试：** 该方法在具有挑战性的长期场景合成基准测试中进行了评估。\n*   **卓越表现：** 与现有方法相比，VMem在保持场景连贯性和相机控制方面展现出卓越的性能。",
      "shortSummary": "VMem提出了一种基于Surfel索引视图记忆的新型机制，旨在解决交互式视频场景生成中长期场景连贯性差和错误累积的问题。通过几何索引过去视图并高效检索相关信息，VMem能以更低的计算成本生成一致的虚拟环境探索。该方法在长期场景合成基准测试中表现出优于现有方法的场景连贯性和相机控制能力。",
      "translated_title": "VMem：基于Surfel索引视图记忆的一致交互式视频场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control."
    },
    {
      "title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成 (原标题: Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations)",
      "link": "https://arxiv.org/abs/2506.18898",
      "pubDate": "Mon, 23 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-23T13:59:14.000Z",
      "creator": "Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang",
      "summary": "## 视觉作为一种方言：通过文本对齐表示统一视觉理解与生成\n\n本文提出了一种多模态框架，旨在通过共享的离散语义表示来统一视觉理解和生成。该框架的核心组件是**文本对齐分词器（Text-Aligned Tokenizer, TA-Tok）**。\n\n### 核心机制\n\n*   **TA-Tok 的功能**：将图像转换为离散的视觉标记（tokens）。\n*   **文本对齐码本**：TA-Tok 使用一个从大型语言模型（LLM）词汇表中投射出来的文本对齐码本，确保视觉标记与文本语义对齐。\n*   **统一表示空间**：通过这种方式，视觉和文本被整合到一个具有扩展词汇量的统一空间中。\n\n### 多模态 LLM (Tar)\n\n*   **跨模态输入输出**：该框架中的多模态 LLM 被命名为 Tar，它通过一个共享接口实现跨模态的输入和输出，无需针对特定模态进行单独设计。\n\n### 创新技术\n\n*   **尺度自适应编码与解码**：为平衡效率和视觉细节，论文提出了尺度自适应的编码和解码机制。\n*   **生成式去分词器**：为了生成高保真度的视觉输出，Tar 采用了生成式去分词器。\n*   **互补的去分词器**：为满足不同的解码需求，系统利用了两种互补的去分词器：一个快速自回归模型和一个基于扩散的模型。\n\n### 预训练与性能\n\n*   **模态融合增强**：研究人员探索了先进的预训练任务，以增强模态间的融合。\n*   **实验结果**：在多个基准测试中，Tar 的表现与现有多模态 LLM 方法持平或超越，并展现出更快的收敛速度和更高的训练效率。",
      "shortSummary": "本文提出了一个名为 Tar 的多模态框架，旨在通过共享的离散语义表示统一视觉理解和生成。其核心是文本对齐分词器（TA-Tok），它将图像转换为与大型语言模型词汇表对齐的离散标记，从而将视觉和文本整合到统一空间。Tar 作为多模态LLM，支持跨模态输入输出，并引入了尺度自适应编码/解码和生成式去分词器。实验表明，Tar 在性能和训练效率上均优于现有方法。",
      "translated_title": "视觉作为一种方言：通过文本对齐表示统一视觉理解与生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com"
    },
    {
      "title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM (原标题: ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs)",
      "link": "https://arxiv.org/abs/2506.18896",
      "pubDate": "Mon, 23 Jun 2025 13:59:02 GMT",
      "isoDate": "2025-06-23T13:59:02.000Z",
      "creator": "Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang",
      "summary": "### ReasonFlux-PRM：LLM中轨迹感知过程奖励模型\n\n**1. 引言与背景**\n*   **现有问题：** 过程奖励模型（PRM）是监督大型语言模型（LLM）中间推理步骤的强大框架。然而，先前的PRM主要基于模型的最终输出响应进行训练，难以鲁棒地评估中间思维轨迹，尤其是在Deepseek-R1等前沿推理模型生成的轨迹-响应输出的新兴场景中。\n\n**2. ReasonFlux-PRM的提出**\n*   **核心创新：** 本文引入了ReasonFlux-PRM，这是一种新颖的轨迹感知PRM，专门设计用于评估轨迹-响应类型的推理痕迹。\n*   **关键特性：** ReasonFlux-PRM整合了**步骤级**和**轨迹级**的监督，从而能够进行与结构化思维链数据对齐的细粒度奖励分配。\n\n**3. 应用场景**\nReasonFlux-PRM被设计用于支持离线和在线设置下的奖励监督，具体包括：\n*   **(i) 数据蒸馏：** 为下游较小模型的监督微调（SFT）选择高质量的模型蒸馏数据。\n*   **(ii) 强化学习：** 在强化学习（RL）过程中提供密集的进程级奖励，以优化策略。\n*   **(iii) 测试时扩展：** 实现奖励引导的Best-of-N测试时扩展。\n\n**4. 实验结果与性能**\n*   **基准测试：** 在AIME、MATH500和GPQA-Diamond等具有挑战性的下游基准测试中进行了实证评估。\n*   **数据质量：** 结果表明，ReasonFlux-PRM-7B选择的数据质量高于强大的PRM（例如Qwen2.5-Math-PRM-72B）和人工策划的基线。\n*   **性能提升：** 派生的ReasonFlux-PRM-7B带来了持续的性能改进，平均增益如下：\n    *   监督微调（SFT）：12.1%\n    *   强化学习（RL）：4.5%\n    *   测试时扩展：6.3%\n\n**5. 模型发布**\n*   为了支持资源受限的应用和边缘部署，研究团队还发布了高效的ReasonFlux-PRM-1.5B模型。",
      "shortSummary": "ReasonFlux-PRM是一种新型轨迹感知过程奖励模型（PRM），旨在解决现有PRM在评估LLM中间思维轨迹方面的不足。它通过整合步骤级和轨迹级监督，为轨迹-响应输出提供细粒度奖励。ReasonFlux-PRM支持数据蒸馏、强化学习和测试时扩展。在AIME、MATH500和GPQA-Diamond等基准测试中，ReasonFlux-PRM-7B在数据选择上优于现有PRM和人工基线，并在SFT、RL和测试时扩展中分别实现了12.1%、4.5%和6.3%的性能提升。团队还发布了轻量级ReasonFlux-PRM-1.5B模型。",
      "translated_title": "ReasonFlux-PRM：用于LLM中长链式思维推理的轨迹感知PRM",
      "images": [],
      "contentSource": "完整文章",
      "content": "Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux"
    },
    {
      "title": "法线之光：通用光度立体视觉的统一特征表示 (原标题: Light of Normals: Unified Feature Representation for Universal Photometric Stereo)",
      "link": "https://arxiv.org/abs/2506.18882",
      "pubDate": "Mon, 23 Jun 2025 13:53:11 GMT",
      "isoDate": "2025-06-23T13:53:11.000Z",
      "creator": "Hong Li, Houyuan Chen, Chongjie Ye, Zhaoxi Chen, Bohan Li, Shaocong Xu, Xianda Guo, Xuhui Liu, Yikai Wang, Baochang Zhang, Satoshi Ikehata, Boxin Shi, Anyi Rao, Hao Zhao",
      "summary": "### 通用光度立体视觉的挑战与进展\n\n本文探讨了通用光度立体视觉（Universal Photometric Stereo, PS）领域面临的核心挑战，该技术旨在在任意光照条件下，无需依赖特定的光照模型，即可恢复物体表面的高质量法线。\n\n尽管近期在通用光度立体视觉方面取得了一些进展，例如SDM-UniPS和Uni MS-PS等方法，但该领域仍存在两个根本性难题：\n\n1.  **光照与表面法线特征的深度耦合**：\n    *   在观察到的强度中存在模糊性，使得难以确定亮度变化是源于光照变化还是表面方向。这种深度耦合使得区分这两种因素变得复杂。\n\n2.  **复杂表面高频几何细节的保留**：\n    *   传统特征处理操作难以准确捕捉复杂几何形状中固有的高频细节。这些细节包括：\n        *   自阴影（self-shadowing）\n        *   相互反射（inter-reflections）\n        *   以及细微的法线变化\n    *   这些复杂性使得准确恢复表面法线变得更具挑战性。",
      "shortSummary": "本文讨论了通用光度立体视觉（PS）面临的挑战。该技术旨在从任意光照下的物体中恢复高质量表面法线，无需特定光照模型。尽管现有方法有所进展，但仍存在两大难题：一是光照与表面法线特征的深度耦合，导致亮度变化来源难以区分；二是难以保留复杂表面（如自阴影、相互反射）的高频几何细节，传统方法难以准确捕捉。",
      "translated_title": "法线之光：通用光度立体视觉的统一特征表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal photometric stereo (PS) aims to recover high-quality surface normals from objects under arbitrary lighting conditions without relying on specific illumination models. Despite recent advances such as SDM-UniPS and Uni MS-PS, two fundamental challenges persist: 1) the deep coupling between varying illumination and surface normal features, where ambiguity in observed intensity makes it difficult to determine whether brightness variations stem from lighting changes or surface orientation; and 2) the preservation of high-frequency geometric details in complex surfaces, where intricate geometries create self-shadowing, inter-reflections, and subtle normal variations that conventional feature processing operations struggle to capture accurately."
    }
  ],
  "lastUpdated": "2025-06-26T09:34:36.074Z"
}