{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "大型语言和多模态模型中的离散扩散：一项综述 (原标题: Discrete Diffusion in Large Language and Multimodal Models: A Survey)",
      "link": "https://arxiv.org/abs/2506.13759",
      "pubDate": "Mon, 16 Jun 2025 13:59:08 GMT",
      "isoDate": "2025-06-16T13:59:08.000Z",
      "creator": "Runpeng Yu, Qi Li, Xinchao Wang",
      "summary": "## 大型语言和多模态模型中的离散扩散：一项综述\n\n本综述系统地概述了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs）。\n\n### 离散扩散模型的特点与优势\n\n与自回归（AR）模型不同，dLLMs和dMLLMs采用多令牌、并行解码范式，利用全注意力机制和基于去噪的生成策略。这种范式自然地带来了以下优势：\n\n*   **并行生成能力**：显著提高生成效率。\n*   **细粒度输出可控性**：能够更精确地控制生成内容的细节。\n*   **动态、响应感知能力**：模型能够根据上下文动态调整响应。\n\n这些能力是AR模型此前难以实现的。\n\n### 性能与发展\n\n近期，越来越多的工业级专有d(M)LLMs以及大量的开源学术d(M)LLMs已展现出与自回归模型相当的性能，同时在推理速度上实现了高达10倍的加速。\n\n离散扩散LLMs和MLLMs的进步主要得益于两个领域的进展：\n\n1.  **自回归LLMs和MLLMs的发展**：积累了大量数据、基准和用于训练与推理的基础设施。\n2.  **离散扩散底层数学模型的演进**：为离散扩散模型提供了坚实的理论基础。\n\n这些进步共同推动了2025年初dLLMs和dMLLMs研究的激增。\n\n### 综述内容概览\n\n本综述全面回顾了dLLM和dMLLM领域的研究，具体涵盖：\n\n*   追溯dLLMs和dMLLMs的历史发展。\n*   形式化其底层的数学框架。\n*   对代表性模型进行分类。\n*   分析关键的训练和推理技术。\n*   总结在语言、视觉-语言和生物等领域的新兴应用。\n*   探讨未来的研究方向和部署前景。",
      "shortSummary": "本综述系统地审视了离散扩散语言模型（dLLMs）和多模态语言模型（dMLLMs）。与自回归模型不同，d(M)LLMs采用并行解码和去噪生成策略，实现了并行生成、精细控制和动态感知。它们在性能上与自回归模型相当，但推理速度提升高达10倍。其发展得益于自回归模型的积累和离散扩散数学模型的进步。综述内容涵盖历史发展、数学框架、模型分类、训练推理技术及未来应用方向。",
      "translated_title": "大型语言和多模态模型中的离散扩散：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output controllability, and dynamic, response-aware perception. These capabilities are previously difficult to achieve with AR models. Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10x acceleration in inference speed.   The advancement of discrete diffusion LLMs and MLLMs has been largely driven by progress in two domains. The first is the development of autoregressive LLMs and MLLMs, which has accumulated vast amounts of data, benchmarks, and foundational infrastructure for training and inference. The second contributing domain is the evolution of the mathematical models underlying discrete diffusion. Together, these advancements have catalyzed a surge in dLLMs and dMLLMs research in early 2025.   In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, and categorize representative models. We further analyze key techniques for training and inference, and summarize emerging applications across language, vision-language, and biological domains. We conclude by discussing future directions for research and deployment.   Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
    },
    {
      "title": "通过预算指导引导LLM思维 (原标题: Steering LLM Thinking with Budget Guidance)",
      "link": "https://arxiv.org/abs/2506.13752",
      "pubDate": "Mon, 16 Jun 2025 13:57:05 GMT",
      "isoDate": "2025-06-16T13:57:05.000Z",
      "creator": "Junyan Li, Wenshuo Zhao, Yang Zhang, Chuang Gan",
      "summary": "# 预算指导：引导LLM思维过程\n\n## 引言与背景\n近期的大型语言模型（LLM）为了提升性能，常常进行广泛的“深度思考”或推理。然而，这种冗长的推理过程并非总是理想的，因为它会带来过高的推理成本，而性能提升却不成比例。因此，在不牺牲性能的前提下控制推理长度变得至关重要，尤其是在思考预算紧张的情况下，这仍然是一个挑战。\n\n## 核心方法：预算指导（Budget Guidance）\n本文提出了一种名为“预算指导”（Budget Guidance）的简单而有效的方法，旨在引导LLM的推理过程，使其符合预设的思考预算，而无需对LLM进行任何微调。\n\n### 工作原理\n*   **轻量级预测器：** 引入一个轻量级的预测器。\n*   **伽马分布建模：** 该预测器在生成下一个令牌（token）时，对剩余的思考长度建模一个伽马分布。\n*   **软性令牌级引导：** 这一信号被用于以软性、令牌级别的方式引导生成过程，确保整体的推理轨迹遵循指定的思考预算。\n\n## 主要成果与优势\n*   **自然控制思维长度：** 预算指导能够自然地控制LLM的思维长度。\n*   **显著提升令牌效率：** 在具有挑战性的数学基准测试中，与基线方法相比，它显著提高了令牌效率。\n*   **性能表现：**\n    *   在MATH-500基准测试中，在预算紧张的情况下，与基线方法相比，准确率提升高达26%。\n    *   在使用完整思考模型63%的思考令牌量下，仍能保持具有竞争力的准确率。\n*   **泛化能力与涌现能力：** 预算指导还能够泛化到更广泛的任务领域，并展现出一些涌现能力，例如估计问题难度。\n\n## 资源可用性\n该方法的源代码可在提供的链接中获取。",
      "shortSummary": "本文提出“预算指导”方法，旨在解决大型语言模型（LLM）深度思考成本高昂且效率不高的问题。该方法通过引入一个轻量级预测器，在不微调LLM的情况下，以令牌级方式引导推理过程，使其符合预设的思考预算。实验表明，预算指导在数学基准测试中显著提升了令牌效率和准确率，例如在MATH-500上准确率提升高达26%，并能以更少令牌保持竞争力，同时具备泛化能力和估计问题难度等涌现能力。",
      "translated_title": "通过预算指导引导LLM思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent deep-thinking large language models often reason extensively to improve performance, but such lengthy reasoning is not always desirable, as it incurs excessive inference costs with disproportionate performance gains. Controlling reasoning length without sacrificing performance is therefore important, but remains challenging, especially under tight thinking budgets. We propose budget guidance, a simple yet effective method for steering the reasoning process of LLMs toward a target budget without requiring any LLM fine-tuning. Our approach introduces a lightweight predictor that models a Gamma distribution over the remaining thinking length during next-token generation. This signal is then used to guide generation in a soft, token-level manner, ensuring that the overall reasoning trace adheres to the specified thinking budget. Budget guidance enables natural control of the thinking length, along with significant token efficiency improvements over baseline methods on challenging math benchmarks. For instance, it achieves up to a 26% accuracy gain on the MATH-500 benchmark under tight budgets compared to baseline methods, while maintaining competitive accuracy with only 63% of the thinking tokens used by the full-thinking model. Budget guidance also generalizes to broader task domains and exhibits emergent capabilities, such as estimating question difficulty. The source code is available at: https://github.com/UMass-Embodied-AGI/BudgetGuidance."
    },
    {
      "title": "Test3R: 在测试时学习重建3D (原标题: Test3R: Learning to Reconstruct 3D at Test Time)",
      "link": "https://arxiv.org/abs/2506.13750",
      "pubDate": "Mon, 16 Jun 2025 13:56:22 GMT",
      "isoDate": "2025-06-16T13:56:22.000Z",
      "creator": "Yuheng Yuan, Qiuhong Shen, Shizun Wang, Xingyi Yang, Xinchao Wang",
      "summary": "# Test3R: 在测试时学习重建3D\n\n## 概述\n\n本文介绍了一种名为 Test3R 的测试时学习技术，旨在显著提升3D重建的几何精度。传统的密集匹配方法（如 DUSt3R）通过回归成对点图进行3D重建，但其对成对预测的依赖以及有限的泛化能力，固有地限制了全局几何一致性。Test3R 通过一种出人意料的简单方法解决了这一问题。\n\n## Test3R 的核心思想与工作原理\n\nTest3R 的核心思想是在测试时通过一个自监督目标来优化网络，以最大化不同重建之间的几何一致性。具体实现如下：\n\n*   **输入数据**：Test3R 使用图像三元组（$I_1, I_2, I_3$）作为输入。\n*   **重建生成**：从图像对（$I_1, I_2$）和（$I_1, I_3$）生成两次独立的3D重建。\n*   **自监督优化**：在测试时，模型通过一个自监督目标进行优化。这个目标是最大化这两次重建相对于共同图像 $I_1$ 的几何一致性。\n*   **目标**：这种优化确保了模型能够生成跨图像对一致的输出，无论输入是什么。\n\n## 性能与优势\n\n广泛的实验证明，Test3R 技术在3D重建和多视角深度估计任务上显著优于现有的最先进方法。其主要优势包括：\n\n*   **显著提升精度**：在3D重建和多视角深度估计任务中，Test3R 表现出卓越的性能。\n*   **普适性**：该技术具有普遍适用性，可以轻松应用于其他模型。\n*   **成本效益**：Test3R 几乎是免费的，因为它只需要极少的测试时训练开销和参数占用。\n\n## 代码可用性\n\n相关代码已在 [this https URL](https://this.https.url) 提供。",
      "shortSummary": "Test3R 是一种创新的测试时学习技术，旨在提升3D重建的几何精度。它通过使用图像三元组，在测试时利用自监督目标优化网络，以最大化不同重建相对于共同图像的几何一致性。该方法显著优于现有技术，在3D重建和多视角深度估计任务上表现出色。Test3R 具有普适性，且成本极低，易于应用于其他模型。",
      "translated_title": "Test3R: 在测试时学习重建3D",
      "images": [],
      "contentSource": "完整文章",
      "content": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D reconstruction. However, the reliance on pairwise prediction and the limited generalization capability inherently restrict the global geometric consistency. In this work, we introduce Test3R, a surprisingly simple test-time learning technique that significantly boosts geometric accuracy. Using image triplets (I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and (I_1,I_3). The core idea is to optimize the network at test time via a self-supervised objective: maximizing the geometric consistency between these two reconstructions relative to the common image I_1. This ensures the model produces cross-pair consistent outputs, regardless of the inputs. Extensive experiments demonstrate that our technique significantly outperforms previous state-of-the-art methods on the 3D reconstruction and multi-view depth estimation tasks. Moreover, it is universally applicable and nearly cost-free, making it easily applied to other models and implemented with minimal test-time training overhead and parameter footprint. Code is available at https://github.com/nopQAQ/Test3R."
    },
    {
      "title": "Ego-R1：用于超长第一人称视频推理的工具链思维 (原标题: Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning)",
      "link": "https://arxiv.org/abs/2506.13654",
      "pubDate": "Mon, 16 Jun 2025 12:17:08 GMT",
      "isoDate": "2025-06-16T12:17:08.000Z",
      "creator": "Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, Ziwei Liu",
      "summary": "### Ego-R1：用于超长第一人称视频推理的工具链思维框架\n\n本文介绍了 **Ego-R1**，一个用于对超长（即数天或数周）第一人称视频进行推理的新型框架。该框架利用了由强化学习（RL）训练的 **Ego-R1 Agent** 所协调的结构化 **工具链思维（Chain-of-Tool-Thought, CoTT）** 过程。\n\n**核心理念与方法：**\n\n*   **灵感来源：** CoTT 过程受到人类解决问题策略的启发。\n*   **分解复杂推理：** CoTT 将复杂的推理任务分解为模块化的步骤。\n*   **工具调用：** Ego-R1 Agent 在每个步骤中调用特定的工具，以迭代和协作地回答子问题，从而处理诸如时间检索和多模态理解等任务。\n\n**训练范式：**\n\n*   **两阶段训练：** Ego-R1 采用两阶段训练范式，旨在使 Agent 能够动态地提出逐步的工具来支持长距离推理。\n    1.  **监督微调（SFT）：** 使用 CoTT 数据对预训练语言模型进行监督微调。\n    2.  **强化学习（RL）：** 进一步通过强化学习训练 Agent。\n\n**数据集：**\n\n*   为了促进训练，研究团队构建了名为 **Ego-R1 Data** 的数据集，其中包括：\n    *   **Ego-CoTT-25K：** 用于监督微调（SFT）。\n    *   **Ego-QA-4.4K：** 用于强化学习（RL）。\n\n**评估基准：**\n\n*   Ego-R1 Agent 在新策划的 **Ego-R1 Bench** 上进行评估。这是一个为期一周的视频问答基准，包含来自混合来源的人工验证问答对。\n\n**实验结果：**\n\n*   广泛的实验结果表明，Ego-R1 Agent 动态的、工具增强的思维链推理方法能够有效应对理解超长第一人称视频的独特挑战。\n*   该方法显著扩展了时间覆盖范围，从数小时延长至一周。",
      "shortSummary": "Ego-R1 是一个用于超长第一人称视频推理的新框架，它引入了由强化学习训练的 Ego-R1 Agent 协调的工具链思维（CoTT）过程。CoTT 将复杂推理分解为模块化步骤，Agent 动态调用工具解决子问题。该框架采用两阶段训练（SFT和RL），并构建了Ego-R1 Data数据集（Ego-CoTT-25K和Ego-QA-4.4K）。在Ego-R1 Bench上的评估显示，Ego-R1能有效处理超长视频，将时间覆盖从几小时扩展到一周。",
      "translated_title": "Ego-R1：用于超长第一人称视频推理的工具链思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week."
    },
    {
      "title": "MiniMax-M1：利用闪电注意力机制高效扩展测试时计算 (原标题: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention)",
      "link": "https://arxiv.org/abs/2506.13585",
      "pubDate": "Mon, 16 Jun 2025 11:08:02 GMT",
      "isoDate": "2025-06-16T11:08:02.000Z",
      "creator": "MiniMax, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang Fan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze Wenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang Yu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, Zijun Sun",
      "summary": "## MiniMax-M1：高效扩展测试时计算的新范式\n\nMiniMax团队推出了MiniMax-M1，这是全球首个开源、大规模混合注意力推理模型，旨在高效扩展测试时计算能力。\n\n### 模型架构与核心特性\n\n*   **混合MoE架构与闪电注意力**：MiniMax-M1结合了混合专家混合（MoE）架构和创新的闪电注意力机制。这一组合使其能够高效处理复杂任务。\n*   **基于MiniMax-Text-01**：该模型基于MiniMax团队先前的MiniMax-Text-01模型开发，该模型总参数量达4560亿，每个token激活459亿参数。\n*   **超长上下文支持**：MiniMax-M1原生支持100万token的上下文长度，是DeepSeek R1上下文大小的8倍，使其特别适用于需要处理长输入和进行深度思考的复杂任务。\n*   **高效测试时计算**：闪电注意力机制是M1的关键创新，它使得测试时计算能够高效扩展。\n\n### 训练方法与效率\n\n*   **大规模强化学习（RL）**：MiniMax-M1通过大规模强化学习进行训练，涵盖了沙盒环境和真实世界软件工程环境等多样化问题。\n*   **新型RL算法CISPO**：为了进一步提升RL训练效率，MiniMax团队提出了CISPO（Clipped Importance Sampling Policy Optimization）算法。CISPO通过裁剪重要性采样权重而非token更新，在性能上超越了其他竞争性RL变体。\n*   **显著的训练成本效益**：结合混合注意力机制和CISPO算法，MiniMax-M1在512块H800 GPU上仅用三周时间就完成了完整的RL训练，租用成本仅为534,700美元。\n\n### 模型版本与性能表现\n\n*   **发布版本**：MiniMax-M1发布了两个版本，分别具有40K和80K的“思考预算”（thinking budgets），其中40K版本代表了80K训练的中间阶段。\n*   **基准测试表现**：在标准基准测试中，MiniMax-M1模型与DeepSeek-R1和Qwen3-235B等强大的开源模型相比，表现出相当或更优的性能，尤其在复杂的软件工程、工具利用和长上下文任务方面具有显著优势。\n\n### 可用性\n\nMiniMax-M1已作为开源项目公开发布。",
      "shortSummary": "MiniMax团队推出了MiniMax-M1，这是首个开源、大规模混合注意力推理模型。它结合了混合MoE架构和创新的闪电注意力机制，原生支持100万token的上下文长度，并能高效扩展测试时计算。通过大规模强化学习和新型CISPO算法，模型训练成本效益显著。MiniMax-M1在软件工程、工具利用和长上下文任务上表现出色，已公开发布。",
      "translated_title": "MiniMax-M1：利用闪电注意力机制高效扩展测试时计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1."
    },
    {
      "title": "基于图像的剩余寿命不确定性感知预测 (原标题: Uncertainty-Aware Remaining Lifespan Prediction from Images)",
      "link": "https://arxiv.org/abs/2506.13430",
      "pubDate": "Mon, 16 Jun 2025 08:47:37 GMT",
      "isoDate": "2025-06-16T08:47:37.000Z",
      "creator": "Tristan Kenneweg, Philip Kenneweg, Barbara Hammer",
      "summary": "## 基于图像的剩余寿命不确定性感知预测\n\n### 引言\n从图像预测与死亡率相关的结果，为可及、无创且可扩展的健康筛查提供了前景。\n\n### 方法概述\n*   本文提出了一种新方法，利用预训练的视觉Transformer基础模型来估计个体的剩余寿命。\n*   该方法能够从面部和全身图像中提取信息进行预测。\n*   核心创新在于提供了鲁棒的不确定性量化，即模型不仅给出预测值，还能评估其预测的置信度。\n*   研究发现，预测的不确定性与真实的剩余寿命之间存在系统性变化。\n*   为了有效建模这种不确定性，该方法为每个样本学习一个高斯分布，从而量化预测的置信区间。\n\n### 性能表现\n*   在现有且已建立的数据集上，该方法实现了7.48年的平均绝对误差（MAE），达到了当前最先进的水平。\n*   研究团队还整理并发布了两个新的、更高质量的数据集，在这些新数据集上，模型的MAE进一步显著提高，分别达到4.79年和5.07年。\n\n### 不确定性校准\n*   模型提供的剩余寿命不确定性估计是经过良好校准的。\n*   通过分桶预期校准误差（bucketed expected calibration error）衡量，其值为0.62年，表明模型的不确定性估计与实际误差吻合度高。\n\n### 意义与未来方向\n*   尽管该方法目前不打算用于临床部署，但其研究结果突出显示了从日常图像中提取具有医学相关性信号的巨大潜力。\n*   为了促进后续研究，所有相关的代码和数据集均已公开。",
      "shortSummary": "该研究提出一种利用预训练视觉Transformer模型，从面部和全身图像预测剩余寿命的方法，并提供鲁棒的不确定性量化。该方法在现有数据集上实现了7.48年的最先进平均绝对误差，并在新数据集上进一步提升至4.79年和5.07年。模型提供良好校准的不确定性估计。尽管不用于临床，但结果表明从图像中提取医学信号的巨大潜力。所有代码和数据集均已公开。",
      "translated_title": "基于图像的剩余寿命不确定性感知预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Predicting mortality-related outcomes from images offers the prospect of accessible, noninvasive, and scalable health screening. We present a method that leverages pretrained vision transformer foundation models to estimate remaining lifespan from facial and whole-body images, alongside robust uncertainty quantification. We show that predictive uncertainty varies systematically with the true remaining lifespan, and that this uncertainty can be effectively modeled by learning a Gaussian distribution for each sample. Our approach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on an established Dataset, and further improves to 4.79 and 5.07 years MAE on two new, higher-quality datasets curated and published in this work. Importantly, our models provide well-calibrated uncertainty estimates, as demonstrated by a bucketed expected calibration error of 0.62 years. While not intended for clinical deployment, these results highlight the potential of extracting medically relevant signals from images. We make all code and datasets available to facilitate further research."
    },
    {
      "title": "AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词 (原标题: Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns)",
      "link": "https://arxiv.org/abs/2506.13172",
      "pubDate": "Mon, 16 Jun 2025 03:34:31 GMT",
      "isoDate": "2025-06-16T03:34:31.000Z",
      "creator": "Evgeny Markhasin",
      "summary": "### AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词\n\n本研究提出并评估了一套概念验证（PoC）的结构化工作流提示，旨在引导大型语言模型（LLMs）对学术手稿进行高级语义和语言分析，同时激发类似人类的层次推理能力。这些提示主要针对两个复杂的分析任务：\n\n*   **识别无根据的主张（信息完整性）**：在摘要中找出缺乏支持的陈述。\n*   **标记模糊的代词指代（语言清晰度）**：识别指代不明确的代词。\n\n#### 研究方法\n\n研究对两种前沿模型（Gemini Pro 2.5 Pro 和 ChatGPT Plus o3）在不同上下文条件下进行了系统、多次运行的评估。\n\n#### 关键发现\n\n**1. 信息完整性任务（识别无根据的主张）**\n\n*   **模型性能差异显著**：\n    *   两种模型都能成功识别无根据的名词短语头部（95%的成功率）。\n    *   然而，ChatGPT 始终未能识别无根据的形容词修饰语（0%的成功率），而 Gemini 则正确标记了（95%的成功率）。\n*   **句法角色影响**：这一结果引发了关于目标句法角色可能对模型性能产生影响的问题。\n\n**2. 语言分析任务（标记模糊代词）**\n\n*   **完整手稿上下文**：在提供完整手稿上下文的情况下，两种模型都表现良好（80-90%的成功率）。\n*   **仅摘要上下文**：\n    *   ChatGPT 在仅摘要的设置下达到了完美的100%成功率。\n    *   而 Gemini 的性能则大幅下降。\n\n#### 结论与启示\n\n*   **结构化提示的可行性**：研究结果表明，结构化提示是进行复杂文本分析的一种可行方法。\n*   **性能依赖性**：提示的性能可能高度依赖于模型、任务类型和上下文之间的相互作用。\n*   **测试必要性**：这突出强调了进行严格、针对特定模型的测试的必要性，以优化LLM在复杂分析任务中的应用。",
      "shortSummary": "本研究评估了AI（Gemini Pro 2.5 Pro, ChatGPT Plus o3）在识别学术摘要中无根据主张和模糊代词方面的能力。结果显示，AI在不同任务和上下文条件下表现各异：ChatGPT在识别形容词修饰语方面表现不佳，而Gemini在仅摘要的代词任务中性能下降。研究表明结构化提示可行，但模型、任务和上下文的交互作用对性能影响显著，强调了进行严格、针对特定模型的测试的重要性。",
      "translated_title": "AI辅助的摘要和结论分析：标记无根据的主张和模糊的代词",
      "images": [],
      "contentSource": "RSS",
      "content": "We present and evaluate a suite of proof-of-concept (PoC), structured workflow prompts designed to elicit human-like hierarchical reasoning while guiding Large Language Models (LLMs) in high-level semantic and linguistic analysis of scholarly manuscripts. The prompts target two non-trivial analytical tasks: identifying unsubstantiated claims in summaries (informational integrity) and flagging ambiguous pronoun references (linguistic clarity). We conducted a systematic, multi-run evaluation on two frontier models (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context conditions. Our results for the informational integrity task reveal a significant divergence in model performance: while both models successfully identified an unsubstantiated head of a noun phrase (95% success), ChatGPT consistently failed (0% success) to identify an unsubstantiated adjectival modifier that Gemini correctly flagged (95% success), raising a question regarding potential influence of the target's syntactic role. For the linguistic analysis task, both models performed well (80-90% success) with full manuscript context. In a summary-only setting, however, ChatGPT achieved a perfect (100%) success rate, while Gemini's performance was substantially degraded. Our findings suggest that structured prompting is a viable methodology for complex textual analysis but show that prompt performance may be highly dependent on the interplay between the model, task type, and context, highlighting the need for rigorous, model-specific testing."
    },
    {
      "title": "通过基于分块的提示和分解利用大型语言模型进行时间序列预测 (原标题: Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition)",
      "link": "https://arxiv.org/abs/2506.12953",
      "pubDate": "Sun, 15 Jun 2025 15:42:58 GMT",
      "isoDate": "2025-06-15T15:42:58.000Z",
      "creator": "Mayank Bumb, Anshul Vemulapalli, Sri Harsha Vardhan Prasad Jella, Anish Gupta, An La, Ryan A. Rossi, Hongjie Chen, Franck Dernoncourt, Nesreen K. Ahmed, Yu Wang",
      "summary": "## 通过基于分块的提示和分解利用大型语言模型进行时间序列预测\n\n### 背景与挑战\n\n*   **大型语言模型（LLMs）在时间序列分析中的潜力：** 近期LLMs的进展为准确高效的时间序列分析带来了新的可能性。\n*   **现有方法的局限性：** 先前的工作通常需要大量的微调，并且/或者忽略了序列间（inter-series）的关联性。\n\n### 研究目标\n\n*   **探索简单灵活的提示策略：** 本研究旨在探索能够使LLMs执行时间序列预测的简单灵活的基于提示的策略，而无需进行大量的再训练或使用复杂的外部架构。\n\n### 核心方法与技术\n\n*   **方法名称：** 提出了一种名为 **PatchInstruct** 的方法。\n*   **关键技术：** 通过探索专门的提示方法，该方法利用了以下技术：\n    *   **时间序列分解（Time series decomposition）：** 将时间序列分解为不同的组成部分（如趋势、季节性、残差）。\n    *   **基于分块的标记化（Patch-based tokenization）：** 将时间序列数据分割成小的“分块”进行处理和标记。\n    *   **基于相似度的邻居增强（Similarity-based neighbor augmentation）：** 通过引入与当前序列相似的邻居序列来增强模型的能力。\n\n### 优势与成果\n\n*   **提升预测质量：** 发现通过上述方法可以增强LLM的时间序列预测质量。\n*   **保持简洁性：** 在提升质量的同时，保持了方法的简洁性。\n*   **最小化数据预处理：** 该方法仅需要最少的数据预处理。\n*   **实现精确有效预测：** PatchInstruct 方法使LLMs能够进行精确有效的预测。\n\n### 相关领域\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "该研究提出PatchInstruct方法，旨在通过基于分块的提示和分解，使大型语言模型（LLMs）无需大量微调或复杂架构即可进行时间序列预测。它利用时间序列分解、基于分块的标记化和基于相似度的邻居增强技术，显著提升了LLMs的预测质量，同时保持了方法的简洁性并最大程度地减少了数据预处理需求，实现了精确有效的预测。",
      "translated_title": "通过基于分块的提示和分解利用大型语言模型进行时间序列预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions."
    },
    {
      "title": "PersonaFeedback：一个用于个性化的大规模人工标注基准 (原标题: PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization)",
      "link": "https://arxiv.org/abs/2506.12915",
      "pubDate": "Sun, 15 Jun 2025 13:19:19 GMT",
      "isoDate": "2025-06-15T13:19:19.000Z",
      "creator": "Meiling Tao, Chenghao Zhu, Dongyi Ding, Tiannan Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou",
      "summary": "### PersonaFeedback：LLM个性化评估的新基准\n\n**1. 背景与问题挑战**\n*   随着大型语言模型（LLMs）能力的飞速提升，构建能够生成针对不同用户画像（persona）的个性化响应或服务的LLM系统，即“LLM个性化”，已成为日益重要的研究和工程问题。\n*   然而，与评估LLM通用能力和推理能力的新基准不断涌现不同，当前缺乏高质量的基准来评估LLM的个性化能力，这严重阻碍了该领域的发展。\n*   现有的一些基准通常要求模型从历史交互中推断隐式用户画像，这使得评估焦点不够明确。\n\n**2. PersonaFeedback基准的引入**\n*   为解决上述问题，研究人员引入了**PersonaFeedback**，这是一个旨在直接评估LLM在给定预定义用户画像和查询时提供个性化响应能力的全新基准。\n*   **核心特点：**\n    *   **解耦评估：** PersonaFeedback将“用户画像推断”与“个性化生成”这两个任务解耦，专注于评估模型根据**显式**用户画像生成定制化响应的能力。\n    *   **大规模人工标注：** 该基准包含8298个人工标注的测试用例。\n    *   **分层难度：** 测试用例根据用户画像的上下文复杂性以及区分两个个性化响应之间细微差别的难度，被划分为“简单（easy）”、“中等（medium）”和“困难（hard）”三个层级。\n\n**3. 综合评估结果与发现**\n*   研究团队对广泛的模型进行了全面的评估。\n*   **主要发现：**\n    *   即使是能够解决复杂现实世界推理任务的最新LLM，在PersonaFeedback的“困难”层级上表现也可能不佳，甚至人类评估者也可能发现区分这些响应具有挑战性。\n    *   对各种系统故障模式的深入分析表明，当前流行的检索增强框架（Retrieval-Augmented Framework）不应被视为个性化任务的默认解决方案。\n\n**4. 数据可用性与未来展望**\n*   所有基准数据、标注协议和评估流程都将公开发布，以促进未来LLM个性化领域的研究。\n*   该工作目前处于进行中（Work in progress）。",
      "shortSummary": "PersonaFeedback是一个新颖的大规模人工标注基准，旨在直接评估大型语言模型（LLM）根据显式用户画像生成个性化响应的能力。它包含8298个分层难度的测试用例，将用户画像推断与个性化生成解耦。评估结果显示，即使是最先进的LLM在“困难”层级上仍面临挑战，且现有检索增强框架并非个性化任务的默认解决方案。该基准数据将公开发布，以推动LLM个性化研究。",
      "translated_title": "PersonaFeedback：一个用于个性化的大规模人工标注基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the rapid improvement in the general capabilities of LLMs, LLM personalization, i.e., how to build LLM systems that can generate personalized responses or services that are tailored to distinct user personas, has become an increasingly important research and engineering problem. However, unlike many new challenging benchmarks being released for evaluating the general/reasoning capabilities, the lack of high-quality benchmarks for evaluating LLM personalization greatly hinders progress in this field. To address this, we introduce PersonaFeedback, a new benchmark that directly evaluates LLMs' ability to provide personalized responses given pre-defined user personas and queries. Unlike existing benchmarks that require models to infer implicit user personas from historical interactions, PersonaFeedback decouples persona inference from personalization, focusing on evaluating the model's ability to generate responses tailored to explicit personas. PersonaFeedback consists of 8298 human-annotated test cases, which are categorized into easy, medium, and hard tiers based on the contextual complexity of the user personas and the difficulty in distinguishing subtle differences between two personalized responses. We conduct comprehensive evaluations across a wide range of models. The empirical results reveal that even state-of-the-art LLMs that can solve complex real-world reasoning tasks could fall short on the hard tier of PersonaFeedback where even human evaluators may find the distinctions challenging. Furthermore, we conduct an in-depth analysis of failure modes across various types of systems, demonstrating that the current retrieval-augmented framework should not be seen as a de facto solution for personalization tasks. All benchmark data, annotation protocols, and the evaluation pipeline will be publicly available to facilitate future research on LLM personalization."
    },
    {
      "title": "MS4UI：一个用于用户界面教学视频多模态摘要的数据集 (原标题: MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos)",
      "link": "https://arxiv.org/abs/2506.12623",
      "pubDate": "Sat, 14 Jun 2025 16:39:32 GMT",
      "isoDate": "2025-06-14T16:39:32.000Z",
      "creator": "Yuan Zang, Hao Tan, Seunghyun Yoon, Franck Dernoncourt, Jiuxiang Gu, Kushal Kafle, Chen Sun, Trung Bui",
      "summary": "### MS4UI：用户界面教学视频多模态摘要数据集\n\n**研究背景与目标**\n\n本研究致力于教学视频的多模态摘要，旨在为用户提供一种高效学习技能的方式，即通过文本指令和关键视频帧的形式。研究人员观察到，现有基准主要关注通用语义级视频摘要，但对于教学视频中至关重要的分步可执行指令和插图，这些基准并不适用。\n\n**MS4UI 数据集提案**\n\n为了填补这一空白，研究人员提出了一个针对用户界面（UI）教学视频摘要的新型基准——MS4UI 数据集。\n\n**数据集详情**\n\n*   **规模与时长**：MS4UI 数据集共收集了2,413个UI教学视频，总时长超过167小时。\n*   **人工标注**：这些视频经过了细致的人工标注，涵盖了以下方面：\n    *   **视频分割**：将视频内容划分为逻辑步骤或片段。\n    *   **文本摘要**：为视频内容提供简洁的文本指令。\n    *   **视频摘要**：识别并提取关键视频帧作为视觉说明。\n*   **评估目的**：这些全面的标注使得对简洁且可执行的视频摘要方法进行评估成为可能。\n\n**实验结果与发现**\n\n研究人员在MS4UI数据集上进行了广泛的实验。结果表明，当前最先进的多模态摘要方法在UI视频摘要任务上表现不佳。这一发现突显了开发专门针对UI教学视频摘要的新方法的重要性。",
      "shortSummary": "本研究提出了MS4UI数据集，旨在解决现有基准不适用于用户界面（UI）教学视频多模态摘要的问题。该数据集包含2,413个UI教学视频，总时长超过167小时，并进行了人工标注以支持视频分割、文本和视频摘要。实验表明，当前最先进的多模态摘要方法在UI视频摘要方面表现不佳，强调了开发新方法的重要性。",
      "translated_title": "MS4UI：一个用于用户界面教学视频多模态摘要的数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "We study multi-modal summarization for instructional videos, whose goal is to provide users an efficient way to learn skills in the form of text instructions and key video frames. We observe that existing benchmarks focus on generic semantic-level video summarization, and are not suitable for providing step-by-step executable instructions and illustrations, both of which are crucial for instructional videos. We propose a novel benchmark for user interface (UI) instructional video summarization to fill the gap. We collect a dataset of 2,413 UI instructional videos, which spans over 167 hours. These videos are manually annotated for video segmentation, text summarization, and video summarization, which enable the comprehensive evaluations for concise and executable video summarization. We conduct extensive experiments on our collected MS4UI dataset, which suggest that state-of-the-art multi-modal summarization methods struggle on UI video summarization, and highlight the importance of new methods for UI instructional video summarization."
    },
    {
      "title": "多语言大型语言模型中的语言手术 (原标题: Language Surgery in Multilingual Large Language Models)",
      "link": "https://arxiv.org/abs/2506.12450",
      "pubDate": "Sat, 14 Jun 2025 07:09:50 GMT",
      "isoDate": "2025-06-14T07:09:50.000Z",
      "creator": "Joanito Agili Lopo, Muhammad Ravi Shulthan Habibi, Tack Hwa Wong, Muhammad Ilham Ghozali, Fajri Koto, Genta Indra Winata, Peerat Limkonchotiwat, Alham Fikri Aji, Samuel Cahyawijaya",
      "summary": "### 多语言大型语言模型中的语言手术\n\n本文深入探讨了大型语言模型（LLMs）中自然涌现的表征对齐现象，特别是在其中间层，及其对解耦语言特异性信息和语言无关信息的影响。\n\n**核心发现与分析：**\n\n*   **表征对齐的存在：** 研究经验性地证实了LLMs中存在这种自然形成的表征对齐。\n*   **行为分析：** 对比了这种自然对齐与显式设计的对齐模型，并分析了其行为特性。\n*   **语言特异性操作潜力：** 证明了这种对齐具有在不损害语义完整性的前提下，进行语言特异性操作的潜力。\n\n**推理时语言控制（ITLC）方法：**\n\n*   **方法提出：** 基于上述发现，本文提出了一种名为“推理时语言控制”（Inference-Time Language Control, ITLC）的新颖方法。\n*   **核心机制：** ITLC利用潜在注入（latent injection）技术。\n*   **主要目标：**\n    *   实现LLMs中精确的跨语言控制。\n    *   缓解LLMs中存在的语言混淆问题。\n\n**实验结果与效果：**\n\n*   **强大的跨语言控制能力：** 实验结果突出显示了ITLC在跨语言控制方面的强大能力。\n*   **语义完整性保持：** 在实现跨语言控制的同时，ITLC能够有效保持目标语言的语义完整性。\n*   **缓解语言混淆：** 本文进一步证明了ITLC在缓解跨语言语言混淆问题上的有效性。这个问题即使在当前大规模LLMs中也普遍存在，并导致生成内容语言不一致。\n\n**研究贡献：**\n\n*   深化了对LLMs中表征对齐的理解。\n*   引入了一种实用的解决方案，以增强LLMs的跨语言性能。",
      "shortSummary": "本文研究了多语言大型语言模型（LLMs）中自然涌现的表征对齐现象，并提出了一种名为“推理时语言控制”（ITLC）的新方法。ITLC利用潜在注入技术，实现了对LLMs的精确跨语言控制，并有效缓解了困扰LLMs的跨语言混淆问题。实验证明，ITLC在保持语义完整性的同时，显著提升了LLMs的跨语言性能，为增强其跨语言能力提供了实用方案。",
      "translated_title": "多语言大型语言模型中的语言手术",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across tasks and languages, revolutionizing natural language processing. This paper investigates the naturally emerging representation alignment in LLMs, particularly in the middle layers, and its implications for disentangling language-specific and language-agnostic information. We empirically confirm the existence of this alignment, analyze its behavior in comparison to explicitly designed alignment models, and demonstrate its potential for language-specific manipulation without semantic degradation. Building on these findings, we propose Inference-Time Language Control (ITLC), a novel method that leverages latent injection to enable precise cross-lingual language control and mitigate language confusion in LLMs. Our experiments highlight ITLC's strong cross-lingual control capabilities while preserving semantic integrity in target languages. Furthermore, we demonstrate its effectiveness in alleviating the cross-lingual language confusion problem, which persists even in current large-scale LLMs, leading to inconsistent language generation. This work advances our understanding of representation alignment in LLMs and introduces a practical solution for enhancing their cross-lingual performance."
    },
    {
      "title": "QGuard：基于问题的零样本多模态大型语言模型安全防护 (原标题: QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety)",
      "link": "https://arxiv.org/abs/2506.12299",
      "pubDate": "Fri, 13 Jun 2025 21:23:50 GMT",
      "isoDate": "2025-06-13T21:23:50.000Z",
      "creator": "Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, Yunho Maeng",
      "summary": "# QGuard：基于问题的零样本多模态大型语言模型安全防护\n\n## 引言\n大型语言模型（LLMs）的快速发展在广泛领域产生了显著影响。然而，伴随这些进步而来的是恶意用户利用有害和越狱提示进行攻击的潜在风险显著增加。尽管已有多项努力旨在预防此类恶意提示，但保护LLMs免受此类攻击仍然是一项重要且充满挑战的任务。\n\n## QGuard 方法\n本文提出了一种名为QGuard的简单而有效的安全防护方法，旨在解决上述挑战。该方法的核心特点和优势包括：\n*   **核心机制**：QGuard利用“问题提示”（question prompting）以零样本（zero-shot）的方式阻止有害提示。这意味着它无需针对特定的有害提示进行预先训练或微调，即可应对新的威胁。\n*   **防护范围**：该方法不仅能够防御基于文本的有害提示，还能有效抵御多模态有害提示攻击，扩展了其应用场景。\n*   **鲁棒性**：通过多样化和修改防护问题，QGuard能够保持对最新有害提示的鲁棒性，而无需进行额外的微调，从而提高了其适应性和持久性。\n\n## 实验结果\n实验结果表明，QGuard模型在纯文本和多模态有害数据集上均表现出具有竞争力的性能，验证了其在不同攻击场景下的有效性。\n\n## 独特优势与意义\n*   **白盒分析**：通过对问题提示的分析，QGuard能够实现对用户输入的白盒分析，这为理解和诊断潜在的恶意输入提供了透明度。\n*   **实际应用价值**：研究人员认为，QGuard方法为现实世界中的LLM服务在缓解与有害提示相关的安全风险方面提供了宝贵的见解。",
      "shortSummary": "QGuard是一种基于问题提示的零样本安全防护方法，旨在保护大型语言模型（LLMs）免受有害提示攻击。它能有效防御文本和多模态有害提示，并通过多样化问题保持对新威胁的鲁棒性，无需微调。实验证明其在多种数据集上表现出色，并能提供用户输入的白盒分析。QGuard为缓解LLM安全风险提供了重要见解。",
      "translated_title": "QGuard：基于问题的零样本多模态大型语言模型安全防护",
      "images": [],
      "contentSource": "完整文章",
      "content": "The recent advancements in Large Language Models(LLMs) have had a significant impact on a wide range of fields, from general domains to specialized areas. However, these advancements have also significantly increased the potential for malicious users to exploit harmful and jailbreak prompts for malicious attacks. Although there have been many efforts to prevent harmful prompts and jailbreak prompts, protecting LLMs from such malicious attacks remains an important and challenging task. In this paper, we propose QGuard, a simple yet effective safety guard method, that utilizes question prompting to block harmful prompts in a zero-shot manner. Our method can defend LLMs not only from text-based harmful prompts but also from multi-modal harmful prompt attacks. Moreover, by diversifying and modifying guard questions, our approach remains robust against the latest harmful prompts without fine-tuning. Experimental results show that our model performs competitively on both text-only and multi-modal harmful datasets. Additionally, by providing an analysis of question prompting, we enable a white-box analysis of user inputs. We believe our method provides valuable insights for real-world LLM services in mitigating security risks associated with harmful prompts."
    },
    {
      "title": "EgoPrivacy：你的第一人称摄像头透露了你什么信息？ (原标题: EgoPrivacy: What Your First-Person Camera Says About You?)",
      "link": "https://arxiv.org/abs/2506.12258",
      "pubDate": "Fri, 13 Jun 2025 18:19:54 GMT",
      "isoDate": "2025-06-13T18:19:54.000Z",
      "creator": "Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos",
      "summary": "### EgoPrivacy：第一人称摄像头对佩戴者的隐私威胁研究\n\n本文深入探讨了穿戴式摄像头普及所带来的独特隐私问题，特别是针对摄像头佩戴者自身的隐私威胁。以往的研究大多忽视了这一方面。\n\n**核心研究问题**\n\n*   第一人称视角视频能推断出佩戴者多少隐私信息？\n\n**EgoPrivacy 基准的引入**\n\n*   **定义：** EgoPrivacy 是首个大规模基准，旨在全面评估以自我为中心视觉中的隐私风险。\n*   **覆盖范围：** 它涵盖了三种类型的隐私：\n    *   **人口统计隐私 (Demographic Privacy)**\n    *   **个体隐私 (Individual Privacy)**\n    *   **情境隐私 (Situational Privacy)**\n*   **任务：** EgoPrivacy 定义了七项任务，旨在恢复从细粒度（如佩戴者身份）到粗粒度（如年龄组）的私人信息。\n\n**新型攻击策略：检索增强攻击 (Retrieval-Augmented Attack)**\n\n*   为进一步强调以自我为中心视觉固有的隐私威胁，研究提出了一种新颖的攻击策略——检索增强攻击。\n*   该策略利用从外部非自我中心视频池中进行的“自我到非自我”检索，以提高人口统计隐私攻击的有效性。\n\n**主要发现与隐私泄露风险**\n\n*   对所有威胁模型下不同攻击方式的广泛比较表明，佩戴者的私人信息极易泄露。\n*   **具体示例：** 研究结果表明，即使在零样本设置下，基础模型也能有效损害佩戴者的隐私，以70-80%的准确率恢复身份、场景、性别和种族等属性。\n\n**数据与代码可用性**\n\n*   本研究的代码和数据已公开可用。",
      "shortSummary": "“EgoPrivacy”研究探讨了第一人称摄像头对佩戴者的隐私威胁。该研究引入了EgoPrivacy基准，评估人口统计、个体和情境隐私风险，并提出了“检索增强攻击”策略。结果显示，佩戴者的隐私信息极易泄露，基础模型即使在零样本设置下也能以70-80%的准确率推断出身份、性别、种族等敏感信息，凸显了以自我为中心视觉的严重隐私风险。",
      "translated_title": "EgoPrivacy：你的第一人称摄像头透露了你什么信息？",
      "images": [],
      "contentSource": "完整文章",
      "content": "While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70-80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy."
    },
    {
      "title": "超新星事件数据集：通过关键事件分析解读大型语言模型的个性 (原标题: Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis)",
      "link": "https://arxiv.org/abs/2506.12189",
      "pubDate": "Fri, 13 Jun 2025 15:31:52 GMT",
      "isoDate": "2025-06-13T15:31:52.000Z",
      "creator": "Pranav Agarwal, Ioana Ciucă",
      "summary": "### 超新星事件数据集：通过关键事件分析解读大型语言模型的个性\n\n随着大型语言模型（LLMs）日益融入日常应用，理解它们的决策过程和潜在“个性”变得至关重要。本研究提出了一种新颖的方法，通过使用“超新星事件数据集”（Supernova Event Dataset）来解读模型的个性。\n\n**超新星事件数据集**\n*   该数据集包含多样化的文章，涵盖传记、历史事件、新闻和科学发现等领域。\n*   它被用于基准测试LLM从文本中提取和排序关键事件的能力。\n*   这项任务具有主观性和复杂性，需要模型进行长距离上下文推理和建模因果链。\n\n**评估框架与模型**\n*   研究评估了多种模型，包括小型模型如Phi-4、Orca 2和Qwen 2.5，以及大型且更强大的模型如Claude 3.7、Gemini 2.5和OpenAI o3。\n*   研究提出了一种独特的框架：利用另一个LLM作为“评判者”，根据目标模型对事件的选择和分类来推断其个性。\n\n**观察到的模型个性特征**\n分析揭示了不同模型独特的个性特征：\n*   **Orca 2**：展现出情感推理能力，侧重于人际动态。\n*   **Qwen 2.5**：表现出更具战略性、分析性的风格。\n*   在分析科学发现事件时：\n    *   **Claude Sonnet 3.7**：强调概念框架。\n    *   **Gemini 2.5 Pro**：优先考虑经验验证。\n    *   **OpenAI o3**：偏好循序渐进的因果推理。\n\n**研究意义**\n这项分析显著提高了模型的解释性，使其在广泛多样的应用中更具用户友好性。",
      "shortSummary": "本研究引入“超新星事件数据集”，旨在通过分析大型语言模型（LLMs）从文本中提取和排序关键事件的能力，来解读其内在“个性”。研究评估了多种LLM，并发现它们展现出独特的个性特征，例如Orca 2侧重情感推理，Qwen 2.5偏向分析风格，而不同模型在处理科学事件时各有侧重。这项工作有助于提高LLM的解释性，使其更易于理解和应用。",
      "translated_title": "超新星事件数据集：通过关键事件分析解读大型语言模型的个性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) are increasingly integrated into everyday applications. As their influence grows, understanding their decision making and underlying personality becomes essential. In this work, we interpret model personality using our proposed Supernova Event Dataset, a novel dataset with diverse articles spanning biographies, historical events, news, and scientific discoveries. We use this dataset to benchmark LLMs on extracting and ranking key events from text, a subjective and complex challenge that requires reasoning over long-range context and modeling causal chains. We evaluate small models like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as Claude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another LLM acts as a judge to infer each model's personality based on its selection and classification of events. Our analysis shows distinct personality traits: for instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal dynamics, while Qwen 2.5 displays a more strategic, analytical style. When analyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual framing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors step-by-step causal reasoning. This analysis improves model interpretability, making them user-friendly for a wide range of diverse applications."
    },
    {
      "title": "VGR：视觉基础推理 (原标题: VGR: Visual Grounded Reasoning)",
      "link": "https://arxiv.org/abs/2506.11991",
      "pubDate": "Fri, 13 Jun 2025 13:47:43 GMT",
      "isoDate": "2025-06-13T13:47:43.000Z",
      "creator": "Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, Jun Xiao",
      "summary": "## VGR：视觉基础推理模型\n\n### 现有挑战\n\n*   **局限性**：当前多模态思维链（CoT）推理方法主要依赖纯语言空间进行推理，这导致其固有地受到语言偏差的影响，并且很大程度上局限于数学或科学领域。\n*   **视觉推理不足**：这种狭隘的关注限制了它们处理需要全面理解图像细节的复杂视觉推理任务的能力。\n\n### VGR模型介绍\n\n*   **创新点**：本文引入了VGR（Visual Grounded Reasoning），这是一种新型的推理多模态大型语言模型（MLLM），具有增强的细粒度视觉感知能力。\n*   **工作机制**：\n    *   与传统MLLM仅在语言空间回答问题或推理不同，VGR首先检测可能有助于解决问题的相关区域。\n    *   然后，它基于“重放”（replayed）的图像区域提供精确的答案。\n*   **推理流程**：VGR的推理流程允许模型选择用于视觉参考的边界框，并引入了一个“重放”阶段，将相应的区域整合到推理过程中，从而增强了多模态理解。\n\n### VGR-SFT数据集\n\n*   **目的**：为了实现VGR的功能，研究团队构建了一个名为VGR-SFT的大规模监督微调（SFT）数据集。\n*   **内容**：该数据集包含混合了视觉基础（vision grounding）和语言推导（language deduction）的推理数据。\n\n### 实验结果\n\n*   **基线对比**：在LLaVA-NeXT-7B基线上进行的实验表明，VGR在需要全面图像细节理解的多模态基准测试中取得了卓越的性能。\n*   **效率与性能提升**：\n    *   与基线模型相比，VGR仅使用了30%的图像token数量。\n    *   在MMStar上得分提升了+4.1。\n    *   在AI2D上得分提升了+7.1。\n    *   在ChartQA上取得了+12.9的显著改进。",
      "shortSummary": "VGR是一种新型多模态大型语言模型，旨在解决现有CoT推理在复杂视觉任务中依赖纯语言和语言偏差的局限性。VGR通过检测并基于重放的图像区域进行推理，增强了细粒度视觉感知能力。它利用VGR-SFT数据集进行训练，并在LLaVA-NeXT-7B基线上表现出色，在MMStar、AI2D和ChartQA等基准测试中显著提升了性能，同时仅使用30%的图像token。",
      "translated_title": "VGR：视觉基础推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA."
    },
    {
      "title": "DeepResearch Bench：深度研究智能体的综合基准测试 (原标题: DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents)",
      "link": "https://arxiv.org/abs/2506.11763",
      "pubDate": "Fri, 13 Jun 2025 09:17:32 GMT",
      "isoDate": "2025-06-13T09:17:32.000Z",
      "creator": "Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao",
      "summary": "# DeepResearch Bench：深度研究智能体的综合基准测试\n\n## 深度研究智能体 (DRAs) 概述\n深度研究智能体 (Deep Research Agents, DRAs) 是一类基于大型语言模型 (LLM) 的重要智能体。它们能够自主地协调多步骤的网页探索、目标性信息检索以及高阶信息合成，从而将海量的在线信息转化为分析师级别的、富含引用的报告。这种能力将数小时的手动桌面研究压缩到短短几分钟内完成。\n\n## 当前挑战\n尽管DRAs潜力巨大，但目前仍缺乏一个系统性评估其能力的综合性基准测试。\n\n## DeepResearch Bench 介绍\n为了弥补这一空白，研究人员提出了 DeepResearch Bench。这是一个包含100项博士级别研究任务的基准测试，每项任务都由22个不同领域的领域专家精心设计。\n\n## 评估方法论\n评估DRAs的性能本质上是复杂且劳动密集型的。为此，研究人员提出了两种新颖的评估方法，这些方法与人类判断具有高度一致性：\n\n*   **基于引用的报告质量评估：** 第一种方法是基于引用的方法，采用自适应标准来评估生成研究报告的质量。\n*   **信息检索与收集能力评估：** 另一个框架旨在通过评估有效引用数量和整体引用准确性来评估DRA的信息检索和收集能力。\n\n## 开源贡献\nDeepResearch Bench 及其核心框架组件已开源，旨在加速实用型LLM智能体的开发。",
      "shortSummary": "DeepResearch Bench 旨在解决深度研究智能体（DRAs）缺乏综合评估基准的问题。该基准包含100项博士级研究任务，并引入两种新颖的评估方法：一种基于引用评估报告质量，另一种评估信息检索和引用准确性。DeepResearch Bench 已开源，旨在加速实用型LLM智能体的开发，将数小时手动研究压缩至几分钟。",
      "translated_title": "DeepResearch Bench：深度研究智能体的综合基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents."
    },
    {
      "title": "通过注意力头选择实现细粒度扰动引导 (原标题: Fine-Grained Perturbation Guidance via Attention Head Selection)",
      "link": "https://arxiv.org/abs/2506.10978",
      "pubDate": "Thu, 12 Jun 2025 13:59:51 GMT",
      "isoDate": "2025-06-12T13:59:51.000Z",
      "creator": "Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Minjae Kim, Jaewon Min, Wooseok Jang, Saungwu Lee, Sayak Paul, Susung Hong, Seungryong Kim",
      "summary": "本文深入探讨了扩散模型中注意力扰动引导的细粒度控制，并提出了名为“HeadHunter”的系统框架，以实现对生成质量和视觉属性的精细控制。\n\n*   **背景与问题：**\n    *   扩散模型中的引导方法通过扰动模型来构建隐式弱模型，从而引导生成。\n    *   注意力扰动在无条件生成场景（不适用无分类器引导）中表现出强大的经验性能。\n    *   然而，现有注意力扰动方法缺乏确定扰动应用位置的原则性方法，尤其是在Diffusion Transformer (DiT) 架构中，因为质量相关的计算分布在不同层中。\n\n*   **核心发现：**\n    *   研究发现，注意力扰动的粒度可以从层级细化到单个注意力头。\n    *   特定的注意力头控制着不同的视觉概念，例如结构、风格和纹理质量。\n\n*   **提出的方法：**\n    *   **HeadHunter框架：** 基于上述发现，本文提出了“HeadHunter”，这是一个系统框架，用于迭代选择与用户中心目标对齐的注意力头。这使得对生成质量和视觉属性的精细控制成为可能。\n    *   **SoftPAG：** 引入SoftPAG，它将每个选定注意力头的注意力图线性插值到单位矩阵。这提供了一个连续的旋钮来调整扰动强度并抑制伪影。\n\n*   **主要优势与贡献：**\n    *   **缓解过平滑问题：** 该方法不仅缓解了现有层级扰动导致的过平滑问题。\n    *   **实现目标化风格操作：** 通过组合式注意力头选择，能够实现对特定视觉风格的目标化操作。\n    *   **验证：** 在现代大型DiT文本到图像模型（包括Stable Diffusion 3和FLUX.1）上进行了验证，在通用质量增强和风格特定引导方面均表现出卓越性能。\n    *   **首次头级分析：** 本文首次对扩散模型中的注意力扰动进行了头级分析，揭示了注意力层内可解释的专业化分工，并为设计有效的扰动策略提供了实用指导。",
      "shortSummary": "本文提出“HeadHunter”框架，通过选择特定注意力头实现扩散模型中细粒度扰动引导。研究发现不同注意力头控制结构、风格等视觉概念。结合SoftPAG，该方法能有效缓解过平滑，并对生成质量和视觉风格进行精确控制。这是首次对扩散模型注意力扰动进行头级分析，并在Stable Diffusion 3和FLUX.1等模型上验证了其卓越性能。",
      "translated_title": "通过注意力头选择实现细粒度扰动引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent guidance methods in diffusion models steer reverse sampling by perturbing the model to construct an implicit weak model and guide generation away from it. Among these approaches, attention perturbation has demonstrated strong empirical performance in unconditional scenarios where classifier-free guidance is not applicable. However, existing attention perturbation methods lack principled approaches for determining where perturbations should be applied, particularly in Diffusion Transformer (DiT) architectures where quality-relevant computations are distributed across layers. In this paper, we investigate the granularity of attention perturbations, ranging from the layer level down to individual attention heads, and discover that specific heads govern distinct visual concepts such as structure, style, and texture quality. Building on this insight, we propose \"HeadHunter\", a systematic framework for iteratively selecting attention heads that align with user-centric objectives, enabling fine-grained control over generation quality and visual attributes. In addition, we introduce SoftPAG, which linearly interpolates each selected head's attention map toward an identity matrix, providing a continuous knob to tune perturbation strength and suppress artifacts. Our approach not only mitigates the oversmoothing issues of existing layer-level perturbation but also enables targeted manipulation of specific visual styles through compositional head selection. We validate our method on modern large-scale DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1, demonstrating superior performance in both general quality enhancement and style-specific guidance. Our work provides the first head-level analysis of attention perturbation in diffusion models, uncovering interpretable specialization within attention layers and enabling practical design of effective perturbation strategies."
    },
    {
      "title": "AutoMind：用于自动化数据科学的自适应知识型智能体 (原标题: AutoMind: Adaptive Knowledgeable Agent for Automated Data Science)",
      "link": "https://arxiv.org/abs/2506.10974",
      "pubDate": "Thu, 12 Jun 2025 13:59:32 GMT",
      "isoDate": "2025-06-12T13:59:32.000Z",
      "creator": "Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, Ningyu Zhang",
      "summary": "# AutoMind：用于自动化数据科学的自适应知识型智能体\n\n## 摘要\n\n大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力，有望自动化整个机器学习流程。然而，现有框架存在局限性，其依赖于僵化、预定义的工作流和不灵活的编码策略，导致它们仅在相对简单、经典的问题上表现出色，而无法捕捉人类实践者在处理复杂、创新任务时所具备的经验知识。\n\n## AutoMind 框架介绍\n\n本文引入了 **AutoMind**，一个自适应、知识型的 LLM 智能体框架，旨在克服上述缺陷。AutoMind 通过以下三个关键进展实现了其目标：\n\n1.  **精选专家知识库：**\n    *   该知识库将智能体建立在领域专家知识的基础上，为智能体提供坚实的基础，使其能够理解和应用专业的经验和最佳实践。\n\n2.  **智能体知识型树搜索算法：**\n    *   此算法能够策略性地探索可能的解决方案，通过结构化的搜索过程，智能体可以更有效地发现和评估不同的方法，从而找到最优解。\n\n3.  **自适应编码策略：**\n    *   这种策略能够根据任务的复杂性动态调整代码生成。这意味着智能体可以根据具体需求灵活地生成定制化的代码，而不是遵循一成不变的模式，从而提高解决复杂问题的能力。\n\n## 性能评估\n\nAutoMind 在两个自动化数据科学基准测试上进行了评估，结果表明其性能优于现有最先进的基线方法。\n\n## 额外分析与结论\n\n进一步的分析证实了 AutoMind 在以下方面的优势：\n\n*   **有效性：** 解决方案的质量和准确性。\n*   **效率：** 解决问题所需的时间和资源。\n*   **定性解决方案质量：** 生成代码和方法的实用性和优越性。\n\n这些结果突出表明，AutoMind 是迈向完全自动化数据科学的有效且稳健的一步。",
      "shortSummary": "AutoMind 是一个自适应、知识型的LLM智能体框架，旨在克服现有数据科学自动化工具的局限性。它通过整合精选专家知识库、智能体知识型树搜索算法和自适应编码策略，解决了传统框架僵化、缺乏经验知识的问题。在基准测试中，AutoMind 表现出优越的性能、效率和解决方案质量，是实现完全自动化数据科学的重要进展。",
      "translated_title": "AutoMind：用于自动化数据科学的自适应知识型智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science."
    },
    {
      "title": "ChineseHarm-Bench：一个中文有害内容检测基准 (原标题: ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark)",
      "link": "https://arxiv.org/abs/2506.10960",
      "pubDate": "Thu, 12 Jun 2025 13:57:05 GMT",
      "isoDate": "2025-06-12T13:57:05.000Z",
      "creator": "Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng",
      "summary": "## ChineseHarm-Bench：一个中文有害内容检测基准\n\n### 引言\n\n大型语言模型（LLMs）在自动化有害内容检测任务中扮演着越来越重要的角色，它们能够协助内容审核员识别违规内容，从而提高内容审核的整体效率和准确性。然而，当前有害内容检测的现有资源主要集中在英文领域，中文数据集则相对稀缺且范围有限，这限制了LLMs在中文有害内容检测方面的应用和发展。\n\n### ChineseHarm-Bench：中文有害内容检测基准\n\n为了解决中文有害内容检测数据集稀缺的问题，本文提出了一个名为“ChineseHarm-Bench”的综合性、专业标注的中文内容有害性检测基准。该基准具有以下特点：\n\n*   **数据来源**：完全基于真实世界数据构建，确保了数据的实用性和代表性。\n*   **覆盖范围**：涵盖了六个具有代表性的有害内容类别，提供了广泛的检测范围。\n*   **标注质量**：经过专业标注，保证了数据集的准确性和可靠性。\n\n### 知识规则库\n\n在ChineseHarm-Bench的标注过程中，研究人员还额外产出了一个**知识规则库**。这个规则库提供了明确的专家知识，旨在辅助LLMs更有效地进行中文有害内容检测，弥补了模型在处理复杂中文语境时可能存在的知识空白。\n\n### 知识增强基线模型\n\n此外，文章还提出了一种**知识增强基线模型**。该模型创新性地整合了人工标注的知识规则和大型语言模型中蕴含的隐式知识。通过这种结合，即使是规模较小的模型也能够实现与当前最先进（state-of-the-art, SOTA）LLMs相媲美的性能，这为资源受限的环境下进行高效有害内容检测提供了新的可能性。\n\n### 资源可用性与项目状态\n\n*   **代码与数据**：相关代码和数据已公开可用，方便研究人员进行复现和进一步研究。\n*   **项目状态**：该工作目前仍在进行中（Work in progress）。\n*   **相关领域**：该研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、密码学与安全（cs.CR）、信息检索（cs.IR）和机器学习（cs.LG）等多个学科领域。",
      "shortSummary": "针对中文有害内容检测数据集稀缺的问题，本文提出了“ChineseHarm-Bench”——一个综合性、专业标注的中文有害内容检测基准。该基准基于真实世界数据构建，涵盖六个代表性类别。研究还产出了一个知识规则库，并提出了一种知识增强基线模型，该模型结合人工规则和LLM隐式知识，使小型模型也能达到SOTA LLM的性能。代码和数据已公开。",
      "translated_title": "ChineseHarm-Bench：一个中文有害内容检测基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench."
    },
    {
      "title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂 (原标题: SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks)",
      "link": "https://arxiv.org/abs/2506.10954",
      "pubDate": "Thu, 12 Jun 2025 13:54:17 GMT",
      "isoDate": "2025-06-12T13:54:17.000Z",
      "creator": "Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, Zibin Zheng",
      "summary": "## SWE-Factory：自动化GitHub问题解决数据集构建\n\n### 引言\n\n为大型语言模型（LLMs）构建用于GitHub问题解决任务的大规模数据集，对于训练和评估其软件工程能力至关重要。然而，传统的基准创建过程面临巨大挑战且劳动密集，尤其是在设置评估环境、测试结果评分和验证任务实例等阶段。\n\n### SWE-Factory 解决方案\n\n本文提出 **SWE-Factory**，一个旨在解决这些挑战的自动化流程。该流程集成了三个核心自动化组件：\n\n1.  **SWE-Builder：** 一个多智能体系统，用于自动化评估环境的构建。它包含四个专业智能体，以协作、迭代循环的方式工作，并利用环境内存池来提高效率。\n2.  **标准化、基于退出码的评分方法：** 这种方法消除了手动编写自定义解析器的需要，简化了评分过程。\n3.  **自动化 fail2pass 验证：** 利用可靠的退出码信号，实现任务实例的自动化验证。\n\n### 实验与结果\n\n研究团队对四种编程语言的671个问题进行了实验，结果表明 SWE-Factory 流程能够有效地构建有效的任务实例：\n\n*   使用 GPT-4.1-mini 时，SWE-Builder 能够构建269个有效实例，每个实例的成本为0.045美元。\n*   使用 Gemini-2.5-flash 时，其性能与GPT-4.1-mini相当，但成本更低，每个实例仅为0.024美元。\n\n此外，实验还验证了 SWE-Factory 各组件的准确性：\n\n*   基于退出码的评分方法与人工检查相比，达到了100%的准确率。\n*   自动化 fail2pass 验证的精确度为0.92，召回率为1.00。\n\n### 结论与展望\n\nSWE-Factory 有望加速大规模、高质量GitHub问题解决数据集的收集，从而促进LLM在软件工程领域的训练和评估。相关代码和数据集已发布。",
      "shortSummary": "SWE-Factory是一个自动化流程，旨在解决为大型语言模型构建GitHub问题解决训练数据和评估基准的挑战。它通过SWE-Builder自动化评估环境构建，采用基于退出码的标准化评分方法，并实现自动化fail2pass验证。实验证明，SWE-Factory能有效构建高质量任务实例，显著降低成本，并实现高准确率，从而加速大规模数据集的收集。",
      "translated_title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂",
      "images": [],
      "contentSource": "完整文章",
      "content": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory."
    }
  ],
  "lastUpdated": "2025-06-17T09:41:08.463Z"
}