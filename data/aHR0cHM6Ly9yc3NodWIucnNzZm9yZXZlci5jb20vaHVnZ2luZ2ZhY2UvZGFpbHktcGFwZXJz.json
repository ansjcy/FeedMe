{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "GLM-4.1V-Thinking：迈向通用多模态推理的可扩展强化学习 (原标题: GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.01006",
      "pubDate": "Tue, 01 Jul 2025 13:55:04 GMT",
      "isoDate": "2025-07-01T13:55:04.000Z",
      "creator": "Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianle Gong, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",
      "summary": "GLM-4.1V-Thinking 是一种旨在推进通用多模态推理的视觉-语言模型（VLM）。该报告分享了其以推理为中心的训练框架的关键发现。\n\n*   **模型开发与训练：**\n    *   首先，通过大规模预训练开发了一个具有巨大潜力的视觉基础模型，这为最终性能设定了上限。\n    *   随后，采用“课程采样强化学习”（Reinforcement Learning with Curriculum Sampling, RLCS）技术，充分释放了模型的潜力。RLCS 显著提升了模型在多种任务上的综合能力。\n\n*   **增强的推理能力：**\n    GLM-4.1V-Thinking 在广泛的任务中展现出全面的能力增强，包括：\n    *   科学、技术、工程和数学（STEM）问题解决\n    *   视频理解\n    *   内容识别\n    *   编码\n    *   接地（Grounding）\n    *   基于图形用户界面（GUI）的智能代理\n    *   长文档理解\n\n*   **开源与性能表现：**\n    *   为了促进该领域的研究，研究团队开源了 GLM-4.1V-9B-Thinking 模型，该模型在同等规模的模型中达到了最先进的性能。\n    *   在一项涵盖28个公共基准的全面评估中，GLM-4.1V-9B-Thinking 在几乎所有任务上都超越了 Qwen2.5-VL-7B。\n    *   与规模显著更大的 Qwen2.5-VL-72B 相比，GLM-4.1V-9B-Thinking 在18个基准测试中表现出可比甚至更优的性能。\n    *   值得注意的是，GLM-4.1V-9B-Thinking 在长文档理解和 STEM 推理等挑战性任务上，与 GPT-4o 等闭源模型相比，也展现出竞争或更优的性能，进一步凸显了其强大的能力。\n\n*   **资源可用性：**\n    模型的代码、模型文件和更多信息已在该报告中提及的链接上发布。",
      "shortSummary": "GLM-4.1V-Thinking 是一种新型视觉-语言模型，旨在提升通用多模态推理能力。其核心创新在于采用课程采样强化学习（RLCS）框架，显著增强了模型在STEM问题解决、视频理解、长文档理解等多样化任务上的表现。开源的GLM-4.1V-9B-Thinking在同等规模模型中达到SOTA，并在多项基准测试中超越Qwen2.5-VL-7B，甚至在某些方面媲美或优于更大的Qwen2.5-VL-72B和GPT-4o等闭源模型，展现出强大实力。",
      "translated_title": "GLM-4.1V-Thinking：迈向通用多模态推理的可扩展强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then unlocks the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding, among others. To facilitate research in this field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at https://github.com/THUDM/GLM-4.1V-Thinking."
    },
    {
      "title": "SciArena：一个用于科学文献任务中基础模型的开放评估平台 (原标题: SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks)",
      "link": "https://arxiv.org/abs/2507.01001",
      "pubDate": "Tue, 01 Jul 2025 13:51:59 GMT",
      "isoDate": "2025-07-01T13:51:59.000Z",
      "creator": "Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Charles McGrady, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan",
      "summary": "## SciArena：一个开放评估平台\n\n### 核心介绍\nSciArena是一个开放且协作的平台，旨在评估基础模型在科学文献任务中的表现。与传统的科学文献理解和综合基准测试不同，SciArena采用类似Chatbot Arena的评估方法，通过社区投票直接让研究界参与模型比较。该平台利用集体智慧，对需要基于文献的长篇响应的开放式科学任务进行社区驱动的模型性能评估。\n\n### 平台现状与数据\n*   **支持模型**：目前，SciArena支持23种开源和专有基础模型。\n*   **数据收集**：平台已从不同科学领域的可信研究人员那里收集了超过13,000张投票。\n\n### 数据分析与发现\n*   **问题多样性**：对已收集数据的分析证实，提交的问题具有多样性，并且与实际的文献需求高度吻合。\n*   **评估质量**：参与评估的研究人员表现出高度的自我一致性和注释者间一致性，这表明评估结果的可靠性。\n*   **模型排名**：平台根据模型排名排行榜讨论了评估结果和由此获得的见解。\n\n### 额外贡献：SciArena-Eval基准测试\n*   **目的**：为进一步促进构建基于模型的文献任务自动化评估系统的研究，SciArena发布了SciArena-Eval。这是一个基于平台收集到的偏好数据的元评估基准测试。\n*   **评估方法**：SciArena-Eval通过比较模型与人类投票的成对评估，来衡量模型判断答案质量的准确性。\n*   **挑战与需求**：通过该基准测试进行的实验突显了其固有的挑战性，并强调了开发更可靠的自动化评估方法的需求。",
      "shortSummary": "SciArena是一个开放协作平台，用于评估科学文献任务中的基础模型。它采用社区投票方式，已收集来自可信研究人员的13,000多张投票，评估23种模型。平台分析显示，问题多样且评估一致性高。为促进自动评估研究，SciArena还发布了SciArena-Eval基准测试，旨在衡量模型判断答案质量的准确性，并强调了开发更可靠自动评估方法的必要性。",
      "translated_title": "SciArena：一个用于科学文献任务中基础模型的开放评估平台",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods."
    },
    {
      "title": "超越令牌思考：从类脑智能到通用人工智能的认知基础及其社会影响 (原标题: Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact)",
      "link": "https://arxiv.org/abs/2507.00951",
      "pubDate": "Tue, 01 Jul 2025 12:52:25 GMT",
      "isoDate": "2025-07-01T12:52:25.000Z",
      "creator": "Rizwan Qureshi, Ranjan Sapkota, Abbas Shah, Amgad Muneer, Anas Zafar, Ashmal Vayani, Maged Shoman, Abdelrahman B. M. Eldaly, Kai Zhang, Ferhat Sadak, Shaina Raza, Xinqi Fan, Ravid Shwartz-Ziv, Hong Yan, Vinjia Jain, Aman Chadha, Manoj Karkee, Jia Wu, Philip Torr, Seyedali Mirjalili",
      "summary": "## 超越令牌思考：通用人工智能的认知基础与未来路径\n\n### 引言\n\n本文深入探讨了通用人工智能（AGI）的核心问题：机器能否真正像人类一样思考、推理和行动。尽管当前的大型模型，如GPT-4.5、DeepSeek、Claude 3.5 Sonnet、Phi-4和Grok 3，展现出多模态流畅性和部分推理能力，但它们仍受限于对令牌级预测的依赖以及缺乏具身能动性。\n\n### 跨学科综合视角\n\n为了推动AGI的发展，本文提供了一个跨学科的综合分析，涵盖了人工智能、认知神经科学、心理学、生成模型和基于代理的系统等多个领域。\n\n### 通用智能的架构与认知基础\n\n文章强调了构建通用智能的关键架构和认知基础，包括：\n\n*   **模块化推理：** 能够将复杂问题分解为可管理的部分并进行独立推理的能力。\n*   **持久记忆：** 能够长期存储和检索信息，以支持持续学习和决策。\n*   **多智能体协调：** 多个智能体之间协同工作以实现共同目标的能力。\n\n### 新兴框架与泛化策略\n\n*   **Agentic RAG框架：** 本文特别强调了Agentic RAG（检索增强生成）框架的兴起，该框架结合了检索、规划和动态工具使用，从而实现更具适应性的行为。\n*   **泛化策略：** 为了实现灵活、领域无关的智能，文章讨论了多种泛化策略，包括：\n    *   信息压缩\n    *   测试时适应\n    *   免训练方法\n\n### 视觉-语言模型（VLMs）的重新审视\n\n视觉-语言模型（VLMs）被重新审视为不仅仅是感知模块，更是具身理解和协作任务完成的演进接口。\n\n### 对“真正智能”的理解\n\n文章认为，真正的智能并非单纯来自规模的扩大，而是源于记忆与推理的深度整合。这是一种模块化、交互式和自我改进组件的协同作用，其中信息压缩是实现适应性行为的关键。\n\n### 弥合差距的途径\n\n通过借鉴神经符号系统、强化学习和认知支架等领域的最新进展，研究人员正在探索如何弥合统计学习与目标导向认知之间的鸿沟。\n\n### 未来挑战\n\n最后，文章指出了在实现AGI道路上所面临的关键科学、技术和伦理挑战。",
      "shortSummary": "本文探讨通用人工智能（AGI）的进展与挑战。当前模型虽能力增强，但受限于令牌预测和缺乏具身能动性。文章提出，AGI需超越令牌，整合模块化推理、持久记忆和多智能体协调。Agentic RAG框架和信息压缩是实现适应性行为的关键。真正的智能源于记忆与推理的整合，而非单纯规模。神经符号系统等技术正弥合统计学习与目标导向认知间的鸿沟。文章最后指出AGI面临的科学、技术和伦理挑战。",
      "translated_title": "超越令牌思考：从类脑智能到通用人工智能的认知基础及其社会影响",
      "images": [],
      "contentSource": "完整文章",
      "content": "Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI."
    },
    {
      "title": "数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性 (原标题: Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning)",
      "link": "https://arxiv.org/abs/2507.00432",
      "pubDate": "Tue, 01 Jul 2025 01:23:05 GMT",
      "isoDate": "2025-07-01T01:23:05.000Z",
      "creator": "Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, Xiang Yue",
      "summary": "# 数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性\n\n## 引言\n大型语言模型（LLMs）在数学推理基准测试（如MATH和AIME）上取得了显著进展，其表现迅速超越了人类水平。然而，随着数学排行榜的每周更新，一个关键问题浮出水面：这些进步是反映了更广泛的问题解决能力，还是仅仅是狭隘的过拟合？\n\n## 研究目的与方法\n为了回答这一问题，本研究评估了20多个开源的推理调优模型，涵盖了广泛的任务套件，包括数学、科学问答、智能体规划、编码和标准指令遵循。为了更严格地研究这一现象，研究人员对Qwen3-14B模型进行了受控实验，这些模型仅使用数学数据进行训练，但采用了不同的调优方法。此外，研究还通过潜在空间表示和token空间分布偏移分析来深入理解模型行为。\n\n## 主要发现\n*   **迁移性不足：** 令人惊讶的是，大多数在数学任务上取得成功的模型未能将其优势迁移到其他领域。这表明数学推理能力的提升可能并未带来普遍的泛化能力。\n*   **调优方法的影响：**\n    *   **强化学习（RL）调优的模型：** 表现出良好的跨领域泛化能力。\n    *   **监督微调（SFT）调优的模型：** 经常会遗忘其通用能力。\n*   **机制分析：**\n    *   **SFT的影响：** SFT会引起显著的表示（latent-space representation）和输出（token-space distribution）漂移。这意味着SFT可能会改变模型的内部结构和输出分布，使其偏离通用领域。\n    *   **RL的优势：** RL则能保留通用领域结构，这可能是其泛化能力更好的原因。\n\n## 结论与建议\n本研究结果表明，有必要重新思考标准的后训练（post-training）方法，特别是目前在推进推理模型时对SFT蒸馏数据的依赖。为了实现更广泛的通用能力提升，可能需要探索或优化其他调优策略，例如强化学习。",
      "shortSummary": "本研究探讨了大型语言模型（LLM）在数学推理上的进步是否能提升其通用能力。结果发现，大多数在数学上表现出色的模型未能将优势迁移到其他领域。受控实验表明，强化学习（RL）调优的模型泛化能力良好，而监督微调（SFT）调优的模型常遗忘通用能力。分析显示SFT导致模型表示和输出漂移，而RL则保留通用结构。研究建议重新思考依赖SFT蒸馏数据的后训练方法。",
      "translated_title": "数学推理能力是否能提升大型语言模型的通用能力？理解大型语言模型推理的迁移性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Math reasoning has become the poster child of progress in large language models (LLMs), with new models rapidly surpassing human-level performance on benchmarks like MATH and AIME. But as math leaderboards improve week by week, it is worth asking: do these gains reflect broader problem-solving ability or just narrow overfitting? To answer this question, we evaluate over 20 open-weight reasoning-tuned models across a broad suite of tasks, including math, scientific QA, agent planning, coding, and standard instruction-following. We surprisingly find that most models that succeed in math fail to transfer their gains to other domains. To rigorously study this phenomenon, we conduct controlled experiments on Qwen3-14B models using math-only data but different tuning methods. We find that reinforcement learning (RL)-tuned models generalize well across domains, while supervised fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space representation and token-space distribution shift analyses reveal that SFT induces substantial representation and output drift, while RL preserves general-domain structure. Our results suggest a need to rethink standard post-training recipes, particularly the reliance on SFT-distilled data for advancing reasoning models."
    },
    {
      "title": "FreeLong++：通过多波段频谱融合实现免训练长视频生成 (原标题: FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion)",
      "link": "https://arxiv.org/abs/2507.00162",
      "pubDate": "Mon, 30 Jun 2025 14:11:21 GMT",
      "isoDate": "2025-06-30T14:11:21.000Z",
      "creator": "Yu Lu, Yi Yang",
      "summary": "## FreeLong++：通过多波段频谱融合实现免训练长视频生成\n\n### 引言\n\n近年来，视频生成模型在从文本提示生成高质量短视频方面取得了显著进展。然而，将这些模型扩展到更长的视频仍然是一个重大挑战，主要问题在于时间一致性和视觉保真度的下降。初步观察表明，简单地将短视频生成模型应用于长序列会导致明显的质量下降。进一步分析发现，随着视频长度的增加，高频分量会变得越来越扭曲，我们将此问题称为“高频失真”。\n\n### FreeLong框架\n\n为了解决高频失真问题，我们提出了FreeLong，一个免训练的框架，旨在在去噪过程中平衡长视频特征的频率分布。FreeLong通过以下方式实现这一点：\n\n*   **融合全局低频特征**：这些特征捕捉整个视频的整体语义。\n*   **融合局部高频特征**：这些特征从短时间窗口中提取，以保留精细细节。\n\n### FreeLong++扩展\n\nFreeLong++在FreeLong双分支设计的基础上进行了扩展，形成了一个多分支架构，包含多个注意力分支，每个分支在不同的时间尺度上操作。通过安排从全局到局部的多种窗口大小，FreeLong++实现了从低频到高频的多波段频率融合，从而确保了长视频序列的语义连续性和精细的运动动态。\n\n### 主要优势与应用\n\nFreeLong++无需任何额外训练，即可即插即用于现有视频生成模型（例如Wan2.1和LTX-Video），以生成具有显著改善的时间一致性和视觉保真度的长视频。我们的方法在长视频生成任务（例如，原生长度的4倍和8倍）上表现优于现有方法。它还支持：\n\n*   **连贯的多提示视频生成**：具有平滑的场景过渡。\n*   **可控视频生成**：使用长深度或姿态序列进行控制。\n\n### 当前状态\n\n该研究目前正在评审中。",
      "shortSummary": "FreeLong++ 是一种免训练框架，旨在解决长视频生成中时间一致性和视觉保真度下降（特别是高频失真）的问题。它通过多波段频谱融合，结合全局低频特征与局部高频特征，平衡视频特征的频率分布。该方法可即插即用于现有视频生成模型，显著提升长视频的质量，并支持多提示和可控生成，表现优于现有方法。",
      "translated_title": "FreeLong++：通过多波段频谱融合实现免训练长视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences."
    },
    {
      "title": "MoCa：模态感知持续预训练提升双向多模态嵌入效果 (原标题: MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings)",
      "link": "https://arxiv.org/abs/2506.23115",
      "pubDate": "Sun, 29 Jun 2025 02:41:00 GMT",
      "isoDate": "2025-06-29T02:41:00.000Z",
      "creator": "Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, Zhicheng Dou",
      "summary": "## MoCa：提升双向多模态嵌入效果的新框架\n\n### 引言\n当前基于因果视觉语言模型（VLM）构建的多模态嵌入模型在多项任务中展现出潜力。然而，现有方法面临三大关键局限：\n\n*   **注意力机制不匹配**：VLM 主干中使用的因果注意力机制对于嵌入任务而言并非最优。\n*   **可扩展性问题**：对比学习过度依赖高质量的标注配对数据，导致可扩展性受限。\n*   **训练多样性不足**：训练目标和数据多样性有限。\n\n### MoCa 框架\n为解决上述问题，我们提出了 MoCa，一个两阶段框架，旨在将预训练的 VLM 转化为高效的双向多模态嵌入模型。\n\n#### 第一阶段：模态感知持续预训练 (Modality-aware Continual Pre-training)\n\n*   **目标**：引入一个联合重建目标。\n*   **机制**：同时对交错的文本和图像输入进行去噪处理。\n*   **效果**：显著增强了模型的双向上下文感知推理能力。\n\n#### 第二阶段：异构对比微调 (Heterogeneous Contrastive Fine-tuning)\n\n*   **数据利用**：利用多样化、语义丰富的多模态数据，超越了简单的图像-文本对。\n*   **效果**：进一步增强了模型的泛化能力和对齐效果。\n\n### MoCa 如何解决现有局限\nMoCa 通过以下方式有效解决了现有方法的局限性：\n\n*   **引入双向注意力**：通过持续预训练引入了双向注意力机制，优化了嵌入任务的性能。\n*   **提升可扩展性**：通过联合重建目标，能够有效利用大规模未标注数据集进行训练，解决了可扩展性问题。\n*   **增强表示鲁棒性**：利用多样化的多模态数据，提升了模型表示的鲁棒性。\n\n### 实验结果\n实验证明，MoCa 在 MMEB 和 ViDoRe-v2 基准测试中持续提升性能，取得了新的最先进（SOTA）结果。此外，MoCa 在 MMEB 上，无论是在模型规模还是训练数据量方面，都展现出强大的可扩展性。",
      "shortSummary": "MoCa是一个两阶段框架，旨在提升双向多模态嵌入效果。它通过模态感知持续预训练引入联合重建目标，增强双向推理能力；并通过异构对比微调利用多样化数据提升泛化和对齐。MoCa解决了现有模型因果注意力不足、可扩展性差和数据多样性有限等问题，并在MMEB和ViDoRe-v2基准测试中取得了最先进的性能和强大的可扩展性。",
      "translated_title": "MoCa：模态感知持续预训练提升双向多模态嵌入效果",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB."
    },
    {
      "title": "Peccavi：一种针对AI生成图像的视觉释义攻击安全且无失真的图像水印技术 (原标题: Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images)",
      "link": "https://arxiv.org/abs/2506.22960",
      "pubDate": "Sat, 28 Jun 2025 13:34:08 GMT",
      "isoDate": "2025-06-28T13:34:08.000Z",
      "creator": "Shreyas Dixit, Ashhar Aziz, Shashwat Bajpai, Vasu Sharma, Aman Chadha, Vinija Jain, Amitava Das",
      "summary": "### PECCAVI：应对AI生成内容挑战的新型水印技术\n\n随着人工智能生成内容（如文本、图像、视频和音频）的迅速增长，政策制定者对其可能被用于政治虚假信息传播的担忧日益加剧。欧盟执法机构预测，到2026年，高达90%的在线内容可能是合成生成的。为应对这一挑战，加利福尼亚州通过了AB 3211法案，强制要求对AI生成的图像、视频和音频进行水印处理。\n\n然而，现有的隐形水印技术面临着被篡改和绕过的风险，特别是新出现的“视觉释义攻击”（visual paraphrase attack）。这种攻击能够完全去除水印，同时生成原始图像的释义版本，即在改变图像的同时保留其核心语义区域，这些区域被称为“非熔点”（Non-Melting Points, NMPs）。\n\n**PECCAVI技术介绍：**\n\n本文提出了一种名为PECCAVI的新型图像水印技术，它是首个能够抵御视觉释义攻击且无失真的水印方案。PECCAVI的主要特点和创新点包括：\n\n*   **视觉释义攻击安全**：PECCAVI通过策略性地将水印嵌入到图像的非熔点（NMPs）中，从而有效抵御视觉释义攻击。NMPs是图像中即使在视觉释义攻击下也能保持语义不变的核心区域。\n*   **无失真**：该技术旨在确保水印的嵌入不会对原始图像造成可见的失真。\n*   **多通道频域水印**：PECCAVI采用多通道频域水印技术，增强了水印的鲁棒性。\n*   **噪声烧蚀（Noisy Burnishing）**：为对抗旨在定位NMPs以破坏嵌入水印的逆向工程尝试，PECCAVI引入了噪声烧蚀机制，进一步提升了水印的耐久性。\n*   **模型无关性**：PECCAVI是一种模型无关的技术，意味着它不依赖于特定的AI生成模型。\n*   **开源资源**：所有相关资源和代码都将开源，以促进研究和应用。\n\nPECCAVI的提出为AI生成内容的真实性验证和虚假信息治理提供了新的解决方案，旨在确保AI生成内容的来源可追溯性和完整性。",
      "shortSummary": "随着AI生成内容激增，虚假信息传播风险加剧，现有水印技术易受“视觉释义攻击”影响。本文提出PECCAVI，一种新型无失真图像水印技术，能有效抵御视觉释义攻击。PECCAVI通过将水印嵌入图像的“非熔点”（NMPs），并结合多通道频域水印和噪声烧蚀，增强了水印的安全性与耐久性。该技术模型无关，旨在应对AI内容真实性挑战，所有资源将开源。",
      "translated_title": "Peccavi：一种针对AI生成图像的视觉释义攻击安全且无失真的图像水印技术",
      "images": [],
      "contentSource": "完整文章",
      "content": "A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that \"Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality.\" In response, California's Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced."
    },
    {
      "title": "全身条件下的第一人称视频预测 (原标题: Whole-Body Conditioned Egocentric Video Prediction)",
      "link": "https://arxiv.org/abs/2506.21552",
      "pubDate": "Thu, 26 Jun 2025 13:59:59 GMT",
      "isoDate": "2025-06-26T13:59:59.000Z",
      "creator": "Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik",
      "summary": "## 全身条件下的第一人称视频预测 (PEVA)\n\n这项研究介绍了一种名为PEVA（Predict Ego-centric Video from human Actions）的模型，旨在根据过去的视频和由相对3D身体姿态表示的人体动作来预测第一人称视角视频。\n\n### 核心目标与方法\n*   **目标**：训练模型，使其能够模拟人类的物理动作如何从第一人称视角塑造环境。\n*   **输入条件**：模型以运动学姿态轨迹为条件，这些轨迹通过身体的关节层级结构化，从而捕捉动作的细节。\n*   **模型架构**：采用了一种自回归条件扩散Transformer模型进行训练。\n\n### 数据集\n*   **名称**：Nymeria\n*   **特点**：这是一个大规模的真实世界数据集，包含了第一人称视角视频和相应的身体姿态捕捉数据，为模型的训练提供了丰富的素材。\n\n### 评估协议\n*   **设计**：研究团队设计了一个分层的评估协议。\n*   **目的**：通过设置挑战性逐渐增加的任务，对模型的具身预测和控制能力进行全面而深入的分析。\n\n### 研究意义\n*   这项工作代表了在从人类视角建模复杂真实世界环境和具身智能体行为的视频预测领域，迈出了初步但重要的尝试。",
      "shortSummary": "这项研究提出了PEVA模型，利用自回归条件扩散Transformer，根据过去的视频和相对3D身体姿态预测第一人称视角视频。模型旨在模拟人类物理动作如何从第一人称视角塑造环境。它在大型真实数据集Nymeria上训练，并通过分层协议评估其具身预测和控制能力。这是首次尝试从人类视角对复杂真实世界环境和具身智能体行为进行视频预测。",
      "translated_title": "全身条件下的第一人称视频预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human."
    },
    {
      "title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程 (原标题: Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test)",
      "link": "https://arxiv.org/abs/2506.21551",
      "pubDate": "Thu, 26 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-26T13:59:58.000Z",
      "creator": "Ziyue Li, Chenrui Fan, Tianyi Zhou",
      "summary": "### Grokking现象在大型语言模型预训练中的发现与机制\n\n**1. 研究背景与Grokking现象**\n*   **Grokking定义：** 指的是神经网络训练中，测试性能在训练损失收敛后仍持续提升的现象。这一现象使得泛化能力和推理等新兴能力的机制变得神秘。\n*   **以往研究局限：** 多数以往研究集中于在少量玩具任务或高度特定任务上训练小型模型，且通常需要数千个epoch。\n\n**2. 本研究的创新点与发现**\n*   **首次大规模研究：** 本研究首次在7B大型语言模型（LLM）OLMoE的单次预训练检查点上，对Grokking现象进行了研究。这与以往小型模型的研究形成对比。\n*   **验证Grokking存在：** 研究首次证实，Grokking现象确实存在于大规模基础模型的预训练中，尽管不同数据可能异步进入Grokking阶段。\n*   **评估方法：** 在研究中，计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索等多样化基准任务上评估了模型的泛化能力。\n\n**3. Grokking的机制解释：记忆到泛化的转变**\n*   **内部动态调查：** 通过深入调查LLM的内部动态，本研究揭示了Grokking中“泛化出现”的内在机制。\n*   **通路演变：** 训练样本的通路（即跨层专家选择）在Grokking过程中，从随机、实例特定演变为更结构化且样本间可共享。\n*   **通路复杂性降低：** 尽管训练损失已经收敛，但样本通路的复杂性却降低了。\n*   **核心结论：** 这些发现共同表明了一个从“记忆”到“泛化”的转换过程，为延迟泛化提供了机械性的解释。\n\n**4. 实用工具与理论贡献**\n*   **新型度量指标：** 本研究开发了两个新颖的度量指标，用于量化通路距离和单个通路的复杂性。\n*   **预测泛化能力：** 这些指标被证明能够有效预测模型在各种下游任务上的泛化改进。\n*   **高效实用：** 这些指标计算高效、简单，并且仅依赖于训练数据，无需额外的测试集或微调。\n*   **实际价值：** 它们对于预训练过程具有重要的实际价值，使得在不进行微调和测试的情况下，也能有效监控模型的泛化性能。\n*   **理论支持：** 理论上，研究表明更结构化的通路能够降低模型复杂性，并改善泛化界限。",
      "shortSummary": "本研究首次在7B大型语言模型（OLMoE）的预训练中证实了Grokking现象的存在。研究发现，Grokking过程中LLM内部的训练样本通路从随机演变为结构化且可共享，通路复杂性降低，这表明了从“记忆”到“泛化”的转变。为监控这一过程，研究开发了两个新颖的度量指标，它们仅依赖训练数据，能有效预测泛化改进，从而无需微调和测试即可监控模型泛化性能，具有重要的实际和理论价值。",
      "translated_title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's \"emergence of generalization\" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound."
    },
    {
      "title": "SAM4D：在相机和激光雷达流中分割一切 (原标题: SAM4D: Segment Anything in Camera and LiDAR Streams)",
      "link": "https://arxiv.org/abs/2506.21547",
      "pubDate": "Thu, 26 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-26T13:59:14.000Z",
      "creator": "Jianyun Xu, Song Wang, Ziqian Ni, Chunyong Hu, Sheng Yang, Jianke Zhu, Qiang Li",
      "summary": "### SAM4D：多模态与时序基础模型\n\nSAM4D是一个为相机和激光雷达流设计的可提示分割的多模态和时序基础模型。它旨在解决自动驾驶场景中动态变化环境下的鲁棒分割问题，并克服传统标注瓶颈。\n\n**核心创新与技术：**\n\n*   **统一多模态位置编码 (UMPE)**\n    *   功能：将相机和激光雷达特征对齐到共享的3D空间中。\n    *   目的：实现无缝的跨模态提示和交互。\n*   **运动感知跨模态记忆注意力 (MCMA)**\n    *   方法：利用自我运动补偿。\n    *   目的：增强时间一致性，实现长距离特征检索，确保在动态变化的自动驾驶场景中进行鲁棒分割。\n*   **多模态自动化数据引擎**\n    *   目的：避免标注瓶颈。\n    *   工作原理：\n        *   协同VFM（Video Foundation Model）驱动的视频masklets。\n        *   结合时空4D重建。\n        *   进行跨模态masklet融合。\n    *   成果：生成相机-激光雷达对齐的伪标签，其速度比人工标注快几个数量级，同时保留了VFM在点云表示中导出的语义保真度。\n\n**实验与成果：**\n\n*   在构建的Waymo-4DSeg数据集上进行了广泛实验。\n*   实验结果证明了SAM4D强大的跨模态分割能力。\n*   展示了其在数据标注方面的巨大潜力。\n\n**其他信息：**\n\n*   该研究已被ICCV2025接受。",
      "shortSummary": "SAM4D是一个多模态、时序基础模型，专为相机和激光雷达流中的可提示分割设计。它引入了统一多模态位置编码（UMPE）以对齐跨模态特征，并采用运动感知跨模态记忆注意力（MCMA）增强时间一致性。为解决标注瓶颈，SAM4D开发了一个自动化数据引擎，能高效生成高质量伪标签。在Waymo-4DSeg上的实验验证了其强大的跨模态分割能力和数据标注潜力。",
      "translated_title": "SAM4D：在相机和激光雷达流中分割一切",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D."
    },
    {
      "title": "语言模型训练中的数据效能 (原标题: Data Efficacy for Language Model Training)",
      "link": "https://arxiv.org/abs/2506.21545",
      "pubDate": "Thu, 26 Jun 2025 13:59:07 GMT",
      "isoDate": "2025-06-26T13:59:07.000Z",
      "creator": "Yalun Dai, Yangyu Huang, Xin Zhang, Wenshan Wu, Chong Li, Wenhui Lu, Shijie Cao, Li Dong, Scarlett Li",
      "summary": "# 语言模型训练中的数据效能\n\n本文提出了“数据效能”（Data Efficacy）的概念，旨在通过优化训练数据的组织方式来最大化语言模型（LM）的性能。这与现有研究主要关注的“数据效率”（Data Efficiency，通过选择最小或最优的数据子集来最大化性能）形成互补，后者主要依赖数据过滤、采样和选择等技术。数据效能作为一个相对未被充分探索的领域，强调了训练数据组织的重要性。\n\n## DELT范式：数据效能的通用框架\n\n为了在语言模型训练中考虑数据效能，本文引入了一个名为DELT（Data Efficacy for Language Model Training）的通用范式。DELT范式包含三个核心组成部分：\n\n*   **数据评分（Data Scoring）**：评估每个数据样本的价值或特性。\n*   **数据选择（Data Selection）**：根据特定标准从数据集中选择子集。\n*   **数据排序（Data Ordering）**：优化数据在训练过程中的呈现顺序。\n\n## 关键创新：LQS和FO\n\n在DELT范式中，本文设计了两个具体的实例来提升数据效能：\n\n*   **可学习性-质量评分（Learnability-Quality Scoring, LQS）**：\n    *   作为数据评分的一个新实例。\n    *   从梯度一致性的角度，同时考虑每个数据样本的“可学习性”（learnability）和“质量”（quality）。\n*   **折叠排序（Folding Ordering, FO）**：\n    *   作为数据排序的一个新颖实例。\n    *   旨在解决语言模型训练中常见的模型遗忘（model forgetting）和数据分布偏差（data distribution bias）等问题。\n\n## 实验验证与发现\n\n全面的实验验证了数据效能在语言模型训练中的有效性，并得出以下主要发现：\n\n1.  **性能提升**：所提出的DELT范式中的各种实例，在不增加数据规模和模型大小的情况下，都能不同程度地提升语言模型的性能。\n2.  **最佳组合**：在这些实例中，结合使用本文提出的LQS进行数据评分和Folding进行数据排序，能够实现最显著的性能提升。\n3.  **兼容性**：数据效能可以与数据效率相结合，通过应用数据选择技术共同实现性能优化。\n\n## 结论\n\n本文认为数据效能是语言模型训练中一个有前景的基础研究领域，其对优化模型性能具有重要意义。",
      "shortSummary": "本文提出“数据效能”概念，旨在通过优化训练数据组织提升语言模型性能，与“数据效率”互补。引入DELT范式，包含数据评分、选择和排序。主要贡献是：基于梯度一致性的可学习性-质量评分（LQS）和解决模型遗忘/分布偏差的折叠排序（FO）。实验证明，DELT实例能显著提升LM性能，特别是LQS与FO的结合效果最佳，且可与数据效率结合。数据效能是LM训练中一个有前景的基础领域。",
      "translated_title": "语言模型训练中的数据效能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Data is fundamental to the training of language models (LM). Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. Techniques such as data filtering, sampling, and selection play a crucial role in this area. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored. This work introduces a general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Among these components, we design Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. We also devise Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias. Comprehensive experiments validate the data efficacy in LM training, which demonstrates the following: Firstly, various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. Secondly, among these instances, the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. Lastly, data efficacy can be achieved together with data efficiency by applying data selection. Therefore, we believe that data efficacy is a promising foundational area in LM training."
    },
    {
      "title": "WorldVLA：迈向自回归动作世界模型 (原标题: WorldVLA: Towards Autoregressive Action World Model)",
      "link": "https://arxiv.org/abs/2506.21539",
      "pubDate": "Thu, 26 Jun 2025 13:55:40 GMT",
      "isoDate": "2025-06-26T13:55:40.000Z",
      "creator": "Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, Hao Chen",
      "summary": "## WorldVLA：统一动作与图像理解与生成的自回归世界模型\n\nWorldVLA是一种创新的自回归动作世界模型，旨在将动作与图像的理解和生成功能整合到一个单一框架中。该模型通过融合视觉-语言-动作（VLA）模型和世界模型，实现了对复杂环境的全面感知和交互能力。\n\n### 模型核心机制\n\n*   **世界模型（World Model）**：利用对动作和图像的理解来预测未来的图像。其主要目的是学习环境的底层物理规律，从而提升动作生成的质量和准确性。\n*   **动作模型（Action Model）**：根据图像观察生成后续动作。这不仅有助于视觉理解，反过来也支持世界模型的视觉生成能力。\n\n### 关键发现与优势\n\n*   **相互增强**：研究表明，WorldVLA的整体性能优于独立的动作模型和世界模型。这突出显示了世界模型与动作模型之间存在的相互促进和增强关系。\n*   **自回归动作生成挑战**：在自回归地生成一系列动作时，动作模型的性能会出现下降。这一现象归因于模型在动作预测方面的泛化能力有限，导致早期动作的错误会传播并累积到后续动作中。\n\n### 解决方案\n\n*   **注意力掩码策略（Attention Mask Strategy）**：为了解决自回归动作生成中的性能下降问题，研究者提出了一种注意力掩码策略。该策略在生成当前动作时，选择性地掩盖先前的动作信息。实验证明，这一策略显著提升了动作块生成任务的性能。",
      "shortSummary": "WorldVLA是一个统一动作与图像理解和生成的自回归世界模型。它将VLA模型和世界模型整合，通过世界模型预测未来图像以学习物理规律，并由动作模型生成动作以辅助视觉理解和生成。研究发现WorldVLA优于独立模型，展现了模型间的相互增强。针对自回归动作生成中性能下降的问题，WorldVLA提出了一种注意力掩码策略，通过选择性掩盖先前动作来显著提升动作块生成任务的性能。",
      "translated_title": "WorldVLA：迈向自回归动作世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task."
    },
    {
      "title": "MADrive：记忆增强的驾驶场景建模 (原标题: MADrive: Memory-Augmented Driving Scene Modeling)",
      "link": "https://arxiv.org/abs/2506.21520",
      "pubDate": "Thu, 26 Jun 2025 13:41:07 GMT",
      "isoDate": "2025-06-26T13:41:07.000Z",
      "creator": "Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, Dmitry Baranchuk",
      "summary": "## MADrive：记忆增强的驾驶场景建模\n\n### 引言\n\n近期，3D高斯泼溅技术在自动驾驶（AD）环境的场景重建方面取得了显著进展，实现了高度逼真的建模。然而，这些重建结果与原始观测数据紧密相关，难以支持对显著改变或全新驾驶场景进行真实感合成。\n\n### MADrive框架概述\n\n本文介绍了MADrive，一个记忆增强的重建框架。该框架旨在扩展现有场景重建方法的能力，其核心思想是通过从一个大型外部记忆库中检索视觉上相似的3D资产，来替换场景中观察到的车辆。\n\n### 关键组成部分\n\n1.  **MAD-Cars数据集**\n    *   MAD-Cars是一个精心策划的数据集，包含约7万个在野外捕获的360°汽车视频。\n2.  **检索模块**\n    *   该模块负责在记忆库中查找与目标车辆最相似的汽车实例。\n    *   接着，它从相应的视频中重建这些汽车的3D资产。\n    *   最后，通过方向对齐和重新打光（relighting）技术，将重建的3D资产无缝整合到目标场景中。\n\n### 实验结果与优势\n\nMADrive所替换的车辆提供了完整的、多视角的车辆表示。这使得该框架能够实现对大幅改变的配置进行真实感合成，其有效性已在实验中得到验证。\n\n### 相关信息\n\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2506.21520 [cs.CV]",
      "shortSummary": "MADrive是一个记忆增强的驾驶场景建模框架，旨在解决现有3D重建方法难以合成新颖或大幅改变场景的问题。它通过从大型MAD-Cars记忆库中检索并整合视觉相似的3D车辆资产，替换原始场景中的车辆。该方法能实现车辆的完整多视图表示和真实感合成，扩展了自动驾驶环境建模的能力。",
      "translated_title": "MADrive：记忆增强的驾驶场景建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/"
    },
    {
      "title": "Mind2Web 2：使用代理作为评判者评估代理式搜索 (原标题: Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge)",
      "link": "https://arxiv.org/abs/2506.21506",
      "pubDate": "Thu, 26 Jun 2025 13:32:50 GMT",
      "isoDate": "2025-06-26T13:32:50.000Z",
      "creator": "Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su",
      "summary": "## Mind2Web 2：代理式搜索评估的新基准与方法\n\n### 背景与挑战\n\n代理式搜索系统，例如深度研究系统，利用大型语言模型（LLMs）自主浏览网页、综合信息并提供带有引用的全面答案，这代表了用户与海量网络信息交互方式的重大转变。尽管代理式搜索有望提高效率并减轻认知负担，但其日益增长的复杂性和开放性已超越了现有评估基准和方法的能力。现有方法大多假设搜索范围短且答案静态，无法有效评估代理式搜索的动态和复杂性。\n\n### Mind2Web 2 基准\n\n为了应对这一挑战，本文引入了 **Mind2Web 2**，这是一个全新的评估基准，其特点如下：\n\n*   **任务数量与质量**：包含130个真实、高质量、长周期的任务。\n*   **任务要求**：这些任务需要实时网页浏览和广泛的信息综合。\n*   **构建投入**：耗费超过1,000小时的人工劳动构建而成。\n\n### 代理作为评判者（Agent-as-a-Judge）框架\n\n为了解决评估时变和复杂答案的难题，研究团队提出了一种新颖的 **代理作为评判者（Agent-as-a-Judge）框架**。该方法通过以下方式实现自动评估：\n\n*   **评判代理构建**：基于树状结构的评分标准设计，构建特定任务的评判代理。\n*   **评估内容**：自动评估答案的正确性以及来源归属。\n\n### 综合评估与发现\n\n研究团队对九个前沿的代理式搜索系统以及人类表现进行了全面评估，并进行了详细的错误分析，以期为未来的发展提供见解。评估结果显示：\n\n*   **最佳表现系统**：OpenAI Deep Research 系统。\n*   **性能对比**：该系统已能达到人类表现的50-70%。\n*   **效率优势**：同时，其耗时仅为人类的一半。\n*   **发展潜力**：这表明代理式搜索系统具有巨大的发展潜力。\n\n### 结论\n\n总而言之，Mind2Web 2 为下一代代理式搜索系统的开发和基准测试提供了坚实而严谨的基础。",
      "shortSummary": "Mind2Web 2引入了一个新的基准和“代理作为评判者”框架，以评估复杂的代理式搜索系统。该基准包含130个需实时浏览和信息综合的长周期任务。其“代理作为评判者”框架能自动评估答案正确性和来源归属。评估显示，OpenAI Deep Research等领先系统已能以一半时间达到人类表现的50-70%，展现了代理式搜索的巨大潜力。Mind2Web 2为未来代理式搜索系统的发展和基准测试奠定了基础。",
      "translated_title": "Mind2Web 2：使用代理作为评判者评估代理式搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems."
    },
    {
      "title": "HumanOmniV2：从理解到基于上下文的全模态推理 (原标题: HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context)",
      "link": "https://arxiv.org/abs/2506.21277",
      "pubDate": "Thu, 26 Jun 2025 10:01:03 GMT",
      "isoDate": "2025-06-26T10:01:03.000Z",
      "creator": "Qize Yang, Shimin Yao, Weixuan Chen, Shenghao Fu, Detao Bai, Jiaxing Zhao, Boyuan Sun, Bowen Yin, Xihan Wei, Jingren Zhou",
      "summary": "### HumanOmniV2：从理解到基于上下文的全模态推理\n\n本文探讨了多模态大型语言模型（LLMs）在理解和解释人类意图方面的关键能力，这需要详细而周密的推理。尽管强化学习（RL）在增强LLM推理能力方面展现了潜力，但将其应用于多模态数据和格式的挑战仍未得到充分解决。\n\n**现有问题识别：**\n研究人员在现有多模态推理模型中识别出两个主要问题：\n\n*   **全局上下文理解不足：** 当模型错误地解释多模态上下文时发生，导致答案不正确。\n*   **捷径问题：** 当模型忽视多模态输入中的关键线索，直接回答查询而未充分考虑多模态信息时发生。\n\n**解决方案与方法：**\n为解决上述问题，本文强调模型必须在清晰理解多模态输入中全局上下文的基础上进行推理。这种全局上下文理解能有效防止模型忽视关键的多模态线索，并确保推理过程的彻底性。\n\n为了确保多模态上下文信息的准确解释，研究人员实施了以下奖励机制：\n\n*   **上下文奖励：** 由大型语言模型（LLM）判断。\n*   **格式奖励。**\n*   **准确性奖励。**\n\n此外，为提升复杂推理能力，研究人员利用LLM评估**逻辑奖励**，以判断推理过程是否成功地将多模态信息与逻辑方法相结合。\n\n**新基准与性能：**\n\n*   **IntentBench：** 引入了一个新的推理全模态基准，旨在评估模型理解复杂人类意图和情感的能力。\n*   **性能表现：** 提出的方法在多个全模态基准测试中，相比其他开源全模态模型，展现出更先进的性能。",
      "shortSummary": "本文提出HumanOmniV2，旨在解决多模态大语言模型在推理中存在的全局上下文理解不足和捷径问题。为增强模型对人类意图的深度理解和推理能力，该方法强调基于全局上下文的推理，并引入LLM判断的上下文、格式、准确性及逻辑奖励。同时，推出了评估复杂人类意图和情感的新基准IntentBench。实验表明，HumanOmniV2在多个全模态基准测试中表现出先进性能。",
      "translated_title": "HumanOmniV2：从理解到基于上下文的全模态推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "With the rapid evolution of multimodal large language models, the capacity to deeply understand and interpret human intentions has emerged as a critical capability, which demands detailed and thoughtful reasoning. In recent studies, Reinforcement Learning (RL) has demonstrated potential in enhancing the reasoning capabilities of Large Language Models (LLMs). Nonetheless, the challenges associated with adapting RL to multimodal data and formats remain largely unaddressed. In this paper, we identify two issues in existing multimodal reasoning models: insufficient global context understanding and shortcut problems. Insufficient context understanding can happen when a model misinterprets multimodal context, resulting in incorrect answers. The shortcut problem occurs when the model overlooks crucial clues in multimodal inputs, directly addressing the query without considering the multimodal information. To tackle these issues, we emphasize the necessity for the model to reason with a clear understanding of the global context within multimodal inputs. This global context understanding can effectively prevent the model from overlooking key multimodal cues and ensure a thorough reasoning process. To ensure the accurate interpretation of multimodal context information, we implement a context reward judged by a large language model, alongside format and accuracy rewards. Additionally, to improve complex reasoning capability, we employ the LLM to assess the logical reward, determining whether the reasoning process successfully integrates multimodal information with logical methods. We also introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating models in understanding complex human intentions and emotions. Our proposed method demonstrates advanced performance across multiple omni-modal benchmarks compared to other open-source omni-modal models."
    },
    {
      "title": "FairyGen：从单一儿童手绘角色生成故事驱动的卡通视频 (原标题: FairyGen: Storied Cartoon Video from a Single Child-Drawn Character)",
      "link": "https://arxiv.org/abs/2506.21272",
      "pubDate": "Thu, 26 Jun 2025 09:58:16 GMT",
      "isoDate": "2025-06-26T09:58:16.000Z",
      "creator": "Jiayi Zheng, Xiaodong Cun",
      "summary": "## FairyGen：从儿童手绘角色生成故事驱动的卡通视频\n\nFairyGen是一个创新的自动化系统，旨在从单一的儿童手绘角色草图生成故事驱动的卡通视频，同时忠实地保留其独特的艺术风格。该系统超越了以往仅关注角色一致性和基本动作的叙事方法，通过解耦角色建模与风格化背景生成，并融入电影化的镜头设计，以支持更具表现力和连贯性的故事讲述。\n\n### 核心创新与特点\n\n*   **风格忠实性**：精确捕捉并保留儿童手绘角色的独特视觉风格，并将其应用于整个视频场景。\n*   **故事驱动**：生成具有叙事结构和自然动作的动画，支持个性化和引人入胜的故事动画。\n*   **电影化设计**：通过镜头设计模块增强视觉多样性和电影质量。\n\n### 系统工作流程\n\n1.  **故事板生成**：\n    *   接收单一角色草图作为输入。\n    *   利用多模态大语言模型（MLLM）生成结构化的故事板，其中包含镜头级别的描述，详细说明环境设置、角色动作和摄像机视角。\n\n2.  **视觉一致性与风格传播**：\n    *   引入一个“风格传播适配器”，用于捕捉角色的视觉风格。\n    *   将捕获的风格应用于背景生成，确保在合成风格一致的场景时，忠实地保留角色的完整视觉特征。\n\n3.  **电影化镜头设计**：\n    *   “镜头设计模块”根据故事板，通过帧裁剪和多视角合成，进一步提升视频的视觉多样性和电影质量。\n\n4.  **动画与动作生成**：\n    *   **3D代理重建**：重建角色的3D代理，以推导出符合物理规律的动作序列。\n    *   **模型微调**：利用这些动作序列对基于MMDiT的图像到视频扩散模型进行微调。\n    *   **两阶段动作定制适配器**：\n        *   **第一阶段**：从时间上无序的帧中学习外观特征，从而将身份（identity）与动作（motion）解耦。\n        *   **第二阶段**：在冻结身份权重的情况下，使用时间步移位策略（timestep-shift strategy）对时间动态进行建模。\n\n### 成果与潜力\n\n一旦训练完成，FairyGen能够直接渲染出与故事板对齐的、多样且连贯的视频场景。广泛的实验证明，该系统生成的动画在风格上忠实于原始手绘，叙事上结构清晰，动作自然流畅，突显了其在个性化和沉浸式故事动画领域的巨大潜力。\n\n### 可用性\n\n项目代码和页面将公开提供。",
      "shortSummary": "FairyGen是一个创新系统，能将单一儿童手绘角色转化为故事驱动的卡通视频，并忠实保留其艺术风格。它通过MLLM生成故事板，利用风格传播适配器确保视觉一致性，并通过电影化镜头设计增强视觉效果。系统通过重建角色3D代理和两阶段动作定制适配器生成自然动作，最终渲染出与故事板对齐的连贯视频，展现了在个性化故事动画方面的巨大潜力。",
      "translated_title": "FairyGen：从单一儿童手绘角色生成故事驱动的卡通视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen"
    },
    {
      "title": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架 (原标题: DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster)",
      "link": "https://arxiv.org/abs/2506.21263",
      "pubDate": "Thu, 26 Jun 2025 09:45:04 GMT",
      "isoDate": "2025-06-26T09:45:04.000Z",
      "creator": "Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich",
      "summary": "### DiLoCoX：一种用于去中心化集群的低通信大规模训练框架\n\n**1. 问题背景与挑战**\n\n*   **高通信需求：** 基础模型（特别是大型语言模型LLM）的分布式训练需要极高的通信带宽。\n*   **中心化依赖：** 这使得训练高度依赖于具有快速可靠互连的中心化集群。\n*   **核心挑战：** 如何在慢速网络上进行大规模模型（参数量超过1000亿）的训练，从而释放去中心化集群的潜力？\n\n**2. 提出的解决方案：DiLoCoX**\n\n*   DiLoCoX是一种创新的低通信大规模去中心化集群训练框架。\n*   其核心目标是解决在慢速网络环境下，对超大规模模型进行高效预训练的难题。\n\n**3. DiLoCoX的关键技术组成**\n\nDiLoCoX通过结合以下多种策略，显著提升了参数规模和模型预训练速度：\n\n*   **流水线并行（Pipeline Parallelism）：** 有效地将模型划分到不同的设备上，实现并行计算。\n*   **双优化器策略（Dual Optimizer Policy）：** 优化器策略的创新应用，以提高训练效率。\n*   **通信与本地训练的一步延迟重叠（One-Step-Delay Overlap of Communication and Local Training）：** 精心设计的通信与计算重叠机制，最大限度地减少空闲时间。\n*   **自适应梯度压缩方案（Adaptive Gradient Compression Scheme）：** 根据网络条件和模型状态动态调整梯度压缩，以减少通信量。\n\n**4. 理论分析与实验验证**\n\n*   **理论分析：** 通过对收敛性的理论分析，证明了“通信与本地训练的一步延迟重叠”以及“自适应梯度压缩方案”的有效性和益处。\n*   **经验验证：**\n    *   DiLoCoX成功通过1Gbps网络预训练了一个1070亿参数的基础模型。\n    *   与传统的AllReduce方法相比，DiLoCoX在分布式训练中实现了惊人的357倍速度提升。\n    *   在取得显著速度提升的同时，模型收敛性退化可忽略不计，保持了模型性能。\n\n**5. 创新与意义**\n\n*   据作者所知，DiLoCoX是首个成功应用于参数量超过1000亿模型的去中心化训练框架。\n*   这为在资源受限或网络条件不佳的环境下，进行超大规模模型训练开辟了新的可能性。",
      "shortSummary": "DiLoCoX是一种创新的低通信大规模训练框架，旨在解决在慢速网络上训练超千亿参数模型的挑战。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩。实验证明，DiLoCoX能在1Gbps网络上预训练1070亿参数模型，相较于AllReduce提速357倍，且收敛性退化可忽略。这是首个应用于超千亿参数模型的去中心化训练框架。",
      "translated_title": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters."
    },
    {
      "title": "学习跳过Transformer的中间层 (原标题: Learning to Skip the Middle Layers of Transformers)",
      "link": "https://arxiv.org/abs/2506.21103",
      "pubDate": "Thu, 26 Jun 2025 05:01:19 GMT",
      "isoDate": "2025-06-26T05:01:19.000Z",
      "creator": "Tim Lawson, Laurence Aitchison",
      "summary": "## 学习跳过Transformer的中间层：一种条件计算方法\n\n本文提出了一种新的Transformer架构，旨在通过动态跳过其中间层来提高计算效率。该方法基于对Transformer内部工作原理的洞察：\n\n### 背景与动机\n\n*   **条件计算**：是提高Transformer效率的常用策略。\n*   **现有方法**：通常针对单个模块（如专家混合层）或独立跳过层。\n*   **研究洞察**：可解释性研究表明，Transformer的**中间层表现出更大的冗余性**，而早期层则负责将信息聚合到token位置。\n\n### 提出的架构\n\n受上述洞察启发，作者提出了一种**从中间向外动态跳过可变数量层**的新型架构：\n\n*   **学习门控机制**：根据输入，该机制决定是否绕过对称的中心块范围。\n*   **门控注意力机制**：防止后续的token关注被跳过的token位置。\n*   **残差范数控制**：通过“三明治”（'sandwich'）或“逐层范数”（'perilayernorm'）方案来控制残差范数。\n*   **门控稀疏性**：通过自适应正则化损失来控制门控的稀疏性。\n\n### 目标与结果\n\n*   **预期目标**：旨在减少“更简单”token的计算需求，并可能促进出现多级表示层次结构。\n*   **实验结果**：在所研究的规模下，与层数较少的密集基线相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面**未能实现改进**。\n\n### 代码发布\n\n*   作者已在 [this https URL](https://this.https.url/) 发布了相关代码。",
      "shortSummary": "本文提出一种新的Transformer架构，通过学习门控机制动态跳过其中间层，以提高计算效率。该方法基于中间层冗余的洞察，并结合门控注意力机制。尽管旨在减少计算量，但在实验规模下，该方法在计算效率与性能权衡方面未能超越传统的密集基线。代码已开源。",
      "translated_title": "学习跳过Transformer的中间层",
      "images": [],
      "contentSource": "完整文章",
      "content": "Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle."
    },
    {
      "title": "PhysRig：用于真实可变形物体建模的可微分物理骨骼蒙皮与绑定框架 (原标题: PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling)",
      "link": "https://arxiv.org/abs/2506.20936",
      "pubDate": "Wed, 25 Jun 2025 21:58:09 GMT",
      "isoDate": "2025-06-25T21:58:09.000Z",
      "creator": "Hao Zhang, Haolan Xu, Chun Feng, Varun Jampani, Narendra Ahuja",
      "summary": "## PhysRig：可微分物理骨骼蒙皮与绑定框架\n\n### 引言与背景\n\n骨骼蒙皮（Skinning）和绑定（Rigging）是动画、可变形物体重建、动作迁移和4D生成等领域的关键组成部分。当前主流方法，如线性混合蒙皮（LBS），因其简单性和可微分性而被广泛采用。然而，LBS存在显著缺陷，包括：\n\n*   **体积损失（Volume Loss）**：在关节弯曲时，模型体积会不自然地缩小。\n*   **不自然变形（Unnatural Deformations）**：无法准确模拟复杂或弹性材料的形变。\n*   **无法模拟弹性材料**：对于软组织、毛发、象鼻和耳朵等柔性或脂肪组织，LBS难以实现真实的物理效果。\n\n### PhysRig框架概述\n\n为了克服LBS的局限性，本文提出了 **PhysRig**：一个可微分的、基于物理的骨骼蒙皮与绑定框架。PhysRig的核心思想是将刚性骨架嵌入到一个体积表示（例如四面体网格）中，并将其模拟为一个由动画骨架驱动的可变形软体结构。\n\n### 核心技术与方法\n\nPhysRig通过以下关键技术实现其功能：\n\n*   **连续介质力学**：该方法利用连续介质力学原理来模拟物体的变形行为。\n*   **欧拉背景网格离散化**：物体被离散化为嵌入在欧拉背景网格中的粒子。这种离散化方法确保了框架对于材料属性和骨骼运动都具有可微分性，这对于优化和学习至关重要。\n*   **材料原型（Material Prototypes）**：引入材料原型概念，显著减少了学习空间，同时保持了高表达能力，使得模型能够更高效地学习和表示不同材料的物理特性。\n\n### 性能评估与应用\n\n为了全面评估PhysRig框架的有效性，研究团队构建了一个综合的合成数据集。该数据集使用了来自Objaverse、The Amazing Animals Zoo和MixaMo的网格模型，涵盖了多样化的物体类别和运动模式。\n\n评估结果显示：\n\n*   **优于传统LBS**：PhysRig方法在生成更真实、更符合物理规律的结果方面，持续优于传统的基于LBS的方法。\n*   **物理真实性**：能够更好地模拟弹性材料和复杂形变，解决了LBS在体积保持和自然变形方面的不足。\n\n此外，研究还通过姿态迁移（Pose Transfer）任务展示了PhysRig框架的适用性，突显了其在可变形物体建模方面的多功能性。\n\n### 结论\n\nPhysRig为动画和可变形物体建模提供了一个创新的解决方案，通过结合物理模拟和可微分性，显著提升了动画的真实感和物理准确性。该研究已被ICCV 2025接受。",
      "shortSummary": "PhysRig是一个创新的可微分物理骨骼蒙皮与绑定框架，旨在克服传统线性混合蒙皮（LBS）在体积损失和弹性材料模拟方面的局限。它通过将骨架嵌入可变形的体积表示中，并利用连续介质力学和欧拉网格实现物理模拟和可微分性。PhysRig引入材料原型以提高效率和表达力。实验证明，该框架在生成更真实、物理更可信的动画效果方面优于LBS，并在姿态迁移任务中展现了其多功能性，已被ICCV 2025接受。",
      "translated_title": "PhysRig：用于真实可变形物体建模的可微分物理骨骼蒙皮与绑定框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: a differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh), which is simulated as a deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct a comprehensive synthetic dataset using meshes from Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling."
    },
    {
      "title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑 (原标题: FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing)",
      "link": "https://arxiv.org/abs/2506.20911",
      "pubDate": "Wed, 25 Jun 2025 20:33:43 GMT",
      "isoDate": "2025-06-25T20:33:43.000Z",
      "creator": "Advait Gupta, Rishie Raj, Dang Nguyen, Tianyi Zhou",
      "summary": "FaSTA^* 是一种成本效益高的神经符号代理，旨在解决复杂的多轮图像编辑任务，例如“检测图像中的长凳并将其重新着色为粉红色。同时，为了更清晰的视图，移除猫并将墙壁重新着色为黄色。”\n\n*   **核心方法论**\n    *   FaSTA^* 结合了大型语言模型（LLMs）的快速、高层次子任务规划能力，以及每项子任务的慢速、准确、工具使用和局部 A* 搜索，以找到成本效益高的工具路径（即一系列对 AI 工具的调用）。\n*   **子程序挖掘与重用**\n    *   为了节省在类似子任务上进行 A* 搜索的成本，FaSTA^* 通过 LLMs 对先前成功的工具路径进行归纳推理，持续提取和优化频繁使用的子程序。\n    *   这些子程序被作为新工具重用于未来的任务中，实现了自适应的快速-慢速规划。\n    *   在规划过程中，首先探索高层次的子程序；只有当这些子程序失败时，才会激活低层次的 A* 搜索。\n*   **“快速-慢速”规划机制**\n    *   可重用的符号子程序显著节省了在应用于类似图像的相同类型子任务上的探索成本。\n    *   FaSTA^* 遵循一种“快速-慢速”工具路径代理模式：\n        *   首先，LLMs 尝试进行快速的子任务规划，并为每个子任务选择基于规则的子程序，预计能覆盖大多数任务。\n        *   只有对于新颖和具有挑战性的子任务，才会触发慢速的 A* 搜索。\n*   **性能优势**\n    *   与近期图像编辑方法相比，FaSTA^* 在计算效率上显著更高，同时在成功率方面与最先进的基线保持竞争力。",
      "shortSummary": "FaSTA^* 是一种高效的神经符号代理，用于处理复杂的多轮图像编辑任务。它结合了LLM的快速高层规划和A*搜索的慢速精确工具使用。通过LLM挖掘并重用常用子程序，FaSTA^* 显著降低了探索成本，实现了自适应的“快速-慢速”规划。该方法在计算效率上远超现有技术，同时保持了与最先进方法相当的成功率。",
      "translated_title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as \"Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A^* search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A^* on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A^* search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A^* search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA^* is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate."
    }
  ],
  "lastUpdated": "2025-07-02T09:34:27.893Z"
}