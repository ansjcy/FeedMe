{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "TTT3R: 将3D重建视为测试时训练 (原标题: TTT3R: 3D Reconstruction as Test-Time Training)",
      "link": "https://arxiv.org/abs/2509.26645",
      "pubDate": "Tue, 30 Sep 2025 13:59:51 GMT",
      "isoDate": "2025-09-30T13:59:51.000Z",
      "creator": "Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen",
      "summary": "## TTT3R：测试时训练的3D重建\n\n### 摘要\n\n现代循环神经网络（RNNs）因其线性时间复杂度，已成为3D重建领域具有竞争力的架构。然而，当它们被应用于超出训练上下文长度的场景时，性能会显著下降，这暴露了其有限的长度泛化能力。\n\n### 核心方法：测试时训练（TTT3R）\n\n本研究从测试时训练（Test-Time Training, TTT）的角度重新审视了3D重建的基础模型，将其设计框架为一个在线学习问题。基于这一视角，该方法提出了TTT3R：\n\n*   **自适应学习率：** TTT3R利用记忆状态与传入观测值之间的对齐置信度，推导出一个闭式学习率，用于记忆更新。\n*   **平衡历史与适应：** 这个学习率旨在平衡保留历史信息和适应新观测值之间的需求。\n\n### 关键成果与优势\n\nTTT3R作为一种无需训练的干预措施，带来了显著的性能提升：\n\n*   **长度泛化能力显著提高：** 大幅改善了模型在处理长序列时的泛化能力，在全球姿态估计方面比基线模型实现了2倍的提升。\n*   **高效运行：** 能够在20 FPS（每秒帧数）的速度下运行。\n*   **资源占用低：** 仅需6 GB的GPU内存即可处理数千张图像。\n\n### 代码可用性\n\n相关代码已公开提供。",
      "shortSummary": "TTT3R提出了一种将3D重建基础模型视为测试时在线学习问题的方法。针对现代RNNs在超出训练上下文长度时泛化能力差的问题，TTT3R利用记忆状态与新观测之间的对齐置信度，推导出一个闭式学习率来更新记忆。这种无需训练的干预显著提高了长度泛化能力，在全球姿态估计上实现了2倍的提升，同时以20 FPS运行，仅需6 GB GPU内存即可处理数千张图像。",
      "translated_title": "TTT3R: 将3D重建视为测试时训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a 2times improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R"
    },
    {
      "title": "注意力作为指南针：推理模型中过程监督强化学习的有效探索 (原标题: Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models)",
      "link": "https://arxiv.org/abs/2509.26628",
      "pubDate": "Tue, 30 Sep 2025 13:58:34 GMT",
      "isoDate": "2025-09-30T13:58:34.000Z",
      "creator": "Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai",
      "summary": "# 注意力作为指南针：推理模型中过程监督强化学习的有效探索\n\n## 1. 研究背景与问题\n*   **强化学习（RL）** 在提升大型语言模型（LLMs）的推理能力方面取得了显著成功。\n*   **过程监督强化学习（PSRL）** 被认为是比基于结果的RL更有效的范式。\n*   **现有PSRL方法的局限性**：在探索效率方面存在不足，主要体现在分支位置选择和采样效率上。\n\n## 2. 提出的方法：AttnRL框架\n本文引入了一个名为 **AttnRL** 的新型PSRL框架，旨在解决现有方法的探索效率问题。\n\n### 2.1 核心思想与动机\n*   **初步观察**：研究发现，表现出高注意力分数的步骤与推理行为密切相关。\n*   **启发**：将注意力分数作为“指南针”，指导探索过程。\n\n### 2.2 主要组成部分\n*   **基于高注意力值的分支策略**：\n    *   提出从具有高注意力值的推理步骤位置进行分支，以提高探索的有效性。\n*   **自适应采样策略**：\n    *   该策略考虑了问题难度和历史批次大小。\n    *   目标是确保整个训练批次都能保持非零的优势值（advantage values），从而提高采样的有效性。\n*   **一步离策略训练流程**：\n    *   为进一步提升采样效率，设计了一种一步离策略（one-step off-policy）训练流程。\n\n## 3. 实验结果\n*   **实验基准**：在多个具有挑战性的数学推理基准上进行了广泛实验。\n*   **性能表现**：AttnRL方法在性能、采样效率和训练效率方面均持续优于现有方法。\n\n## 4. 作者与分类\n*   **作者**：Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai\n*   **主题**：机器学习 (cs.LG); 计算与语言 (cs.CL)",
      "shortSummary": "本文提出AttnRL框架，旨在解决过程监督强化学习（PSRL）在推理模型中探索效率低下的问题。AttnRL利用高注意力分数指导分支位置选择，并引入自适应采样策略和一步离策略训练流程，以提高采样和训练效率。在数学推理基准上的实验表明，AttnRL在性能、采样和训练效率方面均优于现有方法，为LLMs的推理能力提供了更有效的探索机制。",
      "translated_title": "注意力作为指南针：推理模型中过程监督强化学习的有效探索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency."
    },
    {
      "title": "未见先学：揭示大语言模型从语言预训练中获得的视觉先验知识 (原标题: Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training)",
      "link": "https://arxiv.org/abs/2509.26625",
      "pubDate": "Tue, 30 Sep 2025 13:57:44 GMT",
      "isoDate": "2025-09-30T13:57:44.000Z",
      "creator": "Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos",
      "summary": "### 引言\n尽管大语言模型（LLMs）仅通过文本进行训练，但它们却出人意料地发展出了丰富的视觉先验知识。这些先验知识使得LLMs能够以相对少量多模态数据，甚至在某些情况下从未见过图像的情况下，解锁并执行视觉任务。\n\n### 核心发现：视觉先验的构成与来源\n通过系统分析，研究揭示了视觉先验——即在语言预训练期间获得的关于视觉世界的隐式、涌现知识——由可分离的感知先验和推理先验组成，它们具有独特的扩展趋势和起源。\n\n*   **视觉推理先验：**\n    *   主要通过在以推理为中心的数据（例如代码、数学、学术论文）上进行预训练而发展。\n    *   其能力呈渐进式扩展。\n    *   这种从语言预训练中获得的推理先验是可迁移的，并普遍适用于视觉推理任务。\n\n*   **感知先验：**\n    *   更广泛地从通用语料库中涌现。\n    *   其感知能力对视觉编码器和视觉指令微调数据更为敏感。\n\n*   **描述视觉世界的文本：**\n    *   被证明至关重要，但其性能影响会迅速饱和。\n\n### 研究方法与验证\n基于这些洞察，研究提出了一种以数据为中心的预训练视觉感知LLMs的方法，并在1万亿（1T）token规模的预训练中进行了验证。研究结果基于以下大规模实验：\n\n*   **实验规模：** 超过100项受控实验，耗费了500,000 GPU小时。\n*   **涵盖流程：** 实验涵盖了多模态大语言模型（MLLM）构建的完整流程，包括LLM预训练、视觉对齐和有监督多模态微调。\n*   **多样性：** 实验跨越了五种模型规模、多种数据类别和混合方式，以及多种适应性设置。\n\n### 其他贡献\n*   除了主要发现外，研究还提出并调查了若干假设。\n*   引入了多级存在基准（Multi-Level Existence Bench, MLE-Bench）。\n\n### 结论与展望\n这项工作提供了一种有意地从语言预训练中培养视觉先验的新方法，为下一代多模态大语言模型的发展铺平了道路。",
      "shortSummary": "该研究揭示了大型语言模型（LLMs）即使仅通过文本训练也能获得丰富的视觉先验知识。这些先验由可分离的感知和推理能力组成。视觉推理先验主要来自推理数据（如代码、数学），具有可迁移性；感知先验则从广泛语料库中涌现。研究提出了一种数据中心方法来培养视觉感知LLMs，并通过大规模实验验证，为未来多模态LLMs的发展奠定基础。",
      "translated_title": "未见先学：揭示大语言模型从语言预训练中获得的视觉先验知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs."
    },
    {
      "title": "DA^2：任意方向的深度估计 (原标题: DA^2: Depth Anything in Any Direction)",
      "link": "https://arxiv.org/abs/2509.26618",
      "pubDate": "Tue, 30 Sep 2025 13:55:37 GMT",
      "isoDate": "2025-09-30T13:55:37.000Z",
      "creator": "Haodong Li, Wangguangdong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, Ying-Cong Chen, Chunchao Guo",
      "summary": "## DA^2：任意方向的深度估计\n\n### 引言\n\n全景图像（360°×180°）因其提供比透视图像更完整的视觉描述而备受关注，使得全景深度估计在3D视觉领域日益重要。然而，当前方法面临两大挑战：\n\n*   **数据稀缺性**：全景数据不足导致现有方法在零样本泛化能力上表现不佳，通常局限于特定领域设置。\n*   **球形畸变**：全景图像固有的球形畸变使得许多方法依赖于透视分割（如立方体贴图），从而导致效率低下。\n\n### DA^2 方法概述\n\n为解决这些挑战，研究人员提出了 **DA^2**（Depth Anything in Any Direction），这是一种准确、具有零样本泛化能力且完全端到端的全景深度估计器。\n\n### 关键创新点\n\n1.  **大规模全景数据生成引擎**\n    *   为了解决全景数据稀缺问题，DA^2 引入了一个数据整理引擎。\n    *   该引擎能够从透视图像生成高质量的全景深度数据。\n    *   通过此引擎，研究人员创建了约 **54.3 万对**全景 RGB-深度图像对，使总数据集规模达到约 **60.7 万对**。\n\n2.  **SphereViT 模型**\n    *   为进一步减轻全景图像固有的球形畸变，DA^2 提出了 **SphereViT**。\n    *   SphereViT 明确利用球坐标来强制执行全景图像特征中的球形几何一致性，从而显著提高了性能。\n\n### 性能与优势\n\n*   **最先进的性能 (SoTA)**：在多个数据集上的综合基准测试清晰地表明，DA^2 取得了最先进的性能。\n*   **卓越的零样本泛化能力**：\n    *   相较于最强的零样本基线，DA^2 在 AbsRel 指标上平均提升了 **38%**。\n    *   令人惊讶的是，DA^2 甚至超越了先前的域内方法，充分证明了其卓越的零样本泛化能力。\n*   **高效率**：作为一种端到端解决方案，DA^2 比基于融合的方法展现出更高的效率。\n\n### 资源发布\n\nDA^2 的代码和整理后的全景数据都将公开发布。该项目页面为：this https URL。\n\n### 背景\n\n这项工作主要在腾讯混元实习期间完成。",
      "shortSummary": "DA^2：任意方向的深度估计，旨在解决全景深度估计中数据稀缺和球形畸变问题。该方法通过数据整理引擎生成了约60.7万对全景RGB-深度数据集，并引入SphereViT模型利用球坐标增强几何一致性。DA^2在多个数据集上实现了最先进的性能，相较于零样本基线在AbsRel上提升38%，甚至超越了域内方法，展现出卓越的零样本泛化能力和更高的效率。代码和数据将公开发布。",
      "translated_title": "DA^2：任意方向的深度估计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Panorama has a full FoV (360^circtimes180^circ), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose DA^{2}: Depth Anything in Any Direction, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create sim543K panoramic RGB-depth pairs, bringing the total to sim607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data will be released. Project page: https://depth-any-in-any-dir.github.io/."
    },
    {
      "title": "DeepScientist：逐步推进前沿科学发现 (原标题: DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively)",
      "link": "https://arxiv.org/abs/2509.26603",
      "pubDate": "Tue, 30 Sep 2025 13:49:32 GMT",
      "isoDate": "2025-09-30T13:49:32.000Z",
      "creator": "Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, Yue Zhang",
      "summary": "### DeepScientist：一种自主科学发现系统\n\nDeepScientist 是一个旨在克服现有 AI 科学家系统局限性的新系统，它专注于进行目标导向的、完全自主的科学发现，以解决人类定义的紧迫挑战。\n\n**核心创新与方法论：**\n\n*   **目标导向的发现：** 与以往缺乏重点的 AI 科学家系统不同，DeepScientist 致力于产生具有科学价值的贡献。\n*   **贝叶斯优化框架：** 将科学发现形式化为一个贝叶斯优化问题。\n*   **分层评估过程：** 通过“假设、验证、分析”的循环来操作，智能地平衡新假设的探索与现有发现的利用。\n*   **累积发现记忆（Findings Memory）：** 利用这一机制，系统能够选择性地将最有前景的发现提升到更高保真度的验证级别。\n\n**主要成果与突破：**\n\n*   **资源消耗：** 系统运行消耗了超过 20,000 GPU 小时。\n*   **思想生成与验证：** 生成了大约 5,000 个独特的科学想法，并对其中约 1,100 个进行了实验验证。\n*   **超越人类 SOTA：** 在三个前沿 AI 任务上，DeepScientist 显著超越了人类设计的最新（SOTA）方法，性能提升分别为 183.7%、1.9% 和 7.9%。\n*   **里程碑式证据：** 这是首次大规模证据表明 AI 能够逐步实现超越人类 SOTA 的科学发现，产生了真正推动科学前沿的有价值成果。\n\n**未来展望：**\n\n*   为了促进对这一发现过程的进一步研究，所有实验日志和系统代码都将开源。",
      "shortSummary": "DeepScientist 是一个目标导向的 AI 系统，通过将科学发现形式化为贝叶斯优化问题，并采用“假设、验证、分析”的分层评估过程，实现完全自主的科学发现。它利用累积发现记忆平衡探索与利用。该系统消耗了超过 20,000 GPU 小时，生成并验证了数千个科学想法，最终在三个前沿 AI 任务上超越了人类设计的最新技术（SOTA），提升幅度达 183.7%、1.9% 和 7.9%。这是 AI 首次大规模证明能逐步超越人类 SOTA，推动科学前沿。",
      "translated_title": "DeepScientist：逐步推进前沿科学发现",
      "images": [],
      "contentSource": "完整文章",
      "content": "While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/."
    },
    {
      "title": "探测人工智能推理的临界点（CritPt）：一个前沿物理研究基准 (原标题: Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics Research Benchmark)",
      "link": "https://arxiv.org/abs/2509.26574",
      "pubDate": "Tue, 30 Sep 2025 13:34:03 GMT",
      "isoDate": "2025-09-30T13:34:03.000Z",
      "creator": "Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Yaïr Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng",
      "summary": "### 探测人工智能推理的临界点（CritPt）：一个前沿物理研究基准\n\n**引言：**\n\n本研究旨在评估大型语言模型（LLMs）在应对前沿物理研究中复杂、开放式挑战时的推理能力，并探讨物理学家期望LLMs提供何种协助。\n\n**CritPt基准的提出与设计：**\n\n*   **名称与目的：** 提出了CritPt（Complex Research using Integrated Thinking - Physics Test，意为“使用综合思维进行复杂研究——物理测试”，发音为“critical point”），这是首个专门为测试LLMs在未发表、研究级推理任务上表现而设计的基准。\n*   **覆盖领域：** CritPt广泛涵盖了现代物理学的多个研究领域，包括但不限于：\n    *   凝聚态物理\n    *   量子物理\n    *   原子、分子与光学物理\n    *   天体物理\n    *   高能物理\n    *   数学物理\n    *   统计物理\n    *   核物理\n    *   非线性动力学\n    *   流体力学\n    *   生物物理\n*   **任务构成：**\n    *   包含71个复合研究挑战，这些挑战旨在模拟入门级的完整研究项目。\n    *   为了提供更细致的洞察，这些复合挑战被进一步分解为190个更简单的检查点任务。\n*   **问题来源与质量控制：**\n    *   所有问题均由50多位活跃的物理研究人员根据他们各自的研究领域全新创建。\n    *   每个问题都经过人工精心策划，以确保其答案难以猜测，并且能够通过机器进行验证。\n*   **评估机制：**\n    *   采用高度定制化的自动化评分流程，该流程专门针对高级物理学中特有的输出格式进行了优化。\n\n**主要研究发现：**\n\n*   **LLMs的当前表现：** 研究发现，尽管目前最先进的LLMs在处理孤立的检查点任务时展现出了一定的早期潜力，但它们在可靠解决完整研究规模的挑战方面仍存在显著差距。\n*   **准确率数据：**\n    *   在基础模型中，GPT-5（高）的平均准确率仅为4.0%。\n    *   当LLMs配备编码工具时，其准确率适度提升至约10%。\n\n**研究意义与结论：**\n\n*   通过CritPt提供的真实且标准化的评估，本研究揭示了当前大型语言模型的能力与实际物理研究需求之间存在的巨大鸿沟。\n*   CritPt为指导未来科学严谨的AI工具开发奠定了坚实的基础，旨在弥合这一差距，推动AI在科学研究领域的应用。",
      "shortSummary": "CritPt是一个新颖的基准，旨在评估大型语言模型（LLMs）在前沿物理研究级推理任务上的能力。该基准包含71个复合挑战和190个检查点任务，由50多位物理研究人员创建，涵盖广泛的物理领域。研究发现，尽管LLMs在简单检查点上表现出潜力，但在解决完整研究挑战时仍远未达到要求，最佳基础模型准确率仅为4.0%，配备编码工具后也仅约10%。CritPt揭示了当前AI能力与物理研究需求之间的巨大差距，为未来AI工具的开发提供了指导。",
      "translated_title": "探测人工智能推理的临界点（CritPt）：一个前沿物理研究基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced \"critical point\"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular &amp; optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools."
    },
    {
      "title": "Stable Cinemetrics：专业视频生成的结构化分类和评估 (原标题: Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation)",
      "link": "https://arxiv.org/abs/2509.26555",
      "pubDate": "Tue, 30 Sep 2025 13:22:18 GMT",
      "isoDate": "2025-09-30T13:22:18.000Z",
      "creator": "Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani",
      "summary": "### Stable Cinemetrics：专业视频生成的结构化分类和评估\n\n本文介绍了 **Stable Cinemetrics**，这是一个旨在解决现有视频生成模型和基准无法捕捉专业视频生成复杂性和需求的问题的评估框架。\n\n#### 核心问题\n\n*   尽管视频生成技术取得了显著进展，但现有模型和评估标准未能充分反映专业视频制作的复杂性和具体要求。\n\n#### 解决方案：Stable Cinemetrics\n\n*   **结构化评估框架**：Stable Cinemetrics 是一个结构化的评估框架，它将电影制作控制形式化为四个相互独立、分层的分类法。\n*   **四大分类法**：\n    *   **Setup (设置)**\n    *   **Event (事件)**\n    *   **Lighting (灯光)**\n    *   **Camera (摄像机)**\n*   **细粒度控制节点**：这些分类法共同定义了76个基于行业实践的细粒度控制节点。\n\n#### 评估方法与实施\n\n*   **基准构建**：利用这些分类法，研究人员构建了一个与专业用例对齐的提示基准。\n*   **自动化评估流程**：开发了一个自动化流程，用于提示分类和问题生成，从而能够独立评估每个控制维度。\n*   **大规模人工研究**：\n    *   涵盖了10多个模型和20,000多个视频。\n    *   由80多名电影专业人士进行标注。\n*   **主要发现**：\n    *   粗粒度和细粒度分析均显示，即使是当前最强大的模型也存在显著差距，尤其是在**事件（Events）**和**摄像机（Camera）**相关的控制方面。\n*   **自动化评估器**：\n    *   为了实现可扩展的评估，研究人员训练了一个自动评估器。\n    *   这是一个与专家标注对齐的视觉-语言模型，其性能优于现有的零样本基线。\n\n#### 意义与贡献\n\n*   **首次将专业视频生成置于视频生成模型的背景下**：Stable Cinemetrics 是第一个将专业视频生成与视频生成模型相结合的方法。\n*   **引入以电影控制为中心的分类法**：它引入了以电影控制为核心的分类法。\n*   **支持结构化评估流程和详细分析**：通过结构化的评估流程和详细的分析，为未来的研究提供了指导。",
      "shortSummary": "Stable Cinemetrics 引入了一个结构化评估框架，旨在解决现有视频生成模型在专业电影制作控制方面的不足。该框架将电影制作控制分为四大分类（设置、事件、灯光、摄像机），包含76个细粒度控制节点。通过一项涵盖10+模型、20K视频和80+专业人士的大规模研究，发现当前模型在“事件”和“摄像机”控制方面存在显著差距。研究还训练了一个性能优越的自动化评估器，为未来专业视频生成研究提供了指导。",
      "translated_title": "Stable Cinemetrics：专业视频生成的结构化分类和评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research."
    },
    {
      "title": "语音推理能力评估：诊断模态引发的性能差距 (原标题: Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap)",
      "link": "https://arxiv.org/abs/2509.26542",
      "pubDate": "Tue, 30 Sep 2025 13:17:09 GMT",
      "isoDate": "2025-09-30T13:17:09.000Z",
      "creator": "Yueqian Lin, Zhengmian Hu, Qinsi Wang, Yudong Liu, Hengfan Zhang, Jayakumar Subramanian, Nikos Vlassis, Hai Helen Li, Yiran Chen",
      "summary": "## 语音推理能力评估基准：VERA\n\n### 引言\n本文介绍了**语音推理能力评估（VERA）**，这是一个专门为在实时对话约束下评估语音交互系统推理能力而设计的基准。\n\n### VERA的构成\n*   **情景数量与来源**：VERA包含2,931个语音原生（voice-native）的评估情景，这些情景均改编自已建立的文本基准。\n*   **轨道分类**：这些情景被组织成五个不同的评估轨道：\n    *   **数学 (Math)**\n    *   **网络 (Web)**\n    *   **科学 (Science)**\n    *   **长上下文 (Long-Context)**\n    *   **事实 (Factual)**\n*   **适应性**：每个评估项都经过精心调整，以适应语音交互模式，同时确保其固有的推理难度得以保留。\n\n### VERA的用途\n*   **模态比较**：VERA能够直接在同一模型家族内部进行文本与语音模态之间的性能比较。\n*   **架构分析**：它支持深入分析不同的架构选择如何影响系统的可靠性。\n\n### 评估结果：显著的模态差距\n研究评估了12个当代语音系统以及强大的文本基线模型，并观察到存在巨大且一致的模态性能差距：\n*   **竞赛数学任务**：在竞赛数学任务上，领先的文本模型准确率达到**74.8%**，而其语音对应模型的准确率仅为**6.1%**。\n*   **宏观平均表现**：在所有评估轨道上进行宏观平均后，最佳文本模型的准确率为**54.0%**，而最佳语音模型的准确率仅为**11.3%**。\n\n### 延迟-准确率分析\n*   分析揭示了一个被称为“**低延迟平台**”的现象。\n*   快速语音系统通常集中在约**10%**的准确率水平。\n*   要使语音系统接近文本模型的性能，往往需要牺牲实时交互能力。\n\n### 诊断实验与缓解措施\n*   **常见缓解措施的不足**：实验表明，常见的缓解措施不足以弥补模态差距。\n*   **增加“思考时间”**：增加系统的“思考时间”所带来的性能提升微乎其微。\n*   **解耦级联架构**：将推理与叙述分离的解耦级联（decoupled cascade）架构虽然能提高准确率，但仍远低于文本模型，并且会引入特有的接地（grounding）和一致性（consistency）错误。\n\n### 错误分析\n对原生流式（native streaming）、端到端（end-to-end）和级联（cascade）设计进行了错误分析，结果显示不同设计之间存在明显的错误特征。\n\n### 结论与贡献\nVERA提供了一个可复现的测试平台和针对性的诊断工具，特别适用于将“思考”与“说话”解耦的架构。它为衡量实时语音助手在流畅性和可靠推理能力方面取得的进展提供了一种有原则的方法。",
      "shortSummary": "本文介绍了VERA，一个评估语音交互系统推理能力的基准。研究发现，语音系统与文本系统之间存在巨大的模态性能差距：在竞赛数学任务上，文本模型准确率达74.8%，而语音模型仅为6.1%。宏观平均后，最佳文本模型准确率为54.0%，语音模型为11.3%。增加思考时间或采用解耦架构等缓解措施效果有限，且会引入新错误。VERA为诊断和提升实时语音助手的推理能力提供了工具。",
      "translated_title": "语音推理能力评估：诊断模态引发的性能差距",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing \"thinking time\" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned."
    },
    {
      "title": "Ferret-UI Lite：构建小型设备端GUI代理的经验教训 (原标题: Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents)",
      "link": "https://arxiv.org/abs/2509.26539",
      "pubDate": "Tue, 30 Sep 2025 13:13:56 GMT",
      "isoDate": "2025-09-30T13:13:56.000Z",
      "creator": "Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, Ram Ramrakhya, Chao Jia, Jeffrey Nichols, Alexander Toshev, Yinfei Yang, Zhe Gan",
      "summary": "### Ferret-UI Lite：构建小型设备端GUI代理的经验教训\n\n本文介绍了Ferret-UI Lite，一个紧凑、端到端的GUI（图形用户界面）代理，旨在解决小型设备端模型在有效交互GUI方面面临的挑战。\n\n**背景与挑战**\n\n开发能够与图形用户界面有效交互的自主代理是一个持续存在的难题，特别是对于资源受限的小型设备端模型而言。\n\n**Ferret-UI Lite 介绍**\n\n*   **模型特性**：Ferret-UI Lite是一个3B参数的紧凑型、端到端GUI代理。\n*   **跨平台能力**：它能够在多种平台上运行，包括移动设备、网页和桌面环境。\n*   **开发方法与优化**：\n    *   **数据混合**：通过整合来自真实和合成来源的多样化GUI数据混合物来训练模型。\n    *   **推理性能提升**：利用思维链推理（chain-of-thought reasoning）和视觉工具使用（visual tool-use）来增强推理时的性能。\n    *   **强化学习**：通过设计奖励机制，采用强化学习方法进行优化。\n\n**性能表现**\n\nFerret-UI Lite在与同类小型GUI代理的竞争中展现出有竞争力的性能。\n\n*   **GUI定位（GUI Grounding）**：\n    *   在ScreenSpot-V2基准测试中达到91.6%的得分。\n    *   在ScreenSpot-Pro基准测试中达到53.3%的得分。\n    *   在OSWorld-G基准测试中达到61.2%的得分。\n*   **GUI导航（GUI Navigation）**：\n    *   在AndroidWorld基准测试中达到28.0%的成功率。\n    *   在OSWorld基准测试中达到19.8%的成功率。\n\n**总结**\n\n本文分享了开发紧凑型设备端GUI代理的方法和经验教训，为该领域的研究提供了宝贵的见解。",
      "shortSummary": "Ferret-UI Lite是一个紧凑的3B参数设备端GUI代理，旨在解决小型模型与GUI交互的挑战。它通过多样化数据、思维链推理、视觉工具使用和强化学习进行构建，可在移动、网页和桌面平台运行。Ferret-UI Lite在GUI定位和导航任务上表现出竞争力，例如在ScreenSpot-V2上达到91.6%，在AndroidWorld上达到28.0%的成功率。本文分享了开发此类代理的经验教训。",
      "translated_title": "Ferret-UI Lite：构建小型设备端GUI代理的经验教训",
      "images": [],
      "contentSource": "完整文章",
      "content": "Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents."
    },
    {
      "title": "OceanGym：水下具身智能体的基准环境 (原标题: OceanGym: A Benchmark Environment for Underwater Embodied Agents)",
      "link": "https://arxiv.org/abs/2509.26536",
      "pubDate": "Tue, 30 Sep 2025 13:09:32 GMT",
      "isoDate": "2025-09-30T13:09:32.000Z",
      "creator": "Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen",
      "summary": "# OceanGym：水下具身智能体的基准环境\n\nOceanGym是首个为海洋水下具身智能体设计的综合性基准环境，旨在推动人工智能在最具挑战性的真实世界环境中的发展。与陆地或空中领域不同，水下环境带来了极端的感知和决策挑战，包括低能见度和动态洋流，使得智能体的有效部署异常困难。\n\n## OceanGym的特点与功能\n*   **任务领域：** OceanGym涵盖了八个真实的任务领域。\n*   **统一框架：** 它提供了一个由多模态大型语言模型（MLLMs）驱动的统一智能体框架。\n*   **核心能力：** 该框架整合了感知、记忆和序列决策能力。\n*   **智能体要求：** 智能体需要理解光学和声纳数据，自主探索复杂环境，并在恶劣条件下完成长期目标。\n\n## 实验结果与挑战\n广泛的实验揭示了当前最先进的MLLM驱动智能体与人类专家之间存在显著差距。这突出表明，在海洋水下环境中，感知、规划和适应性仍然是持续存在的难题。\n\n## OceanGym的意义\n*   **高保真平台：** OceanGym提供了一个高保真、严谨设计的平台。\n*   **测试平台：** 它建立了一个测试平台，用于开发鲁棒的具身AI。\n*   **实际应用：** 旨在将这些能力转移到真实的自主海洋水下航行器。\n*   **探索前沿：** 这标志着向能够在地球上最后未探索前沿运行的智能智能体迈出了决定性一步。\n\n## 资源可用性\n相关代码和数据已公开。",
      "shortSummary": "OceanGym是首个针对海洋水下具身智能体的综合基准环境，旨在应对水下低能见度和动态洋流等极端挑战。它包含八个任务领域和一个由多模态大型语言模型（MLLMs）驱动的统一框架，整合了感知、记忆和决策。实验表明，现有智能体与人类专家之间存在显著差距，凸显了水下感知、规划和适应性的困难。OceanGym为开发鲁棒的具身AI和实际水下航行器提供了测试平台，推动智能体在地球未探索前沿的运行。",
      "translated_title": "OceanGym：水下具身智能体的基准环境",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym."
    },
    {
      "title": "OffTopicEval：当大型语言模型几乎总是进入错误的聊天时！ (原标题: OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!)",
      "link": "https://arxiv.org/abs/2509.26495",
      "pubDate": "Tue, 30 Sep 2025 12:39:17 GMT",
      "isoDate": "2025-09-30T12:39:17.000Z",
      "creator": "Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria",
      "summary": "# OffTopicEval：大型语言模型操作安全性的评估与改进\n\n## 引言与背景\n大型语言模型（LLM）的广泛部署面临着严峻的安全挑战。除了常见的通用危害（如协助用户进行自我伤害或伤害他人）外，企业更关注LLM代理是否能安全地用于其预期用例。为此，本研究引入了“操作安全性”的概念，将其定义为LLM在被赋予特定任务时，恰当地接受或拒绝用户查询的能力。\n\n## OffTopicEval：评估套件与基准\n为了衡量LLM的通用操作安全性以及在特定代理用例中的操作安全性，研究提出了一个名为 **OffTopicEval** 的评估套件和基准。\n\n## 评估方法与发现\n研究对六个模型家族共20个开源LLM进行了评估。结果显示，尽管模型性能各异，但所有模型在操作安全性方面都表现出高度不安全。\n\n### 主要评估结果：\n*   **最强模型表现不佳**：即使是表现最好的模型，如Qwen-3 (235B) 达到77.77%和Mistral (24B) 达到79.96%，也远未达到可靠的操作安全性水平。\n*   **GPT模型表现中等**：GPT模型家族的得分在62%至73%之间。\n*   **Phi模型居中**：Phi模型的得分在48%至70%之间。\n*   **Gemma和Llama-3表现最差**：Gemma模型的得分降至39.53%，而Llama-3模型更是低至23.84%。\n\n这些结果突显了LLM在处理与特定任务无关的查询时，普遍存在“离题”响应或不恰当接受查询的问题。\n\n## 提示词引导方法：提升操作安全性\n鉴于操作安全性是模型对齐的核心问题，为抑制这些失败，研究提出了两种基于提示词的引导方法：\n1.  **查询接地 (Query Grounding, Q-ground)**：通过调整用户查询来引导模型。\n2.  **系统提示接地 (System-prompt Grounding, P-ground)**：通过调整系统提示来引导模型。\n\n### 引导方法的效果：\n*   **Q-ground**：提供了持续的性能提升，最高可达23%。\n*   **P-ground**：带来了更大的提升，例如将Llama-3.3 (70B) 的操作安全性提高了41%，将Qwen-3 (30B) 提高了27%。\n\n这些结果表明，基于提示词的引导方法是提高LLM操作安全性、使其更可靠的第一步。\n\n## 结论\n本研究强调了对LLM操作安全性进行干预的紧迫性，并展示了基于提示词的引导方法作为解决这一问题有前景的初步方案。未来的工作需要进一步探索更鲁棒的模型对齐技术，以确保LLM代理能够安全、可靠地执行其预期任务。",
      "shortSummary": "本研究引入“操作安全性”概念，定义为大型语言模型（LLM）在特定任务中恰当接受或拒绝查询的能力。通过OffTopicEval基准评估20个主流LLM，发现所有模型在操作安全性方面均表现不佳，即使最强模型也远未达可靠水平。为解决此问题，提出了查询接地（Q-ground）和系统提示接地（P-ground）两种基于提示词的引导方法，显著提升了模型的离题拒绝能力，最高可达41%。研究强调了LLM操作安全性干预的紧迫性及提示词引导方法的潜力。",
      "translated_title": "OffTopicEval：当大型语言模型几乎总是进入错误的聊天时！",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma and Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents."
    },
    {
      "title": "VitaBench：使用真实世界应用中的多功能交互任务对大型语言模型代理进行基准测试 (原标题: VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications)",
      "link": "https://arxiv.org/abs/2509.26490",
      "pubDate": "Tue, 30 Sep 2025 12:33:49 GMT",
      "isoDate": "2025-09-30T12:33:49.000Z",
      "creator": "Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, Yunke Zhao",
      "summary": "## VitaBench：大型语言模型代理的基准测试\n\n### 引言\n\n随着基于大型语言模型（LLM）的代理在现实生活中日益广泛地部署，现有基准测试在捕捉其处理大量信息、利用多样化资源和管理动态用户交互的固有复杂性方面显得不足。为了弥补这一空白，研究人员引入了VitaBench。\n\n### VitaBench 概述\n\nVitaBench 是一个具有挑战性的基准测试，旨在评估代理在基于真实世界场景的多功能交互任务中的表现。它从日常应用中汲取灵感，包括：\n\n*   **外卖服务**\n*   **店内消费**\n*   **在线旅行服务**\n\n### 模拟环境与工具\n\nVitaBench 提供了迄今为止最复杂的生命服务模拟环境，包含 **66 种工具**。通过一个消除了领域特定策略的框架，VitaBench 能够灵活地组合这些场景和工具，从而生成多样化的任务。\n\n### 任务类型与要求\n\nVitaBench 包含：\n\n*   **100 个跨场景任务**（主要结果）\n*   **300 个单场景任务**\n\n每个任务都源自多个真实用户请求，并要求代理具备以下能力：\n\n*   **跨时间和空间维度进行推理**\n*   **利用复杂的工具集**\n*   **主动澄清模糊的指令**\n*   **在多轮对话中跟踪不断变化的用户意图**\n\n### 评估方法\n\n研究人员提出了一种基于 **评分标准（rubric-based）的滑动窗口评估器**。这种评估器能够对复杂环境和随机交互中多样化的解决方案路径进行稳健的评估。\n\n### 评估结果\n\n全面的评估揭示了当前大型语言模型代理的局限性：\n\n*   即使是最先进的模型，在**跨场景任务**上的成功率也仅为 **30%**。\n*   在其他任务上的成功率也**低于 50%**。\n\n### 结论与资源\n\nVitaBench 被认为是一个宝贵的资源，将有助于推动人工智能代理在实际真实世界应用中的发展。相关的代码、数据集和排行榜已公开提供。\n\n*   **代码、数据集和排行榜**：[https://this.https/URL](https://this.https/URL)",
      "shortSummary": "VitaBench是一个新的基准测试，旨在解决现有LLM代理基准在处理真实世界复杂性方面的不足。它模拟了外卖、店内消费和在线旅行等场景，包含66种工具，生成了100个跨场景和300个单场景任务。这些任务要求代理进行复杂推理、工具使用和多轮交互。评估结果显示，即使是先进模型在跨场景任务上的成功率也仅为30%，凸显了当前代理的局限性。VitaBench被视为推动AI代理发展的宝贵资源，其代码、数据集和排行榜已公开。",
      "translated_title": "VitaBench：使用真实世界应用中的多功能交互任务对大型语言模型代理进行基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/"
    },
    {
      "title": "dParallel：dLLM的可学习并行解码 (原标题: dParallel: Learnable Parallel Decoding for dLLMs)",
      "link": "https://arxiv.org/abs/2509.26488",
      "pubDate": "Tue, 30 Sep 2025 12:32:52 GMT",
      "isoDate": "2025-09-30T12:32:52.000Z",
      "creator": "Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang",
      "summary": "## dParallel：dLLM的可学习并行解码\n\n### 摘要\n\n扩散大语言模型（dLLMs）作为自回归生成的有前景替代方案，因其并行令牌预测和更低的推理延迟而受到研究界的广泛关注。然而，其并行解码潜力尚未得到充分探索，现有开源模型仍需接近令牌长度的解码步骤才能确保性能。\n\n### dParallel 方法介绍\n\n为了解决这一问题，研究人员引入了 **dParallel**，这是一种简单而有效的方法，旨在释放 dLLM 的内在并行性以实现快速采样。该研究发现，并行解码的关键瓶颈在于掩码令牌的顺序确定性收敛。\n\n### 核心方法：确定性强制蒸馏\n\n基于这一洞察，dParallel 的核心方法是 **确定性强制蒸馏（certainty-forcing distillation）**。这是一种新颖的训练策略，它通过以下方式对模型进行蒸馏：\n\n*   使其遵循原始的采样轨迹。\n*   同时强制模型在掩码令牌上更快、更并行地达到高确定性。\n\n### 实验结果与性能提升\n\n在各种基准测试中进行的广泛实验表明，dParallel 方法能够显著减少解码步骤，同时保持性能。\n\n*   **GSM8K 基准测试**：当应用于 LLaDA-8B-Instruct 模型时，dParallel 将解码步骤从 256 减少到 30，实现了 **8.5 倍的加速**，且没有性能下降。\n*   **MBPP 基准测试**：解码步骤从 256 减少到 24，带来了 **10.5 倍的加速**，同时保持了准确性。\n\n### 代码可用性\n\n该项目的代码可在提供的链接中获取。",
      "shortSummary": "dParallel 是一种新方法，旨在解锁扩散大语言模型（dLLMs）的并行解码潜力，以实现快速采样。它通过“确定性强制蒸馏”训练策略，加速掩码令牌的确定性收敛。实验证明，dParallel 能显著减少解码步骤，例如在 LLaDA-8B-Instruct 模型上，GSM8K 任务提速 8.5 倍，MBPP 任务提速 10.5 倍，同时保持性能，有效解决了现有 dLLM 并行解码效率低下的问题。",
      "translated_title": "dParallel：dLLM的可学习并行解码",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel"
    },
    {
      "title": "用于代码的回归语言模型 (原标题: Regression Language Models for Code)",
      "link": "https://arxiv.org/abs/2509.26476",
      "pubDate": "Tue, 30 Sep 2025 12:25:23 GMT",
      "isoDate": "2025-09-30T12:25:23.000Z",
      "creator": "Yash Akhauri, Xingyou Song, Arissa Wongpanich, Bryan Lewandowski, Mohamed S. Abdelfattah",
      "summary": "## 用于代码的回归语言模型 (RLM)\n\n### 摘要\n\n本文研究了“代码到指标的回归”任务，即预测代码执行的数值结果。由于编程语言的开放性，这项任务极具挑战性，传统方法通常依赖于繁重且领域特定的特征工程。本文提出了一种统一的回归语言模型 (RLM)，能够直接从文本进行预测，从而克服了传统方法的局限性。\n\n### RLM 的预测能力\n\nRLM 展现了其在多个关键领域的预测能力：\n\n*   **内存占用：** 能够预测 Python 和 C++ 等多种高级语言代码的内存占用。\n*   **延迟：** 能够预测 Triton GPU 内核的延迟。\n*   **神经网络性能：** 能够预测以 ONNX 格式表示的已训练神经网络的准确性和速度。\n\n### 关键成果与性能\n\nRLM 在多项任务中取得了显著的性能：\n\n*   一个相对较小的 3 亿参数 RLM（由 T5Gemma 初始化）在 APPS 竞争性编程提交中获得了 **> 0.9 的 Spearman-rank 相关系数**。\n*   一个统一模型在 CodeNet 的 17 种不同语言上实现了 **> 0.5 的平均 Spearman-rank 相关系数**。\n*   在五个经典的 NAS（神经架构搜索）设计空间上，RLM 获得了 **0.46 的最高平均 Kendall-Tau 相关系数**，这些设计空间此前主要由图神经网络主导。\n*   RLM 还能同时预测多种硬件平台上的架构延迟。\n\n### 结论\n\nRLM 提供了一种通用且高效的方法来预测代码的各种数值结果，无需复杂的特征工程，并在多语言、多任务和多硬件平台上展现了卓越的性能和广泛的适用性。",
      "shortSummary": "本文提出了一种统一的回归语言模型（RLM），能够直接从代码文本预测其执行的数值结果。该模型解决了传统方法依赖复杂特征工程的挑战，并能同时预测多种高级语言（如Python、C++）的内存占用、Triton GPU内核的延迟以及ONNX神经网络的准确性和速度。一个3亿参数的RLM在竞争性编程和CodeNet上表现出色，并在神经架构搜索任务中超越了图神经网络，展现了其在代码到指标回归任务中的广泛适用性和卓越性能。",
      "translated_title": "用于代码的回归语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains &gt; 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves &gt; 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms."
    },
    {
      "title": "MotionRAG：运动检索增强的图像到视频生成 (原标题: MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation)",
      "link": "https://arxiv.org/abs/2509.26391",
      "pubDate": "Tue, 30 Sep 2025 11:26:04 GMT",
      "isoDate": "2025-09-30T11:26:04.000Z",
      "creator": "Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang",
      "summary": "## MotionRAG：运动检索增强的图像到视频生成\n\n### 摘要\n\n图像到视频生成在扩散模型的推动下取得了显著进展，但生成具有真实感运动的视频仍然面临巨大挑战。这主要是因为准确建模运动的复杂性，它涉及捕捉物理约束、对象交互以及难以在不同场景中泛化的领域特定动态。为了解决这一问题，本文提出了 MotionRAG，一个检索增强框架，旨在通过上下文感知运动适应（Context-Aware Motion Adaptation, CAMA）从相关参考视频中适应运动先验，从而增强运动的真实感。\n\n### 关键技术创新\n\nMotionRAG 框架的核心包含以下技术创新：\n\n*   **1. 基于检索的管道：**\n    *   利用视频编码器和专门的重采样器来提取高级运动特征。\n    *   其目的是提炼出语义运动表示，为后续的运动适应提供基础。\n\n*   **2. 运动适应的上下文学习方法：**\n    *   通过一种因果Transformer架构实现，允许模型在上下文中学习和适应运动模式。\n\n*   **3. 基于注意力的运动注入适配器：**\n    *   该适配器能够将传输的运动特征无缝集成到预训练的视频扩散模型中，确保运动信息的有效融合。\n\n### 实验结果与优势\n\n广泛的实验证明，MotionRAG 方法在多个领域和各种基础模型上均取得了显著的改进。该方法的主要优势包括：\n\n*   **性能提升：** 在生成视频的运动真实感方面表现出显著提升。\n*   **计算效率：** 在推理过程中，计算开销可忽略不计，确保了其实用性。\n*   **模块化设计与泛化能力：** 其模块化设计使得通过简单更新检索数据库即可实现对新领域的零样本泛化，而无需重新训练任何组件。\n\n### 研究意义\n\n这项研究通过实现运动先验的有效检索和传输，增强了视频生成系统的核心能力，从而促进了真实运动动态的合成。该工作属于计算机视觉与模式识别（cs.CV）领域。",
      "shortSummary": "MotionRAG 提出了一种检索增强框架，旨在解决图像到视频生成中运动真实感不足的挑战。该框架通过上下文感知运动适应（CAMA）从参考视频中提取并适应运动先验。其核心创新包括基于检索的运动特征提取、通过因果Transformer实现的上下文学习运动适应，以及将运动特征注入预训练扩散模型的注意力适配器。实验表明，MotionRAG 在多个领域显著提升了视频运动的真实感，且推理开销极低，并支持零样本泛化。",
      "translated_title": "MotionRAG：运动检索增强的图像到视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics."
    },
    {
      "title": "TAU：超越语义的文化声音理解基准 (原标题: TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics)",
      "link": "https://arxiv.org/abs/2509.26329",
      "pubDate": "Tue, 30 Sep 2025 10:40:45 GMT",
      "isoDate": "2025-09-30T10:40:45.000Z",
      "creator": "Yi-Cheng Lin, Yu-Hua Chen, Jia-Kai Dong, Yueh-Hsuan Huang, Szu-Chi Chen, Yu-Chen Chen, Chih-Yao Chen, Yu-Jung Lin, Yu-Ling Chen, Zih-Yu Chen, I-Ning Tsai, Hsiu-Hsuan Wang, Ho-Lam Chung, Ke-Han Lu, Hung-yi Lee",
      "summary": "## TAU：超越语义的文化声音理解基准\n\n### 引言\n\n当前大型音频-语言模型（LALMs）在语音和全球通用声音处理方面取得了显著进展。然而，这些模型的评估往往侧重于普遍性的声音或语音，而忽视了具有特定文化背景和地域特色的声音线索。这引发了一个关键问题：现有模型能否有效泛化并理解那些本地社区能立即识别，但外部人员却无法仅凭语义理解的本地化、非语义音频？\n\n### TAU基准的提出\n\n为解决上述挑战，研究团队提出了TAU（Taiwan Audio Understanding）基准。TAU是一个专门为评估模型对日常台湾“声音地标”（soundmarks）理解能力而设计的基准。这些“声音地标”是台湾当地居民耳熟能详，但对非本地人而言可能难以理解的声音。\n\n### TAU的构建过程与特点\n\nTAU基准的构建过程结合了多项策略，以确保其质量和有效性：\n\n*   **精选来源：** 从多样化的来源中精心挑选具有代表性的台湾日常声音。\n*   **人工编辑：** 经过专业人员的人工编辑和筛选，确保音频内容的准确性和文化相关性。\n*   **LLM辅助问题生成：** 利用大型语言模型（LLM）辅助生成多项选择题，这些问题旨在测试对声音的文化理解，而非仅仅是语义识别。\n\n最终，TAU基准包含了**702个音频片段**和**1,794个多项选择题**。这些题目被设计成无法仅通过文本转录来解决，从而强调了其“超越语义”的文化理解维度。\n\n### 实验结果\n\n研究团队对包括Gemini 2.5和Qwen2-Audio在内的最先进LALMs进行了实验。结果显示，这些模型在TAU基准上的表现远低于本地人类的水平。这表明，即使是当前最先进的模型，在理解特定文化背景下的非语义声音方面也存在显著的局限性。\n\n### 研究意义\n\nTAU基准的提出具有重要的研究意义：\n\n*   **揭示文化盲点：** 它证明了需要本地化基准来揭示现有LALMs在文化理解方面的盲点。\n*   **指导公平评估：** 有助于指导更公平、更全面的多模态评估方法，确保模型不仅能在全球通用任务上表现良好，也能适应地域和文化差异。\n*   **服务多元社区：** 确保模型能够更好地服务于全球主流之外的多元化社区，满足其特定的文化和语言需求。\n\n### 其他信息\n\n该文章共5页，已提交至ICASSP 2026会议。研究领域涵盖音频与语音处理（eess.AS）、计算与语言（cs.CL）、机器学习（cs.LG）和声音（cs.SD）。",
      "shortSummary": "大型音频-语言模型在理解文化特色声音方面存在不足。为解决此问题，研究团队推出了TAU（Taiwan Audio Understanding）基准，这是一个包含702个台湾日常“声音地标”片段和1,794个多选题的集合，这些题目无法仅凭文本转录解决。实验表明，包括Gemini 2.5在内的最先进模型表现远逊于本地人类。TAU强调了本地化基准的重要性，以揭示模型的文化盲点，指导更公平的多模态评估，并确保模型能服务于全球主流之外的社区。",
      "translated_title": "TAU：超越语义的文化声音理解基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream."
    },
    {
      "title": "ProfVLM：一种用于多视角熟练度估计的轻量级视频-语言模型 (原标题: ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation)",
      "link": "https://arxiv.org/abs/2509.26278",
      "pubDate": "Tue, 30 Sep 2025 10:00:41 GMT",
      "isoDate": "2025-09-30T10:00:41.000Z",
      "creator": "Edoardo Bianchi, Jacopo Staiano, Antonio Liotta",
      "summary": "### ProfVLM：一种用于多视角熟练度估计的轻量级视频-语言模型\n\n**背景与现有问题：**\n*   现有的技能熟练度估计方法主要依赖于黑盒视频分类器。\n*   这些方法往往忽略了多视角上下文信息，导致评估不够全面。\n*   它们普遍缺乏可解释性，难以提供清晰的反馈。\n\n**ProfVLM 介绍：**\n*   ProfVLM 是一种紧凑的视觉-语言模型（VLM），旨在解决上述问题。\n*   它将技能熟练度估计任务重新定义为一种生成式推理过程。\n*   该模型能够联合预测技能水平，并从第一人称（egocentric）和第三人称（exocentric）视频中生成专家级的反馈。\n\n**核心方法与架构：**\n*   ProfVLM 的核心是一个名为 **AttentiveGatedProjector** 的组件。\n*   该组件负责动态融合来自不同视角的特征。\n*   这些特征首先从一个冻结的 TimeSformer 骨干网络中提取并投影出来。\n*   随后，这些特征被输入到一个经过专门微调的语言模型中，该语言模型负责生成详细的反馈。\n\n**训练与性能表现：**\n*   ProfVLM 在包含专家评论的 EgoExo4D 数据集上进行了训练。\n*   **卓越的性能提升：**\n    *   它在性能上超越了现有最先进的方法。\n    *   在实现更高准确性的同时，ProfVLM 的参数量减少了高达20倍。\n    *   训练时间也大幅缩短，最高可达60%。\n\n**结果与未来方向：**\n*   ProfVLM 在多种活动中均展现出卓越的准确性。\n*   它能够输出与实际表现高度一致的自然语言评论，从而提供了透明且可解释的推理过程。\n*   这些成果强调了生成式视觉-语言建模在技能评估领域是一个强大且充满潜力的新方向。",
      "shortSummary": "ProfVLM是一种轻量级视频-语言模型，用于多视角技能熟练度估计。它通过生成式推理，从第一人称和第三人称视频中联合预测技能水平并生成专家级反馈。ProfVLM超越了现有最先进方法，参数量减少20倍，训练时间缩短60%。它提供高准确性及与表现一致的自然语言评论，展现了生成式视觉-语言建模在技能评估方面的新潜力。",
      "translated_title": "ProfVLM：一种用于多视角熟练度估计的轻量级视频-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment."
    },
    {
      "title": "IMG: 通过隐式多模态引导校准扩散模型 (原标题: IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance)",
      "link": "https://arxiv.org/abs/2509.26231",
      "pubDate": "Tue, 30 Sep 2025 09:27:03 GMT",
      "isoDate": "2025-09-30T09:27:03.000Z",
      "creator": "Jiayi Guo, Chuanhao Yan, Xingqian Xu, Yulin Wang, Kai Wang, Gao Huang, Humphrey Shi",
      "summary": "## IMG: 通过隐式多模态引导校准扩散模型\n\n### 摘要\n\n本文提出了一种名为“隐式多模态引导”（Implicit Multimodal Guidance, IMG）的新型框架，旨在解决扩散模型生成图像与输入提示之间精确多模态对齐的长期挑战。与现有方法（如微调或基于编辑的方法）不同，IMG无需额外数据或编辑操作，通过一种新颖的再生成（re-generation）机制来提升对齐效果。\n\n### 现有挑战\n\n*   **多模态对齐困难**：扩散模型生成的图像与文本提示之间的精确对齐一直是一个难题。\n*   **现有方法局限**：\n    *   **微调方法**：依赖高质量的偏好数据来微调扩散权重，但此类数据通常有限且难以大规模获取。\n    *   **基于编辑的方法**：虽然可以细化生成图像的局部区域，但可能损害整体图像质量。\n\n### IMG 方法概述\n\nIMG 是一种基于再生成的多模态对齐框架，其核心在于无需额外数据或编辑操作，通过以下三个关键步骤实现对齐：\n\n1.  **识别错位**：\n    *   给定一个生成的图像及其对应的提示，IMG 利用多模态大语言模型（Multimodal Large Language Model, MLLM）来识别图像与提示之间存在的错位（misalignments）。\n\n2.  **引入隐式对齐器**：\n    *   IMG 引入了一个“隐式对齐器”（Implicit Aligner）。该对齐器通过巧妙地操纵扩散模型的条件特征（diffusion conditioning features），以减少识别出的错位，并为图像的再生成提供指导。\n\n3.  **制定可训练目标**：\n    *   将再对齐的目标公式化为一个可训练的优化目标，即“迭代更新偏好目标”（Iteratively Updated Preference Objective）。这使得对齐过程能够通过学习进行优化。\n\n### 实验结果与优势\n\n*   **性能超越**：在 SDXL、SDXL-DPO 和 FLUX 等主流扩散模型上的广泛定性和定量评估表明，IMG 的性能优于现有的对齐方法。\n*   **即插即用适配器**：IMG 作为一个灵活的即插即用（plug-and-play）适配器，能够无缝地增强先前基于微调的对齐方法，进一步提升其效果。\n\n### 其他信息\n\n*   **代码可用性**：项目代码将在未来提供。\n*   **会议信息**：本文已被 ICCV 2025 接收。\n*   **研究领域**：计算机视觉与模式识别（cs.CV）。",
      "shortSummary": "本文提出IMG（隐式多模态引导）框架，旨在解决扩散模型图像与提示对齐难题。IMG无需额外数据或编辑，通过MLLM识别错位，引入隐式对齐器操纵扩散特征进行再生成，并制定可训练的“迭代更新偏好目标”。实验证明，IMG在SDXL等模型上优于现有对齐方法，并可作为即插即用适配器增强其他微调方法。",
      "translated_title": "IMG: 通过隐式多模态引导校准扩散模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment."
    },
    {
      "title": "Mem-α: 通过强化学习构建记忆 (原标题: Mem-α: Learning Memory Construction via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.25911",
      "pubDate": "Tue, 30 Sep 2025 04:02:34 GMT",
      "isoDate": "2025-09-30T04:02:34.000Z",
      "creator": "Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, Xiaojian Wu",
      "summary": "### Mem-α: 通过强化学习构建记忆\n\n**背景与问题：**\n\n*   大型语言模型（LLM）代理受限于其有限的上下文窗口，这使得它们需要外部记忆系统来理解和处理长期信息。\n*   当前记忆增强代理通常依赖于预定义的指令和工具进行记忆更新。\n*   然而，语言模型可能缺乏自主决定以下事项的能力：\n    *   哪些信息需要存储。\n    *   如何有效地组织和构建这些信息。\n    *   何时进行记忆更新。\n*   尤其是在记忆系统变得复杂时，这种不足会导致次优的记忆构建和信息丢失。\n\n**Mem-α 框架：**\n\n*   **提出：** Mem-alpha 是一种新颖的强化学习（RL）框架，旨在通过交互和反馈训练代理，使其能够有效地管理复杂的记忆系统。\n*   **训练机制：**\n    *   **专用数据集：** 构建了一个专门的训练数据集，其中包含多样化的多轮交互模式，并配有全面的评估问题，旨在教授代理有效的记忆管理策略。\n    *   **信息处理：** 在训练过程中，代理会处理顺序的信息块，学习从中提取和存储相关内容，并适时更新记忆系统。\n    *   **奖励信号：** 奖励信号直接来源于对完整交互历史的下游问答准确性，从而直接优化记忆构建过程。\n\n**记忆架构示例：**\n\n*   为了说明该训练框架的有效性，研究人员设计了一个包含以下组件的记忆架构：\n    *   核心（Core）组件。\n    *   情景（Episodic）组件。\n    *   语义（Semantic）组件。\n*   该架构还配备了多种工具，用于执行记忆操作。\n\n**实验结果与泛化能力：**\n\n*   **显著改进：** 经验评估表明，Mem-alpha 在现有记忆增强代理基线上取得了显著的性能改进。\n*   **卓越泛化：** 尽管代理仅在最大长度为30k tokens的实例上进行训练，但它们对超过400k tokens（即训练长度的13倍以上）的序列展现出卓越的泛化能力。\n*   **鲁棒性：** 这一结果突显了Mem-alpha框架的强大鲁棒性。\n\n**研究领域：**\n\n*   计算与语言 (cs.CL)。",
      "shortSummary": "Mem-alpha 提出了一种强化学习框架，旨在解决大型语言模型在复杂记忆系统中的记忆构建和管理问题。通过专门的训练数据集和基于问答准确性的奖励信号，Mem-alpha 训练代理学习如何有效存储、组织和更新信息。实验证明，该框架显著优于现有基线，并展现出对远超训练长度的序列的强大泛化能力，突显了其鲁棒性。",
      "translated_title": "Mem-α: 通过强化学习构建记忆",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha."
    },
    {
      "title": "更多思考，更低准确度？关于视觉-语言模型中推理的双重性质 (原标题: More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models)",
      "link": "https://arxiv.org/abs/2509.25848",
      "pubDate": "Tue, 30 Sep 2025 02:37:47 GMT",
      "isoDate": "2025-09-30T02:37:47.000Z",
      "creator": "Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang",
      "summary": "## 视觉-语言模型中推理的双重性质\n\n### 引言\n\n推理能力已成为大型语言模型（LLMs）的关键能力，通过强化学习（如GRPO）解决了数学和代码生成等复杂任务。基于这些进展，近期研究致力于将推理能力扩展到视觉-语言模型（VLMs），并在各种视觉任务中取得了可喜的成果。\n\n### 研究发现：推理的双重性质\n\n尽管取得了上述进展，本研究揭示了多模态推理的“双重性质”：\n\n*   **积极作用：** 推理能力显著增强了逻辑推理，并有助于解决具有挑战性的问题。\n*   **消极作用：** 推理过程可能逐渐损害模型的感知基础（perceptual grounding），导致在原本基本的视觉问题上出现识别失败。\n\n### 原因分析：视觉遗忘\n\n研究进一步分析并将这种现象归因于“视觉遗忘”（visual forgetting）。这意味着长时间的推理过程会导致模型逐渐忽视视觉输入，从而影响其对视觉信息的依赖和准确性。\n\n### 解决方案：视觉锚定策略优化（VAPO）\n\n为了解决视觉遗忘问题，本研究提出了一种简单而有效的方法：**视觉锚定策略优化（Vision-Anchored Policy Optimization, VAPO）**。VAPO明确地将推理过程引导至以视觉为基础的轨迹，确保模型在推理时能够持续关注并利用视觉信息。\n\n### 成果与贡献\n\n基于VAPO训练的模型 **VAPO-Thinker-7B** 取得了显著成果：\n\n*   它显著增强了模型对视觉信息的依赖。\n*   在一系列既定的基准测试中取得了新的最先进（state-of-the-art）结果。",
      "shortSummary": "本研究揭示了视觉-语言模型（VLMs）中推理的双重性质：它虽能增强逻辑推理和解决复杂任务，但也可能因“视觉遗忘”而损害感知基础，导致视觉识别失败。为解决此问题，本文提出了视觉锚定策略优化（VAPO），该方法明确引导推理过程以视觉为基础。VAPO-Thinker-7B模型显著增强了对视觉信息的依赖，并在多项基准测试中取得了最先进的结果。",
      "translated_title": "更多思考，更低准确度？关于视觉-语言模型中推理的双重性质",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/"
    }
  ],
  "lastUpdated": "2025-10-01T09:36:28.049Z"
}