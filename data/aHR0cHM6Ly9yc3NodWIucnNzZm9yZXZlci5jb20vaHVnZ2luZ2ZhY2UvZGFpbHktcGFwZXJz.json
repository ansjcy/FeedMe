{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LayerFlow：一种用于分层视频生成的统一模型 (原标题: LayerFlow: A Unified Model for Layer-aware Video Generation)",
      "link": "https://arxiv.org/abs/2506.04228",
      "pubDate": "Wed, 04 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-04T13:59:58.000Z",
      "creator": "Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, Hengshuang Zhao",
      "summary": "## LayerFlow：分层视频生成的统一解决方案\n\nLayerFlow 是一种创新的统一模型，旨在解决分层视频生成问题。它能够根据每层的提示词，生成透明前景、干净背景以及混合场景的视频。\n\n### 核心功能与应用场景\n\nLayerFlow 支持多种灵活的视频生成变体，包括：\n*   **分层生成**：根据前景、背景和混合场景的提示词生成相应视频。\n*   **视频分解**：将混合视频分解为独立的前景和背景层。\n*   **条件生成**：在给定前景的情况下生成背景视频，或在给定背景的情况下生成前景视频。\n\n### 技术实现与统一框架\n\n该模型以文本到视频扩散Transformer为基础，通过以下方式实现其统一性：\n*   **子剪辑组织**：将不同层的视频组织为独立的子剪辑。\n*   **层嵌入**：利用层嵌入来区分每个剪辑及其对应的分层提示词。\n\n这种设计使得所有上述功能都可以在一个统一的框架内无缝支持。\n\n### 多阶段训练策略\n\n鉴于高质量分层训练视频数据的稀缺性，LayerFlow 采用了一种精巧的多阶段训练策略，以充分利用带有高质量层注释的静态图像数据：\n\n1.  **低质量视频数据预训练**：首先使用低质量视频数据对模型进行初步训练。\n2.  **运动LoRA微调**：接着，微调一个运动 LoRA（Low-Rank Adaptation）模块，使模型能够兼容静态帧，从而引入运动能力。\n3.  **内容LoRA训练**：最后，在高质量分层图像数据与复制粘贴的视频数据混合数据集上训练内容 LoRA，以提升内容生成质量。\n\n### 推理过程\n\n在推理阶段，模型会移除运动 LoRA，从而生成具有所需分层的平滑视频。",
      "shortSummary": "LayerFlow是一个统一的分层视频生成模型。它能根据提示词生成前景、背景和混合场景视频，并支持视频分解及条件生成。该模型基于文本到视频扩散Transformer，通过将不同层视频组织为子剪辑并利用层嵌入实现统一框架。为解决高质量数据稀缺问题，LayerFlow采用多阶段训练策略，结合低质量视频和高质量分层图像数据进行训练，最终生成平滑的分层视频。",
      "translated_title": "LayerFlow：一种用于分层视频生成的统一模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers."
    },
    {
      "title": "Voyager：用于可探索3D场景生成的长距离和世界一致性视频扩散 (原标题: Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation)",
      "link": "https://arxiv.org/abs/2506.04225",
      "pubDate": "Wed, 04 Jun 2025 13:59:04 GMT",
      "isoDate": "2025-06-04T13:59:04.000Z",
      "creator": "Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson W. H. Lau, Wangmeng Zuo, Chunchao Guo",
      "summary": "# Voyager：用于可探索3D场景生成的长距离和世界一致性视频扩散\n\n本文介绍了 **Voyager**，一个新颖的视频扩散框架，旨在解决生成长距离、3D一致性且可探索的3D场景的复杂挑战。这类场景在视频游戏和虚拟现实等实际应用中需求量很大，但现有方法在3D重建方面面临困难。\n\n**核心创新与优势：**\nVoyager 能够从单张图像和用户定义的摄像机路径生成世界一致的3D点云序列。与现有方法不同，Voyager 实现了端到端的场景生成和重建，具有固有的帧间一致性，从而无需传统的3D重建管道（例如，运动结构或多视图立体）。\n\n**主要组成部分：**\nVoyager 框架集成了三个关键组件，共同实现了其卓越的性能：\n\n1.  **世界一致性视频扩散 (World-Consistent Video Diffusion)：**\n    *   这是一个统一的架构，能够联合生成对齐的RGB和深度视频序列。\n    *   它以现有的世界观察为条件，以确保全局连贯性。\n\n2.  **长距离世界探索 (Long-Range World Exploration)：**\n    *   包含一个高效的世界缓存，具有点剔除功能。\n    *   采用自回归推理和平滑视频采样，用于迭代地扩展场景，同时保持上下文感知的一致性。\n\n3.  **可扩展数据引擎 (Scalable Data Engine)：**\n    *   一个视频重建管道，能够自动化摄像机姿态估计和度量深度预测，适用于任意视频。\n    *   这使得无需手动3D标注即可策划大规模、多样化的训练数据。\n\n**成果与应用：**\n这些设计共同使得 Voyager 在视觉质量和几何精度方面相较于现有方法有了显著提升，并具有广泛的应用前景。",
      "shortSummary": "Voyager 是一种新颖的视频扩散框架，旨在从单张图像和用户定义路径生成长距离、世界一致且可探索的3D场景。它通过端到端生成和重建，消除了对传统3D重建管道的需求，实现了固有的帧间一致性。该框架整合了世界一致性视频扩散、长距离世界探索和可扩展数据引擎，显著提升了视觉质量和几何精度，具有广泛的实际应用价值。",
      "translated_title": "Voyager：用于可探索3D场景生成的长距离和世界一致性视频扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "Real-world applications like video gaming and virtual reality often demand the ability to model 3D scenes that users can explore along custom camera trajectories. While significant progress has been made in generating 3D objects from text or images, creating long-range, 3D-consistent, explorable 3D scenes remains a complex and challenging problem. In this work, we present Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Unlike existing approaches, Voyager achieves end-to-end scene generation and reconstruction with inherent consistency across frames, eliminating the need for 3D reconstruction pipelines (e.g., structure-from-motion or multi-view stereo). Our method integrates three key components: 1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence 2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency, and 3) Scalable Data Engine: A video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Collectively, these designs result in a clear improvement over existing methods in visual quality and geometric accuracy, with versatile applications."
    },
    {
      "title": "推进多模态推理：从优化冷启动到分阶段强化学习 (原标题: Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.04207",
      "pubDate": "Wed, 04 Jun 2025 13:51:08 GMT",
      "isoDate": "2025-06-04T13:51:08.000Z",
      "creator": "Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng",
      "summary": "## ReVisual-R1：推进多模态推理\n\n### 背景与挑战\n\n受Deepseek-R1在复杂文本任务中卓越推理能力的启发，许多研究尝试通过直接应用强化学习（RL）来提升多模态大型语言模型（MLLMs）的推理能力。然而，这些尝试在激活复杂推理方面仍面临挑战。\n\n### 关键发现与洞察\n\n本文深入探讨了当前MLLM的训练流程，并识别出三个关键现象，这些现象对于提升多模态推理能力至关重要：\n\n1.  **有效的冷启动初始化至关重要：** 研究发现，即使在多模态强化学习之前，仅使用精心挑选的文本数据进行初始化，其性能就可能超越许多近期多模态推理模型。\n2.  **标准GRPO在多模态RL中存在梯度停滞问题：** 将标准GRPO（Generalized Reinforcement Learning with Policy Optimization）应用于多模态强化学习时，会出现梯度停滞现象，这会降低训练的稳定性和性能。\n3.  **分阶段训练的有效性：** 在多模态强化学习阶段之后，进行后续的纯文本强化学习训练，能够进一步增强多模态推理能力。这种分阶段的训练方法有效地平衡了感知基础（perceptual grounding）和认知推理（cognitive reasoning）的发展。\n\n### ReVisual-R1模型\n\n通过整合上述洞察并解决多模态强化学习中的问题，研究者引入了**ReVisual-R1**模型。\n\n### 成果\n\nReVisual-R1在多个具有挑战性的基准测试中，包括MathVerse、MathVision、WeMath、LogicVista、DynaMath，以及具有挑战性的AIME2024和AIME2025，在开源7B MLLM中取得了新的最先进（state-of-the-art）性能。",
      "shortSummary": "本文提出ReVisual-R1模型，旨在提升多模态大型语言模型（MLLMs）的复杂推理能力。研究发现，有效的冷启动初始化、解决多模态强化学习中的梯度停滞问题，以及采用多模态RL后接纯文本RL的分阶段训练方法，对提升性能至关重要。ReVisual-R1整合这些洞察，在多个挑战性基准测试中，在开源7B MLLM中取得了最先进的性能。",
      "translated_title": "推进多模态推理：从优化冷启动到分阶段强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025."
    },
    {
      "title": "SuperWriter：基于反思的大语言模型长文本生成 (原标题: SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models)",
      "link": "https://arxiv.org/abs/2506.04180",
      "pubDate": "Wed, 04 Jun 2025 13:27:42 GMT",
      "isoDate": "2025-06-04T13:27:42.000Z",
      "creator": "Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee",
      "summary": "### SuperWriter：基于反思的大语言模型长文本生成\n\n*   **挑战：** 大语言模型（LLMs）在长文本生成方面面临显著挑战，尤其是在文本长度增加时，难以保持连贯性、逻辑一致性和整体质量。\n\n*   **解决方案：** 论文提出了 SuperWriter-Agent 框架，这是一个基于智能体（agent-based）的框架，旨在提升长文本生成的质量和一致性。\n\n*   **SuperWriter-Agent 机制：**\n    *   该框架在生成流程中引入了明确的结构化思考阶段，包括规划（planning）和精炼（refinement）。\n    *   这种设计旨在引导模型遵循一种更深思熟虑、更具认知基础的生成过程，类似于专业作家的写作方式。\n\n*   **模型训练与优化：**\n    *   基于 SuperWriter-Agent 框架，研究人员构建了一个监督微调数据集，用于训练一个7B参数的 SuperWriter-LM 模型。\n    *   进一步开发了一种分层直接偏好优化（Hierarchical Direct Preference Optimization, DPO）过程，该过程利用蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）来传播最终的质量评估，并据此优化每个生成步骤。\n\n*   **实验结果：**\n    *   在多个不同基准测试上的实证结果表明，SuperWriter-LM 实现了最先进的性能（state-of-the-art），在自动评估和人工评估中均超越了规模更大的基线模型。\n    *   全面的消融研究（ablation studies）证实了分层 DPO 的有效性，并强调了整合结构化思考步骤对于提升长文本生成质量的价值。",
      "shortSummary": "SuperWriter-Agent 框架旨在解决大语言模型长文本生成中连贯性、一致性和质量下降的问题。该框架引入了类似专业作家的结构化思考（规划与精炼）阶段。通过构建数据集训练7B SuperWriter-LM，并采用分层DPO与MCTS进行优化，SuperWriter-LM在多项基准测试中取得了最先进的性能，超越了更大规模的模型，证明了其方法和优化策略的有效性。",
      "translated_title": "SuperWriter：基于反思的大语言模型长文本生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation."
    },
    {
      "title": "使用扩散模型进行图像编辑即程序 (原标题: Image Editing As Programs with Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.04158",
      "pubDate": "Wed, 04 Jun 2025 12:57:24 GMT",
      "isoDate": "2025-06-04T12:57:24.000Z",
      "creator": "Yujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, Xinchao Wang",
      "summary": "### 使用扩散模型进行图像编辑即程序 (Image Editing As Programs with Diffusion Models)\n\n*   **引言**\n    *   扩散模型在文本到图像生成方面取得了显著成功。\n    *   然而，它们在指令驱动的图像编辑方面面临重大挑战，尤其是在涉及重大布局变化的结构不一致编辑上。\n\n*   **Image Editing As Programs (IEAP) 框架**\n    *   **目的：** 弥补扩散模型在复杂图像编辑方面的不足。\n    *   **核心思想：** IEAP 采用还原论视角处理指令式编辑，将复杂的编辑指令分解为一系列原子操作。\n    *   **架构：**\n        *   IEAP 是一个统一的图像编辑框架，构建于 Diffusion Transformer (DiT) 架构之上。\n        *   每个原子操作都通过一个轻量级适配器实现，该适配器共享相同的 DiT 主干网络，并专门用于特定类型的编辑。\n    *   **操作编程：** 这些操作由一个基于视觉-语言模型 (VLM) 的智能体进行编程。\n    *   **协作支持：** 通过这种方式，原子操作可以协同支持任意和结构不一致的图像转换。\n\n*   **优势与性能**\n    *   **泛化能力：** 通过模块化和序列化编辑，IEAP 在广泛的编辑任务中表现出强大的泛化能力，从简单的调整到实质性的结构变化。\n    *   **实验结果：** 广泛的实验表明，IEAP 在各种编辑场景的标准基准测试中显著优于现有最先进的方法。\n    *   **准确性和语义保真度：** 在这些评估中，IEAP 框架提供了卓越的准确性和语义保真度，尤其适用于复杂的多步骤指令。",
      "shortSummary": "扩散模型在指令驱动的复杂图像编辑中面临挑战。为解决此问题，研究提出了“使用扩散模型进行图像编辑即程序”（IEAP）框架。IEAP将复杂编辑指令分解为原子操作序列，每个操作由基于DiT的适配器实现，并由VLM智能体编程。该框架在广泛的编辑任务中表现出强大的泛化能力，并在实验中显著优于现有最先进方法，尤其在处理复杂多步骤指令时展现出卓越的准确性和语义保真度。",
      "translated_title": "使用扩散模型进行图像编辑即程序",
      "images": [],
      "contentSource": "完整文章",
      "content": "While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP."
    },
    {
      "title": "通过捷径神经元分析建立可信赖的LLM评估 (原标题: Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis)",
      "link": "https://arxiv.org/abs/2506.04142",
      "pubDate": "Wed, 04 Jun 2025 12:33:44 GMT",
      "isoDate": "2025-06-04T12:33:44.000Z",
      "creator": "Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao",
      "summary": "### 通过捷径神经元分析建立可信赖的LLM评估\n\n**1. 背景与问题**\n*   大型语言模型（LLM）的发展依赖于可信赖的评估。\n*   当前大多数评估依赖于公共基准测试，但这些基准容易受到数据污染问题的影响，严重损害了评估的公平性。\n*   数据污染导致模型性能被高估。\n\n**2. 现有解决方案及其局限性**\n*   以往研究主要通过构建动态基准来解决污染问题。\n*   然而，持续构建新基准成本高昂且具有周期性。\n\n**3. 本文提出的方法**\n*   本文旨在通过分析受污染模型本身的机制来解决污染问题。\n*   **发现：** 实验表明，受污染模型性能被高估的原因可能在于参数在训练过程中获得了“捷径解”（shortcut solutions）。\n*   **方法：** 提出了一种新颖的方法，通过比较分析和因果分析来识别“捷径神经元”（shortcut neurons）。\n*   **评估方法：** 基于此，引入了一种名为“捷径神经元修补”（shortcut neuron patching）的评估方法，用于抑制捷径神经元的影响。\n\n**4. 实验验证与结果**\n*   实验验证了该方法在缓解数据污染方面的有效性。\n*   评估结果与最近发布的可信赖基准MixEval表现出很强的线性相关性，斯皮尔曼相关系数（$\\rho$）超过0.95。\n*   高相关性表明该方法能密切揭示模型的真实能力，并具有可信赖性。\n*   进一步的实验证明了该方法在不同基准测试和超参数设置下的泛化能力。\n\n**5. 结论**\n*   通过分析和抑制模型内部的捷径神经元，本文提供了一种有效且可信赖的LLM评估新范式，克服了传统基准测试中数据污染的挑战。",
      "shortSummary": "本文提出一种通过分析“捷径神经元”来建立可信赖LLM评估的新方法，以解决公共基准测试中数据污染导致模型性能被高估的问题。研究发现，污染源于模型参数学习了捷径解。通过比较和因果分析识别捷径神经元，并采用“捷径神经元修补”进行抑制。实验证明该方法能有效缓解污染，且与可信赖基准MixEval高度相关（$\\rho > 0.95$），表明其能准确揭示模型真实能力，具有良好的泛化性。",
      "translated_title": "通过捷径神经元分析建立可信赖的LLM评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation"
    },
    {
      "title": "MMR-V：未尽之言？一个视频多模态深度推理基准 (原标题: MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos)",
      "link": "https://arxiv.org/abs/2506.04141",
      "pubDate": "Wed, 04 Jun 2025 12:33:41 GMT",
      "isoDate": "2025-06-04T12:33:41.000Z",
      "creator": "Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
      "summary": "## MMR-V：视频多模态深度推理基准\n\n### 引言\n\n视频的顺序结构对多模态大型语言模型（MLLMs）在定位多帧证据和进行多模态推理方面构成了显著挑战。然而，现有视频基准主要侧重于理解任务，这些任务仅要求模型匹配问题中提及的帧（“问题帧”）并感知少数相邻帧，未能充分解决深度推理的需求。\n\n### MMR-V：新基准的提出\n\n为弥补这一空白，研究者提出了 **MMR-V：一个用于视频多模态深度推理的基准**。该基准旨在推动模型在视频理解中更深层次的推理能力。\n\n### MMR-V 的主要特点\n\nMMR-V 基准具有以下显著特征：\n\n1.  **长程、多帧推理：** 要求模型推断和分析可能远离问题帧的证据帧，而非仅限于相邻帧。\n2.  **超越感知：** 问题不能仅通过直接感知来回答，而是需要模型对视频中隐藏的信息进行推理和分析。\n3.  **可靠性：** 所有任务均经过人工标注，并参考了广泛的真实世界用户理解，以确保基准与普遍认知保持一致。\n4.  **混淆性：** 采用精心设计的干扰项标注策略，旨在减少模型通过捷径或表面线索进行回答的可能性。\n\n### 基准规模\n\nMMR-V 包含 **317 个视频** 和 **1,257 个任务**，为多模态深度推理研究提供了丰富的数据集。\n\n### 实验结果与发现\n\n实验结果揭示了当前模型在多模态推理方面的局限性：\n\n*   **当前模型表现：** 实验表明，当前模型在多模态推理方面仍面临困难。即使是表现最佳的模型 o4-mini，在 MMR-V 上的准确率也仅为 **52.5%**。\n*   **推理增强策略的局限性：** 当前的推理增强策略，例如思维链（Chain-of-Thought）和扩展测试时计算，所带来的性能提升非常有限。\n*   **CoT 的差异性：** 进一步分析表明，多模态推理所需的思维链与文本推理中的思维链有所不同，这部分解释了性能提升有限的原因。\n\n### 研究展望\n\n研究者希望 MMR-V 能够激发未来在增强多模态推理能力方面的进一步研究，从而推动多模态人工智能领域的发展。",
      "shortSummary": "MMR-V是一个针对视频多模态深度推理的新基准，旨在解决现有模型在长程、多帧推理和超越直接感知的隐藏信息推理方面的不足。它包含317个视频和1257个任务，具有长程推理、超越感知、高可靠性和混淆性等特点。实验显示，当前模型（如o4-mini）在MMR-V上表现不佳，准确率仅52.5%，且现有推理增强策略效果有限。研究表明多模态推理的思维链与文本推理不同。MMR-V旨在推动多模态推理能力的研究。",
      "translated_title": "MMR-V：未尽之言？一个视频多模态深度推理基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as \"question frame\") and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities."
    },
    {
      "title": "Agentic AI的TRiSM：LLM多智能体系统中的信任、风险与安全管理综述 (原标题: TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems)",
      "link": "https://arxiv.org/abs/2506.04133",
      "pubDate": "Wed, 04 Jun 2025 12:26:11 GMT",
      "isoDate": "2025-06-04T12:26:11.000Z",
      "creator": "Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis",
      "summary": "Agentic AI系统基于大型语言模型（LLM）构建，并以多智能体配置部署，正在重新定义企业和社会领域的智能自主性、协作和决策制定。本综述对LLM驱动的Agentic多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）进行了结构化分析。\n\n### 1. Agentic AI的概念基础\n*   文章首先探讨了Agentic AI的概念基础。\n*   阐明了其与传统AI智能体在架构上的差异。\n*   讨论了实现可扩展、工具使用型自主性的新兴系统设计。\n\n### 2. TRiSM框架的四大支柱\nAgentic AI框架中的TRiSM通过以下四个核心支柱进行详细阐述，每个支柱都针对Agentic LLM进行了情境化：\n*   **治理 (Governance)**\n*   **可解释性 (Explainability)**\n*   **模型操作 (ModelOps)**\n*   **隐私与安全 (Privacy/Security)**\n\n### 3. 风险与威胁管理\n*   识别了独特的威胁向量。\n*   引入了针对Agentic AI应用的全面风险分类法。\n*   通过案例研究说明了现实世界的漏洞。\n\n### 4. 信任构建与透明度\n*   调查了信任构建机制。\n*   探讨了透明度与监督技术。\n*   回顾了分布式LLM智能体系统中最先进的可解释性策略。\n\n### 5. 评估指标与挑战\n*   审查了评估信任、可解释性和以人为中心性能的指标。\n*   讨论了开放的基准测试挑战。\n\n### 6. 安全与隐私措施\n*   通过加密、对抗性防御和遵守不断演进的AI法规来解决安全和隐私问题。\n\n### 7. 负责任的Agentic AI路线图\n*   文章最后提出了负责任的Agentic AI路线图。\n*   建议了研究方向，旨在使新兴多智能体系统与强大的TRiSM原则保持一致，以实现安全、负责任和透明的部署。",
      "shortSummary": "本综述分析了LLM多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）。文章探讨了Agentic AI基础、TRiSM的四大支柱（治理、可解释性、ModelOps、隐私/安全）、独特的威胁向量和风险分类。同时，综述还调查了信任构建机制、可解释性策略、评估指标以及安全隐私措施。最后，提出了负责任的Agentic AI路线图，强调安全、负责和透明的部署。",
      "translated_title": "Agentic AI的TRiSM：LLM多智能体系统中的信任、风险与安全管理综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment."
    },
    {
      "title": "修正稀疏注意力 (原标题: Rectified Sparse Attention)",
      "link": "https://arxiv.org/abs/2506.04108",
      "pubDate": "Wed, 04 Jun 2025 12:01:48 GMT",
      "isoDate": "2025-06-04T12:01:48.000Z",
      "creator": "Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei",
      "summary": "### 修正稀疏注意力 (ReSA)：解决长序列生成中的KV缓存错位问题\n\n**1. 核心挑战与问题**\n*   **挑战**：大型语言模型 (LLMs) 的高效长序列生成是一个关键难题。\n*   **现有问题**：近期提出的稀疏解码方法虽然提高了效率，但存在KV缓存错位问题，导致近似误差累积，从而降低了生成质量。\n\n**2. 提出的解决方案：修正稀疏注意力 (ReSA)**\n*   **方法**：ReSA 是一种简单而有效的方法，它将块稀疏注意力与周期性密集修正相结合。\n*   **机制**：通过在固定间隔内使用密集前向传播刷新KV缓存，ReSA 能够限制误差累积，并保持与预训练分布的对齐。\n\n**3. 实验结果与优势**\n*   **性能**：在数学推理、语言建模和检索任务中的实验表明，ReSA 实现了接近无损的生成质量，并显著提高了效率。\n*   **效率提升**：在256K序列长度的解码下，ReSA 实现了高达2.42倍的端到端加速。\n*   **实际意义**：这使得 ReSA 成为可扩展长上下文推理的实用解决方案。\n\n**4. 代码可用性**\n*   相关代码已公开提供。",
      "shortSummary": "修正稀疏注意力 (ReSA) 旨在解决大型语言模型长序列生成中稀疏解码导致的KV缓存错位问题。ReSA 通过结合块稀疏注意力与周期性密集修正，定期刷新KV缓存，从而限制误差累积并保持生成质量。实验证明，ReSA 在保持接近无损生成质量的同时，显著提高了效率，在256K序列长度下可实现高达2.42倍的加速，为长上下文推理提供了实用方案。",
      "translated_title": "修正稀疏注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM."
    },
    {
      "title": "AmbiK：厨房环境中模糊任务数据集 (原标题: AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment)",
      "link": "https://arxiv.org/abs/2506.04089",
      "pubDate": "Wed, 04 Jun 2025 11:47:07 GMT",
      "isoDate": "2025-06-04T11:47:07.000Z",
      "creator": "Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov",
      "summary": "## AmbiK：厨房环境中模糊任务数据集\n\n### 引言\n\n大型语言模型（LLMs）在具身智能体中常用于根据用户的自然语言指令进行行为规划。然而，LLMs在真实世界环境中处理模糊指令的能力仍是一个挑战。尽管已经提出了多种任务模糊性检测方法，但由于它们在不同数据集上进行测试且缺乏统一的基准，导致难以进行有效的比较。\n\n### AmbiK数据集的提出\n\n为了解决现有模糊性检测方法缺乏统一比较基准的问题，研究人员提出了AmbiK（Ambiguous Tasks in Kitchen Environment）数据集。该数据集旨在为模糊性检测方法提供一个标准化的、可统一比较的平台。\n\n### AmbiK数据集特点\n\n*   **数据类型：** AmbiK是一个完全文本形式的数据集。\n*   **应用场景：** 它专注于厨房环境中机器人所接收的模糊指令。\n*   **数据收集与验证：** 数据集的收集过程得到了LLMs的辅助，并经过了人工验证，确保了数据的质量和准确性。\n*   **内容构成：**\n    *   AmbiK包含1000对模糊任务及其对应的明确任务，总计2000个任务。\n    *   这些任务根据模糊性类型进行了分类，主要包括：\n        *   **人类偏好（Human Preferences）：** 涉及用户个人喜好导致的模糊性。\n        *   **常识知识（Common Sense Knowledge）：** 缺乏普遍常识导致的模糊性。\n        *   **安全性（Safety）：** 与操作安全相关的模糊性。\n    *   除了任务本身，每个任务还附带了丰富的上下文信息，包括：\n        *   环境描述\n        *   澄清问题和答案\n        *   用户意图\n        *   任务计划\n\n### 预期影响与可用性\n\n研究人员希望AmbiK数据集能够使模糊性检测方法的研究人员进行统一的比较和评估，从而推动该领域的发展。AmbiK数据集可在线获取。\n\n### 相关信息\n\n*   该研究已被ACL 2025（主会议）接收。\n*   研究领域涵盖：机器学习（cs.LG）、人工智能（cs.AI）、计算与语言（cs.CL）和机器人学（cs.RO）。\n*   引用信息：arXiv:2506.04089。",
      "shortSummary": "AmbiK是一个针对厨房环境中机器人模糊任务的文本数据集。为解决LLM处理模糊指令的挑战及模糊性检测方法缺乏统一基准的问题，AmbiK提供了标准化比较平台。它包含1000对模糊与明确任务（共2000个），按人类偏好、常识知识和安全性分类，并附有环境描述、澄清问答、用户意图和任务计划。该数据集由LLM辅助收集并经人工验证，旨在推动模糊性检测研究的统一比较。",
      "translated_title": "AmbiK：厨房环境中模糊任务数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset."
    },
    {
      "title": "Rex-Thinker：通过思维链推理实现接地对象指代 (原标题: Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning)",
      "link": "https://arxiv.org/abs/2506.04034",
      "pubDate": "Wed, 04 Jun 2025 10:56:57 GMT",
      "isoDate": "2025-06-04T10:56:57.000Z",
      "creator": "Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Lei Zhang",
      "summary": "## Rex-Thinker：通过思维链推理实现接地对象指代\n\n### 引言：对象指代任务的挑战与目标\n\n对象指代（Object referring）旨在检测图像中所有与给定自然语言描述匹配的对象。一个鲁棒的对象指代模型应具备“接地”特性，即其预测应可解释并忠实于视觉内容。具体而言，它应满足以下两个关键属性：\n\n*   **可验证性（Verifiable）**：通过生成可解释的推理过程来证明其预测的合理性，并明确地将其与视觉证据联系起来。\n*   **可信赖性（Trustworthy）**：当图像中没有对象满足给定表达时，模型应学会拒绝输出（即“弃权”）。\n\n然而，大多数现有方法将对象指代视为直接的边界框预测任务，这导致解释性有限，并且难以拒绝没有匹配对象的表达。\n\n### Rex-Thinker 模型：基于思维链（CoT）的解决方案\n\n为了解决上述问题，本文提出了 **Rex-Thinker** 模型。该模型将对象指代任务公式化为一个显式的思维链（Chain-of-Thought, CoT）推理任务。\n\n**Rex-Thinker 的推理流程如下：**\n\n1.  给定一个指代表达，模型首先识别所有与所指对象类别对应的候选对象实例。\n2.  然后，Rex-Thinker 对每个候选对象执行逐步推理，评估其是否与给定表达匹配。\n3.  最后，模型做出最终预测。\n\n### HumanRef-CoT 数据集\n\n为了支持这一范式，研究人员构建了一个名为 **HumanRef-CoT** 的大规模CoT风格指代数据集。该数据集通过在HumanRef数据集上提示GPT-4o生成。\n\n*   每个推理轨迹都遵循结构化的“规划-行动-总结”格式。\n*   这种格式使模型能够学习分解式、可解释的对对象候选的推理。\n\n### 模型训练策略\n\nRex-Thinker 的训练分为两个阶段：\n\n1.  **冷启动监督微调（Cold-start supervised fine-tuning）**：此阶段旨在教授模型如何执行结构化推理。\n2.  **基于GRPO的强化学习（GRPO-based RL learning）**：此阶段用于提高模型的准确性和泛化能力。\n\n### 实验结果\n\n实验结果表明，Rex-Thinker 方法在以下方面表现出色：\n\n*   在域内评估中，其在精度和可解释性方面均优于标准基线。\n*   显著提高了拒绝幻觉输出的能力。\n*   在域外设置中展现出强大的泛化能力。",
      "shortSummary": "Rex-Thinker 是一种新的对象指代模型，通过将任务重构为显式的思维链（CoT）推理过程，解决了现有方法解释性差和无法拒绝无匹配描述的问题。它首先识别候选对象，然后逐步推理以评估匹配度。该模型利用大型CoT风格的HumanRef-CoT数据集进行训练，并采用两阶段学习策略。实验表明，Rex-Thinker 在精度、可解释性、拒绝幻觉输出以及泛化能力方面均优于现有基线。",
      "translated_title": "Rex-Thinker：通过思维链推理实现接地对象指代",
      "images": [],
      "contentSource": "完整文章",
      "content": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings."
    },
    {
      "title": "持续学习前的适应 (原标题: Adapt before Continual Learning)",
      "link": "https://arxiv.org/abs/2506.03956",
      "pubDate": "Wed, 04 Jun 2025 09:46:33 GMT",
      "isoDate": "2025-06-04T09:46:33.000Z",
      "creator": "Aojun Lu, Tao Feng, Hangjie Yuan, Chunhui Ding, Yanan Sun",
      "summary": "## 持续学习前的适应 (ACL) 框架\n\n### 核心挑战：稳定性-可塑性权衡\n\n*   **持续学习 (CL) 目标**：使神经网络能够逐步获取新知识（可塑性）同时保留现有知识（稳定性）。\n*   **预训练模型 (PTM) 在 CL 中的应用**：PTM 已成为 CL 的关键组成部分。\n*   **现有方法的局限性**：\n    *   **冻结 PTM 主干**：为保持稳定性，常见做法是冻结 PTM 主干，但这限制了模型的可塑性，尤其是在增量任务中遇到显著领域差异时。\n    *   **顺序微调整个 PTM**：若对整个 PTM 进行顺序微调，则存在灾难性遗忘通用知识的风险。\n*   **问题核心**：上述两种方法都暴露了 CL 中关键的稳定性-可塑性权衡问题。\n\n### 提出的解决方案：持续学习前的适应 (ACL)\n\n*   **新颖框架**：我们提出了一种名为“持续学习前的适应 (ACL)”的新颖框架，旨在解决上述挑战。\n*   **工作原理**：ACL 在核心 CL 过程之前，通过一个即插即用的适应阶段来优化 PTM 主干。这意味着在利用现有 CL 方法（例如提示微调）学习每个新任务之前，先对 PTM 进行适应性调整。\n\n### ACL 的优势与机制\n\n*   **增强可塑性**：ACL 通过将嵌入与它们的原始类别原型对齐，同时使它们远离其他类别原型，从而增强了模型的可塑性。\n*   **平衡稳定性与可塑性**：理论和实证研究均表明，ACL 能够有效地平衡模型的稳定性和可塑性。\n\n### 实验结果\n\n*   **性能提升**：广泛的实验证明，ACL 显著提高了跨基准和集成方法的 CL 性能。\n*   **通用解决方案**：ACL 为基于 PTM 的持续学习提供了一个多功能的解决方案。",
      "shortSummary": "针对持续学习（CL）中预训练模型（PTM）面临的稳定性-可塑性权衡问题，本文提出了“持续学习前的适应”（ACL）框架。ACL 在核心CL过程前，通过即插即用的适应阶段优化PTM主干，增强模型可塑性，同时平衡稳定性。实验证明，ACL显著提升了CL性能，为基于PTM的CL提供了一种通用且有效的解决方案。",
      "translated_title": "持续学习前的适应",
      "images": [],
      "contentSource": "完整文章",
      "content": "Continual Learning (CL) seeks to enable neural networks to incrementally acquire new knowledge (plasticity) while retaining existing knowledge (stability). While pre-trained models (PTMs) have become pivotal in CL, prevailing approaches freeze the PTM backbone to preserve stability, limiting their plasticity, particularly when encountering significant domain gaps in incremental tasks. Conversely, sequentially finetuning the entire PTM risks catastrophic forgetting of generalizable knowledge, exposing a critical stability-plasticity trade-off. To address this challenge, we propose Adapting PTMs before the core CL process (ACL), a novel framework that refines the PTM backbone through a plug-and-play adaptation phase before learning each new task with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by aligning embeddings with their original class prototypes while distancing them from others, theoretically and empirically shown to balance stability and plasticity. Extensive experiments demonstrate that ACL significantly improves CL performance across benchmarks and integrated methods, offering a versatile solution for PTM-based CL."
    },
    {
      "title": "从架构视角重新思考持续学习中的稳定性-可塑性权衡 (原标题: Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective)",
      "link": "https://arxiv.org/abs/2506.03951",
      "pubDate": "Wed, 04 Jun 2025 09:40:41 GMT",
      "isoDate": "2025-06-04T09:40:41.000Z",
      "creator": "Aojun Lu, Hangjie Yuan, Tao Feng, Yanan Sun",
      "summary": "### 持续学习中稳定性-可塑性权衡的架构视角再思考\n\n**1. 持续学习（CL）与稳定性-可塑性困境**\n\n*   持续学习（CL）旨在赋予神经网络增量学习和适应的能力。\n*   核心挑战在于解决“稳定性-可塑性困境”，即在保留先前学习到的知识（稳定性）和获取新知识（可塑性）这两个相互冲突的目标之间取得平衡。\n\n**2. 现有方法的局限性**\n\n*   尽管许多CL方法致力于解决这一权衡问题，但它们通常忽视了网络架构对稳定性和可塑性的影响。\n*   这些方法往往将权衡限制在参数层面，未能充分利用架构层面的潜力。\n\n**3. 架构层面的发现**\n\n*   本文深入探讨了架构层面的稳定性与可塑性冲突。\n*   研究揭示，在相同的参数约束下：\n    *   **更深的网络**表现出更好的**可塑性**（即学习新知识的能力）。\n    *   **更宽的网络**则具有更优的**稳定性**（即保留旧知识的能力）。\n\n**4. 提出的解决方案：Dual-Arch 框架**\n\n*   为解决这一架构层面的困境，论文引入了一个名为 **Dual-Arch** 的新颖框架。\n*   Dual-Arch 可作为现有CL方法的即插即用组件。\n*   该框架的核心思想是利用两个独立网络的互补优势：\n    *   一个网络专门用于处理**可塑性**。\n    *   另一个网络专门用于处理**稳定性**。\n*   每个网络都设计有专门且轻量级的架构，以最大化其各自的目标。\n\n**5. 实验结果与优势**\n\n*   广泛的实验证明，Dual-Arch 能够显著增强现有CL方法的性能。\n*   同时，Dual-Arch 在参数方面更为紧凑，最多可减少87%的参数，从而提高了效率。",
      "shortSummary": "本文从架构视角重新思考持续学习中的稳定性-可塑性权衡。研究发现，在相同参数约束下，更深网络可塑性更优，而更宽网络稳定性更佳。为解决此架构困境，论文提出Dual-Arch框架，利用两个独立的轻量级网络分别处理可塑性和稳定性。实验表明，Dual-Arch能提升现有持续学习方法的性能，并显著减少参数，最高可达87%。",
      "translated_title": "从架构视角重新思考持续学习中的稳定性-可塑性权衡",
      "images": [],
      "contentSource": "完整文章",
      "content": "The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. While numerous CL methods aim to achieve this trade-off, they often overlook the impact of network architecture on stability and plasticity, restricting the trade-off to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Arch, which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments demonstrate that Dual-Arch enhances the performance of existing CL methods while being up to 87% more compact in terms of parameters."
    },
    {
      "title": "VisCoder：为可执行Python可视化代码生成微调大型语言模型 (原标题: VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation)",
      "link": "https://arxiv.org/abs/2506.03930",
      "pubDate": "Wed, 04 Jun 2025 09:24:44 GMT",
      "isoDate": "2025-06-04T09:24:44.000Z",
      "creator": "Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen",
      "summary": "大型语言模型（LLMs）在可视化任务（如绘制图表）中面临挑战，因为成功不仅依赖于代码的正确性，还依赖于视觉语义。现有的指令微调数据集缺乏基于执行的监督，并且对迭代代码修正的支持有限，导致生成的图表脆弱且不可靠。\n\n为了解决这些问题，研究人员提出了以下方案：\n\n*   **VisCode-200K数据集**：\n    *   这是一个大规模的指令微调数据集，专为Python可视化和自我修正设计。\n    *   包含超过20万个示例。\n    *   数据来源包括：\n        1.  来自开源代码库的经过验证的绘图代码，并配有自然语言指令和渲染后的图表。\n        2.  来自Code-Feedback的4.5万个多轮修正对话，使模型能够利用运行时反馈来修改错误代码。\n\n*   **VisCoder模型**：\n    *   通过在VisCode-200K数据集上对Qwen2.5-Coder-Instruct进行微调而创建。\n\n*   **评估与结果**：\n    *   VisCoder在PandasPlotBench上进行了评估。\n    *   结果显示，VisCoder显著优于强大的开源基线模型，并且性能接近专有模型如GPT-4o-mini。\n    *   研究团队还采用了自我调试评估协议来评估迭代修复能力，这进一步证明了反馈驱动学习对于生成可执行、视觉准确代码的益处。",
      "shortSummary": "大型语言模型在可视化任务中面临代码正确性和视觉语义挑战，现有数据集缺乏执行监督和迭代修正支持。为解决此问题，研究人员提出了VisCode-200K数据集和VisCoder模型。VisCode-200K包含20万+示例，涵盖验证绘图代码和多轮修正对话。VisCoder是基于Qwen2.5-Coder-Instruct微调而来，在PandasPlotBench上表现出色，显著超越开源基线，并接近GPT-4o-mini的性能，证明了反馈驱动学习在生成可执行、视觉准确代码方面的优势。",
      "translated_title": "VisCoder：为可执行Python可视化代码生成微调大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation."
    },
    {
      "title": "主动学习超参数调查：来自大规模实验网格的见解 (原标题: Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid)",
      "link": "https://arxiv.org/abs/2506.03817",
      "pubDate": "Wed, 04 Jun 2025 06:41:37 GMT",
      "isoDate": "2025-06-04T06:41:37.000Z",
      "creator": "Julius Gonsior, Tim Rieß, Anja Reusch, Claudio Hartmann, Maik Thiele, Wolfgang Lehner",
      "summary": "# 主动学习超参数调查：来自大规模实验网格的见解\n\n## 引言\n数据标注是监督机器学习中一项耗时且成本高昂的任务。主动学习（Active Learning, AL）是一种旨在通过迭代选择最具信息量的未标注样本进行专家标注，从而最大限度地减少人工标注工作并提高整体分类性能的方法。尽管AL技术已存在数十年，但在实际应用中仍鲜有使用。\n\n## 核心问题\n根据对NLP社区进行的两次关于AL的社区网络调查，阻碍从业者使用AL的两个主要原因持续存在：\n1.  设置AL的复杂性。\n2.  对其有效性缺乏信任。\n\n本研究假设这两个原因都源于同一个症结：AL巨大的超参数空间。这个大部分未被探索的超参数空间常常导致误导性和不可复现的AL实验结果。\n\n## 研究方法\n为了解决上述问题，本研究进行了迄今为止规模最大的AL研究，具体步骤如下：\n1.  **构建超参数网格：** 首次编译了一个包含超过460万种超参数组合的大规模超参数网格。\n2.  **性能记录：** 记录了所有这些组合的性能表现。\n3.  **影响分析：** 分析了实验结果中每个超参数的具体影响。\n\n## 主要发现与贡献\n本研究最终提供了以下见解和建议：\n*   **超参数影响建议：** 针对每个超参数的影响给出了具体建议。\n*   **策略实现影响：** 揭示了具体AL策略实现方式的惊人影响。\n*   **可复现实验设计：** 概述了一种实验研究设计，旨在以最小的计算量实现可复现的AL实验。\n\n## 研究意义\n本研究旨在通过提供关于超参数影响的深入见解和可复现的实验设计，促进未来AL研究的可复现性和可信度。",
      "shortSummary": "本研究调查了主动学习（AL）超参数，旨在解决其在实际应用中因设置复杂和缺乏信任而鲜被使用的问题。研究假设这源于AL巨大的、未被探索的超参数空间。通过构建包含超过460万种组合的大规模实验网格，本研究分析了各超参数的影响，并揭示了AL策略实现方式的意外作用。最终，研究提供了超参数使用建议，并提出了一种可复现且计算量小的AL实验设计，以提升AL研究的可信度。",
      "translated_title": "主动学习超参数调查：来自大规模实验网格的见解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future."
    },
    {
      "title": "VLMs 可以聚合分散的训练补丁 (原标题: VLMs Can Aggregate Scattered Training Patches)",
      "link": "https://arxiv.org/abs/2506.03614",
      "pubDate": "Wed, 04 Jun 2025 02:46:06 GMT",
      "isoDate": "2025-06-04T02:46:06.000Z",
      "creator": "Zhanhui Zhou, Lingjie Chen, Chao Yang, Chaochao Lu",
      "summary": "### 核心问题：VLMs 的数据审核规避风险\n\n视觉-语言模型（VLMs）在训练数据中面临一个重大风险：即使有害图像被分割成看似无害的小块并分散在多个训练样本中，VLMs 也能在训练过程中将这些碎片重新组合起来。这使得模型在推理时，无论是从完整图像还是文本引用，都可能生成有害响应，从而轻易绕过数据审核。\n\n例如，如果 VLM 训练时接收到来自血腥场景的图像补丁，并配以“安全”的描述，那么它之后可能会将完整的图像或对该场景的文本引用描述为“安全”。\n\n### 视觉拼接（Visual Stitching）能力\n\n文章将 VLM 具备的这种核心能力定义为“视觉拼接”——即整合分散在多个共享相同文本描述的训练样本中的视觉信息的能力。\n\n### 实验验证\n\n1.  **能力演示**：研究首先在常见的开源 VLMs 上演示了视觉拼接能力。实验使用了三个数据集，其中每张图像都带有一个唯一的合成 ID。\n2.  **训练过程**：研究将每个（图像，ID）对拆分为多个（补丁，ID）对，并以此对模型进行微调。\n3.  **实验结果**：结果发现，经过微调的模型能够从完整的图像或文本引用中正确地识别出对应的 ID，这证明了 VLMs 确实具备将分散的视觉信息拼接起来的能力。\n\n### 模拟对抗性数据投毒场景\n\n基于上述发现，研究模拟了前述的对抗性数据投毒场景：\n\n1.  **有害内容处理**：研究使用了来自危险图像的补丁，并将这些补丁的 ID 替换为“安全”或“不安全”等文本描述。\n2.  **风险揭示**：实验结果表明，有害内容确实可以以补丁的形式规避数据审核，并随后通过视觉拼接被 VLM 重建。这揭示了视觉拼接对 VLM 安全构成的严重风险。\n\n### 结论\n\nVLMs 的视觉拼接能力是一个严重的安全隐患，它允许模型在训练过程中从分散的、看似无害的图像补丁中学习并重构有害信息，从而规避现有的数据审核机制。相关研究代码已公开。",
      "shortSummary": "视觉-语言模型（VLMs）存在“视觉拼接”风险，即能将分散在多个训练样本中、看似无害的图像补丁重新组合，从而学习并生成有害内容，绕过数据审核。研究通过实验验证了这一能力，并模拟了对抗性数据投毒场景，揭示了其对VLM安全构成的严重威胁。这种能力使得有害内容能以碎片形式规避审查，并在模型内部被重建，对VLM的安全性构成挑战。",
      "translated_title": "VLMs 可以聚合分散的训练补丁",
      "images": [],
      "contentSource": "完整文章",
      "content": "One way to mitigate risks in vision-language models (VLMs) is to remove dangerous samples in their training data. However, such data moderation can be easily bypassed when harmful images are split into small, benign-looking patches, scattered across many training samples. VLMs may then learn to piece these fragments together during training and generate harmful responses at inference, either from full images or text references. For instance, if trained on image patches from a bloody scene paired with the descriptions \"safe,\" VLMs may later describe, the full image or a text reference to the scene, as \"safe.\" We define the core ability of VLMs enabling this attack as visual stitching -- the ability to integrate visual information spread across multiple training samples that share the same textual descriptions. In our work, we first demonstrate visual stitching abilities in common open-source VLMs on three datasets where each image is labeled with a unique synthetic ID: we split each (image, ID) pair into {(patch, ID)} pairs at different granularity for finetuning, and we find that tuned models can verbalize the correct IDs from full images or text reference. Building on this, we simulate the adversarial data poisoning scenario mentioned above by using patches from dangerous images and replacing IDs with text descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can evade moderation in patches and later be reconstructed through visual stitching, posing serious VLM safety risks. Code is available at https://github.com/ZHZisZZ/visual-stitching."
    },
    {
      "title": "MiMo-VL 技术报告 (原标题: MiMo-VL Technical Report)",
      "link": "https://arxiv.org/abs/2506.03569",
      "pubDate": "Wed, 04 Jun 2025 00:32:54 GMT",
      "isoDate": "2025-06-04T00:32:54.000Z",
      "creator": "Xiaomi LLM-Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia",
      "summary": "## MiMo-VL 技术报告：先进视觉语言模型的开源与评估\n\n本技术报告详细介绍了小米LLM-Core团队开源的MiMo-VL-7B-SFT和MiMo-VL-7B-RL两款强大的视觉语言模型。这些模型在通用视觉理解和多模态推理方面均展现出最先进的性能。\n\n### 核心性能亮点\n\n*   **MiMo-VL-7B-RL的卓越表现：**\n    *   在40项评估任务中的35项上超越了Qwen2.5-VL-7B。\n    *   在OlympiadBench上得分高达59.4，甚至超越了参数量高达78B的模型。\n    *   在GUI（图形用户界面）接地应用方面，于OSWorld-G数据集上取得了56.1的新标准，性能优于UI-TARS等专用模型。\n\n### 训练方法与关键发现\n\n*   **四阶段预训练：** 模型经历了2.4万亿（2.4 trillion）tokens的四阶段预训练。\n*   **混合在策略强化学习（MORL）：** 训练过程整合了多种奖励信号，采用了混合在策略强化学习（Mixed On-policy Reinforcement Learning）。\n*   **数据质量的重要性：** 研究发现，在预训练阶段整合包含长链式思考（Chain-of-Thought）的高质量推理数据至关重要。\n*   **混合RL的益处：** 尽管在同时优化多个领域时面临挑战，但混合强化学习带来了显著益处。\n\n### 研究贡献\n\n*   **开源模型：** 提供了MiMo-VL-7B-SFT和MiMo-VL-7B-RL的模型检查点。\n*   **综合评估套件：** 贡献了一个包含50多项任务的综合评估套件，旨在促进研究的可复现性并推动领域发展。\n*   所有模型检查点和完整的评估套件均已开源。",
      "shortSummary": "小米开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL视觉语言模型，在视觉理解和多模态推理上表现卓越。MiMo-VL-7B-RL在OlympiadBench和OSWorld-G等基准测试中超越了Qwen2.5-VL-7B及更大模型。其训练结合四阶段预训练和混合强化学习，强调高质量推理数据。团队还贡献了包含50多项任务的评估套件。",
      "translated_title": "MiMo-VL 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL."
    },
    {
      "title": "使用非对称双3D高斯泼溅在野外实现鲁棒神经渲染 (原标题: Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2506.03538",
      "pubDate": "Tue, 03 Jun 2025 23:40:33 GMT",
      "isoDate": "2025-06-03T23:40:33.000Z",
      "creator": "Chengqi Li, Zhihao Shi, Yangdi Lu, Wenbo He, Xiangyu Xu",
      "summary": "### 文章标题：使用非对称双3D高斯泼溅在野外实现鲁棒神经渲染\n\n本文介绍了一种名为“非对称双3D高斯泼溅”（Asymmetric Dual 3DGS）的新型框架，旨在解决在野外图像中进行3D重建所面临的挑战。这些挑战主要源于不一致的光照条件和瞬态干扰物，它们常常导致重建结果不稳定、不一致并产生视觉伪影。\n\n#### 核心问题与现有方法的局限性：\n\n*   **挑战性任务：** 在野外图像中进行3D重建是一项极具挑战性的任务。\n*   **主要障碍：** 不一致的光照和瞬态干扰物是导致重建质量低下的主要原因。\n*   **现有方法不足：** 现有方法通常依赖启发式策略来处理低质量训练数据，但难以生成稳定一致的重建结果，并常伴有视觉伪影。\n\n#### Asymmetric Dual 3DGS 框架：\n\nAsymmetric Dual 3DGS 利用了这些伪影的随机性（即它们在不同训练运行中因微小随机性而变化）来提升重建的鲁棒性。\n\n*   **并行训练与一致性约束：**\n    *   该方法并行训练两个3D高斯泼溅（3DGS）模型。\n    *   通过施加一致性约束，鼓励模型收敛于可靠的场景几何结构，同时抑制不一致的伪影。\n\n*   **发散掩蔽策略（Divergent Masking Strategy）：**\n    *   为防止两个模型因确认偏差而陷入相似的失败模式，引入了发散掩蔽策略。\n    *   该策略应用两种互补的掩码：\n        *   **多线索自适应掩码（Multi-cue adaptive mask）**\n        *   **自监督软掩码（Self-supervised soft mask）**\n    *   这两种掩码导致两个模型进行非对称的训练过程，从而减少了共享的错误模式。\n\n*   **动态EMA代理（Dynamic EMA Proxy）以提高效率：**\n    *   为了提高模型训练效率，引入了轻量级的“动态EMA代理”变体。\n    *   该变体用一个动态更新的指数移动平均（EMA）代理替换了两个模型中的一个。\n    *   采用交替掩蔽策略来保持模型的发散性，进一步提升了效率。\n\n#### 实验结果与结论：\n\n*   在具有挑战性的真实世界数据集上进行了广泛的实验。\n*   结果表明，Asymmetric Dual 3DGS 方法始终优于现有方法，并实现了高效率。\n*   代码和训练好的模型将发布。",
      "shortSummary": "Asymmetric Dual 3DGS 是一种新颖的神经渲染框架，旨在解决野外3D重建中因光照不一致和干扰物导致的伪影问题。它通过并行训练两个3DGS模型，并施加一致性约束来收敛可靠几何。引入发散掩蔽策略和动态EMA代理，确保训练过程非对称并提高效率，有效抑制伪影。实验证明，该方法在真实世界数据上表现优异且高效，显著提升了鲁棒性。",
      "translated_title": "使用非对称双3D高斯泼溅在野外实现鲁棒神经渲染",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released."
    },
    {
      "title": "DenseDPO：视频扩散模型的细粒度时间偏好优化 (原标题: DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.03517",
      "pubDate": "Tue, 03 Jun 2025 23:06:08 GMT",
      "isoDate": "2025-06-03T23:06:08.000Z",
      "creator": "Ziyi Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ashkan Mirzaei, Igor Gilitschenski, Sergey Tulyakov, Aliaksandr Siarohin",
      "summary": "# DenseDPO：视频扩散模型的细粒度时间偏好优化\n\n## 现有DPO方法的局限性\n\n*   **训练数据获取方式：** 当前的直接偏好优化（DPO）方法在文本到视频扩散模型中，通过让标注者在由独立噪声生成的两个视频之间提供偏好来获取训练数据。\n*   **缺乏细粒度比较：** 这种方法限制了细粒度的比较。\n*   **运动偏见：** 它会导致标注者偏向于低运动片段，因为这些片段通常包含较少的视觉伪影。\n\n## DenseDPO的贡献\n\nDenseDPO旨在解决上述缺点，主要有以下三点贡献：\n\n1.  **视频对的创建：**\n    *   DenseDPO通过对真实视频的损坏副本进行去噪来创建DPO的每个视频对。\n    *   这产生了具有相似运动结构但局部细节不同的对齐视频对，从而有效地中和了运动偏见。\n2.  **细粒度偏好标注：**\n    *   该方法利用由此产生的时序对齐，在短视频片段而非整个视频剪辑上标注偏好。\n    *   这产生了更密集、更精确的学习信号。\n3.  **自动化偏好标注：**\n    *   DenseDPO支持使用现成的视觉语言模型（VLM）进行自动化偏好标注。\n    *   GPT能够准确预测片段级别的偏好，其效果与专门为任务微调的视频奖励模型相似。\n    *   使用这些自动标签训练的DenseDPO模型，其性能接近使用人类标签训练的模型。\n\n## 性能提升\n\n*   仅使用三分之一的标注数据，DenseDPO在运动生成方面相比传统的DPO有显著提升。\n*   同时，它在文本对齐、视觉质量和时间一致性方面与传统DPO持平。",
      "shortSummary": "DenseDPO是一种针对视频扩散模型的细粒度时间偏好优化方法，旨在解决现有DPO中存在的运动偏见和粗粒度比较问题。它通过对齐的视频对进行去噪，并在短片段上标注偏好，从而提供更密集、更精确的学习信号。此外，DenseDPO还支持使用视觉语言模型进行自动化偏好标注。实验表明，DenseDPO仅用三分之一的数据就能显著改善运动生成，同时保持文本对齐、视觉质量和时间一致性。",
      "translated_title": "DenseDPO：视频扩散模型的细粒度时间偏好优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Direct Preference Optimization (DPO) has recently been applied as a post-training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one-third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels."
    },
    {
      "title": "RefEdit：一个用于改进基于指令的图像编辑模型在指代表达方面的基准和方法 (原标题: RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions)",
      "link": "https://arxiv.org/abs/2506.03448",
      "pubDate": "Tue, 03 Jun 2025 19:20:24 GMT",
      "isoDate": "2025-06-03T19:20:24.000Z",
      "creator": "Bimsara Pathiraja, Maitreya Patel, Shivam Singh, Yezhou Yang, Chitta Baral",
      "summary": "### 背景与问题\n尽管基于指令的图像编辑技术近期取得了进展，但现有方法主要擅长编辑单个突出对象，在处理包含多个实体的复杂场景时表现显著不足。\n\n### RefEdit-Bench：新基准\n为了量化这一差距，研究首先引入了RefEdit-Bench。这是一个基于RefCOCO的严格真实世界基准，即使在数百万样本上训练的基线模型，在该基准上的表现也十分糟糕。\n\n### RefEdit：新方法\n为克服上述局限性，研究引入了RefEdit——一个基于指令的编辑模型。该模型通过可扩展的合成数据生成管道进行训练，仅使用了20,000个编辑三元组。\n\n### 性能表现\n\n*   RefEdit的性能超越了在数百万数据上训练的Flux/SD3模型基线。\n*   在各种基准测试中进行了广泛评估，结果表明RefEdit不仅在指代表达任务中表现出色，还提升了传统基准的性能。\n*   RefEdit取得了与闭源方法相当的最新（SOTA）结果。\n\n### 可复现性\n为了确保研究的可复现性，研究团队已发布了相关数据和检查点。",
      "shortSummary": "针对现有基于指令的图像编辑模型在复杂多实体场景中表现不佳的问题，研究引入了RefEdit-Bench基准和RefEdit模型。RefEdit模型通过可扩展的合成数据管道，仅用2万个编辑三元组训练，却在指代表达任务上显著超越了在数百万数据上训练的Flux/SD3等基线模型，并在多个基准测试中达到了最先进的性能，解决了复杂场景下的图像编辑挑战。",
      "translated_title": "RefEdit：一个用于改进基于指令的图像编辑模型在指代表达方面的基准和方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\&amp; checkpoint for reproducibility."
    }
  ],
  "lastUpdated": "2025-06-05T09:34:30.138Z"
}