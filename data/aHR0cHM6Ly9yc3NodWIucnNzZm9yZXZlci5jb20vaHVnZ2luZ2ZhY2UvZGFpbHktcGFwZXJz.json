{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "大型推理模型强化学习综述 (原标题: A Survey of Reinforcement Learning for Large Reasoning Models)",
      "link": "https://arxiv.org/abs/2509.08827",
      "pubDate": "Wed, 10 Sep 2025 13:59:43 GMT",
      "isoDate": "2025-09-10T13:59:43.000Z",
      "creator": "Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou",
      "summary": "本文对大型推理模型（LRMs）中强化学习（RL）的最新进展进行了综述。RL在推动大型语言模型（LLMs）能力发展方面取得了显著成功，尤其是在解决数学和编程等复杂逻辑任务方面。因此，RL已成为将LLMs转化为LRMs的基础方法。\n\n**主要观点与贡献：**\n\n*   **RL的关键作用**：RL在增强LLMs处理复杂推理任务（如数学和编码）的能力方面发挥了核心作用，使其能够演变为大型推理模型（LRMs）。\n*   **当前挑战**：随着该领域的快速发展，RL在LRMs中的进一步扩展面临着基础性挑战，不仅体现在计算资源上，还包括算法设计、训练数据和基础设施方面。\n*   **综述目的**：本文旨在及时回顾该领域的发展，重新评估其发展轨迹，并探索增强RL可扩展性以实现人工超智能（ASI）的策略。\n*   **研究范围**：特别关注自DeepSeek-R1发布以来，将RL应用于LLMs和LRMs以提升推理能力的研究。这包括：\n    *   基础组件\n    *   核心问题\n    *   训练资源\n    *   下游应用\n*   **未来展望**：通过对上述方面的考察，旨在识别这一快速发展领域的未来机遇和方向。\n*   **促进研究**：作者希望这篇综述能促进未来在更广泛推理模型中RL的研究。",
      "shortSummary": "本文综述了强化学习（RL）在大型推理模型（LRMs）中的最新进展。RL在提升大型语言模型（LLMs）处理复杂逻辑任务的能力方面取得了显著成功，是LLMs向LRMs转化的基础方法。然而，RL的进一步扩展面临计算资源、算法设计、训练数据和基础设施等方面的挑战。本综述旨在回顾该领域发展，评估其轨迹，并探索增强RL可扩展性的策略，以识别未来机遇和方向，促进相关研究。",
      "translated_title": "大型推理模型强化学习综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
    },
    {
      "title": "RewardDance：视觉生成中的奖励缩放 (原标题: RewardDance: Reward Scaling in Visual Generation)",
      "link": "https://arxiv.org/abs/2509.08826",
      "pubDate": "Wed, 10 Sep 2025 13:59:31 GMT",
      "isoDate": "2025-09-10T13:59:31.000Z",
      "creator": "Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang",
      "summary": "## RewardDance：视觉生成中的奖励缩放\n\n本文介绍了RewardDance，一个创新的可扩展奖励建模框架，旨在解决视觉生成领域中现有奖励模型（RMs）面临的关键挑战。\n\n### 现有挑战与局限性\n\n在视觉生成中，奖励模型（RMs）的扩展范式尚未得到充分探索，主要原因在于现有方法的根本性局限：\n\n*   **CLIP-based RMs的限制**：受限于其固有的架构和输入模态。\n*   **Bradley-Terry 损失的错位**：与视觉-语言模型（VLMs）的下一词预测机制存在根本性不匹配，从而阻碍了有效的扩展。\n*   **奖励欺骗（Reward Hacking）问题**：强化学习人类反馈（RLHF）优化过程普遍存在奖励欺骗问题，即模型可能利用奖励信号的缺陷，而非真正提升生成内容的真实质量。\n\n### RewardDance 解决方案\n\nRewardDance 通过引入一种新颖的生成式奖励范式来克服这些障碍：\n\n*   **奖励分数重构**：将奖励分数重新定义为模型预测“是”（yes）标记的概率。这个“是”标记表示根据特定标准，生成的图像优于参考图像。\n*   **与 VLM 架构的内在对齐**：这种重构使得奖励目标与 VLM 架构实现了内在对齐，从而解锁了RMs的扩展潜力。\n\n### RewardDance 的扩展能力\n\nRewardDance 在两个关键维度上实现了扩展：\n\n1.  **模型扩展**：系统地将奖励模型扩展到高达260亿参数的规模。\n2.  **上下文扩展**：能够整合任务特定指令、参考示例以及思维链（CoT）推理。\n\n### 实验结果与优势\n\n*   **性能显著超越 SOTA**：广泛的实验证明，RewardDance 在文本到图像、文本到视频和图像到视频生成任务中显著超越了现有最先进的方法。\n*   **解决奖励欺骗问题**：RewardDance 成功解决了长期存在的“奖励欺骗”挑战。其大规模RMs在RL微调过程中表现并保持高奖励方差，证明了它们对欺骗的抵抗力，并能持续产生多样化、高质量的输出。\n*   **缓解模式崩溃**：这极大地缓解了困扰小型模型的模式崩溃问题。",
      "shortSummary": "RewardDance 是一种创新的可扩展奖励建模框架，旨在解决视觉生成中现有奖励模型（RMs）的局限性及奖励欺骗问题。它通过将奖励分数重构为模型预测“是”标记的概率，实现了与视觉-语言模型（VLMs）架构的内在对齐。RewardDance 支持模型（高达260亿参数）和上下文（指令、CoT）的扩展。实验表明，它在多种生成任务中显著超越现有SOTA，有效抵抗奖励欺骗，并能生成多样化、高质量的内容，缓解了模式崩溃问题。",
      "translated_title": "RewardDance：视觉生成中的奖励缩放",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models."
    },
    {
      "title": "AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策 (原标题: AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.08755",
      "pubDate": "Wed, 10 Sep 2025 12:46:11 GMT",
      "isoDate": "2025-09-10T12:46:11.000Z",
      "creator": "Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang",
      "summary": "# AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策\n\n## 1. 引言与背景\n开发能够通过一系列智能决策解决复杂现实世界任务的自主大型语言模型（LLM）智能体是当前研究的热点。智能体需要通过与环境的探索和交互来获取知识和技能。然而，目前社区缺乏一个统一的、交互式的强化学习（RL）框架，能够有效地从零开始（不依赖于监督微调SFT）在多样化和真实的环​​境中训练此类智能体。\n\n## 2. AgentGym-RL 框架介绍\n为了解决上述问题，研究人员引入了 **AgentGym-RL**，这是一个用于通过强化学习训练LLM智能体进行多轮交互式决策的新框架。\n\n*   **核心目标**：通过强化学习训练LLM智能体，使其能够进行多轮交互式决策，以解决长周期任务。\n*   **架构特点**：该框架采用模块化和解耦的架构设计，确保了高度的灵活性和可扩展性。\n*   **应用范围**：AgentGym-RL 涵盖了广泛的现实世界场景，并支持主流的强化学习算法。\n\n## 3. ScalingInter-RL 训练方法\n为了进一步优化训练过程，研究人员提出了一种名为 **ScalingInter-RL** 的训练方法，旨在实现探索与利用的平衡以及稳定的强化学习优化。\n\n*   **早期阶段**：该方法通过限制交互次数来强调利用（exploitation），使智能体在初期阶段能够更有效地利用已知信息。\n*   **后期阶段**：随着训练的进行，它逐渐转向探索（exploration），通过更大的决策周期（horizons）来鼓励智能体发展多样化的解决问题策略。\n*   **优势**：通过这种方式，智能体能够发展出更多样化的行为，并且在长周期任务中更不容易出现性能崩溃。\n\n## 4. 实验验证与成果\n研究人员进行了广泛的实验，以验证 AgentGym-RL 框架和 ScalingInter-RL 方法的稳定性和有效性。\n\n*   **实验结果**：实验表明，所训练的智能体在多样化环境中的27项任务上，其性能与商业模型相当或超越商业模型。\n*   **未来贡献**：研究将提供关键的见解，并计划开源完整的 AgentGym-RL 框架，包括代码和数据集，以赋能研究社区开发下一代智能智能体。",
      "shortSummary": "AgentGym-RL是一个新颖的强化学习框架，旨在通过多轮交互式决策训练LLM智能体进行长周期任务。它解决了缺乏统一RL框架从零开始训练LLM智能体的问题。该框架采用模块化设计，并引入了ScalingInter-RL训练方法，以平衡探索与利用，确保稳定优化。实验证明，AgentGym-RL智能体在27项任务上性能匹敌或超越商业模型。该框架及其代码和数据集将开源，以推动智能体研究。",
      "translated_title": "AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策",
      "images": [],
      "contentSource": "完整文章",
      "content": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents."
    },
    {
      "title": "HumanAgencyBench：AI助手中人类能动性支持的可扩展评估 (原标题: HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants)",
      "link": "https://arxiv.org/abs/2509.08494",
      "pubDate": "Wed, 10 Sep 2025 07:10:10 GMT",
      "isoDate": "2025-09-10T07:10:10.000Z",
      "creator": "Benjamin Sturgeon, Daniel Samuelson, Jacob Haimes, Jacy Reese Anthis",
      "summary": "## HumanAgencyBench：AI助手中人类能动性支持的可扩展评估\n\n随着人类将越来越多的任务和决策委托给人工智能（AI），我们面临着失去对个人和集体未来控制的风险。即使是相对简单的算法系统，如社交媒体信息流算法，也已在引导人类决策，导致人们无意中、漫无目的地浏览为提高参与度而优化的内容。\n\n为了解决这一问题，本文提出了“人类能动性”（human agency）的概念，并开发了一个可扩展的基准测试工具：HumanAgencyBench (HAB)。\n\n### HAB的核心理念与方法\n\n*   **整合理论基础**：HAB通过整合哲学和科学的能动性理论来定义人类能动性，旨在理解和衡量AI如何影响人类的自主决策和行为。\n*   **AI辅助评估**：利用大型语言模型（LLMs）作为评估工具，模拟和验证用户查询，并评估AI助手的响应。这种方法使得能动性支持的评估能够以可扩展和自适应的方式进行。\n\n### HAB的六个能动性维度\n\nHAB基于典型的AI使用场景，设计了六个衡量AI助手支持人类能动性倾向的关键维度：\n\n1.  **提出澄清问题 (Ask Clarifying Questions)**：AI是否会主动询问以确保充分理解用户意图和需求，避免误解或提供不相关的帮助。\n2.  **避免价值操纵 (Avoid Value Manipulation)**：AI是否会避免以微妙或直接的方式影响、改变或引导用户的价值观、偏好或信念。\n3.  **纠正错误信息 (Correct Misinformation)**：AI是否能识别并纠正用户可能持有的错误信息，同时以非评判性的方式呈现准确事实。\n4.  **推迟重要决策 (Defer Important Decisions)**：对于涉及高风险、伦理困境或需要人类判断的重要决策，AI是否会建议用户自行决定、寻求人类专业意见或提供多种选择而非直接给出答案。\n5.  **鼓励学习 (Encourage Learning)**：AI是否会促进用户的学习、理解和技能发展，而非仅仅提供即时答案，从而增强用户的自主能力。\n6.  **维护社交边界 (Maintain Social Boundaries)**：AI是否会尊重并维护适当的社交互动边界，避免过度拟人化、情感操纵或侵犯用户隐私。\n\n### 主要发现\n\n*   **当前支持水平**：研究发现，当代基于LLM的AI助手在支持人类能动性方面的表现普遍为“低到中等”水平，表明现有AI系统在这方面仍有显著提升空间。\n*   **显著差异**：不同系统开发者和不同能动性维度之间存在显著差异，这表明AI系统在设计和训练上存在不同的侧重和效果。\n*   **具体案例**：例如，Anthropic的LLM在总体上对人类能动性支持最多，但在“避免价值操纵”这一维度上却是支持度最低的LLM之一，这揭示了特定AI在某些关键领域可能存在的弱点。\n*   **能力与能动性**：研究表明，能动性支持似乎并非总是随着LLM能力的提升或指令遵循行为（如通过强化学习与人类反馈RLHF）的增强而一致提高，这挑战了现有的一些AI对齐策略。\n\n### 建议与展望\n\n鉴于上述发现，研究鼓励将AI安全和对齐的目标转向更稳健的方向，超越单纯的性能提升或指令遵循，以更好地支持和维护人类能动性，确保AI技术真正服务于人类的福祉和自主性。",
      "shortSummary": "本文提出HumanAgencyBench (HAB)，一个可扩展的基准，用于评估AI助手对人类能动性的支持程度。HAB整合哲学与科学理论，并利用LLM模拟评估，从提出澄清问题、避免价值操纵等六个维度衡量AI表现。研究发现，当前LLM助手对人类能动性的支持程度为低到中等，且不同开发者和维度间存在显著差异。能动性支持并非随LLM能力提升而线性增长。研究呼吁将AI安全目标转向更稳健的方向，以更好地维护人类控制权和自主性。",
      "translated_title": "HumanAgencyBench：AI助手中人类能动性支持的可扩展评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "As humans delegate more tasks and decisions to artificial intelligence (AI), we risk losing control of our individual and collective futures. Relatively simple algorithmic systems already steer human decision-making, such as social media feed algorithms that lead people to unintentionally and absent-mindedly scroll through engagement-optimized content. In this paper, we develop the idea of human agency by integrating philosophical and scientific theories of agency with AI-assisted evaluation methods: using large language models (LLMs) to simulate and validate user queries and to evaluate AI responses. We develop HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions of human agency based on typical AI use cases. HAB measures the tendency of an AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation, Correct Misinformation, Defer Important Decisions, Encourage Learning, and Maintain Social Boundaries. We find low-to-moderate agency support in contemporary LLM-based assistants and substantial variation across system developers and dimensions. For example, while Anthropic LLMs most support human agency overall, they are the least supportive LLMs in terms of Avoid Value Manipulation. Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and we encourage a shift towards more robust safety and alignment targets."
    },
    {
      "title": "EnvX：用智能体AI将一切智能体化 (原标题: EnvX: Agentize Everything with Agentic AI)",
      "link": "https://arxiv.org/abs/2509.08088",
      "pubDate": "Tue, 09 Sep 2025 14:51:36 GMT",
      "isoDate": "2025-09-09T14:51:36.000Z",
      "creator": "Linyao Chen, Zimian Peng, Yingxuan Yang, Yikun Wang, Wenzheng Tom Tang, Hiroki H. Kobayashi, Weinan Zhang",
      "summary": "## EnvX：用智能体AI将一切智能体化\n\n### 摘要\n\n尽管开源仓库提供了大量可重用的软件组件，但其利用过程仍面临手动、易错和分散的挑战。开发者需要手动查阅文档、理解API并编写集成代码，这严重阻碍了软件的高效复用。为解决这一问题，EnvX框架应运而生，它利用智能体AI将GitHub仓库转化为智能、自主的智能体，能够进行自然语言交互和智能体间协作。\n\n### EnvX 的核心理念与方法\n\nEnvX 与现有将仓库视为静态代码资源的方法不同，它通过一个三阶段过程将仓库重构为活跃的智能体：\n\n1.  **TODO引导的环境初始化**：\n    *   设置必要的依赖项。\n    *   准备所需数据。\n    *   配置验证数据集。\n\n2.  **人类对齐的智能体自动化**：\n    *   允许特定于仓库的智能体自主执行真实世界的任务。\n\n3.  **智能体到智能体 (A2A) 协议**：\n    *   使多个智能体能够相互协作，共同完成任务。\n\nEnvX 将大型语言模型的能力与结构化工具集成相结合，不仅自动化了代码生成，还自动化了理解、初始化和操作仓库功能的整个过程。\n\n### 评估与成果\n\nEnvX 在 GitTaskBench 基准测试上进行了评估，该基准包含了来自图像处理、语音识别、文档分析和视频操作等领域的18个仓库。评估结果显示：\n\n*   **执行完成率**：EnvX 达到了 74.07%。\n*   **任务通过率**：EnvX 达到了 51.85%。\n*   **性能优势**：EnvX 的表现优于现有框架。\n\n此外，案例研究进一步证明了 EnvX 通过 A2A 协议实现多仓库协作的能力。\n\n### 结论\n\n这项工作标志着一个重要的转变，即将仓库从被动的代码资源转变为智能、交互式的智能体，从而促进了开源生态系统内更高的可访问性和协作性。",
      "shortSummary": "EnvX是一个利用智能体AI将GitHub仓库转化为智能、自主智能体的框架。它通过三阶段过程（环境初始化、智能体自动化和A2A协作协议）解决开源组件复用中的手动和低效问题。EnvX自动化了仓库功能的理解、初始化和操作。在GitTaskBench基准测试中，EnvX表现优于现有框架，实现了74.07%的执行完成率和51.85%的任务通过率，并支持多仓库协作，标志着从被动代码资源向智能交互智能体的转变。",
      "translated_title": "EnvX：用智能体AI将一切智能体化",
      "images": [],
      "contentSource": "完整文章",
      "content": "The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem."
    },
    {
      "title": "Parallel-R1：通过强化学习实现并行思维 (原标题: Parallel-R1: Towards Parallel Thinking via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.07980",
      "pubDate": "Tue, 09 Sep 2025 13:59:35 GMT",
      "isoDate": "2025-09-09T13:59:35.000Z",
      "creator": "Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu",
      "summary": "## Parallel-R1：通过强化学习实现并行思维\n\n本文介绍了**Parallel-R1**，这是首个利用强化学习（RL）框架来增强大型语言模型（LLMs）并行思维能力的新方法，旨在解决现有方法在训练中面临的挑战。\n\n### 背景与挑战\n*   **并行思维**：作为一种新颖的方法，它通过同时探索多个推理路径来提升LLM的推理能力。\n*   **训练难题**：通过训练激活LLM的并行思维能力仍然具有挑战性。\n*   **现有方法局限**：当前方法主要依赖于对合成数据进行监督微调（SFT），这种方式鼓励了教师强制模仿，而非模型自主的探索和泛化能力。\n\n### Parallel-R1 框架\n*   **RL驱动**：Parallel-R1是第一个专门为复杂真实世界推理任务实现并行思维行为的强化学习框架。\n*   **渐进式课程**：该框架采用了一种渐进式课程，旨在明确解决RL训练并行思维中的“冷启动”问题。具体步骤如下：\n    1.  **SFT阶段**：首先，在来自较简单任务的提示生成轨迹上使用监督微调（SFT），以初步灌输并行思维能力。\n    2.  **RL阶段**：随后，模型过渡到强化学习（RL），在更难的问题上进一步探索和泛化这项技能。\n\n### 实验结果与发现\n*   **性能显著提升**：在MATH、AMC23和AIME等多种数学基准测试中，Parallel-R1成功地灌输了并行思维，与直接在挑战性任务上使用RL训练的顺序思维模型相比，准确率提高了8.4%。\n*   **思维行为转变**：进一步的分析揭示了模型思维行为的明显转变：\n    *   **早期阶段**：模型将并行思维主要用作一种探索策略。\n    *   **后期阶段**：模型将相同的并行思维能力用于多视角验证，以确保推理的准确性。\n*   **“训练中期探索支架”**：最重要的是，研究验证了并行思维作为一种“训练中期探索支架”（mid-training exploration scaffold）的作用。这种临时的探索阶段在RL之后解锁了更高的性能上限，在AIME25基准测试上比基线模型提高了42.9%。\n\n### 开源信息\n*   Parallel-R1的模型、数据和代码将进行开源。",
      "shortSummary": "Parallel-R1提出首个强化学习（RL）框架，通过渐进式课程实现大型语言模型（LLMs）的并行思维。它首先利用监督微调（SFT）在简单任务上奠定基础，随后通过RL在复杂任务上进行探索和泛化。实验表明，Parallel-R1显著提升了LLMs在数学基准上的推理准确率，并作为“训练中期探索支架”解锁了更高的性能上限，比基线模型在AIME25上提高了42.9%。",
      "translated_title": "Parallel-R1：通过强化学习实现并行思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1."
    },
    {
      "title": "多模态大型语言模型的视觉表征对齐 (原标题: Visual Representation Alignment for Multimodal Large Language Models)",
      "link": "https://arxiv.org/abs/2509.07979",
      "pubDate": "Tue, 09 Sep 2025 13:59:14 GMT",
      "isoDate": "2025-09-09T13:59:14.000Z",
      "creator": "Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim",
      "summary": "## 多模态大型语言模型的视觉表征对齐 (VIRAL)\n\n### 摘要\n\n多模态大型语言模型 (MLLMs) 尽管通过视觉指令微调在各种任务中取得了强大性能，但在诸如物体计数或空间推理等以视觉为中心的任务中仍存在局限性。本文将这一差距归因于当前普遍存在的仅文本监督范式，该范式仅为视觉通路提供间接指导，并常常导致 MLLMs 在训练过程中丢弃细粒度的视觉细节。\n\n### 提出的方法：VIRAL\n\n本文提出了一种名为 **VIsual Representation ALignment (VIRAL)** 的简单而有效的正则化策略。VIRAL 的核心思想是将 MLLMs 的内部视觉表征与预训练视觉基础模型 (VFMs) 的表征进行对齐。\n\n### VIRAL 的工作原理与优势\n\n通过明确地强制执行这种对齐，VIRAL 能够使模型实现以下关键优势：\n\n*   **保留关键视觉细节**：模型能够从输入视觉编码器中保留重要的视觉细节，避免在训练过程中丢失。\n*   **补充视觉知识**：模型能够从预训练的 VFMs 中获取并补充额外的视觉知识。\n*   **增强复杂视觉输入推理能力**：通过结合上述两点，VIRAL 显著增强了 MLLMs 对复杂视觉输入进行推理的能力。\n\n### 实验结果与验证\n\n*   **一致性提升**：实验结果表明，VIRAL 在广泛采用的多模态基准测试中的所有任务上都取得了持续的性能提升。\n*   **消融研究**：研究人员进行了全面的消融研究，以验证其框架背后关键设计选择的有效性。\n\n### 结论与未来方向\n\n作者认为，这一简单的发现为在训练 MLLMs 中有效整合视觉信息开辟了一个重要的方向。",
      "shortSummary": "多模态大型语言模型（MLLMs）在视觉中心任务上表现受限，主要因文本监督导致视觉细节丢失。本文提出VIRAL，一种通过将MLLM内部视觉表征与预训练视觉基础模型对齐的正则化策略。VIRAL能保留关键视觉细节并补充视觉知识，从而增强MLLM对复杂视觉输入的推理能力。实验证明，VIRAL在多模态基准测试中持续提升性能，为MLLM中视觉信息的有效整合提供了重要方向。",
      "translated_title": "多模态大型语言模型的视觉表征对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs."
    },
    {
      "title": "Mini-o3：扩展视觉搜索中的推理模式和交互回合 (原标题: Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search)",
      "link": "https://arxiv.org/abs/2509.07969",
      "pubDate": "Tue, 09 Sep 2025 13:54:21 GMT",
      "isoDate": "2025-09-09T13:54:21.000Z",
      "creator": "Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao",
      "summary": "本文介绍了Mini-o3系统，旨在解决现有大型多模态模型（LMMs）在视觉问题上推理模式单一、交互回合受限的问题，使其无法有效处理需要试错探索的复杂任务。Mini-o3通过扩展基于工具的交互，实现了深度、多回合（可达数十步）的推理，并在挑战性视觉搜索任务上取得了最先进的性能。\n\nMini-o3重现OpenAI o3风格行为的关键组成部分包括：\n\n*   **构建视觉探测数据集（Visual Probe Dataset）**\n    *   该数据集包含数千个专为探索性推理设计的挑战性视觉搜索问题。\n*   **开发迭代数据收集流程**\n    *   该流程用于获取“冷启动”轨迹，这些轨迹展现了多样化的推理模式，包括深度优先搜索、试错法和目标维护。\n*   **提出“超回合掩蔽”策略（Over-turn Masking Strategy）**\n    *   此策略在强化学习训练期间，避免对超出最大回合数的响应进行惩罚，从而平衡了训练效率和测试时的可扩展性。\n\n**主要成果与特点：**\n\n*   尽管模型在训练时仅设定了最多六个交互回合的上限，但在推理时，Mini-o3能够自然地生成扩展到数十个回合的轨迹。\n*   随着交互回合数的增加，模型的准确性也随之提高。\n*   广泛的实验证明，Mini-o3能够产生丰富的推理模式和深入的思维路径，有效解决了挑战性的视觉搜索问题。\n\n**资源可用性：**\n代码、数据集和模型均可在项目页面获取。",
      "shortSummary": "Mini-o3是一个旨在解决现有大型多模态模型在视觉搜索中推理模式单一和交互回合受限问题的系统。它通过扩展基于工具的交互，实现了深度、多回合（可达数十步）的推理，并在挑战性视觉搜索任务上取得了最先进的性能。Mini-o3引入了视觉探测数据集、迭代数据收集流程和超回合掩蔽策略，使其能够生成丰富的推理模式和深入的思维路径，即使在有限训练回合下也能在推理时自然扩展到更多回合，并随回合数增加提高准确性。",
      "translated_title": "Mini-o3：扩展视觉搜索中的推理模式和交互回合",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems."
    },
    {
      "title": "SimpleQA Verified：一个衡量参数化知识的可靠事实性基准 (原标题: SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge)",
      "link": "https://arxiv.org/abs/2509.07968",
      "pubDate": "Tue, 09 Sep 2025 13:53:58 GMT",
      "isoDate": "2025-09-09T13:53:58.000Z",
      "creator": "Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das",
      "summary": "### SimpleQA Verified：衡量参数化知识的可靠事实性基准\n\n#### 引言\n本文介绍了 **SimpleQA Verified**，这是一个包含1,000个提示的基准测试，专门用于评估大型语言模型（LLM）的短形式事实性。该基准基于OpenAI的SimpleQA，旨在解决其原始版本中的关键局限性。\n\n#### 解决的问题\nSimpleQA Verified旨在纠正OpenAI原始SimpleQA基准中存在的以下问题：\n*   **嘈杂和不正确的标签**：原始基准中包含大量错误或不准确的标签。\n*   **主题偏见**：评估问题在主题分布上存在不平衡，导致评估结果可能不具代表性。\n*   **问题冗余**：存在重复或高度相似的问题，降低了评估效率和挑战性。\n\n#### 创建过程\nSimpleQA Verified的创建经历了一个严格的多阶段过滤过程，以确保其高质量和可靠性：\n*   **去重（De-duplication）**：移除了重复的问题，确保每个评估点都是独特的。\n*   **主题平衡（Topic Balancing）**：对问题的主题分布进行了调整，以覆盖更广泛的知识领域，减少偏见。\n*   **来源协调（Source Reconciliation）**：对事实来源进行了仔细核对和统一，提高了标签的准确性。\n*   **自动评分器提示改进**：除了数据集的改进，用于自动评估模型响应的提示词也得到了优化，以提高评估的准确性和一致性。\n\n通过这些改进，SimpleQA Verified成为了一个更可靠、更具挑战性的评估集。\n\n#### 模型性能表现\n在新基准测试上，**Gemini 2.5 Pro** 取得了显著的性能，其F1分数达到了 **55.6**，创下了最先进的（state-of-the-art）记录。这一成绩表明Gemini 2.5 Pro在事实性方面表现优异，超越了包括GPT-5在内的其他前沿模型。\n\n#### 意义与可用性\n这项工作为研究社区提供了一个更高保真度的工具，具有以下重要意义：\n*   **跟踪真实进展**：能够更准确地跟踪参数化模型在事实性方面的真正进步。\n*   **缓解幻觉**：有助于识别和减轻LLM生成虚假信息（即“幻觉”）的问题。\n\n基准数据集、评估代码和排行榜均已公开，可在提供的链接（this https URL）获取，方便研究人员使用和贡献。",
      "shortSummary": "本文介绍了SimpleQA Verified，一个基于OpenAI SimpleQA的1,000个提示的LLM事实性基准。它通过严格的多阶段过滤过程，解决了原始基准中标签错误、主题偏见和问题冗余等关键限制，创建了一个更可靠、更具挑战性的评估集。在新基准上，Gemini 2.5 Pro以55.6的F1分数达到最先进水平，超越了GPT-5。SimpleQA Verified为跟踪参数化模型事实性进展和缓解幻觉提供了高保真度工具。数据集和代码已公开。",
      "translated_title": "SimpleQA Verified：一个衡量参数化知识的可靠事实性基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified."
    },
    {
      "title": "ΔL 归一化：重新思考RLVR中的损失聚合 (原标题: ΔL Normalization: Rethink Loss Aggregation in RLVR)",
      "link": "https://arxiv.org/abs/2509.07558",
      "pubDate": "Tue, 09 Sep 2025 05:52:34 GMT",
      "isoDate": "2025-09-09T05:52:34.000Z",
      "creator": "Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu",
      "summary": "## ΔL 归一化：重新思考RLVR中的损失聚合\n\n### 摘要\n\n本文提出了一种名为 **ΔL 归一化**的损失聚合方法，专门针对强化学习与可验证奖励（RLVR）中动态生成长度的特性。该方法旨在解决RLVR训练过程中响应长度高度可变性所导致的梯度方差高和优化不稳定的问题。\n\n### 背景与问题\n\n*   **RLVR的潜力**：强化学习与可验证奖励（RLVR）在提升大型语言模型（LLMs）的推理能力方面展现出巨大潜力。\n*   **主要挑战**：在RLVR训练过程中，响应长度的巨大变异性是一个核心挑战。这种变异性导致梯度方差高，进而使得优化过程不稳定。\n\n### 现有方法及其局限性\n\n*   **现有方法**：GRPO、DAPO和Dr. GRPO等方法曾引入不同的损失归一化项来尝试解决这一问题。\n*   **局限性**：这些方法存在以下问题：\n    *   要么产生有偏估计。\n    *   要么仍然面临高梯度方差的问题。\n\n### 本文贡献与提出的方法\n\n*   **问题重构**：通过理论和经验分析不同生成长度对策略损失的影响，本文将问题重新表述为寻找一个**最小方差无偏估计器**。\n*   **ΔL 归一化**：基于此，本文提出了 **ΔL 归一化**方法。\n\n### ΔL 归一化的优势\n\n*   **无偏估计**：ΔL 归一化不仅提供了真实策略损失的无偏估计。\n*   **最小化梯度方差**：理论上，该方法能够最小化梯度方差。\n\n### 实验结果与可用性\n\n*   **实验验证**：广泛的实验表明，ΔL 归一化在不同模型大小、最大生成长度和任务上持续取得优越结果。\n*   **代码公开**：本文的代码将公开发布。",
      "shortSummary": "本文提出ΔL归一化，一种针对强化学习与可验证奖励（RLVR）中动态生成长度问题的损失聚合方法。RLVR训练中响应长度变异性导致梯度方差高和优化不稳定，现有方法存在偏倚或高方差。ΔL归一化通过理论分析，提供策略损失的无偏估计并最小化梯度方差。实验证明，该方法在多种设置下均表现优异，有效解决了RLVR中的损失聚合挑战。",
      "translated_title": "ΔL 归一化：重新思考RLVR中的损失聚合",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed Delta L Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization."
    },
    {
      "title": "用于无数据训练的语言自博弈 (原标题: Language Self-Play For Data-Free Training)",
      "link": "https://arxiv.org/abs/2509.07414",
      "pubDate": "Tue, 09 Sep 2025 01:51:34 GMT",
      "isoDate": "2025-09-09T01:51:34.000Z",
      "creator": "Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan",
      "summary": "## 用于无数据训练的语言自博弈 (Language Self-Play For Data-Free Training)\n\n### 背景与挑战\n\n近年来，大型语言模型（LLMs）在规模、丰富的优质训练数据以及强化学习的推动下取得了飞速发展。然而，这种进步面临一个根本性的瓶颈：模型需要不断增加的数据才能持续学习和改进。\n\n### 提出的方法：语言自博弈（LSP）\n\n为了解决对额外数据的依赖问题，本文提出了一种基于强化学习的方法，使模型无需额外数据即可自我提升。该方法的核心是“语言自博弈”（Language Self-Play, LSP），其主要特点如下：\n\n*   **博弈论框架**：LSP 利用了自博弈的博弈论框架。在这个框架中，模型的各项能力被视为在竞争性博弈中的表现。\n*   **自我对弈机制**：通过让模型与自身进行对弈，可以逐步产生更强大的策略。这一过程使得模型能够在没有新外部数据输入的情况下，通过内部迭代和优化来增强其性能。\n\n### 实验与结果\n\n研究人员使用 Llama-3.2-3B-Instruct 模型在指令遵循基准任务上进行了实验。实验结果表明：\n\n*   **性能提升**：预训练模型不仅能够通过单独的自博弈显著提升其在具有挑战性任务上的表现。\n*   **超越基线**：LSP 方法的提升效果甚至比数据驱动的基线方法更为有效。\n\n### 核心贡献\n\nLSP 提供了一种新颖的范式，使大型语言模型能够摆脱对不断增长的训练数据的依赖，通过自我博弈实现持续的能力增强，从而有望突破当前LLM发展面临的数据瓶颈。",
      "shortSummary": "本文提出“语言自博弈”（LSP）方法，通过强化学习使大型语言模型（LLMs）无需额外数据即可自我提升。LSP利用博弈论框架，让模型与自身对弈以生成更强策略。实验表明，Llama-3.2-3B-Instruct模型通过LSP在指令遵循任务上显著提升性能，甚至优于数据驱动的基线方法，有效解决了LLM对数据日益增长的依赖瓶颈。",
      "translated_title": "用于无数据训练的语言自博弈",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines."
    },
    {
      "title": "带前瞻键的因果注意力 (原标题: Causal Attention with Lookahead Keys)",
      "link": "https://arxiv.org/abs/2509.07301",
      "pubDate": "Mon, 08 Sep 2025 20:15:23 GMT",
      "isoDate": "2025-09-08T20:15:23.000Z",
      "creator": "Zhuoqing Song, Peng Sun, Huizhuo Yuan, Quanquan Gu",
      "summary": "## 带前瞻键的因果注意力 (CASTLE)\n\n### 引言：标准因果注意力的局限性\n\n在标准的因果注意力机制中，每个token的查询（Query）、键（Key）和值（Value, QKV）是静态的，并且仅编码其自身之前（preceding context）的信息。这限制了模型在处理序列数据时对未来上下文的潜在利用。\n\n### CASTLE机制的引入\n\n文章提出了一种名为“带前瞻键的因果注意力”（CAuSal aTtention with Lookahead kEys, 简称CASTLE）的新型注意力机制。CASTLE旨在克服标准因果注意力的上述局限性。\n\n### 前瞻键（Lookahead Keys）的定义与特性\n\n*   **核心思想：** CASTLE机制的核心在于，它会随着上下文的展开，持续更新每个token的键（keys）。\n*   **“前瞻键”的含义：** 这些更新后的键被称为“前瞻键”，因为它们虽然属于较早的位置，但却集成了相对于这些位置而言后续出现的token的信息。\n*   **自回归属性的保留：** 尽管前瞻键整合了后续信息，CASTLE机制严格保留了自回归（autoregressive）属性。这意味着在生成当前token时，模型仍然只依赖于之前的token信息，但键的更新过程巧妙地考虑了未来的上下文，从而增强了表示能力。\n\n### 效率与并行训练\n\n*   **表面上的顺序性：** 尽管CASTLE机制在概念上可能显得是顺序执行的，因为它涉及键的持续更新。\n*   **数学等价形式：** 作者推导出了一个数学等价形式，该形式避免了在每个位置显式地实例化前瞻键。\n*   **高效并行训练：** 这一数学等价形式使得CASTLE能够实现高效的并行训练，从而解决了潜在的计算效率问题，使其在大规模模型训练中具有实用性。\n\n### 实验结果与性能提升\n\n*   **语言建模基准测试：** 在语言建模基准测试中，CASTLE在不同模型规模下始终优于标准的因果注意力机制。\n*   **具体性能指标：**\n    *   显著降低了验证困惑度（validation perplexity）。\n    *   提高了一系列下游任务的性能，表明其具有更强的泛化能力和表示学习能力。\n\n### 研究领域\n\n*   计算与语言（cs.CL）\n*   机器学习（cs.LG）",
      "shortSummary": "文章介绍了CASTLE（带前瞻键的因果注意力）机制，旨在解决标准因果注意力中QKV静态且仅编码前文信息的局限。CASTLE通过在上下文展开时持续更新每个token的键（称为“前瞻键”），使其能整合后续信息，同时严格保持自回归特性。尽管机制看似顺序，但通过数学等价实现了高效并行训练。实验表明，CASTLE在语言建模任务中持续优于标准因果注意力，降低了困惑度并提升了下游任务性能。",
      "translated_title": "带前瞻键的因果注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks."
    },
    {
      "title": "重建对齐改进统一多模态模型 (原标题: Reconstruction Alignment Improves Unified Multimodal Models)",
      "link": "https://arxiv.org/abs/2509.07295",
      "pubDate": "Mon, 08 Sep 2025 19:59:32 GMT",
      "isoDate": "2025-09-08T19:59:32.000Z",
      "creator": "Ji Xie, Trevor Darrell, Luke Zettlemoyer, XuDong Wang",
      "summary": "### 重建对齐改进统一多模态模型\n\n本文介绍了一种名为“重建对齐”（Reconstruction Alignment, RecA）的资源高效型后训练方法，旨在解决统一多模态模型（UMMs）在训练过程中面临的挑战。\n\n**背景问题：**\n*   统一多模态模型（UMMs）旨在将视觉理解和生成整合到单一架构中。\n*   传统的UMM训练依赖于图像-文本对，但这些文本描述通常稀疏，即使使用大量词语也难以捕捉图像中细粒度的视觉细节。\n\n**RecA方法介绍：**\n*   **核心思想：** RecA利用视觉理解编码器嵌入作为密集的“文本提示”，提供丰富的监督信息，而无需依赖人工标注的文本描述。\n*   **工作原理：**\n    1.  RecA使UMM以其自身的视觉理解嵌入为条件。\n    2.  通过自监督重建损失，优化UMM以重建输入图像。\n    3.  这一过程有效地重新对齐了模型的理解能力和生成能力。\n*   **特点：**\n    *   资源高效的后训练方法。\n    *   无需依赖稀疏的图像-文本描述。\n    *   广泛适用于各种UMM架构，包括自回归、掩码自回归和基于扩散的UMM。\n\n**实验结果与性能提升：**\n*   **训练效率：** 仅需27个GPU小时进行后训练。\n*   **图像生成性能显著提升：**\n    *   在GenEval基准测试中，性能从0.73提升至0.90。\n    *   在DPGBench基准测试中，性能从80.93提升至88.15。\n*   **图像编辑基准表现优异：**\n    *   在ImgEdit基准测试中，性能从3.38提升至3.75。\n    *   在GEdit基准测试中，性能从6.94提升至7.25。\n\n**结论：**\nRecA方法尽管简单，但其有效性使其能够超越许多更大的开源模型。它作为一种高效且通用的UMM后训练对齐策略，在不同UMM架构中均表现出广泛的适用性，显著提高了生成和编辑的保真度。",
      "shortSummary": "重建对齐（RecA）是一种资源高效的后训练方法，旨在改进统一多模态模型（UMMs）。它通过利用视觉理解编码器嵌入作为密集提示，并优化UMM以自监督方式重建输入图像，从而解决传统训练中稀疏文本描述的问题，重新对齐模型的理解与生成能力。RecA适用于多种UMM架构，仅用27个GPU小时，便显著提升了图像生成（如GenEval从0.73升至0.90）和编辑（如ImgEdit从3.38升至3.75）的性能，超越了许多大型开源模型，是一种高效通用的对齐策略。",
      "translated_title": "重建对齐改进统一多模态模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs"
    },
    {
      "title": "F1：一种连接理解与生成到行动的视觉-语言-行动模型 (原标题: F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions)",
      "link": "https://arxiv.org/abs/2509.06951",
      "pubDate": "Mon, 08 Sep 2025 13:58:30 GMT",
      "isoDate": "2025-09-08T13:58:30.000Z",
      "creator": "Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, Jiangmiao Pang",
      "summary": "# F1：一种连接理解与生成到行动的视觉-语言-行动模型\n\n## 引言\n在具身AI领域，于动态视觉环境中执行语言条件任务仍是一个核心挑战。现有视觉-语言-行动（VLA）模型主要采用反应式的状态-行动映射，这往往导致在动态场景中出现短视行为和较差的鲁棒性。\n\n## F1模型概述\n本文介绍F1，一个预训练的VLA框架，它将视觉预见生成整合到决策流程中，旨在弥合理解、生成与行动之间的鸿沟。\n\n### 核心架构与机制\n*   **架构：** F1采用Transformer混合架构，包含专门的模块用于感知、预见生成和控制。\n*   **核心机制：** F1运用“下一尺度预测”机制来合成目标条件下的视觉预见，将其作为明确的规划目标。\n*   **行动生成：** 通过预测合理的未来视觉状态，F1将行动生成重新定义为“预见引导的逆动力学问题”，从而能够生成隐式实现视觉目标的行动。\n\n## 训练方法与能力\n*   **训练方案：** 为赋予F1鲁棒和泛化能力，研究者提出了一种三阶段训练方法。该方法在一个包含超过33万条轨迹和136项多样化任务的广泛数据集上进行。\n*   **能力提升：** 这种训练方案增强了模型的模块化推理能力，并使其具备可迁移的视觉预见能力，这对于在复杂和动态环境中执行任务至关重要。\n\n## 实验结果\n在真实世界任务和模拟基准测试中，F1持续优于现有方法，在任务成功率和泛化能力方面均取得了显著提升。",
      "shortSummary": "F1是一种创新的预训练视觉-语言-行动（VLA）模型，旨在解决具身AI在动态视觉环境中执行语言条件任务的挑战。它通过将视觉预见生成整合到决策流程中，克服了现有VLA模型的短视和鲁棒性问题。F1采用Transformer混合架构，利用目标条件下的视觉预见作为规划目标，将行动生成重构为预见引导的逆动力学问题。经过大规模数据集的三阶段训练，F1在真实世界和模拟任务中均显著超越现有方法，大幅提升了任务成功率和泛化能力。",
      "translated_title": "F1：一种连接理解与生成到行动的视觉-语言-行动模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability."
    },
    {
      "title": "保持在最佳区域：通过能力自适应提示支架实现响应式推理演化 (原标题: Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding)",
      "link": "https://arxiv.org/abs/2509.06923",
      "pubDate": "Mon, 08 Sep 2025 13:36:21 GMT",
      "isoDate": "2025-09-08T13:36:21.000Z",
      "creator": "Ziheng Li, Zexu Sun, Jinman Zhao, Erxue Min, Yongcheng Zeng, Hui Wu, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Xu Chen, Zhi-Hong Deng",
      "summary": "### SEELE：通过能力自适应提示支架优化大型语言模型推理\n\n**1. 背景与问题**\n*   可验证奖励强化学习（RLVR）在提升大型语言模型（LLM）的推理能力方面取得了显著成功。\n*   然而，现有RLVR方法面临探索效率低下的挑战，主要原因在于训练数据难度与模型自身能力之间存在不匹配。\n*   具体表现为：当问题难度过高时，LLM难以发现有效的推理路径；而当问题过于简单时，模型学习到的新能力有限，导致学习效率不高。\n\n**2. 理论基础**\n*   本文通过量化损失下降速度与rollout准确性之间的关系，形式化地阐明了问题难度对学习过程的影响。\n\n**3. SEELE框架介绍**\n*   **SEELE**是一个新颖的、监督辅助的RLVR框架，其核心目标是动态调整问题难度，以确保模型始终处于高效学习的“最佳区域”。\n*   **核心机制：** SEELE通过在原始问题之后附加一个“提示”（即完整解决方案的一部分）来增强每个训练样本。\n*   **创新之处：** 与以往基于提示的方法不同，SEELE的关键在于它能有意识地、自适应地调整每个问题的提示长度，从而达到一个最优的难度水平。\n\n**4. 提示长度确定机制**\n*   SEELE采用多轮rollout采样策略来确定每个问题的最佳提示长度。\n*   在每一轮采样中，它会利用前几轮收集到的“准确性-提示对”数据，拟合一个项目反应理论（IRT）模型，进而预测下一轮训练所需的最佳提示长度。\n*   这种实例级别、实时的问题难度调整方法，确保了问题难度与模型不断演进的能力保持同步，从而显著提高了探索效率。\n\n**5. 实验结果**\n*   实验结果表明，SEELE在六个数学推理基准测试中表现出色：\n    *   平均优于Group Relative Policy Optimization (GRPO) 11.8个百分点。\n    *   平均优于Supervised Fine-tuning (SFT) 10.5个百分点。\n    *   平均超越之前最佳的监督辅助方法3.6个百分点。",
      "shortSummary": "SEELE是一种新颖的监督辅助强化学习框架，旨在解决大型语言模型（LLM）推理训练中因问题难度与模型能力不匹配导致的探索效率低下问题。该框架通过动态调整每个训练样本的提示长度来优化问题难度，使模型始终处于高效学习区域。SEELE利用多轮rollout采样和项目反应理论模型预测最佳提示长度。实验表明，SEELE在多个数学推理基准测试中显著优于现有方法，有效提升了LLM的推理能力和学习效率。",
      "translated_title": "保持在最佳区域：通过能力自适应提示支架实现响应式推理演化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks."
    },
    {
      "title": "Curia：一种用于放射学的多模态基础模型 (原标题: Curia: A Multi-Modal Foundation Model for Radiology)",
      "link": "https://arxiv.org/abs/2509.06830",
      "pubDate": "Mon, 08 Sep 2025 12:04:12 GMT",
      "isoDate": "2025-09-08T12:04:12.000Z",
      "creator": "Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, François Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul Hérent",
      "summary": "# Curia：一种用于放射学的多模态基础模型\n\n## 1. 引言与背景\n\n*   **当前挑战：** 现有的放射学AI辅助解释主要依赖于狭窄的、单任务模型。这种方法对于覆盖广泛的成像模态、疾病和放射学发现来说是不切实际的。\n*   **基础模型的潜力：** 基础模型（FMs）有望在不同模态和低数据环境下实现广泛泛化，但在放射学领域尚未充分实现这一潜力。\n\n## 2. Curia模型介绍\n\n*   **模型名称：** Curia\n*   **模型类型：** 一种多模态基础模型，专为放射学设计。\n*   **训练数据：**\n    *   训练于一家大型医院多年来的全部横断面成像输出。\n    *   据作者称，这是迄今为止最大的真实世界数据语料库。\n    *   **数据规模：** 包含150,000次检查，总计130 TB的数据。\n\n## 3. 性能与评估\n\n*   **评估基准：** 在一个新策划的19项任务外部验证基准上进行测试。\n*   **核心能力：**\n    *   准确识别器官。\n    *   检测多种病症，例如脑出血和心肌梗死。\n    *   预测肿瘤分期中的结果。\n*   **性能表现：**\n    *   达到或超越了放射科医生和近期其他基础模型的表现。\n    *   在跨模态和低数据条件下展现出具有临床意义的涌现特性。\n\n## 4. 可用性\n\n*   为了加速研究进展，Curia的基础模型权重已被发布。\n\n## 5. 作者与分类\n\n*   **作者：** Corentin Dancette 等多位研究人员。\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)；机器学习 (cs.LG)。\n*   **引用信息：** arXiv:2509.06830。",
      "shortSummary": "Curia是一种用于放射学的多模态基础模型，旨在克服现有单任务AI的局限。它利用一家大型医院15万次检查（130 TB）的真实世界数据进行训练。Curia在一个19项任务的基准测试中表现出色，能准确识别器官、检测脑出血和心肌梗死等病症，并预测肿瘤分期结果。其性能超越了放射科医生及其他基础模型，并在跨模态和低数据环境下展现出重要的涌现特性。模型权重已发布，以加速研究进展。",
      "translated_title": "Curia：一种用于放射学的多模态基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia."
    },
    {
      "title": "UMO：通过匹配奖励扩展图像定制中的多身份一致性 (原标题: UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward)",
      "link": "https://arxiv.org/abs/2509.06818",
      "pubDate": "Mon, 08 Sep 2025 11:54:55 GMT",
      "isoDate": "2025-09-08T11:54:55.000Z",
      "creator": "Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He",
      "summary": "## UMO：通过匹配奖励扩展图像定制中的多身份一致性\n\n### 摘要\n\n近期图像定制技术在能力上取得了显著进步，展现出广泛的应用前景。然而，由于人类对人脸的敏感性，在处理多参考图像时，如何保持一致的身份并避免身份混淆，仍然是一个重大挑战，这限制了定制模型在身份可扩展性方面的表现。\n\n### UMO 框架：统一的多身份优化\n\n为了解决上述问题，研究人员提出了 UMO（Unified Multi-identity Optimization）框架。该框架旨在实现高保真度的身份保留，并有效缓解多身份场景下的身份混淆问题，同时提升模型的可扩展性。\n\n#### 核心方法：多对多匹配范式\n\nUMO 采用了一种“多对多匹配”（multi-to-multi matching）范式，将多身份生成问题重新定义为一个全局分配优化问题。通过在扩散模型上应用强化学习，UMO 能够普遍地增强现有图像定制方法的多身份一致性。\n\n### 辅助开发与评估\n\n1.  **可扩展定制数据集：** 为了促进 UMO 的训练，研究团队开发了一个包含多参考图像的可扩展定制数据集，该数据集结合了合成数据和真实数据。\n2.  **新度量标准：** 此外，研究人员还提出了一种新的度量标准，专门用于量化和衡量身份混淆的程度。\n\n### 实验结果与性能\n\n广泛的实验结果表明，UMO 框架不仅显著提高了身份一致性，而且在多种图像定制方法上有效降低了身份混淆。在身份保留维度上，UMO 在开源方法中达到了新的最先进水平。",
      "shortSummary": "UMO是一个统一的多身份优化框架，旨在解决图像定制中多参考图像的身份一致性保持和身份混淆问题。它通过“多对多匹配”范式，将多身份生成重构为全局分配优化问题，并利用扩散模型上的强化学习来提升现有方法的身份一致性。UMO还引入了新的数据集和评估指标。实验证明，UMO显著提高了身份一致性，减少了身份混淆，并在身份保留方面达到了开源方法的最新水平。",
      "translated_title": "UMO：通过匹配奖励扩展图像定制中的多身份一致性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO"
    },
    {
      "title": "P3-SAM: 原生3D部件分割 (原标题: P3-SAM: Native 3D Part Segmentation)",
      "link": "https://arxiv.org/abs/2509.06784",
      "pubDate": "Mon, 08 Sep 2025 11:12:17 GMT",
      "isoDate": "2025-09-08T11:12:17.000Z",
      "creator": "Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, Chunchao Guo",
      "summary": "本文提出了一种名为P3-SAM的原生3D点提示部件分割模型，旨在全自动地将任何3D对象分割成组件。该模型受SAM启发，由特征提取器、多个分割头和一个IoU预测器组成，从而实现用户的交互式分割。此外，还提出了一种算法来自动选择和合并模型预测的掩码，以进行部件实例分割。该模型在一个包含近370万个具有合理分割标签的模型的新建数据集上进行训练。比较结果表明，该方法在任何复杂对象上都能实现精确的分割结果和强大的鲁棒性，达到最先进的性能。代码即将发布。",
      "shortSummary": "本文提出了一种名为P3-SAM的原生3D点提示部件分割模型，用于全自动地将3D对象分割成组件。该模型包含特征提取器、分割头和IoU预测器，并提出了一种自动选择和合并掩码的算法。该模型在大型数据集上训练，实验结果表明其在复杂对象上实现了精确的分割和强大的鲁棒性，达到最先进的性能。",
      "translated_title": "P3-SAM: 原生3D部件分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D understanding, facilitating model reuse, and supporting various applications such as part generation. However, current methods face limitations such as poor robustness when dealing with complex objects and cannot fully automate the process. In this paper, we propose a native 3D point-promptable part segmentation model termed P3-SAM, designed to fully automate the segmentation of any 3D objects into components. Inspired by SAM, P3-SAM consists of a feature extractor, multiple segmentation heads, and an IoU predictor, enabling interactive segmentation for users. We also propose an algorithm to automatically select and merge masks predicted by our model for part instance segmentation. Our model is trained on a newly built dataset containing nearly 3.7 million models with reasonable segmentation labels. Comparisons show that our method achieves precise segmentation results and strong robustness on any complex objects, attaining state-of-the-art performance. Our code will be released soon."
    },
    {
      "title": "WinT3R：基于窗口的流式重建与相机令牌池 (原标题: WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool)",
      "link": "https://arxiv.org/abs/2509.05296",
      "pubDate": "Fri, 05 Sep 2025 13:59:47 GMT",
      "isoDate": "2025-09-05T13:59:47.000Z",
      "creator": "Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He",
      "summary": "# WinT3R：基于窗口的流式重建与相机令牌池\n\n## 摘要\n本文介绍了WinT3R，一个前馈重建模型，能够在线预测精确的相机姿态和高质量的点云图。\n\n## 现有问题\n*   以往的方法在重建质量和实时性能之间存在权衡，难以同时达到高水平。\n\n## WinT3R 的核心创新与解决方案\nWinT3R模型旨在解决上述问题，其主要设计包括：\n\n1.  **滑动窗口机制：**\n    *   **目的：** 确保窗口内帧之间有足够的信息交换，从而在不增加大量计算的情况下提高几何预测的质量。\n    *   **优势：** 改善重建质量，同时保持计算效率。\n2.  **紧凑的相机表示与全局相机令牌池：**\n    *   **目的：** 增强相机姿态估计的可靠性。\n    *   **优势：** 在不牺牲效率的前提下，提高相机姿态估计的准确性。\n\n## 性能与验证\n*   **成果：** 这些设计使WinT3R在在线重建质量、相机姿态估计和重建速度方面达到了最先进的性能。\n*   **验证：** 通过在各种数据集上进行的大量实验验证了其卓越性能。\n\n## 可用性\n*   WinT3R的代码和模型已公开提供。",
      "shortSummary": "WinT3R是一个前馈模型，用于在线预测精确的相机姿态和高质量点云图。它通过引入滑动窗口机制来增强帧间信息交换，并在不牺牲效率的情况下，利用紧凑的相机表示和全局相机令牌池提高相机姿态估计的可靠性。WinT3R在在线重建质量、相机姿态估计和重建速度方面均达到了最先进的性能。",
      "translated_title": "WinT3R：基于窗口的流式重建与相机令牌池",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R."
    },
    {
      "title": "LatticeWorld: 一个由多模态大型语言模型驱动的交互式复杂世界生成框架 (原标题: LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation)",
      "link": "https://arxiv.org/abs/2509.05263",
      "pubDate": "Fri, 05 Sep 2025 13:22:33 GMT",
      "isoDate": "2025-09-05T13:22:33.000Z",
      "creator": "Yinglin Duan, Zhengxia Zou, Tongwei Gu, Wei Jia, Zhan Zhao, Luyi Xu, Xinzhu Liu, Hao Jiang, Kang Chen, Shuang Qiu",
      "summary": "### LatticeWorld: 多模态大型语言模型赋能的交互式复杂世界生成框架\n\n#### 背景与挑战\n*   **研究焦点**: 近期研究日益关注开发模拟复杂现实世界场景的3D世界模型。\n*   **应用领域**: 世界模型在具身AI、自动驾驶、娱乐等多个领域有广泛应用。\n*   **目标**: 通过更真实的模拟和精确的物理效果，有效缩小“模拟到现实”的差距，并方便地获取关于现实世界的丰富信息。\n*   **传统方法局限**: 传统手动建模虽然能创建虚拟3D场景，但效率不高。\n*   **现代方法**: 现代方法利用先进的机器学习算法进行3D世界生成，最新进展集中于根据用户指令创建虚拟世界的生成式方法。\n\n#### LatticeWorld框架介绍\n*   **核心理念**: 本文提出了LatticeWorld，一个简单而有效的3D世界生成框架，旨在简化3D环境的工业生产流程。\n*   **技术构成**: \n    *   利用轻量级大型语言模型（LLM，如LLaMA-2-7B）。\n    *   结合工业级渲染引擎（如Unreal Engine 5）。\n    *   共同生成动态环境。\n*   **多模态输入**: LatticeWorld接受文本描述和视觉指令作为多模态输入。\n*   **生成内容**: 创建大规模的3D交互式世界，其中包含动态智能体。\n*   **关键特性**: \n    *   具有竞争力的多智能体交互。\n    *   高保真物理模拟。\n    *   实时渲染。\n\n#### 实验与成果\n*   **评估**: 对LatticeWorld进行了全面的实验评估。\n*   **准确性与视觉保真度**: 结果表明，LatticeWorld在场景布局生成和视觉保真度方面表现出色。\n*   **生产效率提升**: 相较于传统手动生产方法，LatticeWorld在保持高创造性质量的同时，将工业生产效率提高了90倍以上。",
      "shortSummary": "LatticeWorld是一个由多模态大型语言模型（如LLaMA-2-7B）和工业级渲染引擎（如Unreal Engine 5）驱动的3D世界生成框架。它接受文本和视觉指令，能高效生成包含动态智能体、高保真物理模拟和实时渲染的大规模交互式3D世界。实验证明，LatticeWorld在场景布局准确性和视觉保真度上表现优异，并能将工业生产效率提升90倍以上，同时保持高创造性质量。",
      "translated_title": "LatticeWorld: 一个由多模态大型语言模型驱动的交互式复杂世界生成框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent research has been increasingly focusing on developing 3D world models that simulate complex real-world scenarios. World models have found broad applications across various domains, including embodied AI, autonomous driving, entertainment, etc. A more realistic simulation with accurate physics will effectively narrow the sim-to-real gap and allow us to gather rich information about the real world conveniently. While traditional manual modeling has enabled the creation of virtual 3D scenes, modern approaches have leveraged advanced machine learning algorithms for 3D world generation, with most recent advances focusing on generative methods that can create virtual worlds based on user instructions. This work explores such a research direction by proposing LatticeWorld, a simple yet effective 3D world generation framework that streamlines the industrial production pipeline of 3D environments. LatticeWorld leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed framework accepts textual descriptions and visual instructions as multimodal inputs and creates large-scale 3D interactive worlds with dynamic agents, featuring competitive multi-agent interaction, high-fidelity physics simulation, and real-time rendering. We conduct comprehensive experiments to evaluate LatticeWorld, showing that it achieves superior accuracy in scene layout generation and visual fidelity. Moreover, LatticeWorld achieves over a 90times increase in industrial production efficiency while maintaining high creative quality compared with traditional manual production methods. Our demo video is available at https://youtu.be/8VWZXpERR18"
    }
  ],
  "lastUpdated": "2025-09-11T09:30:32.621Z"
}