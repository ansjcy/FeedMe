{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VGGT-X：当VGGT遇上密集新视角合成 (原标题: VGGT-X: When VGGT Meets Dense Novel View Synthesis)",
      "link": "https://arxiv.org/abs/2509.25191",
      "pubDate": "Mon, 29 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-29T13:59:59.000Z",
      "creator": "Yang Liu, Chuanchen Luo, Zimo Tang, Junran Peng, Zhaoxiang Zhang",
      "summary": "## VGGT-X：解决密集新视角合成中的3D基础模型挑战\n\n### 研究背景与问题\n\n当前的新视角合成（NVS）技术，如NeRF和3DGS，高度依赖于通过运动恢复结构（SfM）获取的精确3D属性（例如相机姿态和点云）。然而，SfM在低纹理或低重叠的捕获场景中往往速度慢且脆弱。尽管最近的3D基础模型（3DFMs）在速度上比传统流程快了几个数量级，并在在线NVS方面展现出巨大潜力，但其验证和结论大多局限于稀疏视角设置。\n\n### 密集视角下的挑战\n\n研究发现，将3DFMs直接应用于密集视角会遇到两个根本性障碍：\n\n*   **VRAM负担急剧增加**：处理大量图像导致显存消耗过大。\n*   **输出不完善**：3DFMs的初始输出质量不足，会降低对初始化敏感的3D训练效果。\n\n### VGGT-X解决方案\n\n为了克服这些障碍，研究者引入了**VGGT-X**，它包含以下关键组成部分：\n\n1.  **内存高效的VGGT实现**：能够扩展到处理1,000多张图像，有效缓解VRAM负担。\n2.  **自适应全局对齐**：用于增强VGGT的输出质量，提高其准确性。\n3.  **鲁棒的3DGS训练实践**：确保在不完善的初始化条件下也能进行稳定的3D高斯溅射训练。\n\n### 实验结果与贡献\n\n广泛的实验表明，VGGT-X的这些措施显著缩小了与COLMAP初始化管道的保真度差距。它在**密集无COLMAP新视角合成**和**姿态估计**方面取得了最先进（state-of-the-art）的结果。\n\n### 未来展望\n\n研究还分析了与COLMAP初始化渲染之间仍存在的差距原因，为3D基础模型和密集NVS的未来发展提供了有价值的见解。",
      "shortSummary": "本研究提出了VGGT-X，旨在解决将3D基础模型（3DFMs）应用于密集新视角合成（NVS）时遇到的VRAM负担和输出质量问题。VGGT-X通过内存高效的VGGT实现、自适应全局对齐和鲁棒的3DGS训练实践，显著提升了性能。实验证明，VGGT-X在密集无COLMAP NVS和姿态估计方面达到了最先进水平，并为3DFMs和密集NVS的未来发展提供了深入见解。",
      "translated_title": "VGGT-X：当VGGT遇上密集新视角合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/"
    },
    {
      "title": "视觉拼图后训练改进多模态大语言模型 (原标题: Visual Jigsaw Post-Training Improves MLLMs)",
      "link": "https://arxiv.org/abs/2509.25190",
      "pubDate": "Mon, 29 Sep 2025 13:59:57 GMT",
      "isoDate": "2025-09-29T13:59:57.000Z",
      "creator": "Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, Ziwei Liu",
      "summary": "### Visual Jigsaw：一种改进多模态大语言模型视觉理解的自监督后训练框架\n\n当前，基于强化学习的后训练已成为增强多模态大语言模型（MLLMs）对齐和推理能力的重要范式。然而，现有的后训练方法主要以文本为中心，视觉输入通常仅用于提取稀疏线索以进行基于文本的推理，这限制了MLLMs对视觉信号的内在理解。尽管有一些方法试图解决这一问题，但它们往往仍依赖文本作为中间媒介，或引入额外的视觉生成设计。\n\n为了解决这些局限性，本文提出了一种名为 **Visual Jigsaw** 的通用自监督后训练框架，旨在加强MLLMs的视觉理解能力。\n\n**Visual Jigsaw 的核心机制：**\n\n*   **通用排序任务**：该框架被设计为一个通用的排序任务。视觉输入（如图像、视频帧、3D数据块）首先被分割成多个部分，然后这些部分被打乱顺序。\n*   **自然语言重构**：模型的目标是通过生成正确的排列顺序（以自然语言形式），来重建原始的视觉信息。\n*   **与RLVR对齐**：这种设计自然地与可验证奖励的强化学习（RLVR）范式对齐。\n\n**Visual Jigsaw 的主要优势：**\n\n*   **无需额外视觉生成组件**：它不需要引入任何额外的视觉生成组件，简化了模型架构。\n*   **自动监督信号**：其监督信号是自动生成的，无需任何人工标注，大大降低了数据准备的成本。\n*   **纯视觉中心**：它专注于纯粹的视觉理解，避免了对文本作为中间媒介的依赖。\n\n**应用与实验结果：**\n\n*   **多模态支持**：Visual Jigsaw 在三种视觉模态上进行了实例化和验证，包括图像、视频和3D数据。\n*   **显著改进**：广泛的实验结果表明，该框架在以下方面带来了显著的性能提升：\n    *   **细粒度感知**\n    *   **时间推理**\n    *   **3D空间理解**\n\n**研究意义：**\n\n这些发现强调了自监督视觉中心任务在MLLMs后训练中的巨大潜力，并旨在启发未来在视觉中心预训练设计方面的进一步研究。",
      "shortSummary": "Visual Jigsaw 是一种创新的自监督后训练框架，旨在增强多模态大语言模型（MLLMs）的视觉理解能力。它通过将视觉输入（图像、视频、3D数据）分割、打乱，并要求模型以自然语言形式重建正确顺序的通用排序任务来实现。该方法无需额外视觉生成组件或人工标注，并与RLVR对齐。实验证明，Visual Jigsaw显著提升了MLLMs在细粒度感知、时间推理和3D空间理解方面的性能。",
      "translated_title": "视觉拼图后训练改进多模态大语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While vision-centric post-training is crucial for enhancing MLLMs' intrinsic understanding of visual signals, current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce Visual Jigsaw, a generic self-supervised post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs. Project Page: https://penghao-wu.github.io/visual_jigsaw/"
    },
    {
      "title": "PixelCraft：一个用于结构化图像高保真视觉推理的多智能体系统 (原标题: PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images)",
      "link": "https://arxiv.org/abs/2509.25185",
      "pubDate": "Mon, 29 Sep 2025 13:59:49 GMT",
      "isoDate": "2025-09-29T13:59:49.000Z",
      "creator": "Shuoshuo Zhang, Zijian Li, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, Yujiu Yang, Rui Wang",
      "summary": "## PixelCraft：一个用于结构化图像高保真视觉推理的多智能体系统\n\n### 引言\n\n多模态大语言模型（MLLMs）在处理结构化图像（例如图表和几何图）时面临显著挑战。感知上的失误可能导致错误的结论。尽管中间视觉线索可以引导推理，但现有基于线索的方法受限于低保真图像处理和线性、僵化的推理模式，这限制了它们在复杂结构化图像任务上的有效性。\n\n### PixelCraft 系统概述\n\n本文提出了 **PixelCraft**，一个新颖的多智能体系统，旨在实现结构化图像的高保真图像处理和灵活的视觉推理。该系统由以下核心组件构成：\n\n*   **调度器 (Dispatcher)**\n*   **规划器 (Planner)**\n*   **推理器 (Reasoner)**\n*   **评论员 (Critics)**\n*   **一组视觉工具智能体 (Visual Tool Agents)**\n\n### 高保真处理机制\n\n为实现高保真图像处理，PixelCraft 采取了以下策略：\n\n1.  **构建高质量语料库**：用于训练和微调模型。\n2.  **微调 MLLM 为基础模型**：将一个 MLLM 微调成一个基础模型，该模型能够进行像素级的定位。\n3.  **整合传统计算机视觉 (CV) 算法**：将基础模型的像素级定位与传统计算机视觉算法集成到工具智能体中，从而增强图像处理的精确性和细节。\n\n### 灵活视觉推理机制\n\nPixelCraft 通过以下方式促进灵活的视觉推理：\n\n1.  **动态三阶段工作流**：\n    *   **工具选择 (Tool Selection)**\n    *   **智能体讨论 (Agent Discussion)**\n    *   **自我批评 (Self-Criticism)**\n2.  **图像记忆 (Image Memory)**：与以往简单地追加历史图像的线性推理模式不同，PixelCraft 维护一个图像记忆。这使得规划器能够：\n    *   自适应地重新访问早期的视觉步骤。\n    *   探索替代的推理分支。\n    *   在讨论过程中动态调整推理轨迹。\n\n### 实验结果与贡献\n\n在具有挑战性的图表和几何基准测试上进行的广泛实验表明，PixelCraft 显著提升了高级 MLLMs 的视觉推理性能，为结构化图像推理设定了新标准。\n\n### 代码可用性\n\n该项目的代码将在指定链接提供。",
      "shortSummary": "PixelCraft是一个多智能体系统，旨在提升多模态大语言模型（MLLMs）在结构化图像（如图表、几何图）上的高保真视觉推理能力。它通过将微调的MLLM基础模型与传统计算机视觉算法结合，实现精确的图像处理。系统采用动态三阶段工作流（工具选择、智能体讨论、自我批评）和独特的图像记忆机制，支持灵活的推理路径。实验证明，PixelCraft显著改善了MLLMs的视觉推理表现，为结构化图像推理设立了新标准。",
      "translated_title": "PixelCraft：一个用于结构化图像高保真视觉推理的多智能体系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft."
    },
    {
      "title": "SIRI：通过交错压缩扩展迭代强化学习 (原标题: SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression)",
      "link": "https://arxiv.org/abs/2509.25176",
      "pubDate": "Mon, 29 Sep 2025 13:59:08 GMT",
      "isoDate": "2025-09-29T13:59:08.000Z",
      "creator": "Haoming Wen, Yushi Bai, Juanzi Li, Jie Tang",
      "summary": "SIRI：通过交错压缩扩展迭代强化学习\n\n*   **引言**\n    *   现有研究表明，大型推理模型（LRMs）在推理过程中常出现重复的思维模式。\n    *   尝试减少这些重复模式通常会导致性能下降，形成效率与性能之间的权衡。\n\n*   **SIRI方法概述**\n    *   SIRI（Scaling Iterative Reinforcement Learning with Interleaved Compression）是一种简单而有效的强化学习方法，旨在提高LRMs推理的效率和准确性。\n    *   该方法通过在训练过程中动态调整最大输出长度，迭代地交替进行推理预算的“压缩”和“扩展”。\n\n*   **SIRI的工作机制**\n    *   **压缩阶段：**\n        *   缩短模型的输出长度（rollout length）。\n        *   迫使模型在有限的上下文中做出更精确和有价值的决策。\n        *   有效减少冗余token并提高推理密度。\n    *   **扩展阶段：**\n        *   放宽输出长度限制。\n        *   为模型在长周期（long-horizon）设置中进行探索和规划提供充足空间。\n\n*   **关键发现与优势**\n    *   研究发现，在每个压缩-扩展循环之后，模型的性能都会得到提升，同时其输出长度却有所减少。\n    *   这使得模型能够稳步接近性能-效率权衡中的帕累托前沿，有效克服了传统上减少冗余会牺牲性能的问题。\n\n*   **实验结果**\n    *   SIRI在DeepSeek-R1-Distill-Qwen-1.5B模型上进行了训练。\n    *   **SIRI-low：** 经过三次迭代后，在AIME24基准测试上的性能提高了43.2%，同时token使用量减少了46.9%。\n    *   **SIRI-high：** 在所有比较方法中实现了最高的准确性。\n\n*   **结论与启示**\n    *   这些发现揭示了在训练过程中周期性地振荡LRM的输出截断长度的潜力。\n    *   这种方法能够动态平衡推理中的探索能力和效率，使模型收敛到一个最佳的“甜蜜点”。\n\n*   **可用性**\n    *   SIRI模型已公开可用。",
      "shortSummary": "SIRI是一种针对大型推理模型（LRMs）的强化学习方法，通过在训练中迭代交替压缩和扩展推理预算来提高效率和准确性。它动态调整最大输出长度，在压缩阶段强制精确决策，在扩展阶段允许探索。SIRI显著提升了模型性能，同时减少了token使用量，克服了性能与效率的权衡。例如，SIRI-low在AIME24上性能提升43.2%，token使用减少46.9%。",
      "translated_title": "SIRI：通过交错压缩扩展迭代强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available."
    },
    {
      "title": "EasySteer：一个用于高性能和可扩展LLM转向的统一框架 (原标题: EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering)",
      "link": "https://arxiv.org/abs/2509.25175",
      "pubDate": "Mon, 29 Sep 2025 13:59:07 GMT",
      "isoDate": "2025-09-29T13:59:07.000Z",
      "creator": "Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen",
      "summary": "### EasySteer：一个用于高性能和可扩展LLM转向的统一框架\n\n**背景与问题**\n\n*   大型语言模型（LLM）转向（steering）是一种新兴范式，通过在推理时操纵隐藏状态来控制模型行为，为昂贵的模型再训练提供了一种轻量级替代方案。\n*   然而，现有转向框架面临以下关键限制：\n    *   **计算效率低下**：影响处理速度和资源消耗。\n    *   **可扩展性有限**：难以适应新的方法或应用场景。\n    *   **功能受限**：阻碍了研究进展和实际部署。\n\n**EasySteer 框架介绍**\n\nEasySteer 是一个统一的框架，专为实现高性能和可扩展的LLM转向而设计，其核心构建于vLLM之上。\n\n*   **主要特点**：\n    *   **模块化架构**：提供可插拔接口，支持基于分析和基于学习的转向方法，增强了灵活性和可扩展性。\n    *   **细粒度参数控制**：允许用户对转向参数进行精确调整，以满足特定需求。\n    *   **预计算转向向量**：为八个不同的应用领域提供了预先计算好的转向向量，简化了部署和使用。\n    *   **交互式演示系统**：提供用户友好的界面，方便探索和验证转向效果。\n\n**性能优势**\n\n*   通过与vLLM优化推理引擎的深度集成，EasySteer在性能上取得了显著突破。\n*   与现有框架相比，EasySteer实现了**5.5至11.4倍的速度提升**，极大地提高了转向操作的效率。\n\n**应用效果**\n\n*   广泛的实验证明了EasySteer在多个关键应用中的有效性：\n    *   **缓解“过度思考”（overthinking）**：帮助模型避免不必要的冗长或重复推理。\n    *   **减少幻觉（hallucination）**：提高模型生成内容的准确性和事实性。\n    *   以及其他重要的LLM行为控制应用。\n\n**结论与意义**\n\n*   EasySteer将LLM转向从一种主要停留在研究阶段的技术，成功地转变为一种**生产就绪的能力**。\n*   它为开发和部署可控、可信赖的语言模型建立了关键的基础设施，推动了LLM在实际应用中的发展。",
      "shortSummary": "EasySteer是一个基于vLLM的统一框架，旨在解决现有LLM转向框架效率低下和可扩展性差的问题。它通过模块化架构、细粒度控制和预计算转向向量，实现了高性能和可扩展性。与现有框架相比，EasySteer速度提升5.5-11.4倍，并在缓解过度思考、减少幻觉等应用中表现出色。该框架将LLM转向提升为生产级能力，为可控语言模型提供了关键基础设施。",
      "translated_title": "EasySteer：一个用于高性能和可扩展LLM转向的统一框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4times speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models."
    },
    {
      "title": "滚动强制：实时自回归长视频扩散 (原标题: Rolling Forcing: Autoregressive Long Video Diffusion in Real Time)",
      "link": "https://arxiv.org/abs/2509.25161",
      "pubDate": "Mon, 29 Sep 2025 13:57:14 GMT",
      "isoDate": "2025-09-29T13:57:14.000Z",
      "creator": "Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, Shijian Lu",
      "summary": "# 滚动强制：实时自回归长视频扩散\n\n## 摘要\n本文介绍了一种名为“滚动强制”（Rolling Forcing）的新型视频生成技术，旨在解决现有流式视频生成方法中严重的误差累积问题，从而实现高质量、低延迟和时间连贯的长视频流实时生成。\n\n## 背景与问题\n流式视频生成是交互式世界模型和神经游戏引擎中的一个基本组成部分，其目标是生成高质量、低延迟且时间连贯的长视频流。然而，大多数现有工作都面临严重的误差累积问题，这在生成长视频流时会显著降低其质量。\n\n## 滚动强制（Rolling Forcing）的关键创新\n为了克服误差累积的挑战，滚动强制引入了三项新颖设计：\n\n### 1. 联合去噪方案\n*   **问题：** 迭代采样单个帧会加速误差传播。\n*   **解决方案：** 滚动强制设计了一个联合去噪方案，该方案同时对多个帧进行去噪，并采用逐步增加的噪声水平。\n*   **效果：** 这种设计放宽了相邻帧之间严格的因果关系，从而有效地抑制了误差的增长。\n\n### 2. 注意力汇聚机制（Attention Sink Mechanism）\n*   **引入目的：** 增强长程流式视频生成的全局一致性。\n*   **工作原理：** 该机制允许模型将初始帧的关键值状态保留为全局上下文锚点。\n*   **效果：** 显著增强了视频的长期全局一致性。\n\n### 3. 高效训练算法\n*   **目标：** 实现对大幅扩展去噪窗口的少步蒸馏。\n*   **工作原理：** 该算法在非重叠窗口上操作，并缓解了以自生成历史为条件的曝光偏差。\n\n## 实验结果\n广泛的实验表明，“滚动强制”技术能够在单个GPU上实现多分钟视频的实时流式生成，并且显著减少了误差累积。\n\n## 图片说明\n文章内容中未包含有效的实际图片链接，因此本摘要不包含任何图片。",
      "shortSummary": "Rolling Forcing是一种新型视频生成技术，旨在解决现有流式视频生成中严重的误差累积问题。它通过联合去噪方案、注意力汇聚机制和高效训练算法三项创新设计，实现了高质量、低延迟和时间连贯的长视频流实时生成。实验证明，该方法能在单个GPU上实时生成多分钟视频，并显著减少误差累积，提升了长视频的全局一致性。",
      "translated_title": "滚动强制：实时自回归长视频扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation."
    },
    {
      "title": "GSM8K-V：视觉语言模型能否解决视觉语境下的小学数学应用题 (原标题: GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts)",
      "link": "https://arxiv.org/abs/2509.25160",
      "pubDate": "Mon, 29 Sep 2025 13:57:05 GMT",
      "isoDate": "2025-09-29T13:57:05.000Z",
      "creator": "Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
      "summary": "# GSM8K-V：一个用于视觉数学推理的新基准\n\n## 引言\n视觉语言模型（VLMs）通过统一图像和文本建模，在感知、规划和推理等复杂现实世界任务中展现出强大能力。其中，数学推理是VLM高级能力的一个突出体现，它要求模型理解图像中的数学信息并进行复杂推理。\n\n## 现有基准的局限性\n尽管已有多个视觉数学推理基准被提出，但它们普遍存在以下局限：\n*   **范围受限：** 通常局限于几何学问题。\n*   **内容缺失：** 缺乏对数学应用题的覆盖。\n*   **推理单一：** 很少评估跨多图像的推理能力。\n\n## GSM8K-V 基准的引入与构建\n为解决上述空白，研究人员引入了**GSM8K-V**，这是一个纯视觉、多图像的数学推理基准。\n*   **构建方法：** GSM8K-V通过系统地将广泛使用的文本基GSM8K中的每个样本映射为视觉形式来构建。\n*   **生成流程：** 结合了精心设计的自动化图像生成流程和细致的人工标注。\n*   **样本数量：** 最终整理出1,319个高质量的视觉数学推理样本。\n\n## 模型评估与结果\n研究人员在GSM8K-V上评估了多种开源和闭源模型。\n*   **性能对比：** 结果显示，尽管现有VLMs在文本基GSM8K上的表现已接近饱和，但在GSM8K-V上的表现仍有显著提升空间。\n*   **具体示例：** 表现最佳的模型Gemini-2.5-Pro在文本基GSM8K上取得了95.22%的准确率，但在GSM8K-V上其准确率显著下降至46.93%。\n\n## 分析与未来方向\n研究对GSM8K-V进行了全面分析，深入探讨了当前模型的局限性以及潜在的改进方向。\n\n## 意义\nGSM8K-V为视觉数学推理提供了新的视角，并建立了一个重要的基准，旨在指导开发更鲁棒、更具泛化能力的视觉语言模型。",
      "shortSummary": "研究引入了GSM8K-V，一个纯视觉、多图像的数学推理基准，旨在解决现有视觉数学基准在几何限制、应用题缺失及多图像推理方面的不足。该基准通过将文本版GSM8K样本视觉化并结合人工标注构建。评估结果显示，尽管视觉语言模型在文本任务上表现优异，但在GSM8K-V上的性能仍有巨大提升空间（如Gemini-2.5-Pro从95.22%降至46.93%）。GSM8K-V为视觉数学推理提供了新视角，并为开发更强大的视觉语言模型设立了新标准。",
      "translated_title": "GSM8K-V：视觉语言模型能否解决视觉语境下的小学数学应用题",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs."
    },
    {
      "title": "使用NVFP4预训练大型语言模型 (原标题: Pretraining Large Language Models with NVFP4)",
      "link": "https://arxiv.org/abs/2509.25149",
      "pubDate": "Mon, 29 Sep 2025 13:53:17 GMT",
      "isoDate": "2025-09-29T13:53:17.000Z",
      "creator": "NVIDIA, Felix Abecassis, Anjulie Agrusa, Dong Ahn, Jonah Alben, Stefania Alborghetti, Michael Andersch, Sivakumar Arayandi, Alexis Bjorlin, Aaron Blakeman, Evan Briones, Ian Buck, Bryan Catanzaro, Jinhang Choi, Mike Chrzanowski, Eric Chung, Victor Cui, Steve Dai, Bita Darvish Rouhani, Carlo del Mundo, Deena Donia, Burc Eryilmaz, Henry Estela, Abhinav Goel, Oleg Goncharov, Yugi Guvvala, Robert Hesse, Russell Hewett, Herbert Hum, Ujval Kapasi, Brucek Khailany, Mikail Khona, Nick Knight, Alex Kondratenko, Ronny Krashinsky, Ben Lanir, Simon Layton, Michael Lightstone, Daniel Lo, Paulius Micikevicius, Asit Mishra, Tim Moon, Deepak Narayanan, Chao Ni, Abhijit Paithankar, Satish Pasumarthi, Ankit Patel, Mostofa Patwary, Ashwin Poojary, Gargi Prasad, Sweta Priyadarshi, Yigong Qin, Xiaowei Ren, Oleg Rybakov, Charbel Sakr, Sanjeev Satheesh, Stas Sergienko, Pasha Shamis, Kirthi Shankar, Nishant Sharma, Mohammad Shoeybi, Michael Siu, Misha Smelyanskiy, Darko Stosic, Dusan Stosic, Bor-Yiing Su, Frank Sun, Nima Tajbakhsh, Shelby Thomas, Przemek Tredak, Evgeny Tsykunov, Gandhi Vaithilingam, Aditya Vavre, Rangharajan Venkatesan, Roger Waleffe, Qiyu Wan, Hexin Wang, Mengdi Wang, Lizzie Wei, Hao Wu, Evan Wu, Keith Wyss, Ning Xu, Jinze Xue, Charlene Yang, Yujia Zhai, Ruoxi Zhang, Jingyang Zhu, Zhongbo Zhu",
      "summary": "## 使用NVFP4预训练大型语言模型：提升效率与性能\n\n### 引言\n大型语言模型（LLM）在多个领域展现出强大的解决问题能力，并且随着模型规模、训练数据集大小和质量的提升，其性能持续增强。然而，训练前沿模型需要巨大的计算资源（数十到数百yottaflops）、时间和能源投入。因此，提高预训练效率对于开发下一代更强大的LLM至关重要。\n\n### 挑战与机遇\n目前，8位浮点（FP8）训练已广泛采用，但转向更窄的精度，如4位浮点（FP4），有望进一步提升计算速度和资源利用率。然而，在这一精度水平进行量化对训练稳定性、收敛性和实现带来了挑战，特别是对于在长序列上训练的大规模模型。\n\n### NVFP4训练方法\n本研究提出了一种新颖的方法，旨在利用NVFP4格式稳定且准确地训练大型语言模型。该方法整合了多项关键技术：\n\n*   **随机哈达玛变换（RHT）**：用于限制块级异常值，确保数据分布的稳定性。\n*   **二维量化方案**：在正向和反向传播过程中提供一致的表示，维持计算的准确性。\n*   **随机舍入**：用于无偏梯度估计，避免量化引入的系统性误差。\n*   **选择性高精度层**：在模型关键部分保留高精度计算，以维持整体性能。\n\n### 实验验证与结果\n为了验证该方法的有效性，研究人员进行了一项大规模训练实验：\n\n*   **模型规模**：训练了一个120亿参数的模型。\n*   **训练数据**：使用了10万亿个token进行训练，这是迄今为止公开记录的最长的4位精度训练运行。\n*   **性能对比**：实验结果表明，使用NVFP4预训练技术训练的模型，其训练损失和下游任务准确性与FP8基线模型相当。\n\n### 结论\n这些发现强调，NVFP4与本研究提出的训练方法相结合，代表了窄精度LLM训练算法的重大进步。它为在保持性能的同时，显著提高LLM预训练效率和资源利用率开辟了新的途径。",
      "shortSummary": "本研究提出了一种使用NVFP4格式预训练大型语言模型（LLM）的新方法，旨在解决4位精度训练的稳定性挑战。该方法结合了随机哈达玛变换、二维量化、随机舍入和选择性高精度层。通过在10万亿个token上训练一个120亿参数的模型，实验证明其训练损失和下游任务准确性与FP8基线相当。这标志着窄精度LLM训练算法的重大进展，有望大幅提升训练效率和资源利用率。",
      "translated_title": "使用NVFP4预训练大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms."
    },
    {
      "title": "MGM-Omni：将全能大型语言模型扩展到个性化长时程语音 (原标题: MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech)",
      "link": "https://arxiv.org/abs/2509.25131",
      "pubDate": "Mon, 29 Sep 2025 13:48:28 GMT",
      "isoDate": "2025-09-29T13:48:28.000Z",
      "creator": "Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia",
      "summary": "# MGM-Omni：将全能大型语言模型扩展到个性化长时程语音\n\nMGM-Omni 是一种统一的全能大型语言模型（Omni LLM），旨在实现全模态理解和富有表现力的长时程语音生成。它通过创新的架构设计，解决了传统级联管道在语音合成方面的局限性，并建立了高效的端到端范式。\n\n## 核心设计理念\n\n*   **“脑-口”设计**：MGM-Omni 采用独特的“脑-口”设计，其双轨、基于令牌的架构将多模态推理与实时语音生成清晰地解耦。这种设计促进了高效的跨模态交互和低延迟的流式语音生成。\n*   **解耦架构**：通过将推理和生成分离，MGM-Omni 能够更灵活、高效地处理复杂的全模态任务。\n\n## 理解能力\n\n*   **统一训练策略**：结合双音频编码器设计，MGM-Omni 能够实现跨不同声学条件的长篇音频感知。\n*   **长篇音频感知**：模型在理解和处理长时间音频内容方面表现出色，能够适应各种复杂的声学环境。\n\n## 生成能力\n\n*   **分块并行解码方案**：为缩小文本-语音令牌速率差距而设计，显著加速了推理过程。\n*   **流式零样本语音克隆**：支持在长时间内保持稳定音色的流式零样本语音克隆。这意味着模型可以在没有预先训练数据的情况下，实时克隆语音并长时间保持其独特的音色。\n\n## 性能优势与创新点\n\n*   **数据高效训练**：相较于同期工作，MGM-Omni 以显著的数据高效性实现了上述能力。\n*   **卓越的性能**：\n    *   在长时间序列中保持音色一致性方面优于现有开源模型。\n    *   生成自然且上下文感知的语音。\n    *   实现卓越的长篇音频和全模态理解。\n*   **端到端范式**：MGM-Omni 建立了一个高效的端到端范式，用于全模态理解和可控的个性化长时程语音生成。\n\n## 结论\n\nMGM-Omni 代表了全模态理解和语音生成领域的一个重要进展，它通过其创新的架构和训练策略，为实现更自然、个性化和高效的人机交互奠定了基础。",
      "shortSummary": "MGM-Omni 是一种统一的全能大型语言模型（Omni LLM），旨在实现全模态理解和个性化长时程语音生成。它采用“脑-口”双轨架构，将多模态推理与实时语音生成解耦，实现高效跨模态交互和低延迟流式生成。通过统一训练策略和分块并行解码，MGM-Omni 能够进行长篇音频感知、加速推理，并支持音色稳定的流式零样本语音克隆。它以数据高效的方式，在保持音色一致性、生成自然语音及全模态理解方面超越现有模型，建立了高效的端到端范式。",
      "translated_title": "MGM-Omni：将全能大型语言模型扩展到个性化长时程语音",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation."
    },
    {
      "title": "从 f(x) 和 g(x) 到 f(g(x))：大型语言模型通过组合旧技能在强化学习中学习新技能 (原标题: From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones)",
      "link": "https://arxiv.org/abs/2509.25123",
      "pubDate": "Mon, 29 Sep 2025 13:44:27 GMT",
      "isoDate": "2025-09-29T13:44:27.000Z",
      "creator": "Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng",
      "summary": "# 大型语言模型通过组合旧技能在强化学习中学习新技能\n\n## 摘要\n\n本文探讨了强化学习（RL）是否能使大型语言模型（LLM）获得真正的新技能，而不仅仅是激活其现有能力。这一问题是关于RL在LLM后训练中作用的持续争论的核心。本文提供了具体证据，表明LLM在RL过程中可以通过组合现有技能来学习真正的新技能，这与人类获取新认知技能的核心机制之一相呼应。\n\n## 研究方法\n\n为了避免数据污染和其他混淆因素，并精确控制任务复杂性，研究人员开发了一个合成框架。在该框架中，一项“技能”被定义为根据输入 `x` 推断字符串转换函数 `f(x)` 输出的能力。\n\n## 关键发现\n\n*   **组合新技能的能力：** 当LLM在RL之前已经学习了函数 `f` 和 `g` 时，实验表明RL使其能够学习它们未曾见过的组合 `h(x) = g(f(x))`。\n*   **泛化能力：** 这种组合能力可以泛化到更复杂的问题，例如在RL训练中未曾见过的超过两个函数的组合。\n*   **技能迁移：** 令人惊讶的是，在一个源任务上获得的组合技能可以迁移到不同的目标任务。这种迁移甚至不需要在目标任务上进行组合训练，只需LLM预先了解目标任务的基本原子技能。\n*   **RL对推理行为的影响：** 定性分析表明，RL从根本上改变了模型的推理行为。\n*   **与下一词元训练的对比：** 相比之下，使用相同数据进行的下一词元训练未能产生上述任何发现。\n\n## 结论与建议\n\n本研究的系统性实验为LLM学习提供了新的见解，表明了首先构建具有基本技能的基础模型，然后利用RL来激励其学习针对复杂问题的高级、可泛化技能的价值。",
      "shortSummary": "本文通过合成框架证明，大型语言模型（LLM）在强化学习（RL）中能通过组合现有技能来学习真正的新技能。实验显示，LLM在学习了原子函数f(x)和g(x)后，能通过RL学习未曾见过的组合g(f(x))，且此能力可泛化并迁移到新任务。RL从根本上改变了模型的推理行为，这与下一词元训练不同。研究建议先构建具备基本技能的基础模型，再利用RL激励其学习高级、可泛化的复杂问题技能。",
      "translated_title": "从 f(x) 和 g(x) 到 f(g(x))：大型语言模型通过组合旧技能在强化学习中学习新技能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of &gt;2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems."
    },
    {
      "title": "迈向个性化深度研究：基准与评估 (原标题: Towards Personalized Deep Research: Benchmarks and Evaluations)",
      "link": "https://arxiv.org/abs/2509.25106",
      "pubDate": "Mon, 29 Sep 2025 13:39:17 GMT",
      "isoDate": "2025-09-29T13:39:17.000Z",
      "creator": "Yuan Liang, Jiaxian Li, Yuqing Wang, Piaohong Wang, Motong Tian, Pai Liu, Shuofei Qiao, Runnan Fang, He Zhu, Ge Zhang, Minghao Liu, Yuchen Eleanor Jiang, Ningyu Zhang, Wangchunshu Zhou",
      "summary": "## 迈向个性化深度研究：基准与评估\n\n### 背景与挑战\n深度研究智能体（Deep Research Agents, DRAs）在自主进行复杂调查和生成综合报告方面展现出巨大的实际应用潜力。然而，当前对DRAs的评估主要依赖于封闭式基准测试，而开放式深度研究基准仍然稀缺，并且普遍忽视了对个性化场景的考量。这导致现有评估无法充分反映DRAs在处理个性化需求方面的能力。\n\n### 引入个性化深度研究基准（PDRB）\n为了弥补这一空白，本文引入了**个性化深度研究基准（Personalized Deep Research Bench, PDRB）**。这是首个专门用于评估DRAs个性化能力的基准。PDRB的设计旨在模拟真实世界中用户与研究任务的复杂交互，从而提供更具挑战性和真实性的评估环境。\n\n### PDRB 的构成\nPDRB 精心构建了以下核心组件：\n*   **研究任务：** 包含10个不同领域中的50项多样化研究任务，涵盖了广泛的知识和信息需求。\n*   **用户档案：** 结合了25个真实的、多维度的用户档案。每个档案都包含：\n    *   **结构化人物属性：** 如职业、兴趣、背景知识等。\n    *   **动态真实世界情境：** 模拟用户在特定时间点或特定场景下的具体需求和偏好。\n*   **用户-任务查询：** 通过将50项研究任务与25个用户档案进行配对，PDRB 共生成了250个逼真的用户-任务查询。这些查询能够反映出用户在不同情境下对研究结果的个性化需求。\n\n### PQR 评估框架\n为了全面评估系统性能，本文提出了**PQR 评估框架**。该框架联合衡量以下三个关键维度：\n*   **(P) 个性化对齐（Personalization Alignment）：** 评估DRAs生成的研究报告在多大程度上符合特定用户的个性化需求、偏好和背景。\n*   **(Q) 内容质量（Content Quality）：** 评估研究报告的整体质量，包括信息的完整性、清晰度、组织结构和表达流畅性等。\n*   **(R) 事实可靠性（Factual Reliability）：** 评估研究报告中信息的准确性、真实性和来源的可靠性，确保内容没有虚假或误导性信息。\n\n### 实验与未来展望\n研究人员在一系列现有系统上进行了实验，结果突出了当前DRAs在处理个性化深度研究方面的现有能力和局限性。这项工作为开发和评估下一代真正个性化的AI研究助手奠定了坚实而严格的基础，有望推动人工智能在复杂信息获取和报告生成领域的进一步发展。",
      "shortSummary": "本文介绍了“个性化深度研究基准”（PDRB），这是首个用于评估深度研究智能体（DRAs）个性化能力的基准。PDRB包含10个领域的50项研究任务和25个真实用户档案，生成250个逼真的用户-任务查询。同时提出了PQR评估框架，衡量个性化对齐、内容质量和事实可靠性。该工作旨在为开发真正个性化的AI研究助手提供严格的评估基础，并揭示了现有系统的能力与局限。",
      "translated_title": "迈向个性化深度研究：基准与评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants."
    },
    {
      "title": "扩展通用数据分析智能体 (原标题: Scaling Generalist Data-Analytic Agents)",
      "link": "https://arxiv.org/abs/2509.25084",
      "pubDate": "Mon, 29 Sep 2025 13:23:08 GMT",
      "isoDate": "2025-09-29T13:23:08.000Z",
      "creator": "Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen",
      "summary": "## DataMind：扩展通用数据分析智能体的新范式\n\n### 引言与背景\n数据分析智能体正成为自动化科学发现和“创新AI”愿景的关键催化剂。然而，现有方法严重依赖于对专有模型的提示工程，而开源模型在处理多样化格式、大规模数据文件以及真实世界分析所需的长期、多步骤推理方面面临挑战。\n\n### DataMind 解决方案\n本文介绍了 DataMind，一个可扩展的数据合成和智能体训练方案，旨在构建通用数据分析智能体。DataMind 解决了构建开源数据分析智能体面临的三个关键挑战：\n*   数据资源不足\n*   训练策略不当\n*   基于代码的多轮交互不稳定\n\n### DataMind 的核心机制\nDataMind 具体应用了以下创新方法：\n1.  **任务合成机制**：\n    *   采用细粒度的任务分类法。\n    *   通过递归的“由易到难”任务组合机制，增加合成查询的多样性和难度。\n2.  **轨迹采样与过滤**：\n    *   使用知识增强的轨迹采样策略。\n    *   结合基于模型和基于规则的过滤机制。\n3.  **动态可调训练目标**：\n    *   结合了监督微调（SFT）和强化学习（RL）损失。\n4.  **多轮交互框架**：\n    *   设计了一个内存节约且稳定的基于代码的多轮交互框架。\n\n### DataMind-12K 数据集\n基于 DataMind，研究人员精心策划了 DataMind-12K，这是一个高质量的轨迹数据集，涵盖了数据分析任务的多个领域、任务类别和数据文件格式。\n\n### 实验结果与性能\n*   在 DataMind-12K 上训练后，DataMind-14B 在多个数据分析基准测试中取得了平均 71.16% 的最先进（SOTA）分数，超越了最强的专有基线模型 DeepSeek-V3.1 和 GPT-5。\n*   DataMind-7B 在所有开源模型中表现最佳，得分达到 68.10%。\n\n### 社区贡献与未来展望\n*   研究还结合了从探索性试验中获得的一些经验见解，旨在为社区提供关于智能体训练的可操作性见解。\n*   DataMind-12K 和 DataMind-7B、14B 将向社区发布，以促进未来的研究。\n\n**图片说明：**\n文章内容中不包含有效的实际图片链接，因此详细摘要中不包含任何图片。",
      "shortSummary": "本文提出 DataMind，一个可扩展的数据合成与智能体训练方案，旨在解决开源数据分析智能体在数据、训练和多轮交互方面的挑战。DataMind 通过细粒度任务分类、知识增强采样、动态训练目标及稳定多轮框架，构建了高质量数据集 DataMind-12K。基于此训练的 DataMind-14B 在数据分析基准测试中取得 71.16% 的SOTA分数，超越专有模型；DataMind-7B 也在开源模型中表现最佳。DataMind-12K 及模型将开源。",
      "translated_title": "扩展通用数据分析智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research."
    },
    {
      "title": "BRIDGE - 构建用于单目深度估计的强化学习深度到图像数据生成引擎 (原标题: BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation Engine for Monocular Depth Estimation)",
      "link": "https://arxiv.org/abs/2509.25077",
      "pubDate": "Mon, 29 Sep 2025 13:19:45 GMT",
      "isoDate": "2025-09-29T13:19:45.000Z",
      "creator": "Dingning Liu, Haoyu Guo, Jingyi Zhou, Tong He",
      "summary": "## BRIDGE：用于单目深度估计的强化学习深度到图像数据生成引擎\n\n### 背景与挑战\n*   单目深度估计（MDE）是计算机视觉领域的一项基础任务。\n*   传统MDE方法面临数据稀缺和质量不佳的限制，这严重影响了模型的鲁棒性。\n\n### 提出的解决方案：BRIDGE 框架\n*   我们提出了 **BRIDGE**，一个经过强化学习（RL）优化的深度到图像（D2I）生成框架，旨在克服现有MDE方法的局限性。\n\n### 创新数据生成\n*   **大规模合成：** BRIDGE 能够从多样化的源深度图合成超过 2000 万张逼真且几何精确的 RGB 图像。\n*   **内在配对：** 每张合成的 RGB 图像都与其对应的真实深度图进行内在配对，提供了高质量的监督数据。\n\n### 混合监督训练策略\n*   在 BRIDGE 生成的大规模数据集上训练深度估计模型。\n*   采用创新的混合监督策略，该策略将教师伪标签与真实深度相结合，以实现全面且鲁棒的模型训练。\n\n### 主要成果与优势\n*   **突破性进展：** 这种创新的数据生成和训练范式使 BRIDGE 在数据规模和领域多样性方面取得了显著突破。\n*   **超越现有技术：** 在定量评估和复杂场景细节捕捉方面，BRIDGE 持续优于现有最先进（SOTA）的方法。\n*   **通用鲁棒特征：** 框架促进了通用且鲁棒的深度特征的学习，提升了模型的泛化能力。\n\n### 资源可用性\n*   项目的代码和模型已公开，可在提供的链接获取。",
      "shortSummary": "BRIDGE 提出一个强化学习优化的深度到图像（D2I）生成框架，旨在解决单目深度估计（MDE）中数据稀缺和质量问题。它能从多样化深度图合成超过 2000 万张 RGB 图像及其真实深度。结合混合监督训练策略，BRIDGE 在规模和领域多样性上取得突破，性能超越现有最先进方法，促进了通用且鲁棒的深度特征学习。",
      "translated_title": "BRIDGE - 构建用于单目深度估计的强化学习深度到图像数据生成引擎",
      "images": [],
      "contentSource": "完整文章",
      "content": "Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/."
    },
    {
      "title": "我思故我玩：一个通过推理和规划学习游戏的智能体 (原标题: Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning)",
      "link": "https://arxiv.org/abs/2509.25052",
      "pubDate": "Mon, 29 Sep 2025 13:02:31 GMT",
      "isoDate": "2025-09-29T13:02:31.000Z",
      "creator": "Sai Wang, Yu Wu, Zhongwen Xu",
      "summary": "### Cogito, Ergo Ludo (CEL)：一个通过推理和规划学习游戏的智能体\n\n本文介绍了一种名为“Cogito, ergo ludo (CEL)”的新型智能体架构，旨在解决现有深度强化学习方法对大量经验的依赖以及其知识表示不透明的问题。CEL提出了一种不同的范式，即智能体通过推理和规划来学习游戏。\n\n#### 核心理念与方法\n\n*   **问题背景**：当前的人工智能智能体在掌握复杂环境方面取得了显著成功，但主流的深度强化学习方法通常需要海量的经验，并且其知识以不透明的方式编码在神经网络权重中。\n*   **CEL的创新**：CEL利用大型语言模型（LLM）来构建对其环境机制和自身策略的显式、基于语言的理解。\n*   **学习过程**：\n    *   **初始状态**：CEL从“白板”（tabula rasa）状态开始，除了动作集外，没有任何先验知识。\n    *   **交互与反思循环**：智能体通过一个交互和反思的循环进行操作。\n    *   **学习机制**：在每个回合结束后，智能体分析其完整的轨迹，执行两个并发的学习过程：\n        1.  **规则归纳（Rule Induction）**：智能体在此过程中完善其对环境动态的显式模型。\n        2.  **策略与策略手册总结（Strategy and Playbook Summarization）**：智能体将经验提炼成一个可操作的战略手册。\n\n#### 实验评估与结果\n\n*   **评估任务**：研究人员在多种网格世界任务（如扫雷、冰湖和推箱子）上对CEL进行了评估。\n*   **主要发现**：\n    *   CEL智能体成功学会了掌握这些游戏，通过自主发现游戏规则并从稀疏奖励中发展出有效的策略。\n    *   **消融研究（Ablation studies）**证实，迭代过程对于持续学习至关重要。\n\n#### 意义与展望\n\n这项工作展示了一条通向更通用和可解释智能体的路径。这些智能体不仅能有效地行动，还能通过对原始经验的显式推理，构建一个透明且不断改进的世界模型。",
      "shortSummary": "本文提出“Cogito, ergo ludo (CEL)”智能体，旨在通过推理和规划学习游戏，克服深度强化学习对经验的过度依赖和知识不透明性。CEL利用大型语言模型构建显式、基于语言的环境规则和策略理解。它从零开始，通过交互和反思循环，归纳环境规则并总结策略。在扫雷、冰湖和推箱子等网格世界任务中，CEL成功自主学习并掌握游戏，证实了其迭代学习过程的关键性，为开发更通用、可解释的智能体提供了新途径。",
      "translated_title": "我思故我玩：一个通过推理和规划学习游戏的智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience."
    },
    {
      "title": "随机策略评估足以用于可验证奖励的LLM推理 (原标题: Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards)",
      "link": "https://arxiv.org/abs/2509.24981",
      "pubDate": "Mon, 29 Sep 2025 12:09:07 GMT",
      "isoDate": "2025-09-29T12:09:07.000Z",
      "creator": "Haoran He, Yuxiao Ye, Qingpeng Cai, Chen Hu, Binxing Jiao, Daxin Jiang, Ling Pan",
      "summary": "### 随机策略评估足以用于可验证奖励的LLM推理\n\n本文介绍了一种名为ROVER（Random Policy Valuation for Diverse Reasoning）的新方法，旨在改进大型语言模型（LLM）在可验证奖励强化学习（RLVR）范式下的推理能力。\n\n**现有方法的挑战：**\n*   当前RLVR方法（如PPO和GRPO）主要依赖策略优化框架，这些框架遵循广义策略迭代，交替进行策略评估和策略改进。\n*   这些方法常面临训练不稳定和多样性崩溃的问题。\n*   它们需要复杂的启发式技巧和精心的参数调整。\n\n**核心洞察与理论突破：**\n*   作者观察到，数学推理中的标准RLVR可以被形式化为一个特殊的有限视野马尔可夫决策过程（MDP）。\n*   该MDP具有确定性状态转移、树状动态结构和二元终端奖励。\n*   尽管规模庞大，但其底层结构比通用控制设置（PPO等算法为此开发）更简单。\n*   基于此洞察，作者证明了一个令人惊讶的结果：最优动作可以从一个固定的均匀随机策略的Q函数中恢复。\n*   这一发现使得可以绕过广义策略迭代循环及其相关的启发式方法。\n\n**ROVER方法介绍：**\n*   ROVER（Random Policy Valuation for Diverse Reasoning）将上述理论原理转化为一种实用且可扩展的LLM数学推理算法。\n*   ROVER是一种极简但高效的RL方法，它通过对这些均匀策略Q值进行softmax操作来采样动作。\n*   该方法在整个训练过程中保持了多样性，允许持续探索多个有效的推理路径。\n\n**实验结果与优势：**\n*   ROVER在多个基础模型和标准数学推理基准上进行了评估。\n*   与现有复杂方法相比，ROVER展现出卓越的性能：\n    *   **质量提升**：pass@1指标提升+8.2，pass@256指标提升+16.8。\n    *   **多样性提升**：多样性提升+17.6%。\n*   这些显著的改进是在其激进简化（相比于强大且复杂的现有方法）的情况下实现的。",
      "shortSummary": "本文提出ROVER，一种用于LLM数学推理的极简RL方法。针对现有RLVR方法的不稳定性和多样性问题，作者发现数学推理MDP结构更简单，并证明最优动作可从固定随机策略的Q函数中恢复。ROVER利用这一原理，通过对均匀策略Q值进行softmax采样来选择动作，有效绕过复杂的策略迭代。实验表明，ROVER在质量（pass@1提升+8.2，pass@256提升+16.8）和多样性（+17.6%）上均显著优于现有复杂方法。",
      "translated_title": "随机策略评估足以用于可验证奖励的LLM推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both quality (+8.2 on pass@1, +16.8 on pass@256) and diversity (+17.6\\%), despite its radical simplification compared to strong, complicated existing methods."
    },
    {
      "title": "大规模自改进演示学习目标导向的语言引导导航 (原标题: Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale)",
      "link": "https://arxiv.org/abs/2509.24910",
      "pubDate": "Mon, 29 Sep 2025 11:15:54 GMT",
      "isoDate": "2025-09-29T11:15:54.000Z",
      "creator": "Songze Li, Zun Wang, Gengze Zhou, Jialu Li, Xiangyu Zeng, Limin Wang, Yu Qiao, Qi Wu, Mohit Bansal, Yi Wang",
      "summary": "## SID：大规模自改进演示学习目标导向的语言引导导航\n\n本文介绍了一种名为SID（Self-Improving Demonstrations）的学习方法，旨在解决目标导向的语言引导导航中代理探索能力不足的问题，并提升其在未知环境中的导航性能。\n\n### 挑战与背景\n\n*   **核心问题：** 目标导向的语言引导导航要求代理在没有逐步指令的情况下，根据语言描述在未知环境中自主探索并到达指定目标。\n*   **现有方法局限：** 当前方法主要依赖最短路径轨迹进行训练，导致导航代理缺乏有效的探索先验知识，难以应对复杂多变的环境。\n\n### SID方法概述\n\nSID通过一个迭代的自我提升流程来解决上述挑战，其工作机制如下：\n\n1.  **初始代理训练：** 首先，SID利用从环境中采样的最短路径数据训练一个初始导航代理。\n2.  **生成探索轨迹：** 接着，这个初始代理被用来生成新颖的探索轨迹。这些轨迹包含了比单纯的最短路径更强的探索策略。\n3.  **迭代改进：** 这些新生成的探索轨迹作为高质量的演示数据，用于训练一个性能更优的代理。这个更优的代理反过来又能在下一轮训练中生成更高质量的代理演示。\n4.  **自我提升循环：** 这一迭代的自我提升流程持续进行，使得代理的探索能力和导航性能不断增强。\n\n### 关键优势与成果\n\n*   **可扩展性：** SID的迭代自改进流程能够轻松扩展到新的环境，展现出良好的泛化能力。\n*   **可迁移性：** 生成的演示数据可以跨多种语言引导导航任务进行迁移，显著提升了各类目标导向导航任务的性能上限。\n*   **性能显著提升：** 广泛的实验证明，SID显著增强了导航代理的探索能力和泛化能力。\n*   **最先进表现：** 最终的代理在目标导向的语言引导导航任务（包括REVERIE和SOON）上取得了新的最先进（state-of-the-art）性能。\n    *   特别是在SOON任务的未见验证集上，成功率达到了50.9%，比之前领先的方法高出13.9%。\n\n### 结论\n\nSID通过其独特的自改进演示机制，有效解决了语言引导导航中探索能力不足的难题，为代理提供了强大的探索策略，并在多个基准任务上树立了新的性能标杆。",
      "shortSummary": "本文提出SID（Self-Improving Demonstrations）方法，旨在提升目标导向语言引导导航代理的探索能力。SID通过迭代过程，首先用最短路径数据训练初始代理，然后利用该代理生成更具探索性的轨迹作为演示，进而训练出更优代理，实现自我提升。该方法显著增强了代理的探索和泛化能力，并在REVERIE和SOON等任务上取得了最先进的性能，例如在SOON未见验证集上成功率达50.9%，超越现有方法13.9%。",
      "translated_title": "大规模自改进演示学习目标导向的语言引导导航",
      "images": [],
      "contentSource": "完整文章",
      "content": "Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%."
    },
    {
      "title": "BOE-XSUM：西班牙法律法令与通知的清晰语言极端摘要 (原标题: BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal Decrees and Notifications)",
      "link": "https://arxiv.org/abs/2509.24908",
      "pubDate": "Mon, 29 Sep 2025 11:15:17 GMT",
      "isoDate": "2025-09-29T11:15:17.000Z",
      "creator": "Andrés Fernández García, Javier de la Rosa, Julio Gonzalo, Roser Morante, Enrique Amigó, Alejandro Benito-Santos, Jorge Carrillo-de-Albornoz, Víctor Fresno, Adrian Ghajari, Guillermo Marco, Laura Plaza, Eva Sánchez Salido",
      "summary": "## BOE-XSUM：西班牙法律法令与通知的清晰语言极端摘要\n\n### 引言\n\n随着信息过载日益严重，对长文档进行简洁摘要的能力在日常生活中变得愈发重要。然而，目前西班牙语文档，特别是法律领域的文档，普遍缺乏此类摘要。\n\n### BOE-XSUM 数据集介绍\n\n为了解决这一空白，本研究推出了 **BOE-XSUM**，这是一个经过精心整理的数据集，包含3,648份来自西班牙《国家官方公报》（Boletín Oficial del Estado, BOE）的简洁、清晰语言摘要。\n\n*   **数据来源：** 西班牙《国家官方公报》（BOE）。\n*   **数据集规模：** 包含3,648份文档的摘要。\n*   **内容特点：** 摘要以简洁、清晰的语言编写，易于理解。\n*   **每个条目构成：** 数据集中的每个条目都包括一个简短摘要、原始文本及其文档类型标签。\n\n### 模型评估与结果\n\n研究团队对在BOE-XSUM数据集上进行微调的中型大型语言模型（LLMs）的性能进行了评估，并将其与通用生成模型在零样本（zero-shot）设置下进行了比较。\n\n*   **主要发现：** 结果表明，经过微调的模型显著优于其非专业化的通用模型。\n*   **最佳表现模型：** BERTIN GPT-J 6B（32位精度）。\n*   **性能提升：** 最佳表现模型BERTIN GPT-J 6B相较于表现最佳的零样本模型DeepSeek-R1，实现了24%的性能提升。\n*   **准确率对比：** BERTIN GPT-J 6B的准确率为41.6%，而DeepSeek-R1的准确率为33.5%。\n\n### 结论\n\nBOE-XSUM数据集的创建及其在微调大型语言模型方面的应用，为西班牙法律文档的极端摘要提供了一个有效的解决方案，并显著提升了摘要的质量和效率。这对于应对法律信息过载具有重要意义。",
      "shortSummary": "该研究介绍了BOE-XSUM数据集，旨在解决西班牙法律文档缺乏简洁摘要的问题。该数据集包含3,648份来自《国家官方公报》的清晰语言摘要。通过在BOE-XSUM上微调中型大型语言模型，研究发现微调模型显著优于通用模型。其中，BERTIN GPT-J 6B表现最佳，相较于零样本模型DeepSeek-R1，性能提升24%，准确率达41.6%。这表明BOE-XSUM有效提升了西班牙法律文档的摘要能力。",
      "translated_title": "BOE-XSUM：西班牙法律法令与通知的清晰语言极端摘要",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\\% vs.\\ 33.5\\%)."
    },
    {
      "title": "OpenGPT-4o-Image：一个用于高级图像生成和编辑的综合数据集 (原标题: OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing)",
      "link": "https://arxiv.org/abs/2509.24900",
      "pubDate": "Mon, 29 Sep 2025 11:11:09 GMT",
      "isoDate": "2025-09-29T11:11:09.000Z",
      "creator": "Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang",
      "summary": "## OpenGPT-4o-Image：用于高级图像生成和编辑的综合数据集\n\n### 背景与问题\n*   统一多模态模型在图像生成和编辑方面的性能，受到训练数据质量和全面性的根本限制。\n*   现有数据集虽然涵盖了风格迁移和简单对象操作等基本任务，但往往缺乏系统性结构和应对真实世界应用所需的挑战性场景。\n\n### 解决方案：引入OpenGPT-4o-Image数据集\n*   为了解决这一瓶颈，研究人员引入了OpenGPT-4o-Image，一个大规模数据集。\n*   该数据集采用了一种新颖的方法构建，结合了分层任务分类法和自动化数据生成。\n\n### 数据集构建方法\n1.  **分层任务分类法 (Hierarchical Task Taxonomy)**\n    *   不仅包括文本渲染和风格控制等基本能力。\n    *   还引入了高度实用但具有挑战性的类别，例如：\n        *   用于化学插图的科学图像。\n        *   需要同时执行多项操作的复杂指令编辑。\n2.  **自动化数据生成流程 (Automated Data Generation Pipeline)**\n    *   利用结构化资源池和GPT-4o。\n    *   生成了8万对高质量的指令-图像对。\n    *   数据多样性受到严格控制。\n    *   涵盖了11个主要领域和51个子任务。\n\n### 实验结果与性能提升\n*   在OpenGPT-4o-Image数据集上对领先模型进行微调，在多个基准测试中取得了显著的性能提升。\n*   编辑任务（UniWorld-V1 on ImgEdit-Bench）的性能提升高达18%。\n*   生成任务（Harmon on GenEval）的性能提升高达13%。\n\n### 结论\n*   这项工作表明，系统性的数据构建是推动多模态AI能力发展的关键。",
      "shortSummary": "OpenGPT-4o-Image是一个为解决现有数据集局限性而设计的大规模图像生成和编辑数据集。它采用分层任务分类法和GPT-4o驱动的自动化流程，生成了8万对高质量指令-图像对，涵盖11个领域和51个子任务，包括复杂的科学图像和多操作编辑。实验证明，使用该数据集微调模型，在编辑和生成任务上分别实现了高达18%和13%的显著性能提升，强调了系统数据构建对多模态AI发展的重要性。",
      "translated_title": "OpenGPT-4o-Image：一个用于高级图像生成和编辑的综合数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities."
    },
    {
      "title": "RealUnify：统一模型真的从统一中受益吗？一项综合基准测试 (原标题: RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark)",
      "link": "https://arxiv.org/abs/2509.24897",
      "pubDate": "Mon, 29 Sep 2025 11:07:28 GMT",
      "isoDate": "2025-09-29T11:07:28.000Z",
      "creator": "Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, Zuyan Liu, Bohan Zeng, Ruizhe Chen, Qixun Wang, Zhuoran Zhang, Xinlong Chen, Chengzhuo Tong, Bozhou Li, Chaoyou Fu, Qiang Liu, Haotian Wang, Wenjing Yang, Yuanxing Zhang, Pengfei Wan, Yi-Fan Zhang, Ziwei Liu",
      "summary": "## RealUnify：评估统一模型双向能力协同的综合基准\n\n### 引言与研究背景\n\n*   **通用AI的进展：** 将视觉理解和生成整合到统一多模态模型中，是迈向通用人工智能的重要一步。\n*   **现有评估的局限性：** 当前的基准测试主要孤立地评估理解和生成能力，未能解决一个核心问题：这种架构上的统一是否真正促成了构成能力之间的协同互动？\n*   **未被回答的关键问题：** 现有范式无法判断统一模型能否利用其理解能力来增强生成，或者利用生成模拟来促进更深层次的理解。\n\n### RealUnify基准介绍\n\n为解决这一关键空白，研究引入了 **RealUnify**，这是一个专门设计用于评估双向能力协同的基准。\n\n*   **构成：** RealUnify包含1,000个经过人工精心标注的实例。\n*   **覆盖范围：** 涵盖10个类别和32个子任务。\n*   **核心评估轴：** RealUnify围绕两个核心轴构建：\n    1.  **理解增强生成 (Understanding Enhances Generation)：** 这类任务要求模型利用推理能力（例如常识、逻辑）来指导图像生成。\n    2.  **生成增强理解 (Generation Enhances Understanding)：** 这类任务要求模型进行心理模拟或重建（例如对变换或无序的视觉输入）来解决推理任务。\n\n### 双重评估协议\n\nRealUnify的一个关键贡献是其 **双重评估协议**，旨在精确识别性能瓶颈是源于核心能力缺陷还是未能有效整合这些能力。\n\n*   **组成：**\n    *   **直接端到端评估：** 对模型进行整体性能评估。\n    *   **诊断性分步评估：** 将任务分解为独立的理解和生成阶段进行诊断。\n\n### 大规模评估与主要发现\n\n研究对12个领先的统一模型和6个专业基线模型进行了大规模评估，结果揭示了以下关键发现：\n\n*   **协同效应的挑战：** 当前的统一模型在实现有效协同方面仍然面临挑战。\n*   **架构统一的局限性：** 结果表明，仅仅架构上的统一不足以带来预期的协同效应。\n*   **未来方向：** 这些发现强调了需要新的训练策略和归纳偏置，以充分释放统一模型的潜力。",
      "shortSummary": "统一多模态模型旨在整合视觉理解与生成，但现有基准未能评估其能力协同性。为解决此问题，研究引入了RealUnify，这是一个包含1000个实例、涵盖10类32子任务的新基准，专门评估“理解增强生成”和“生成增强理解”的双向协同。通过端到端和诊断性分步评估，研究发现当前统一模型在实现有效协同方面仍面临挑战，表明仅靠架构统一不足，需要新的训练策略和归纳偏置来充分发挥其潜力。",
      "translated_title": "RealUnify：统一模型真的从统一中受益吗？一项综合基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling."
    },
    {
      "title": "LOVE-R1：通过多步推理的自适应缩放机制推进长视频理解 (原标题: LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in Mechanism via Multi-Step Reasoning)",
      "link": "https://arxiv.org/abs/2509.24786",
      "pubDate": "Mon, 29 Sep 2025 09:43:55 GMT",
      "isoDate": "2025-09-29T09:43:55.000Z",
      "creator": "Shenghao Fu, Qize Yang, Yuan-Ming Li, Xihan Wei, Xiaohua Xie, Wei-Shi Zheng",
      "summary": "# LOVE-R1：通过多步推理的自适应缩放机制推进长视频理解\n\n## 挑战与问题\n当前的大型视频-语言模型（LVLMs）在长视频理解方面面临巨大挑战。这主要是由于长时序理解与详细空间感知之间存在冲突。传统的LVLMs采用统一的帧采样机制，即以相同的帧大小和固定的采样率进行采样，这不可避免地会牺牲时间线索或空间细节，从而导致次优解决方案。\n\n## 提出的解决方案：LOVE-R1\n为了缓解这一困境，研究人员提出了LOVE-R1模型，它能够自适应地对视频片段进行“缩放”（zoom in）。\n\n### LOVE-R1的工作原理\n1.  **初始采样**：模型首先接收到密集采样的帧，但这些帧的分辨率较低。\n2.  **自适应缩放**：如果需要获取某些空间细节，模型会根据其推理能力，对感兴趣的片段进行放大，使用高分辨率帧，直到获得关键的视觉信息。\n3.  **多步推理**：整个过程被实现为一个多步推理流程。\n\n### 训练方法\n为了训练模型的推理能力，LOVE-R1采用了以下策略：\n*   **CoT数据微调**：首先，模型在研究人员收集的3.8万条高质量CoT（Chain-of-Thought）数据上进行微调。\n*   **解耦强化微调**：通过解耦强化微调进一步增强模型。由于最终结果奖励无法提供细粒度的过程监督，研究人员将多步推理解耦为多个单步推理，并明确优化内部的缩放能力。\n\n## 实验结果与性能\n在长视频理解基准测试上的实验表明：\n*   **采样机制优势**：LOVE-R1模型采用的慢-快自适应帧采样机制，在采样密度和帧分辨率之间取得了出色的平衡。\n*   **性能提升**：LOVE-R1在4个常见的长视频理解基准测试中，平均比基线模型Qwen2.5-VL高出3.1个百分点。",
      "shortSummary": "LOVE-R1模型旨在解决大型视频-语言模型在长视频理解中面临的时间线索与空间细节冲突。它通过一种自适应缩放机制，首先以低分辨率密集采样帧，然后根据多步推理按需放大感兴趣区域以获取空间细节。该模型通过高质量CoT数据和解耦强化微调进行训练，明确优化了内部缩放能力。实验表明，LOVE-R1在采样密度和分辨率之间取得了良好平衡，并在4个长视频理解基准测试中，平均比基线模型Qwen2.5-VL高出3.1%。",
      "translated_title": "LOVE-R1：通过多步推理的自适应缩放机制推进长视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks."
    }
  ],
  "lastUpdated": "2025-09-30T09:34:38.221Z"
}