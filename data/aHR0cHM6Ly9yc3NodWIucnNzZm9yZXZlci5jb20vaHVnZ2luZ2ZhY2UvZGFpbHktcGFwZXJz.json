{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "InfGen：一种分辨率无关的可扩展图像合成范式 (原标题: InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis)",
      "link": "https://arxiv.org/abs/2509.10441",
      "pubDate": "Fri, 12 Sep 2025 13:48:57 GMT",
      "isoDate": "2025-09-12T13:48:57.000Z",
      "creator": "Tao Han, Wanghan Xu, Junchao Gong, Xiaoyu Yue, Song Guo, Luping Zhou, Lei Bai",
      "summary": "# InfGen：一种分辨率无关的可扩展图像合成范式\n\n本文介绍了 **InfGen**，一种旨在解决当前扩散模型在生成高分辨率图像时计算效率低下问题的创新范式。它提供了一种在不同设备上保持一致视觉体验的方法，对生产者和消费者都具有广泛应用。\n\n## 核心问题\n\n*   **计算需求高昂**：现有扩散模型生成图像的计算需求与分辨率呈二次方增长。\n*   **生成时间长**：例如，生成4K图像可能需要超过100秒，这严重影响了用户体验和应用效率。\n\n## InfGen 解决方案\n\nInfGen 是在潜在扩散模型（第二代）基础上发展而来的一种新方法。其核心思想和工作原理如下：\n\n*   **内容表示**：将扩散模型生成的固定潜在表示视为图像的内容表示。\n*   **一步生成器**：提出了一种一步生成器，用于从紧凑的、固定大小的潜在表示中解码出任意分辨率的图像。\n*   **替换 VAE 解码器**：InfGen 用这个新的生成器替换了传统的 VAE (Variational Autoencoder) 解码器。\n*   **无需重新训练**：关键优势在于，它可以在不重新训练原始扩散模型的情况下，从固定大小的潜在空间生成任意分辨率的图像。\n\n## 主要优势\n\n*   **简化流程**：大大简化了高分辨率图像的生成过程。\n*   **降低计算复杂度**：显著减少了生成高分辨率图像所需的计算资源。\n*   **广泛适用性**：可以应用于任何使用相同潜在空间的模型。\n*   **显著提速**：实验结果表明，InfGen 能够将4K图像的生成时间缩短到10秒以内。\n*   **提升模型能力**：使许多现有模型能够进入任意高分辨率图像生成时代。\n\n## 论文背景\n\n*   该研究已被 ICCV 2025 接受。",
      "shortSummary": "InfGen 提出了一种分辨率无关的图像合成范式，旨在解决现有扩散模型生成高分辨率图像时计算量大、耗时长的痛点。它通过用一步生成器替换 VAE 解码器，使得模型能够在不重新训练的情况下，从固定大小的潜在表示生成任意分辨率的图像。实验证明，InfGen 能将4K图像生成时间缩短至10秒以内，显著提高了效率和可扩展性。",
      "translated_title": "InfGen：一种分辨率无关的可扩展图像合成范式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the InfGen, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds."
    },
    {
      "title": "基于Inpainting引导的扩散大语言模型策略优化 (原标题: Inpainting-Guided Policy Optimization for Diffusion Large Language Models)",
      "link": "https://arxiv.org/abs/2509.10396",
      "pubDate": "Fri, 12 Sep 2025 12:44:31 GMT",
      "isoDate": "2025-09-12T12:44:31.000Z",
      "creator": "Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen",
      "summary": "扩散大语言模型（dLLMs）作为自回归LLMs的替代方案正在兴起，它们不仅性能具有竞争力，还支持Inpainting等独特的生成能力。本文探讨了Inpainting如何指导dLLM的强化学习（RL）算法设计。\n\n**RL对齐LLMs面临的挑战：**\n*   稀疏的奖励信号。\n*   当模型未能发现正确解决方案时，导致样本浪费。\n\n**dLLMs的Inpainting能力带来的独特机遇：**\n*   dLLMs的Inpainting能力可以有效引导探索过程。\n\n**介绍IGPO（Inpainting Guided Policy Optimization）：**\n*   **核心思想：** IGPO是一个RL框架，它在在线采样过程中策略性地插入部分真实推理轨迹。\n*   **工作机制：**\n    *   与提供完整解决方案不同，Inpainting将探索引向有前景的轨迹空间。\n    *   同时保留模型自身生成的推理过程。\n    *   有效连接了监督微调（SFT）和强化学习。\n*   **解决梯度问题：** 在GRPO等基于组的优化方法中，探索失败会导致零优势和梯度。IGPO能够恢复有意义的梯度，并提高样本效率。\n\n**附加技术：**\n*   对合成重写的简洁轨迹进行监督微调，使其更好地与dLLM的生成模式对齐。\n*   采用基于熵的过滤。\n\n**实验结果：**\n*   结合IGPO和上述训练方法，在三个数学基准测试（GSM8K、Math500和AMC）上取得了显著的性能提升。\n*   为全注意力掩码dLLMs实现了新的最先进（SOTA）结果。",
      "shortSummary": "本文提出了IGPO（Inpainting Guided Policy Optimization），一个针对扩散大语言模型（dLLMs）的强化学习框架。IGPO利用dLLMs的Inpainting能力，在在线采样时策略性地插入部分真实推理轨迹，以解决RL探索中的稀疏奖励和样本浪费问题。该方法引导探索、保留自我生成推理、恢复梯度并提高样本效率。结合其他技术，IGPO在GSM8K、Math500和AMC等数学基准测试中为全注意力掩码dLLMs取得了新的最先进成果。",
      "translated_title": "基于Inpainting引导的扩散大语言模型策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new state-of-the-art results for full-attention masked dLLMs."
    },
    {
      "title": "虚拟代理经济 (原标题: Virtual Agent Economies)",
      "link": "https://arxiv.org/abs/2509.10147",
      "pubDate": "Fri, 12 Sep 2025 07:20:11 GMT",
      "isoDate": "2025-09-12T07:20:11.000Z",
      "creator": "Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero",
      "summary": "## 虚拟代理经济：新兴经济层与可控市场设计\n\n### 引言\n\n随着自主AI代理的迅速普及，一个全新的经济层正在形成。在这个新兴的经济层中，AI代理能够以超越人类直接监督的规模和速度进行交易和协调。\n\n### “沙盒经济”框架\n\n文章提出了“沙盒经济”作为分析这一新兴系统的框架。该框架通过两个关键维度来描述和理解这一系统：\n\n*   **起源：** 探讨经济是自发涌现的还是经过有意设计的。\n*   **与人类经济的独立性：** 评估经济与既有的人类经济之间的分离程度，是高度可渗透的（与人类经济紧密相连）还是不可渗透的（独立运行）。\n\n### 当前发展轨迹与挑战\n\n目前的趋势表明，我们正朝着一个庞大且高度可渗透的AI代理经济自发涌现的方向发展。这带来了显著的机遇和挑战：\n\n*   **机遇：** 能够实现前所未有的协调程度。\n*   **挑战：** 包括潜在的系统性经济风险和加剧的社会不平等。\n\n### 可控AI代理市场的设计选择\n\n为了确保新兴技术变革能够与人类的长期集体繁荣相一致，文章讨论了多种可能的设计选择，旨在构建安全可控的AI代理市场：\n\n*   **拍卖机制：** 用于实现公平的资源分配和解决偏好冲突。\n*   **AI“任务经济”：** 旨在围绕实现集体目标进行有效协调。\n*   **社会技术基础设施：** 建立必要的结构以确保信任、安全和问责制。\n\n### 核心论点\n\n文章强调，我们应积极主动地设计可控的代理市场，以引导未来的技术变革，使其与人类的长期集体繁荣相一致。\n\n### 相关领域\n\n本文主要涉及人工智能（cs.AI）领域。",
      "shortSummary": "自主AI代理的快速发展催生了一个超越人类监督的新经济层。文章提出了“沙盒经济”框架来分析这一新兴系统，并指出当前趋势是形成一个庞大且高度可渗透的AI代理经济。这带来了前所未有的协调机遇，但也伴随着系统性经济风险和不平等加剧等挑战。为确保技术变革与人类福祉相符，文章主张通过设计拍卖机制、AI“任务经济”和健全的社会技术基础设施，积极构建可控的代理市场。",
      "translated_title": "虚拟代理经济",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid adoption of autonomous AI agents is giving rise to a new economic layer where agents transact and coordinate at scales and speeds beyond direct human oversight. We propose the \"sandbox economy\" as a framework for analyzing this emergent system, characterizing it along two key dimensions: its origins (emergent vs. intentional) and its degree of separateness from the established human economy (permeable vs. impermeable). Our current trajectory points toward a spontaneous emergence of a vast and highly permeable AI agent economy, presenting us with opportunities for an unprecedented degree of coordination as well as significant challenges, including systemic economic risk and exacerbated inequality. Here we discuss a number of possible design choices that may lead to safely steerable AI agent markets. In particular, we consider auction mechanisms for fair resource allocation and preference resolution, the design of AI \"mission economies\" to coordinate around achieving collective goals, and socio-technical infrastructure needed to ensure trust, safety, and accountability. By doing this, we argue for the proactive design of steerable agent markets to ensure the coming technological shift aligns with humanity's long-term collective flourishing."
    },
    {
      "title": "正确着色：弥合感知色彩空间与文本嵌入以改进扩散生成 (原标题: Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation)",
      "link": "https://arxiv.org/abs/2509.10058",
      "pubDate": "Fri, 12 Sep 2025 04:44:22 GMT",
      "isoDate": "2025-09-12T04:44:22.000Z",
      "creator": "Sung-Lin Tsai, Bo-Lun Huang, Yu Ting Shen, Cheng Yu Yeo, Chiang Tseng, Bo-Kai Ruan, Wen-Sheng Lien, Hong-Han Shuai",
      "summary": "### 解决文本到图像生成中的颜色对齐挑战\n\n**问题背景**\n\n*   在时尚、产品可视化和室内设计等应用中，文本到图像（T2I）生成中的准确颜色对齐至关重要。\n*   然而，当前的扩散模型在处理细微和复合颜色术语（例如，蒂芙尼蓝、酸橙绿、亮粉色）时表现不佳，经常生成与人类意图不符的图像。\n*   现有方法（如交叉注意力操作、参考图像或微调）未能系统地解决模糊颜色描述的问题。\n\n**提出的解决方案：无需训练的框架**\n\n为了在提示模糊的情况下精确渲染颜色，本文提出了一种无需训练的框架，通过以下方式增强颜色保真度：\n\n1.  **大型语言模型（LLM）消歧**：首先，利用大型语言模型（LLM）来解决文本提示中模糊的颜色相关术语。\n2.  **文本嵌入优化**：然后，根据解析出的颜色术语在CIELAB色彩空间中的空间关系，直接在文本嵌入空间中指导颜色混合操作，从而优化文本嵌入。\n\n**主要优势**\n\n*   **无需额外训练**：与现有方法不同，该方法无需额外的模型训练。\n*   **无需外部参考图像**：不依赖外部参考图像。\n*   **提高颜色精度**：在不损害图像质量的前提下，显著提高了颜色对齐的准确性。\n\n**研究成果与影响**\n\n*   实验结果表明，该框架在不影响图像质量的情况下，有效改善了颜色对齐。\n*   成功弥合了文本语义与视觉生成之间的鸿沟，使得T2I模型能够更准确地理解和渲染复杂的颜色描述。\n\n**出版信息**\n\n*   该研究已被 ACM Multimedia 2025 (MM '25) 接收。",
      "shortSummary": "针对文本到图像（T2I）生成中颜色描述不准确的问题，本文提出了一种无需训练的框架。该框架首先利用大型语言模型（LLM）消除提示中模糊的颜色术语，然后根据CIELAB色彩空间中解析出的颜色关系，直接在文本嵌入空间中优化文本嵌入。该方法无需额外训练或参考图像，显著提高了颜色对齐精度，同时保持了图像质量，弥合了文本语义与视觉生成之间的鸿沟。",
      "translated_title": "正确着色：弥合感知色彩空间与文本嵌入以改进扩散生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation."
    },
    {
      "title": "QuantAgent：价格驱动的多智能体LLM在高频交易中的应用 (原标题: QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading)",
      "link": "https://arxiv.org/abs/2509.09995",
      "pubDate": "Fri, 12 Sep 2025 02:35:40 GMT",
      "isoDate": "2025-09-12T02:35:40.000Z",
      "creator": "Fei Xiong, Xiang Zhang, Aosong Feng, Siqi Sun, Chenyu You",
      "summary": "## QuantAgent：高频交易中的多智能体LLM框架\n\n### 背景与挑战\n\n*   **现有LLM的局限性：** 近期大型语言模型（LLMs）在金融推理和市场理解方面展现出强大能力。TradingAgent和FINMEM等多智能体LLM框架通过利用基本面和情绪输入，增强了模型在长期投资任务中的战略决策能力。\n*   **高频交易（HFT）的独特需求：** 然而，这些系统不适用于高频交易（HFT）对高速、精度关键决策的严苛要求。HFT需要基于结构化、短周期信号（如技术指标、图表模式和趋势特征）的快速、风险感知决策，这与传统金融LLM应用中典型的长期语义推理截然不同。\n\n### QuantAgent：专为HFT设计\n\n*   **创新框架：** 我们引入了QuantAgent，这是第一个明确为高频算法交易设计的多智能体LLM框架。\n*   **模块化智能体：** 该系统将交易过程分解为四个专业智能体：\n    *   **指标（Indicator）智能体：** 专注于分析各种技术指标。\n    *   **模式（Pattern）智能体：** 识别图表模式和价格行为。\n    *   **趋势（Trend）智能体：** 捕捉市场趋势和动量。\n    *   **风险（Risk）智能体：** 评估和管理交易风险。\n*   **核心能力：** 每个智能体都配备了领域特定工具和结构化推理能力，以在短时间窗口内捕捉市场动态的不同方面。\n\n### 性能评估与结果\n\n*   **零样本评估：** 在对包括比特币和纳斯达克期货在内的十种金融工具进行的零样本评估中，QuantAgent展现出卓越的性能。\n*   **优异表现：** 在4小时交易间隔内，QuantAgent在预测准确性和累计回报方面均表现出色，超越了强大的神经网络和基于规则的基线模型。\n\n### 结论与展望\n\n*   **潜力揭示：** 我们的研究结果表明，将结构化金融先验知识与语言原生推理相结合，为高频金融市场中可追溯的实时决策系统开辟了新的潜力。",
      "shortSummary": "QuantAgent是首个专为高频交易（HFT）设计的多智能体LLM框架。它通过指标、模式、趋势和风险四个专业智能体，处理短周期市场信号，以实现快速、风险感知的决策。在对包括比特币和纳斯达克期货在内的十种金融工具进行的零样本评估中，QuantAgent在预测准确性和累计回报方面均表现出卓越性能，超越了现有基线。该研究揭示了结合结构化金融先验知识与LLM推理在高频金融市场中的巨大潜力。",
      "translated_title": "QuantAgent：价格驱动的多智能体LLM在高频交易中的应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) have demonstrated impressive capabilities in financial reasoning and market understanding. Multi-agent LLM frameworks such as TradingAgent and FINMEM augment these models to long-horizon investment tasks, leveraging fundamental and sentiment-based inputs for strategic decision-making. However, such systems are ill-suited for the high-speed, precision-critical demands of High-Frequency Trading (HFT). HFT requires rapid, risk-aware decisions based on structured, short-horizon signals, including technical indicators, chart patterns, and trend-based features, distinct from the long-term semantic reasoning typical of traditional financial LLM applications. To this end, we introduce QuantAgent, the first multi-agent LLM framework explicitly designed for high-frequency algorithmic trading. The system decomposes trading into four specialized agents, Indicator, Pattern, Trend, and Risk, each equipped with domain-specific tools and structured reasoning capabilities to capture distinct aspects of market dynamics over short temporal windows. In zero-shot evaluations across ten financial instruments, including Bitcoin and Nasdaq futures, QuantAgent demonstrates superior performance in both predictive accuracy and cumulative return over 4-hour trading intervals, outperforming strong neural and rule-based baselines. Our findings suggest that combining structured financial priors with language-native reasoning unlocks new potential for traceable, real-time decision systems in high-frequency financial markets."
    },
    {
      "title": "CMHG：一个用于中国少数民族语言标题生成的语料库和基准 (原标题: CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China)",
      "link": "https://arxiv.org/abs/2509.09990",
      "pubDate": "Fri, 12 Sep 2025 02:18:44 GMT",
      "isoDate": "2025-09-12T02:18:44.000Z",
      "creator": "Guixian Xu, Zeli Su, Ziyin Zhang, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong",
      "summary": "# CMHG：中国少数民族语言标题生成数据集与基准\n\n## 问题背景\n*   **语言挑战：** 中国的少数民族语言，如藏语、维吾尔语和传统蒙古语，由于其独特的书写系统与国际标准存在差异，面临着显著的挑战。\n*   **语料库匮乏：** 这种差异导致相关语料库严重缺乏，尤其是在标题生成等监督任务方面。\n\n## 解决方案：CMHG数据集\n*   **数据集引入：** 为解决上述空白，研究人员推出了一个名为“中国少数民族标题生成（CMHG）”的新型数据集。\n*   **数据集构成：**\n    *   包含100,000条藏语条目。\n    *   包含50,000条维吾尔语条目。\n    *   包含50,000条蒙古语条目。\n*   **用途：** 这些条目是专门为标题生成任务精心策划的。\n\n## 基准测试集\n*   **高质量测试集：** 除了核心数据集，研究还提出了一个由母语使用者标注的高质量测试集。\n*   **目的：** 该测试集旨在作为该领域未来研究的基准。\n\n## 期望与贡献\n*   研究人员希望CMHG数据集能成为推动中国少数民族语言标题生成领域发展的重要资源。\n*   同时，也期望它能为相关基准的开发做出贡献。",
      "shortSummary": "针对中国少数民族语言（如藏语、维吾尔语、蒙古语）标题生成任务中语料库匮乏的问题，研究人员推出了CMHG数据集。该数据集包含10万条藏语、各5万条维吾尔语和蒙古语条目，并附带一个由母语使用者标注的高质量测试集，旨在作为该领域的基准。CMHG有望推动中国少数民族语言标题生成研究的发展。",
      "translated_title": "CMHG：一个用于中国少数民族语言标题生成的语料库和基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks."
    },
    {
      "title": "LoFT：面向开放世界场景中长尾半监督学习的参数高效微调 (原标题: LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios)",
      "link": "https://arxiv.org/abs/2509.09926",
      "pubDate": "Thu, 11 Sep 2025 22:28:32 GMT",
      "isoDate": "2025-09-11T22:28:32.000Z",
      "creator": "Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su",
      "summary": "## LoFT：面向开放世界场景中长尾半监督学习的参数高效微调\n\n### 1. 背景与挑战\n*   **长尾学习的重要性**：长尾学习因其在现实世界场景中的广泛适用性而受到越来越多的关注。\n*   **长尾半监督学习 (LTSSL)**：通过将大量未标记数据整合到不平衡的标记数据集中，LTSSL 已成为一种有效的解决方案。\n*   **现有方法的局限性**：大多数先前的 LTSSL 方法从头开始训练模型，这常常导致模型过自信和生成低质量的伪标签。\n\n### 2. 提出的方法：LoFT\n*   **范式转变**：为了解决上述挑战，本文将 LTSSL 扩展到基础模型微调范式。\n*   **LoFT 框架**：提出了一个新颖的框架——LoFT（Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning，即通过参数高效微调实现长尾半监督学习）。\n*   **核心优势**：LoFT 证明了微调后的基础模型能够生成更可靠的伪标签，从而显著有益于不平衡学习。\n\n### 3. 开放世界场景下的扩展：LoFT-OW\n*   **更实际的设置**：本文进一步探索了在开放世界条件下的半监督学习，即未标记数据可能包含分布外（Out-of-Distribution, OOD）样本的情况。\n*   **LoFT-OW 方案**：为了处理这一问题，作者提出了 LoFT-OW（LoFT under Open-World scenarios），旨在提高模型在开放世界场景下的判别能力。\n\n### 4. 实验结果\n*   **卓越性能**：在多个基准测试上的实验结果表明，LoFT 方法相比现有方法取得了卓越的性能。\n*   **数据效率**：即使仅使用与以往工作相比 1% 的未标记数据，LoFT 也能达到优越的表现。\n\n### 5. 研究主题\n*   机器学习 (cs.LG)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "本文提出LoFT框架，通过参数高效微调基础模型，解决了长尾半监督学习中模型过自信和伪标签质量低的问题。LoFT能生成更可靠的伪标签，提升不平衡学习效果。针对开放世界场景，LoFT-OW进一步增强了模型对分布外样本的判别能力。实验证明，LoFT在多个基准测试上表现优越，即使仅使用少量未标记数据也能超越现有方法。",
      "translated_title": "LoFT：面向开放世界场景中长尾半监督学习的参数高效微调",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\\% of the unlabeled data compared with previous works."
    },
    {
      "title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准 (原标题: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark)",
      "link": "https://arxiv.org/abs/2509.09680",
      "pubDate": "Thu, 11 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-11T13:59:59.000Z",
      "creator": "Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li",
      "summary": "## FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准\n\n### 引言\n\n当前，开源文本到图像（T2I）模型的发展面临挑战，主要原因在于缺乏大规模、专注于推理能力的数据集以及全面的评估基准。这导致开源模型与领先的闭源系统之间存在显著的性能差距。\n\n### FLUX-Reason-6M 数据集\n\n为解决上述问题，研究团队引入了FLUX-Reason-6M数据集，旨在教授模型复杂的推理能力。\n\n*   **规模与内容：**\n    *   包含600万张由FLUX模型生成的高质量图像。\n    *   附带2000万条双语（英语和中文）描述，这些描述经过精心设计以支持复杂推理任务。\n*   **组织与结构：**\n    *   图像根据六个关键特征进行组织：想象力（Imagination）、实体（Entity）、文本渲染（Text rendering）、风格（Style）、情感（Affection）和构图（Composition）。\n    *   设计了明确的“生成思维链”（Generation Chain-of-Thought, GCoT），为图像生成步骤提供详细的分解，从而帮助模型理解和执行复杂指令。\n*   **资源投入：** 整个数据整理过程耗费了15,000个A100 GPU天，为社区提供了一个此前仅在大型工业实验室才能获得的宝贵资源。\n\n### PRISM-Bench 评估基准\n\n同时推出的PRISM-Bench（Precise and Robust Image Synthesis Measurement Benchmark）提供了一个新颖且全面的评估标准。\n\n*   **评估轨道：** 包含七个不同的评估轨道，其中包括一个极具挑战性的“长文本挑战”，该挑战利用GCoT来评估模型处理复杂长文本提示的能力。\n*   **评估方法：**\n    *   通过精心设计的提示词，利用先进的视觉-语言模型（VLM）进行评估。\n    *   实现对提示词-图像对齐（prompt-image alignment）和图像美学（image aesthetics）进行细致的、与人类评估高度一致的评估。\n\n### 实验与发现\n\n研究团队对19个领先的T2I模型在PRISM-Bench上进行了广泛评估。结果揭示了这些模型在推理能力上的关键性能差距，并明确指出了需要改进的具体领域。\n\n### 贡献与开放资源\n\nFLUX-Reason-6M数据集、PRISM-Bench基准以及相关的评估代码均已公开发布。此举旨在催化下一波面向推理的T2I生成研究，推动开源社区在这一领域取得突破。项目页面：this https URL",
      "shortSummary": "为解决开源文本到图像（T2I）模型在推理能力和评估方面的不足，本文提出了FLUX-Reason-6M数据集和PRISM-Bench基准。FLUX-Reason-6M包含600万张图像和2000万条双语描述，通过“生成思维链”（GCoT）教授复杂推理。PRISM-Bench提供七个评估轨道，包括长文本挑战，利用先进视觉-语言模型进行细致评估。对19个模型的评估揭示了性能差距。该工作发布了数据集、基准和代码，旨在推动面向推理的T2I生成发展。",
      "translated_title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ ."
    },
    {
      "title": "回报递减的假象：衡量大型语言模型（LLMs）的长期执行能力 (原标题: The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs)",
      "link": "https://arxiv.org/abs/2509.09677",
      "pubDate": "Thu, 11 Sep 2025 13:59:34 GMT",
      "isoDate": "2025-09-11T13:59:34.000Z",
      "creator": "Akshit Sinha, Arvindh Arun, Shashwat Goel, Steffen Staab, Jonas Geiping",
      "summary": "## 回报递减的假象：衡量大型语言模型（LLMs）的长期执行能力\n\n本文探讨了大型语言模型（LLMs）在处理长序列任务时是否面临回报递减的问题。研究发现，单步准确率的微小提升可以复合式地转化为模型成功完成任务长度的指数级改进。\n\n### 核心观点与发现：\n\n*   **执行而非推理是关键**：\n    *   当简单任务变得更长时，LLM的失败并非源于推理能力的不足，而是执行过程中出现的错误。\n    *   研究者提出，通过明确提供解决长序列任务所需的知识和计划，可以有效地隔离和衡量模型的执行能力。\n\n*   **模型规模与执行能力**：\n    *   即使小型模型在单步任务中能达到100%的准确率，大型模型也能正确执行显著更多的轮次，这表明模型规模对长期执行能力有显著影响。\n    *   模型的每步准确率会随着步骤数量的增加而下降。\n\n*   **自我条件效应（Self-conditioning Effect）**：\n    *   这种准确率下降不仅仅是由于长上下文限制。\n    *   研究观察到一种“自我条件效应”：当上下文包含模型之前轮次的错误时，模型更容易再次犯错。\n    *   单纯扩大模型规模并不能减少这种自我条件效应。\n\n*   **“思考模型”（Thinking Models）的表现**：\n    *   最近的“思考模型”不会出现自我条件效应。\n    *   这些模型能够在单次执行中完成更长的任务，显示出在长期执行方面的优越性。\n\n### 结论：\n\n通过关注执行能力，本文旨在调和关于LLM在解决复杂推理问题时表现出色，但在简单任务变长时却失败的争论。研究强调了扩大模型规模和在测试时进行顺序计算对于长序列任务的巨大益处。",
      "shortSummary": "本文指出，大型语言模型（LLMs）在长序列任务中的失败主要源于执行错误而非推理不足。单步准确率的微小提升能带来任务长度的指数级改进。研究发现，模型规模越大，执行能力越强，但存在“自我条件效应”——模型会因自身错误而更容易再次犯错，这并非单纯扩大规模能解决。然而，新型“思考模型”能克服此问题，展现出更强的长序列执行能力。这强调了模型规模和顺序计算对长任务的重要性。",
      "translated_title": "回报递减的假象：衡量大型语言模型（LLMs）的长期执行能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks."
    },
    {
      "title": "SpatialVID：一个带有空间标注的大规模视频数据集 (原标题: SpatialVID: A Large-Scale Video Dataset with Spatial Annotations)",
      "link": "https://arxiv.org/abs/2509.09676",
      "pubDate": "Thu, 11 Sep 2025 13:59:31 GMT",
      "isoDate": "2025-09-11T13:59:31.000Z",
      "creator": "Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, Yao Yao",
      "summary": "# SpatialVID：一个带有空间标注的大规模视频数据集\n\n## 摘要\n本文介绍了 **SpatialVID**，一个大规模的视频数据集，旨在解决当前空间智能模型（包括空间重建和世界探索）在可扩展性和真实世界保真度方面受限于高质量训练数据稀缺的问题。现有数据集在规模、多样性和标注丰富性方面存在局限，尤其缺乏真实世界动态场景中的地面真值相机运动信息。\n\n## SpatialVID 数据集概述\n*   **目标**：提供一个大规模、高质量的视频数据集，以促进空间智能领域的发展。\n*   **构成**：包含大量“野外”视频，场景和相机运动多样。\n*   **核心标注**：提供密集的3D标注，如每帧相机姿态、深度信息和运动指令。\n\n## 数据收集与处理\n1.  **原始数据收集**：收集了超过21,000小时的原始视频。\n2.  **分层过滤**：通过一个分层过滤管道，将原始视频处理成270万个视频片段。\n3.  **动态内容总量**：最终数据集包含7,089小时的动态内容。\n\n## 详细标注流程\n在视频片段处理完成后，通过后续的标注管道，为这些片段丰富了详细的空间和语义信息，具体包括：\n*   **相机姿态（Camera Poses）**：每帧的相机位置和方向。\n*   **深度图（Depth Maps）**：场景中物体的深度信息。\n*   **动态掩码（Dynamic Masks）**：识别和标注视频中的动态对象。\n*   **结构化字幕（Structured Captions）**：对视频内容进行描述。\n*   **序列化运动指令（Serialized Motion Instructions）**：描述相机或场景的运动序列。\n\n## 数据意义与贡献\n*   **丰富性和多样性**：SpatialVID的数据统计分析表明其具有丰富的多样性。\n*   **促进模型泛化**：这种丰富性直接有助于提高模型的泛化能力和性能。\n*   **重要资产**：SpatialVID被定位为视频和3D视觉研究社区的关键资产。",
      "shortSummary": "SpatialVID是一个大规模视频数据集，旨在解决空间智能模型训练数据不足的问题。它包含7,089小时的“野外”视频，具有多样化的场景和相机运动。数据集提供密集的3D标注，包括每帧相机姿态、深度图、动态掩码、结构化字幕和运动指令。SpatialVID的丰富性和多样性有望显著提升视频和3D视觉模型的泛化能力和性能，是该研究领域的重要资源。",
      "translated_title": "SpatialVID：一个带有空间标注的大规模视频数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community."
    },
    {
      "title": "SimpleVLA-RL：通过强化学习扩展VLA训练 (原标题: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.09674",
      "pubDate": "Thu, 11 Sep 2025 13:59:17 GMT",
      "isoDate": "2025-09-11T13:59:17.000Z",
      "creator": "Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding",
      "summary": "## SimpleVLA-RL：通过强化学习扩展VLA训练\n\n### VLA模型面临的挑战\n视觉-语言-动作 (VLA) 模型已成为机器人操作领域的强大范式。然而，它们面临两个基本挑战：\n*   **数据稀缺与高成本**：用于监督微调 (SFT) 的大规模人工操作机器人轨迹数据稀缺且成本高昂，限制了SFT的扩展。\n*   **泛化能力有限**：对涉及分布偏移的任务，VLA模型的泛化能力有限。\n\n### 强化学习的启发\n大型推理模型 (LRM) 的最新突破表明，强化学习 (RL) 可以显著增强逐步推理能力。这引发了一个关键问题：RL能否同样改进VLA模型的长周期逐步动作规划能力？\n\n### 引入SimpleVLA-RL框架\n本文提出了SimpleVLA-RL，一个专为VLA模型量身定制的高效强化学习框架。该框架在veRL的基础上，引入了以下VLA特定的改进措施，以提升训练效率和性能：\n*   **VLA特定的轨迹采样**：优化了数据收集和利用方式。\n*   **可扩展的并行化**：实现了高效的计算资源利用。\n*   **多环境渲染**：支持在多个模拟环境中进行训练，提高数据多样性。\n*   **优化的损失计算**：提升了模型学习的效率和稳定性。\n\n### 关键成果与性能\nSimpleVLA-RL在多个基准测试和真实世界任务中展现出卓越性能：\n*   **最先进性能**：应用于OpenVLA-OFT时，SimpleVLA-RL在LIBERO数据集上取得了最先进 (SoTA) 的性能。\n*   **超越基线**：通过引入探索增强策略，SimpleVLA-RL在RoboTwin 1.0和2.0上甚至超越了$\\pi_0$。\n*   **降低数据依赖与增强泛化**：SimpleVLA-RL不仅显著减少了对大规模数据的依赖，还实现了鲁棒的泛化能力。\n*   **超越SFT**：在真实世界任务中，SimpleVLA-RL的表现显著超越了传统的监督微调 (SFT) 方法。\n\n### 新现象“Pushcut”\n研究中还识别出一个新颖的现象，称为“pushcut”。在RL训练过程中，策略能够发现以前训练过程中未曾见过的模式，这表明RL具有超越现有数据分布的探索和发现能力。",
      "shortSummary": "SimpleVLA-RL是一个为视觉-语言-动作 (VLA) 模型设计的高效强化学习框架，旨在解决VLA模型在数据稀缺和泛化能力上的挑战。它通过引入VLA特定轨迹采样、可扩展并行化等改进，在LIBERO上实现了最先进性能，并在RoboTwin上超越了基线。SimpleVLA-RL显著减少了对大规模数据的依赖，增强了泛化能力，并在真实世界任务中表现优于监督微调。研究还发现策略在RL训练中能发现新模式。",
      "translated_title": "SimpleVLA-RL：通过强化学习扩展VLA训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL"
    },
    {
      "title": "理解与生成能否真正互利共赢——抑或仅仅是共存？ (原标题: Can Understanding and Generation Truly Benefit Together -- or Just Coexist?)",
      "link": "https://arxiv.org/abs/2509.09666",
      "pubDate": "Thu, 11 Sep 2025 13:57:59 GMT",
      "isoDate": "2025-09-11T13:57:59.000Z",
      "creator": "Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, Xinyan Xiao, Jingdong Wang, Haifeng Wang, Li Yuan",
      "summary": "## UAE：通过统一框架实现理解与生成的互利共赢\n\n本文提出了一种创新的范式，通过自动编码器（Auto-Encoder）的视角，将图像理解（Image-to-Text, I2T）视为编码器，负责将图像压缩成文本；将图像生成（Text-to-Image, T2I）视为解码器，负责从文本重建图像。以重建保真度作为统一的训练目标，该方法强制理解与生成过程之间实现连贯的双向信息流，从而带来相互增益。\n\n### 核心框架：UAE\n为实现这一目标，作者提出了UAE（Unified Auto-Encoder），一个用于统一多模态学习的新颖框架。\n\n*   **解码器预训练：** 首先，使用大规模、长上下文的图像描述对解码器进行预训练，以捕捉细粒度的语义和复杂的空间关系。\n\n*   **统一的GRPO强化学习：** 随后，通过强化学习（RL）引入了Unified-GRPO，该过程涵盖三个阶段：\n    1.  **冷启动阶段：** 使用语义重建损失温和地初始化编码器和解码器。\n    2.  **生成促进理解：** 在此阶段，训练编码器生成信息丰富的描述，以最大化解码器的重建质量，从而增强编码器的视觉理解能力。\n    3.  **理解促进生成：** 在此阶段，解码器被精炼以从这些描述中进行重建，这迫使解码器利用每一个细节，从而提高其对长上下文指令的遵循能力和生成保真度。\n\n### 评估与发现\n为了评估多模态统一模型（UMMs）的统一程度，作者引入了Unified-Bench，这是首个为此目的量身定制的基准。\n\n一个令人惊讶的“顿悟时刻”出现在多模态学习领域：随着强化学习的进展，编码器自主地生成了更具描述性的文本，而解码器同时展现出深刻理解这些复杂描述的能力，最终实现了惊人的高保真度重建。\n\n### 关键贡献\n*   提出了通过Auto-Encoder视角统一理解与生成的范式。\n*   引入了UAE框架和Unified-GRPO强化学习策略。\n*   创建了Unified-Bench基准来评估统一多模态模型。\n*   展示了理解与生成之间通过重建目标实现的互利共赢。",
      "shortSummary": "本文提出UAE框架，通过自动编码器视角统一图像理解（I2T编码器）和生成（T2I解码器）。以重建保真度为统一目标，并结合Unified-GRPO强化学习，实现理解与生成间的双向信息流。实验表明，随着训练进展，编码器能生成更具描述性的文本，解码器则能实现高保真度重建，证明了理解与生成可相互促进，实现互利共赢。",
      "translated_title": "理解与生成能否真正互利共赢——抑或仅仅是共存？",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity."
    },
    {
      "title": "LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试 (原标题: LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering)",
      "link": "https://arxiv.org/abs/2509.09614",
      "pubDate": "Thu, 11 Sep 2025 12:55:04 GMT",
      "isoDate": "2025-09-11T12:55:04.000Z",
      "creator": "Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang",
      "summary": "# LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试\n\n## 引言\n\n随着上下文窗口扩展到数百万个token的长上下文语言模型（LLM）的出现，为复杂的代码理解和软件开发评估带来了新的机遇。然而，现有的代码评估基准主要关注单函数完成或短上下文任务，这在评估LLM的长上下文能力方面存在显著空白。这些能力包括理解整个代码库、跨多个文件进行推理以及在大规模软件系统中保持架构一致性。\n\n## LoCoBench 概述\n\n为解决这一评估差距，我们提出了 LoCoBench，这是一个专门设计用于在真实、复杂的软件开发场景中评估长上下文LLM的综合性基准测试。\n\n## LoCoBench 的主要特点与构成\n\nLoCoBench 具有以下关键特性：\n\n*   **广泛的评估场景：** 提供了 8,000 个系统生成的评估场景。\n*   **多语言支持：** 涵盖 10 种编程语言。\n*   **可变上下文长度：** 上下文长度从 10K 扩展到 1M token，这种 100 倍的变化范围使得能够精确评估长上下文性能在实际软件开发环境中的下降情况。\n*   **八大任务类别：** LoCoBench 引入了 8 种任务类别，旨在捕捉 LLM 在长上下文环境下的核心能力：\n    1.  **架构理解：** 评估模型对软件系统整体架构的把握。\n    2.  **跨文件重构：** 考察模型在多个文件间进行代码重构的能力。\n    3.  **多会话开发：** 模拟持续开发过程中对上下文的维护和利用。\n    4.  **错误调查：** 评估模型在复杂代码库中定位和诊断错误的能力。\n    5.  **功能实现：** 衡量模型根据需求实现新功能的能力。\n    6.  **代码理解：** 评估模型对现有代码逻辑和意图的理解。\n    7.  **集成测试：** 考察模型在系统集成层面的测试能力。\n    8.  **安全分析：** 评估模型识别和分析代码中安全漏洞的能力。\n*   **高质量场景生成：** 通过一个 5 阶段的流程，我们创建了多样化、高质量的场景，这些场景旨在挑战 LLM 在前所未有的规模上对复杂代码库进行推理。\n\n## 评估框架\n\nLoCoBench 引入了一个全面的评估框架，该框架：\n\n*   包含 4 个维度下的 17 项指标，其中包括 8 项新的评估指标。\n*   所有指标最终综合为一个 LoCoBench 分数 (LCBS)，提供了一个统一的性能衡量标准。\n\n## 研究发现与结论\n\n我们对当前最先进的长上下文模型进行了评估，结果揭示了它们之间存在显著的性能差距。这表明，在复杂软件开发中实现有效且可靠的长上下文理解仍然是一个重大的未解决挑战，需要学术界和工业界投入更多关注和研究。\n\n## 可用性\n\nLoCoBench 基准测试已公开发布。",
      "shortSummary": "LoCoBench是一个综合性基准测试，旨在评估长上下文大型语言模型在复杂软件工程场景中的能力。它通过8000个跨10种编程语言、上下文长度达1M token的场景，弥补了现有基准在理解整个代码库和跨文件推理方面的不足。LoCoBench涵盖架构理解、错误调查等8类核心任务，并引入包含17项指标的评估框架。初步评估显示，当前模型在长上下文理解方面存在显著性能差距，表明这是一个亟待解决的挑战。LoCoBench已公开发布。",
      "translated_title": "LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench."
    },
    {
      "title": "Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成 (原标题: Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis)",
      "link": "https://arxiv.org/abs/2509.09595",
      "pubDate": "Thu, 11 Sep 2025 12:34:57 GMT",
      "isoDate": "2025-09-11T12:34:57.000Z",
      "creator": "Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan",
      "summary": "# Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成\n\n## 引言与背景\n*   **现有问题：** 当前的音频驱动虚拟形象视频生成方法在音视频真实感方面有所提升，但它们将指令条件视为仅由声学或视觉线索驱动的低级跟踪，未能建模指令所传达的交流目的。这导致叙事连贯性和角色表现力不足。\n\n## Kling-Avatar 解决方案\n*   **核心创新：** 引入Kling-Avatar，一个新颖的级联框架，它将多模态指令理解与逼真肖像生成相结合。\n*   **两阶段流水线：**\n    1.  **第一阶段：多模态大语言模型（MLLM）导演**\n        *   根据多样化的指令信号生成“蓝图视频”（blueprint video）。\n        *   负责控制高级语义，如角色动作和情感。\n    2.  **第二阶段：并行子剪辑生成**\n        *   以蓝图关键帧为指导。\n        *   采用“首尾帧策略”（first-last frame strategy）并行生成多个子剪辑。\n\n## 框架优势\n*   **全局到局部：** 这种全局到局部的框架设计能够保留精细细节，同时忠实地编码多模态指令背后更高层次的意图。\n*   **并行架构：** 支持快速稳定地生成长时程视频，适用于数字人直播和视频博客等实际应用。\n\n## 评估与成果\n*   **基准构建：** 构建了一个包含375个精心策划样本的基准，涵盖了多样化的指令和具有挑战性的场景。\n*   **实验结果：**\n    *   Kling-Avatar能够生成生动、流畅、长时程的视频，分辨率高达1080p，帧率达48 fps。\n    *   在以下方面表现出卓越性能：\n        *   唇部同步准确性\n        *   情感和动态表现力\n        *   指令可控性\n        *   身份保持\n        *   跨领域泛化能力\n*   **行业地位：** 这些结果确立了Kling-Avatar作为语义驱动、高保真音频驱动虚拟形象合成新基准的地位。",
      "shortSummary": "Kling-Avatar是一个创新的级联框架，旨在解决现有音频驱动虚拟形象生成方法在叙事连贯性和角色表现力方面的不足。它通过结合多模态指令理解与逼真肖像生成，采用两阶段流水线：首先，MLLM导演根据指令生成蓝图视频以控制高级语义；其次，并行生成子剪辑。该方法能快速稳定地生成长时程、高保真视频（最高1080p，48fps），在唇部同步、情感表达、指令可控性、身份保持和泛化能力上表现卓越，为语义驱动的虚拟形象合成树立了新基准。",
      "translated_title": "Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis."
    },
    {
      "title": "ObjectReact：学习用于视觉导航的物体相对控制 (原标题: ObjectReact: Learning Object-Relative Control for Visual Navigation)",
      "link": "https://arxiv.org/abs/2509.09594",
      "pubDate": "Thu, 11 Sep 2025 12:34:17 GMT",
      "isoDate": "2025-09-11T12:34:17.000Z",
      "creator": "Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid",
      "summary": "# ObjectReact：学习用于视觉导航的物体相对控制\n\n## 1. 背景与问题\n\n*   **视觉导航的兴起：** 近年来，仅使用单个摄像头和拓扑地图进行视觉导航，已成为替代需要额外传感器和3D地图方法的有吸引力选择。\n*   **传统方法的局限性：** 这种方法通常采用“图像相对”的方式，从当前观测图像和子目标图像对中估计控制。然而，图像级别的世界表示存在局限性，因为图像严格受限于智能体的姿态和具体形态。\n\n## 2. 提出的新范式：“物体相对”控制\n\n*   **核心思想：** 与图像不同，物体是地图的固有属性，提供了一种与具体形态和轨迹无关的世界表示。\n*   **“物体相对”控制的优势：**\n    *   **a) 路径遍历灵活性：** 可以在不严格模仿先前经验的情况下遍历新路线。\n    *   **b) 解耦问题：** 将控制预测问题与图像匹配问题解耦。\n    *   **c) 跨形态部署不变性：** 在跨形态部署中实现高度不变性，适用于训练-测试和建图-执行设置中的各种变化。\n\n## 3. 技术实现\n\n*   **拓扑度量地图表示：** 提出了一种以“相对”3D场景图形式存在的拓扑度量地图表示。\n    *   **目的：** 用于获取更具信息量的物体级别全局路径规划成本。\n*   **局部控制器“ObjectReact”：**\n    *   **设计：** 直接以高级别的“WayObject Costmap”表示为条件进行训练。\n    *   **特点：** 消除了对显式RGB输入的需要。\n\n## 4. 实验与成果\n\n*   **性能优势：** 证明了学习“物体相对”控制相对于“图像相对”控制的优势。\n    *   **测试场景：** 涵盖传感器高度变化和挑战底层空间理解能力的多项导航任务（例如，沿反方向遍历地图轨迹）。\n*   **泛化能力：** 进一步表明，仅在模拟环境中训练的策略能够很好地泛化到真实的室内环境。\n\n## 5. 资源\n\n*   **项目页面：** 代码和补充材料可通过项目页面获取。",
      "shortSummary": "本文提出了一种名为“ObjectReact”的新型“物体相对”控制范式，用于单摄像头视觉导航。该方法通过利用物体作为与智能体姿态和形态无关的世界表示，克服了传统“图像相对”控制的局限性。它采用“相对”3D场景图进行路径规划，并训练一个基于“WayObject Costmap”的局部控制器，无需显式RGB输入。实验证明，“ObjectReact”在传感器高度变化和复杂导航任务中优于传统方法，并能从模拟环境泛化到真实世界。",
      "translated_title": "ObjectReact：学习用于视觉导航的物体相对控制",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/"
    },
    {
      "title": "VLA-Adapter：一种用于微型视觉-语言-动作模型的有效范式 (原标题: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2509.09372",
      "pubDate": "Thu, 11 Sep 2025 07:42:21 GMT",
      "isoDate": "2025-09-11T07:42:21.000Z",
      "creator": "Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, Donglin Wang",
      "summary": "## VLA-Adapter：微型视觉-语言-动作模型的有效范式\n\n### 研究背景与问题\n\n传统的视觉-语言-动作（VLA）模型通常通过在机器人数据上预训练大型视觉-语言模型（VLM）来连接感知和动作空间。尽管这种方法显著提升了性能，但也带来了高昂的训练成本和对大规模VLM的严重依赖。\n\n### VLA-Adapter的提出\n\n本文引入了VLA-Adapter，一种旨在有效连接视觉-语言（VL）表示与动作（A）的新范式。其核心目标是减少VLA模型对大规模VLM和大量预训练的依赖。\n\n### 核心方法与创新点\n\n1.  **系统性分析VL条件**：研究人员首先系统地分析了各种VL条件的有效性，并揭示了哪些条件对于连接感知和动作空间至关重要。\n2.  **轻量级策略模块与桥接注意力（Bridge Attention）**：基于上述洞察，VLA-Adapter提出了一个轻量级的策略模块，该模块集成了桥接注意力机制。此机制能够自主地将最优条件注入到动作空间中。\n\n### 实验结果与性能优势\n\nVLA-Adapter在模拟和真实世界的机器人基准测试中进行了广泛实验，结果表明：\n\n*   **高性能与轻量化**：该方法仅使用一个0.5B参数的骨干网络，在没有任何机器人数据预训练的情况下，实现了最先进（SOTA）水平的性能。\n*   **快速推理速度**：VLA-Adapter报告了迄今为止最快的推理速度。\n*   **低训练成本**：得益于其先进的桥接范式，VLA-Adapter能够在单个消费级GPU上仅用8小时就训练出一个强大的VLA模型，极大地降低了VLA模型的部署门槛。",
      "shortSummary": "VLA-Adapter提出了一种新范式，旨在降低视觉-语言-动作（VLA）模型对大规模视觉-语言模型（VLM）和大量预训练的依赖。通过系统分析VL条件并引入带有桥接注意力的轻量级策略模块，VLA-Adapter在仅0.5B参数骨干网络下，无需机器人数据预训练即可达到SOTA性能。它还实现了最快的推理速度，并在单个消费级GPU上8小时内完成训练，显著降低了VLA模型的部署门槛。",
      "translated_title": "VLA-Adapter：一种用于微型视觉-语言-动作模型的有效范式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/."
    },
    {
      "title": "OmniEVA：通过任务自适应3D接地和具身感知推理实现的具身多功能规划器 (原标题: OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning)",
      "link": "https://arxiv.org/abs/2509.09332",
      "pubDate": "Thu, 11 Sep 2025 06:32:22 GMT",
      "isoDate": "2025-09-11T06:32:22.000Z",
      "creator": "Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan",
      "summary": "# OmniEVA：具身多功能规划器\n\n## 引言\n多模态大型语言模型（MLLM）的最新进展为具身智能带来了新的机遇，使其能够实现多模态理解、推理、交互以及连续的空间决策。然而，当前的基于MLLM的具身系统面临两个关键限制。\n\n## 现有MLLM具身系统的局限性\n\n1.  **几何适应性差距（Geometric Adaptability Gap）**：\n    *   模型仅通过2D输入训练，或通过硬编码的3D几何注入，导致空间信息不足或2D泛化能力受限。\n    *   这使得模型在面对具有多样空间需求的任务时，适应性较差。\n\n2.  **具身约束差距（Embodiment Constraint Gap）**：\n    *   现有工作往往忽视真实机器人的物理约束和能力。\n    *   这导致生成的任务规划在理论上有效，但在实践中却无法执行。\n\n## OmniEVA的创新点与解决方案\n为了解决上述差距，研究人员引入了 **OmniEVA**——一个具身多功能规划器。OmniEVA通过两项关键创新，实现了先进的具身推理和任务规划：\n\n1.  **任务自适应3D接地机制（Task-Adaptive 3D Grounding mechanism）**：\n    *   引入了一个门控路由器（gated router），根据上下文需求对3D融合进行显式选择性调节。\n    *   这使得模型能够为多样化的具身任务实现上下文感知的3D接地。\n\n2.  **具身感知推理框架（Embodiment-Aware Reasoning framework）**：\n    *   将任务目标和具身约束共同整合到推理循环中。\n    *   从而产生既以目标为导向又可执行的规划决策。\n\n## 实验结果与性能\n广泛的实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，而且在广泛的下游场景中展现出强大的能力。通过对一系列提出的具身基准（包括原始任务和复合任务）的评估，证实了其鲁棒且多功能的规划能力。\n\n## 相关领域\n*   机器人学 (cs.RO)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "OmniEVA是一个具身多功能规划器，旨在解决当前基于MLLM的具身系统在几何适应性和具身约束方面的局限。它通过引入任务自适应3D接地机制和具身感知推理框架，实现了上下文感知的3D接地和可执行的规划决策。实验证明，OmniEVA在具身推理方面达到了最先进的性能，并展现出强大的鲁棒性和多功能规划能力。",
      "translated_title": "OmniEVA：通过任务自适应3D接地和具身感知推理实现的具身多功能规划器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io"
    },
    {
      "title": "基于Transformer的漏洞检测在开源与工业数据上的跨域评估 (原标题: Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data)",
      "link": "https://arxiv.org/abs/2509.09313",
      "pubDate": "Thu, 11 Sep 2025 05:58:43 GMT",
      "isoDate": "2025-09-11T05:58:43.000Z",
      "creator": "Moritz Mock, Thomas Forrer, Barbara Russo",
      "summary": "# 基于Transformer的漏洞检测在开源与工业数据上的跨域评估\n\n本文旨在解决学术界提出的深度学习漏洞检测方案在实际开发中难以应用，以及其在工业环境中适用性鲜有探讨的问题。将此类技术从学术界转移到工业界面临多重挑战，包括可信度、遗留系统、有限的数字素养以及学术与工业专业知识之间的差距。对于深度学习而言，性能和与现有工作流的集成是额外的关注点。\n\n## 研究目标与方法\n\n本研究的主要目标是评估基于Transformer的深度学习模型（特别是CodeBERT）在漏洞检测方面的跨域泛化能力，并开发一个实用的工业集成解决方案。\n\n研究方法包括：\n*   **CodeBERT性能评估：** 评估CodeBERT在工业和开源软件中检测漏洞函数的能力。\n*   **跨域泛化分析：** 分析CodeBERT在开源数据上进行微调并在工业数据上测试，以及反之，其跨域泛化能力。\n*   **类别不平衡处理：** 探索处理类别不平衡的策略。\n\n## 主要研究发现\n\n研究结果揭示了跨域训练和测试的关键洞察：\n*   **领域内表现：** 在工业数据上训练的模型在其自身领域内能准确检测漏洞。\n*   **跨域性能下降：** 但这些模型在应用于开源代码时，性能会显著下降。\n*   **开放数据微调的优势：** 在开放数据上进行微调并结合适当欠采样技术的深度学习模型，能有效提高漏洞检测能力。\n\n## AI-DO系统：工业集成解决方案\n\n基于上述研究结果，本文开发了AI-DO（Automating vulnerability detection Integration for Developers' Operations）系统。\n\n*   **系统定位：** AI-DO是一个集成到持续集成-持续部署（CI/CD）流程中的推荐系统。\n*   **核心功能：** 它利用经过微调的CodeBERT在代码审查期间检测和定位漏洞。\n*   **设计理念：** 该系统旨在不中断现有开发工作流的前提下，提供高效的漏洞检测能力。\n\n## 工具评估\n\n*   **评估方式：** 通过对公司IT专业人员进行调查，评估了AI-DO工具的感知有用性。\n\n## 结论\n\n本研究深入探讨了基于Transformer的深度学习模型（CodeBERT）在开源和工业数据上进行漏洞检测的跨域泛化能力，并提出了一种实用的CI/CD集成推荐系统AI-DO，旨在弥合学术研究与工业应用之间的鸿沟，提升工业环境中漏洞检测的效率和可用性。",
      "shortSummary": "本文评估了基于Transformer的CodeBERT模型在开源和工业数据上进行漏洞检测的跨域泛化能力。研究发现，工业数据训练的模型在同领域表现良好，但跨域性能下降；而开放数据微调结合欠采样能提高检测能力。基于此，开发了AI-DO系统，一个CI/CD集成的推荐系统，利用CodeBERT在代码审查中检测和定位漏洞，且不中断工作流。该系统通过IT专业人员调查评估了其感知有用性，旨在弥合学术与工业漏洞检测的鸿沟。",
      "translated_title": "基于Transformer的漏洞检测在开源与工业数据上的跨域评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO(Automating vulnerability detection Integration for Developers' Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tool's perceived usefulness through a survey with the company's IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while a deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities."
    },
    {
      "title": "视觉可编程性：图表理解中“代码即思想”的指南 (原标题: Visual Programmability: A Guide for Code-as-Thought in Chart Understanding)",
      "link": "https://arxiv.org/abs/2509.09286",
      "pubDate": "Thu, 11 Sep 2025 05:22:16 GMT",
      "isoDate": "2025-09-11T05:22:16.000Z",
      "creator": "Bohao Tang, Yan Ma, Fei Zhang, Jiadi Su, Ethan Chern, Zhulin Hu, Zhixin Wang, Pengfei Liu, Ya Zhang",
      "summary": "## 视觉可编程性：图表理解中“代码即思想”的指南\n\n### 摘要\n\n本文提出了一种名为“视觉可编程性”的新方法，旨在解决视觉-语言模型（VLMs）在图表理解任务中面临的关键挑战。该方法通过引入“代码即思想”（Code-as-Thought, CaT）和自适应推理路径选择机制，显著提升了VLMs的推理能力和准确性。\n\n### 现有方法的局限性\n\n*   **依赖外部工具：** 现有方法常依赖外部工具，导致系统脆弱且受限于预定义的工具集。\n*   **单一推理策略：** 大多数方法采用单一推理策略，如基于文本的思维链（Chain-of-Thought, CoT），这限制了模型处理复杂图表的能力。\n*   **难以验证中间步骤：** 基于文本的推理中间步骤难以验证，这使得难以利用强化学习信号来奖励事实准确性，从而可能导致数值幻觉。\n\n### 提出“代码即思想”（CaT）方法\n\n*   **目标：** 将图表的视觉信息表示为可验证的、符号化的格式。\n*   **优势：** 符号化表示使得中间推理步骤更易于验证，为强化学习提供了更清晰的反馈信号。\n\n### 关键洞察与“视觉可编程性”\n\n*   **CaT的局限性：** 纯粹的、固定的代码实现（CaT）在面对符号表示不适用的复杂图表时会失效。\n*   **引入“视觉可编程性”：** 本文提出“视觉可编程性”作为一种可学习的属性。它使VLM能够判断特定图表-问题对更适合通过代码推理（CaT）解决，还是通过直接的视觉分析解决。\n\n### 自适应框架的实现\n\n*   **双路径选择：** 构建了一个自适应框架，其中VLM学习在以下两种推理路径之间进行选择：\n    1.  **CaT路径：** 基于代码的符号化推理。\n    2.  **直接视觉推理路径：** 直接进行视觉分析。\n\n### 创新的双重奖励系统\n\n*   **强化学习训练：** 模型的选择策略通过强化学习进行训练。\n*   **双重奖励机制：** 采用了一种新颖的双重奖励系统：\n    *   **数据准确性奖励：** 确保模型基于事实进行推理，防止数值幻觉。\n    *   **决策奖励：** 教导模型何时使用每种策略，防止其默认采用单一推理模式。\n\n### 实验结果与结论\n\n*   **强大而稳健的性能：** 实验证明，该方法在各种图表理解基准测试中展现出强大且稳健的性能。\n*   **核心发现：** 本研究表明，VLMs不仅可以被训练去推理，还可以被训练去学习“如何推理”，即动态地为每个任务选择最优的推理路径。",
      "shortSummary": "针对VLM在图表理解中现有方法的局限性，本文提出“代码即思想”（CaT）方法，以可验证的符号格式表示图表信息。为解决CaT在复杂图表上的不足，引入“视觉可编程性”，使VLM能自适应选择代码推理或直接视觉分析。该框架通过结合数据准确性和决策奖励的双重强化学习系统进行训练。实验证明，此方法在图表理解任务中表现出色，使VLM学会动态选择最佳推理路径。",
      "translated_title": "视觉可编程性：图表理解中“代码即思想”的指南",
      "images": [],
      "contentSource": "完整文章",
      "content": "Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task."
    },
    {
      "title": "驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度 (原标题: Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents)",
      "link": "https://arxiv.org/abs/2509.09265",
      "pubDate": "Thu, 11 Sep 2025 04:50:01 GMT",
      "isoDate": "2025-09-11T04:50:01.000Z",
      "creator": "Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang",
      "summary": "### 驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度\n\n本文提出了一种名为熵调制策略梯度（EMPG）的新框架，旨在解决大型语言模型（LLM）智能体在长周期任务中面临的挑战。\n\n**1. 背景与现有问题**\n*   **稀疏奖励问题：** 在长周期任务中，LLM智能体通常只能获得稀疏的、基于最终结果的奖励，这使得难以准确地将信用分配给中间步骤。\n*   **现有解决方案的局限性：** 之前的研究主要通过生成密集的奖励信号来指导学习，例如使用逆强化学习或过程奖励模型提供分步反馈。\n*   **本文发现的根本问题：** 作者识别出LLM学习动态中的一个基本问题——策略梯度的幅度与熵固有地耦合。这种耦合导致：\n    *   对于自信且正确的动作，更新幅度过小，学习效率低下。\n    *   对于不确定的动作，更新幅度可能过大，导致学习过程不稳定。\n\n**2. 提出的解决方案：熵调制策略梯度 (EMPG)**\n*   **核心思想：** EMPG通过根据分步不确定性和最终任务结果重新校准学习信号来解决上述问题。\n*   **EMPG的工作机制：**\n    *   **放大自信的正确动作：** 对于智能体自信且执行正确的动作，EMPG会放大其学习更新，从而加速学习。\n    *   **惩罚自信的错误：** 对于智能体自信但执行错误的动作，EMPG会施加惩罚，促使其纠正。\n    *   **衰减不确定步骤的更新：** 对于智能体不确定的步骤，EMPG会减弱其更新幅度，以稳定探索过程，避免因不确定性导致的大幅、不稳定更新。\n\n**3. 额外的奖励机制：未来清晰度奖励**\n*   为了进一步优化学习，EMPG还引入了一个“未来清晰度”奖励项。\n*   **目的：** 这个奖励项鼓励智能体寻找更可预测、更清晰的解决方案路径，从而提高任务完成的稳定性和效率。\n\n**4. 实验结果**\n*   **实验任务：** 研究人员在三个具有挑战性的智能体任务上对EMPG进行了全面实验，包括WebShop、ALFWorld和Deep Search。\n*   **性能提升：** 实验结果表明，EMPG取得了显著的性能提升，并显著优于现有的强大策略梯度基线方法。\n\n**5. 结论**\nEMPG通过创新性地解决策略梯度与熵的耦合问题，为长周期LLM智能体的学习提供了一个更稳定、高效的框架，并在多个复杂任务中展现出卓越的性能。",
      "shortSummary": "本文提出熵调制策略梯度（EMPG），以解决长周期LLM智能体中策略梯度幅度与熵耦合的问题。传统方法在稀疏奖励下信用分配困难，且自信正确动作更新小、不确定动作更新不稳定。EMPG根据分步不确定性和最终结果重新校准学习信号，放大自信正确更新，惩罚自信错误，并衰减不确定步骤更新以稳定探索。它还引入未来清晰度奖励。实验表明，EMPG在WebShop、ALFWorld和Deep Search任务上显著优于基线，提升了LLM智能体在复杂任务中的性能。",
      "translated_title": "驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度",
      "images": [],
      "contentSource": "完整文章",
      "content": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/"
    }
  ],
  "lastUpdated": "2025-09-15T09:37:20.128Z"
}