{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益 (原标题: LiveTradeBench: Seeking Real-World Alpha with Large Language Models)",
      "link": "https://arxiv.org/abs/2511.03628",
      "pubDate": "Wed, 05 Nov 2025 11:47:26 GMT",
      "isoDate": "2025-11-05T11:47:26.000Z",
      "creator": "Haofei Yu, Fenghai Li, Jiaxuan You",
      "summary": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益\n\n本文介绍了LiveTradeBench，一个用于在真实且不断变化的市场中评估大型语言模型（LLM）代理的实时交易环境。该研究旨在解决现有LLM基准测试的局限性，即它们通常在静态环境中进行，缺乏真实世界的动态性和不确定性，因此无法有效评估LLM在不确定性下的决策能力。\n\n**LiveTradeBench的设计原则**\n\nLiveTradeBench遵循以下三个核心设计原则，以提供更真实的评估环境：\n\n*   **实时数据流：**\n    *   系统直接流式传输市场价格和新闻数据，消除了对离线回测的依赖。\n    *   这有助于防止信息泄露，并能捕捉市场中的实时不确定性。\n*   **投资组合管理抽象：**\n    *   将控制从单一资产操作扩展到多资产配置。\n    *   整合了风险管理和跨资产推理能力。\n*   **多市场评估：**\n    *   在结构上不同的环境中进行评估，包括美国股票市场和Polymarket预测市场。\n    *   这些市场在波动性、流动性和信息流方面存在显著差异，提供了更全面的测试。\n\n**代理行为与评估过程**\n\n在LiveTradeBench的每一步，LLM代理会观察到最新的价格、新闻及其当前的投资组合状态。然后，代理需要输出百分比分配，以平衡风险和回报。\n\n研究人员使用LiveTradeBench对21个不同系列的LLM进行了为期50天的实时评估。\n\n**主要研究发现**\n\n评估结果揭示了以下关键发现：\n\n1.  **静态基准与实际表现的差异：** 在LMArena等静态基准测试中获得高分的模型，并不意味着它们在实际交易中也能取得优异表现。这表明静态评估与真实世界能力之间存在差距。\n2.  **模型独特的投资组合风格：** 不同的LLM模型展现出独特的投资组合风格，这反映了它们各自的风险偏好和推理动态。\n3.  **LLM利用实时信号的能力：** 一些LLM能够有效地利用实时市场信号来调整其决策，显示出一定的适应性。\n\n**结论与启示**\n\n这些发现强调了静态评估与真实世界能力之间的显著差距，并促使人们重新思考LLM的评估方式。研究呼吁开发新的基准测试，以在实时不确定性下测试LLM的顺序决策能力和一致性。",
      "shortSummary": "LiveTradeBench是一个评估大型语言模型（LLM）在真实、动态市场中决策能力的实时交易环境。它通过实时数据流、投资组合管理和多市场评估来模拟真实交易。研究发现，LLM在静态基准上的高分不代表实际交易成功，但部分模型能有效利用实时信号。这揭示了LLM静态评估与真实世界能力间的差距，呼吁更侧重不确定性下顺序决策的基准测试。",
      "translated_title": "LiveTradeBench：利用大型语言模型寻求真实世界的超额收益",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty."
    },
    {
      "title": "UniAVGen：基于非对称跨模态交互的统一音视频生成 (原标题: UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions)",
      "link": "https://arxiv.org/abs/2511.03334",
      "pubDate": "Wed, 05 Nov 2025 05:06:51 GMT",
      "isoDate": "2025-11-05T05:06:51.000Z",
      "creator": "Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, Limin Wang",
      "summary": "UniAVGen 是一个为解决现有开源音视频生成方法中唇形同步不佳和语义一致性不足问题而提出的统一框架。这些问题通常源于缺乏有效的跨模态建模。\n\n**UniAVGen 框架概述：**\n*   **目标：** 实现联合音视频生成。\n*   **核心架构：** 基于双分支联合合成设计，包含两个并行的 Diffusion Transformers (DiTs)，旨在构建一个内聚的跨模态潜在空间。\n\n**关键机制与创新：**\n1.  **非对称跨模态交互 (Asymmetric Cross-Modal Interaction)：**\n    *   这是 UniAVGen 的核心机制。\n    *   它实现了双向、时间对齐的交叉注意力，从而确保了音视频之间精确的时空同步和语义一致性。\n2.  **面部感知调制模块 (Face-Aware Modulation)：**\n    *   该模块增强了跨模态交互过程。\n    *   它能够动态地优先处理交互中的显著区域，例如面部，以提高生成质量。\n3.  **模态感知无分类器指导 (Modality-Aware Classifier-Free Guidance)：**\n    *   为在推理阶段提升生成保真度而引入。\n    *   这是一种新颖的策略，能够明确地放大跨模态相关性信号。\n\n**UniAVGen 的能力与应用：**\n*   其强大的联合合成设计使得单一模型能够无缝统一多种关键的音视频任务，包括：\n    *   联合音视频生成与续写\n    *   视频到音频的配音\n    *   音频驱动的视频合成\n\n**实验验证：**\n*   全面的实验结果表明，UniAVGen 在使用远少于现有方法的训练样本（130万 vs. 3010万）的情况下，在音视频同步、音色一致性和情感一致性方面均展现出整体优势。",
      "shortSummary": "UniAVGen是一个统一的音视频生成框架，旨在解决现有方法中唇形同步差和语义不一致的问题。它采用双分支架构，核心是非对称跨模态交互机制，通过双向、时间对齐的交叉注意力确保精确的时空同步和语义一致性。此外，面部感知调制和模态感知无分类器指导进一步提升生成质量。UniAVGen能统一多种音视频任务，并在使用更少训练数据的情况下，在同步、音色和情感一致性方面表现出显著优势。",
      "translated_title": "UniAVGen：基于非对称跨模态交互的统一音视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency."
    },
    {
      "title": "扩散语言模型是超级数据学习器 (原标题: Diffusion Language Models are Super Data Learners)",
      "link": "https://arxiv.org/abs/2511.03276",
      "pubDate": "Wed, 05 Nov 2025 03:17:42 GMT",
      "isoDate": "2025-11-05T03:17:42.000Z",
      "creator": "Jinjie Ni, Qian Liu, Longxu Dou, Chao Du, Zili Wang, Hang Yan, Tianyu Pang, Michael Qizhe Shieh",
      "summary": "扩散语言模型（DLMs）在严格控制的预训练设置下，当独特数据有限时，通过增加训练周期，始终超越自回归（AR）模型，展现出一种“交叉点”现象。这一交叉点会随着数据量增加或质量提高而推迟，随着模型增大而提前，并且在密集和稀疏架构中都持续存在。\n\n研究将DLM的性能提升归因于三个复合因素：\n\n*   **任意顺序建模（Any-order modeling）**：允许模型更灵活地处理数据。\n*   **迭代双向去噪带来的超密集计算（Super-dense compute from iterative bidirectional denoising）**：通过迭代过程更有效地利用计算资源。\n*   **内置蒙特卡洛增强（Built-in Monte Carlo augmentation）**：模型内部固有的数据增强机制。虽然输入或参数噪声也能在数据受限时改善AR模型，但无法弥补与DLM之间的差距。\n\n在规模化实验中，一个1.7B参数的DLM在10B独特Python tokens上，使用约1.5T-token的计算预算进行训练，其性能超越了在严格匹配设置下训练的AR编码器。此外，一个1B参数的DLM仅使用1B tokens（通过重复标准预训练数据，未采用任何特殊技巧），就在HellaSwag上取得了超过56%的准确率，在MMLU上取得了超过33%的准确率。\n\n研究还指出，在这种机制下，验证交叉熵的上升并不意味着下游性能的下降。",
      "shortSummary": "当独特训练数据有限时，扩散语言模型（DLMs）通过增加训练周期，持续超越自回归（AR）模型。DLMs的优势源于其任意顺序建模、迭代双向去噪带来的超密集计算以及内置的蒙特卡洛增强。实验表明，DLMs能从有限的重复数据中高效学习，在编码任务和HellaSwag、MMLU等基准测试上表现出色，证明了其作为“超级数据学习器”的能力。",
      "translated_title": "扩散语言模型是超级数据学习器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Under strictly controlled pre-training settings, we observe a Crossover: when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation; input or parameter noise improves AR under data constraint but cannot close the gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B unique Python tokens overtakes an AR coder trained with strictly matched settings. In addition, a 1B-parameter DLM achieves &gt; 56% accuracy on HellaSwag and &gt; 33% on MMLU using only 1B tokens, without any special tricks, just by repeating standard pre-training data. We also show that rising validation cross-entropy does not imply degraded downstream performance in this regime."
    },
    {
      "title": "MME-CC：一个具有挑战性的多模态认知能力评估基准 (原标题: MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity)",
      "link": "https://arxiv.org/abs/2511.03146",
      "pubDate": "Tue, 04 Nov 2025 22:09:16 GMT",
      "isoDate": "2025-11-04T22:09:16.000Z",
      "creator": "Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang, Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang, Ge Zhang, Yi Lin, Guang Shi, Chaoyou Fu, Wenhao Huang",
      "summary": "MME-CC：一个具有挑战性的多模态认知能力评估基准\n\n本文介绍了MME-CC（Multi-Modal Evaluation benchmark of Cognitive Capacity），一个旨在系统评估多模态大语言模型（MLLMs）视觉中心认知能力的新基准。\n\n**背景与问题**\n*   随着推理模型的快速发展，多模态在人类认知中的核心作用日益凸显，促使人们需要深入探究以视觉为中心的认知行为。\n*   然而，现有的多模态基准要么过分强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs认知能力的评估不足。\n\n**MME-CC基准介绍**\n*   **目的：** 解决现有基准的局限性，提供一个以视觉为基础的评估工具。\n*   **结构：** 将11个具有代表性的推理任务组织成三大类视觉信息：\n    *   **空间推理 (Spatial Reasoning)**\n    *   **几何推理 (Geometric Reasoning)**\n    *   **基于知识的推理 (Knowledge-based Reasoning)**\n*   **功能：** 提供对MLLMs在这些维度上认知能力的细粒度分析。\n\n**实验与发现**\n*   **实验对象：** 基于MME-CC，对16个具有代表性的MLLMs进行了广泛实验。\n*   **主要结果：**\n    *   **整体表现：** 闭源模型目前总体领先（例如，Gemini-2.5-Pro得分为42.66，而GLM-4.5V为30.45）。\n    *   **薄弱环节：** 空间推理和几何推理能力普遍较弱（得分低于或等于30%）。\n    *   **常见错误模式：**\n        *   方向错误 (orientation mistakes)\n        *   脆弱的跨视角身份持久性 (fragile cross-view identity persistence)\n        *   对反事实指令依从性差 (poor adherence to counterfactual instructions)\n    *   **思维链（Chain-of-Thought, CoT）分析：** CoT通常遵循一个三阶段过程（提取 -> 推理 -> 验证），并且严重依赖视觉提取。\n\n**展望**\n*   作者希望这项工作能够促使评估和模型设计将MLLMs的认知能力视为核心。",
      "shortSummary": "本文介绍了MME-CC，一个用于评估多模态大语言模型（MLLMs）视觉中心认知能力的新基准。MME-CC将11个推理任务分为空间、几何和知识三大类。实验发现，闭源模型总体表现领先，但MLLMs在空间和几何推理方面普遍较弱（≤30%），常出现方向错误和对反事实指令依从性差等问题。研究强调了将MLLMs认知能力作为评估和模型设计的核心。",
      "translated_title": "MME-CC：一个具有挑战性的多模态认知能力评估基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs' cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt; verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design."
    },
    {
      "title": "LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估 (原标题: LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation)",
      "link": "https://arxiv.org/abs/2511.03001",
      "pubDate": "Tue, 04 Nov 2025 16:13:51 GMT",
      "isoDate": "2025-11-04T16:13:51.000Z",
      "creator": "Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh, Jinyoung Yeo",
      "summary": "## LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估\n\n### 引言与背景\n\n尽管大型语言模型（LLM）在自动生成3D场景方面取得了进展，但生成的场景往往缺乏真实世界环境中应有的空间布局和对象属性。这一问题主要源于指令的粒度过粗，导致场景细节不足。因此，通过更详细、细粒度的指令来指导3D场景合成，使其能反映真实世界环境，变得至关重要。在不真实的场景中训练具身智能体，可能导致它们学习到与现实世界物理和语义显著偏离的先验知识，从而在实际部署时性能下降。因此，验证细粒度指令与生成场景之间的一致性对于有效学习至关重要。\n\n然而，当前的评估方法，如CLIPScore和视觉-语言模型（VLM），通常无法可靠地评估这种一致性。这主要是因为它们对3D场景的理解深度不足，常常导致场景组件的接地（grounding）不当。\n\n### LEGO-Eval 评估框架\n\n为了解决上述问题，本文引入了 **LEGO-Eval**，这是一个配备了多种工具的评估框架。这些工具旨在明确地对场景组件进行接地，从而实现更准确的场景-指令一致性评估。\n\n### LEGO-Bench 基准\n\n除了LEGO-Eval，本文还提出了 **LEGO-Bench**，这是一个包含详细指令的基准。这些指令具体指定了真实世界环境中复杂的布局和属性，为评估3D场景生成提供了标准。\n\n### 实验结果与发现\n\n实验结果表明：\n\n*   **LEGO-Eval 的优越性：** 在评估场景-指令一致性方面，LEGO-Eval 的F1分数比“VLM-as-a-judge”高出0.41，证明了其评估的准确性。\n*   **现有生成方法的局限性：** 使用LEGO-Bench进行基准测试揭示了当前生成方法存在的显著局限性。在所有评估的方法中，能够完全符合细粒度指令的场景生成成功率最高仅为10%。\n\n### 总结\n\n这项工作强调了细粒度评估对于提升3D具身环境合成质量的重要性，并提出了LEGO-Eval框架和LEGO-Bench基准来应对这一挑战。实验结果表明，当前3D场景生成技术在实现与细粒度指令完全一致的场景方面仍有很大的提升空间。\n\n### 其他信息\n\n*   **状态：** 工作进展中 (Work in Progress)\n*   **主题：** 计算与语言 (cs.CL)\n*   **arXiv ID：** arXiv:2511.03001",
      "shortSummary": "针对大型语言模型（LLM）生成的3D场景缺乏真实感和细粒度评估不足的问题，本文提出了LEGO-Eval框架和LEGO-Bench基准。LEGO-Eval利用工具明确地对场景组件进行接地，以实现更准确的场景-指令一致性评估，其F1分数比现有方法高0.41。LEGO-Bench提供包含复杂布局和属性的详细指令。实验表明，当前3D场景生成方法在完全符合细粒度指令方面成功率最高仅为10%，揭示了现有技术的显著局限性。",
      "translated_title": "LEGO-Eval：迈向通过工具增强实现3D具身环境合成的细粒度评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions."
    },
    {
      "title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统 (原标题: TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System)",
      "link": "https://arxiv.org/abs/2511.02832",
      "pubDate": "Tue, 04 Nov 2025 13:58:35 GMT",
      "isoDate": "2025-11-04T13:58:35.000Z",
      "creator": "Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu",
      "summary": "# TWIST2：可扩展、便携、整体式人形机器人数据采集系统\n\n## 1. 背景与挑战\n*   大规模数据已推动机器人技术取得突破，例如语言模型和双臂操作中的视觉-语言-动作模型。\n*   然而，人形机器人领域缺乏同样有效的数据采集框架。\n*   现有的人形机器人遥操作系统存在局限性：\n    *   采用解耦控制。\n    *   依赖昂贵的运动捕捉（mocap）设备。\n\n## 2. TWIST2 系统介绍\n*   **TWIST2** 是一个便携、无需运动捕捉的人形机器人遥操作和数据采集系统。\n*   它在提高可扩展性的同时，保留了完整的人形机器人全身控制能力。\n\n## 3. 核心技术与组件\n*   **全身运动获取**：利用 PICO4U VR 设备实时获取人类全身运动。\n*   **自我中心视觉**：配备定制的2自由度机器人颈部（成本约250美元），用于实现自我中心视觉。\n*   **整体控制**：实现从人类到人形机器人的整体控制。\n\n## 4. 系统性能与成果\n*   **技能演示**：成功展示了长周期、灵巧且移动的人形机器人技能。\n*   **数据采集效率**：\n    *   能够在15分钟内收集100个演示。\n    *   成功率接近100%。\n\n## 5. 分层视觉运动策略框架\n*   基于 TWIST2 采集的数据，研究人员提出了一个分层视觉运动策略框架。\n*   该框架能够基于自我中心视觉自主控制人形机器人的整个身体。\n*   **策略演示**：成功展示了全身灵巧操作和动态踢球任务。\n\n## 6. 开源与可复现性\n*   整个系统是完全可复现的，并已开源。\n*   系统代码库：[this https URL](this https URL)\n*   采集的数据集也已开源：[this https URL](this https URL)",
      "shortSummary": "TWIST2是一个创新的人形机器人数据采集系统，解决了现有系统昂贵或控制解耦的问题。它利用PICO4U VR和定制机器人颈部，实现便携、无需运动捕捉的全身遥操作，并能高效收集数据（15分钟内100个演示，成功率近100%）。基于此，TWIST2提出了一个分层视觉运动策略，使机器人能自主完成灵巧操作和动态踢球任务。系统和数据集均已开源，具有高度可复现性。",
      "translated_title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io ."
    },
    {
      "title": "Orion-MSP：用于表格上下文学习的多尺度稀疏注意力 (原标题: Orion-MSP: Multi-Scale Sparse Attention for Tabular In-Context Learning)",
      "link": "https://arxiv.org/abs/2511.02818",
      "pubDate": "Tue, 04 Nov 2025 13:43:44 GMT",
      "isoDate": "2025-11-04T13:43:44.000Z",
      "creator": "Mohamed Bouadi, Pratinav Seth, Aditya Tanna, Vinay Kumar Sankarapu",
      "summary": "# Orion-MSP：用于表格上下文学习的多尺度稀疏注意力\n\n## 引言与背景\n表格数据是现实世界应用中最主要的数据格式。然而，由于特征类型异构以及多尺度上复杂的交互，开发有效的表格数据神经网络模型仍然面临挑战。\n\n## 现有表格上下文学习（ICL）模型的局限性\n近期在表格上下文学习（ICL）方面的进展，如TabPFN和TabICL，已在无需任务特定微调的情况下，取得了与梯度提升树（GBTs）相当的最新（SOTA）性能。然而，当前架构存在以下关键局限：\n*   **单尺度特征处理：** 忽略了数据中固有的层次依赖性。\n*   **密集注意力机制：** 在表格宽度上呈现二次方缩放，导致在高维表格上效率低下。\n*   **严格顺序的组件处理：** 阻碍了迭代的表示细化和跨组件的信息通信。\n\n## Orion-MSP的创新之处\n为解决上述挑战，本文引入了Orion-MSP，一个全新的表格ICL架构，其核心包含三项关键创新：\n1.  **多尺度处理：** 旨在捕获表格数据中复杂的层次特征交互。\n2.  **块稀疏注意力：** 结合了窗口式、全局和随机注意力模式，以实现可扩展的效率和长程连接能力。\n3.  **Perceiver风格内存：** 实现了组件间安全的双向信息流，促进更丰富的表示学习。\n\n## 性能与结论\n在各种基准测试中，Orion-MSP的性能与最先进的模型相当或超越，同时能够有效扩展到高维表格。这为高效的表格上下文学习树立了新标准。\n\n## 模型可用性\nOrion-MSP模型已公开可用。",
      "shortSummary": "Orion-MSP是一种创新的表格上下文学习（ICL）架构，旨在解决现有模型在处理表格数据时的局限性。它通过引入多尺度处理、块稀疏注意力机制和Perceiver风格内存，有效捕获层次特征交互、提高可扩展性并实现双向信息流。Orion-MSP在多个基准测试中达到了或超越了最先进的性能，并能高效处理高维表格，为表格ICL树立了新标准。",
      "translated_title": "Orion-MSP：用于表格上下文学习的多尺度稀疏注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tabular data remain the predominant format for real-world applications. Yet, developing effective neural models for tabular data remains challenging due to heterogeneous feature types and complex interactions occurring at multiple scales. Recent advances in tabular in-context learning (ICL), such as TabPFN and TabICL, have achieved state-of-the-art performance comparable to gradient-boosted trees (GBTs) without task-specific fine-tuning. However, current architectures exhibit key limitations: (1) single-scale feature processing that overlooks hierarchical dependencies, (2) dense attention with quadratic scaling in table width, and (3) strictly sequential component processing that prevents iterative representation refinement and cross-component communication. To address these challenges, we introduce Orion-MSP, a tabular ICL architecture featuring three key innovations: (1) multi-scale processing to capture hierarchical feature interactions; (2) block-sparse attention combining windowed, global, and random patterns for scalable efficiency and long-range connectivity; and (3) a Perceiver-style memory enabling safe bidirectional information flow across components. Across diverse benchmarks, Orion-MSP matches or surpasses state-of-the-art performance while scaling effectively to high-dimensional tables, establishing a new standard for efficient tabular in-context learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-MSP ."
    },
    {
      "title": "TabTune：一个用于表格基础模型推理和微调的统一库 (原标题: TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models)",
      "link": "https://arxiv.org/abs/2511.02802",
      "pubDate": "Tue, 04 Nov 2025 13:25:17 GMT",
      "isoDate": "2025-11-04T13:25:17.000Z",
      "creator": "Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu",
      "summary": "## TabTune：表格基础模型推理与微调的统一库\n\n### 引言：表格基础模型面临的挑战\n\n表格基础模型（Tabular Foundation Models, TFMs）代表了结构化数据学习中日益增长的范式，将大规模预训练的优势扩展到表格领域。然而，由于以下几个关键问题，TFMs的实际应用和普及受到了限制：\n\n*   **异构的预处理流程：** 缺乏统一的数据预处理标准和工具。\n*   **碎片化的API：** 不同模型和框架之间API不一致，增加了开发难度。\n*   **不一致的微调程序：** 缺乏标准化的微调策略和流程。\n*   **缺乏标准化评估：** 针对校准（calibration）和公平性（fairness）等部署导向指标，缺乏统一的评估方法。\n\n### TabTune：解决方案\n\n为了解决上述挑战，本文提出了 **TabTune**，一个统一的库。TabTune旨在通过一个单一的接口，标准化表格基础模型的完整工作流程，从而简化其采用和部署。\n\n### TabTune 的核心功能与特点\n\nTabTune 提供了一系列功能，以实现表格基础模型的标准化和高效使用：\n\n1.  **统一接口与模型访问：**\n    *   提供对七种最先进的表格基础模型的统一、一致性访问。\n    *   在内部管理不同模型架构的异构性，对用户透明。\n\n2.  **多样的适应策略支持：**\n    *   **零样本推理 (Zero-shot inference)：** 无需额外训练即可直接应用模型。\n    *   **元学习 (Meta-learning)：** 学习如何快速适应新任务。\n    *   **监督式微调 (Supervised Fine-Tuning, SFT)：** 使用带标签数据对预训练模型进行微调。\n    *   **参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)：** 在微调过程中只更新少量参数，提高效率。\n\n3.  **自动化与标准化：**\n    *   **自动化模型感知的预处理：** 根据所选模型的特点自动进行数据预处理。\n    *   **集成评估模块：** 内置了用于评估模型性能、校准和公平性的模块，确保评估的一致性和全面性。\n\n4.  **设计原则：**\n    *   **可扩展性 (Extensibility)：** 方便未来集成新的模型和适应策略。\n    *   **可复现性 (Reproducibility)：** 确保实验结果可以被一致地复现。\n\n### TabTune 的价值\n\nTabTune 的设计旨在促进表格基础模型的广泛采用，并通过提供一个标准化的框架，实现对不同适应策略的一致性基准测试。这有助于研究人员和开发者更有效地比较和选择最适合其特定任务的表格基础模型和微调方法。\n\n### 可用性\n\nTabTune 是一个开源库，可在指定的GitHub仓库获取。",
      "shortSummary": "TabTune是一个统一的开源库，旨在标准化表格基础模型（TFMs）的完整工作流程。它解决了TFMs在预处理、API、微调和评估方面的碎片化问题，通过单一接口提供对七种先进模型的访问，并支持零样本推理、元学习、监督式微调和参数高效微调等多种适应策略。TabTune自动化模型感知预处理，集成性能、校准和公平性评估，从而促进TFMs的广泛采用和一致性基准测试。",
      "translated_title": "TabTune：一个用于表格基础模型推理和微调的统一库",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models. The library is open source and available at https://github.com/Lexsi-Labs/TabTune ."
    },
    {
      "title": "当可视化是推理的第一步：MIRA，一个视觉思维链基准 (原标题: When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought)",
      "link": "https://arxiv.org/abs/2511.02779",
      "pubDate": "Tue, 04 Nov 2025 13:00:51 GMT",
      "isoDate": "2025-11-04T13:00:51.000Z",
      "creator": "Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye",
      "summary": "### MIRA：一个视觉思维链基准\n\nMIRA（When Visualizing is the First Step to Reasoning）是一个新提出的基准，旨在评估模型在需要生成中间视觉图像才能成功推理的场景中的能力。\n\n#### 核心理念与区别\n\n*   **超越传统思维链（CoT）**：与仅依赖文本的传统CoT方法不同，MIRA中的任务要求模型生成并利用草图、结构图或路径图等中间图像来指导其推理过程。\n*   **模拟人类认知**：这种设置紧密模仿了人类通过“边画边思考”来解决复杂问题的方式。\n\n#### 任务特点与数据\n\n*   **挑战性任务**：MIRA专注于本质上具有挑战性、涉及复杂结构、空间关系或难以仅通过语言表达的推理步骤的任务。\n*   **高质量数据**：基准包含546个多模态问题，这些问题都标注了中间视觉图像和最终答案，以确保评估数据的高质量。\n\n#### 评估协议\n\nMIRA提出了一个统一的评估协议，涵盖三个层级的评估输入：\n\n1.  **直接输入**：仅包含图像和问题。\n2.  **纯文本CoT输入**：包含图像和文本思考提示。\n3.  **视觉CoT输入**：同时包含标注的图像线索和文本思考提示。\n\n#### 实验结果与发现\n\n*   **现有模型表现**：实验结果表明，包括最强大的私有模型和强大的开源模型在内的现有多模态大型语言模型，在仅依赖文本提示时表现不佳。\n*   **视觉线索的关键作用**：当提供中间视觉线索时，模型性能持续提升，在所有模型和任务中平均相对增益达到33.7%。\n*   **上限探测**：通过扩展搜索空间和设计与视觉CoT对齐的文本提示来探测模型能力上限，但与视觉CoT设置相比，这些方法仅带来了有限的改进。\n*   **核心结论**：这些结果强调了“想象的视觉信息”在MIRA上实现成功推理的关键作用。",
      "shortSummary": "MIRA是一个新基准，旨在评估模型在需要生成中间视觉图像以进行推理的场景中的能力。与传统文本CoT不同，MIRA要求模型利用草图、图表等视觉线索来指导复杂问题的解决，模拟人类“边画边思考”。实验表明，现有模型在仅依赖文本提示时表现不佳，但当提供中间视觉线索时，性能平均提升33.7%。这凸显了想象的视觉信息在推理中的关键作用。",
      "translated_title": "当可视化是推理的第一步：MIRA，一个视觉思维链基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA."
    },
    {
      "title": "VCode：一个以SVG作为符号视觉表示的多模态编码基准 (原标题: VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation)",
      "link": "https://arxiv.org/abs/2511.02778",
      "pubDate": "Tue, 04 Nov 2025 13:00:18 GMT",
      "isoDate": "2025-11-04T13:00:18.000Z",
      "creator": "Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang",
      "summary": "# VCode：一个以SVG作为符号视觉表示的多模态编码基准\n\n## 引言\n在智能体时代，代码已成为进行推理和行动的精确且可执行的媒介。然而，目前的研究进展主要集中于程序合成和调试等语言中心任务，而视觉中心编码领域仍未得到充分探索。受人类通过草图进行推理的启发，本文倡导将SVG（可缩放矢量图形）代码作为一种紧凑、可解释且可执行的视觉表示形式。\n\n## VCode基准介绍\n本文引入了VCode，这是一个将多模态理解重构为代码生成任务的基准：给定一张图像，模型必须生成SVG代码，以保留其符号意义，从而支持后续的推理任务。VCode基准涵盖了三个主要领域：\n*   **通用常识：** 基于MM-Vet数据集。\n*   **专业学科：** 基于MMMU数据集。\n*   **视觉中心感知：** 基于CV-Bench数据集。\n\n## CodeVQA评估协议\n为了评估SVG代码的符号保真度，研究提出了一种新颖的评估协议——CodeVQA。在该协议中，一个策略模型需要对渲染后的SVG图像回答问题；正确的答案表明SVG代码忠实地保留了原始图像的符号信息。\n\n## 实验发现\n实证结果表明，当前的前沿视觉语言模型（VLMs）在生成忠实的SVG方面表现不佳，这揭示了语言中心编码与视觉中心编码之间存在的显著差距。\n\n## VCoder框架：弥合差距\n为了弥合这一差距，研究引入了VCoder，这是一个智能体框架，它通过两个主要方面增强了VLMs的能力：\n1.  **思考与修订 (Thinking with Revision)：** VCoder能够迭代地分析SVG代码与原始图像之间的差异，并对SVG代码进行精炼和修正。\n2.  **借助视觉工具行动 (Acting with Visual Tools)：** VCoder利用检测器和解析器等视觉工具，提供结构化的线索，如对象、形状和文本信息，这些能力超出了VLM固有的处理范围。\n\n## VCoder性能与人类研究\n*   尽管具有强大推理能力的前沿VLMs在整体上表现良好，但在专业知识和3D推理方面仍存在局限性。\n*   VCoder在基准测试中取得了显著成果，比表现最佳的Claude-4-Opus模型整体性能提升了12.3个百分点。\n*   人类研究表明，人类和VLMs在处理渲染后的SVG时表现均有所下降，但它们之间的一致性揭示了符号视觉表示的巨大潜力。\n\n## 资源可用性\nVCode基准和相关代码已公开发布，可在项目页面和GitHub上获取。",
      "shortSummary": "VCode是一个创新性的多模态编码基准，旨在通过将图像转换为SVG代码来弥补视觉中心编码的不足。它将多模态理解重构为代码生成任务，并引入CodeVQA协议评估SVG的符号保真度。鉴于现有VLMs在此任务上的局限性，研究提出了VCoder框架，通过迭代修订和视觉工具增强VLMs。VCoder显著提升了性能，比Claude-4-Opus高出12.3个百分点，揭示了SVG作为符号视觉表示的巨大潜力，并推动了视觉中心编码领域的发展。",
      "translated_title": "VCode：一个以SVG作为符号视觉表示的多模态编码基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode."
    },
    {
      "title": "VidEmo：基于情感树推理的以情感为中心的视频基础模型 (原标题: VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models)",
      "link": "https://arxiv.org/abs/2511.02712",
      "pubDate": "Tue, 04 Nov 2025 11:31:09 GMT",
      "isoDate": "2025-11-04T11:31:09.000Z",
      "creator": "Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang",
      "summary": "## VidEmo：基于情感树推理的以情感为中心的视频基础模型\n\n### 摘要\n\n本文介绍了一种名为 VidEmo 的新型视频情感基础模型，旨在解决视频中情感理解和预测的固有挑战。随着视频大语言模型（VideoLLMs）的进步，视频情感分析受到了广泛关注，但情感的动态性和对线索的依赖性使得理解复杂且不断演变的情感状态变得困难。\n\n### 核心方法：情感线索引导推理框架\n\n为了应对这些挑战，研究团队提出了一种新颖的“情感线索引导推理框架”。该框架以分阶段的方式统一了以下三个关键方面：\n\n*   **基本属性感知：** 识别视频中的基础视觉和听觉属性。\n*   **表情分析：** 深入分析面部表情、肢体语言等情感表达。\n*   **高层情感理解：** 基于感知和分析结果，进行复杂的情感状态推理。\n\n### VidEmo 模型设计与训练\n\nVidEmo 是该方法的核心，是一系列专门为情感推理和指令遵循设计的视频情感基础模型。这些模型经过一个独特的两阶段调优过程：\n\n1.  **课程情感学习（Curriculum Emotion Learning）：** 在此阶段，模型被注入基础的情感知识，逐步学习识别和理解各种情感线索。\n2.  **情感树强化学习（Affective-Tree Reinforcement Learning）：** 这一阶段侧重于情感推理能力的培养。模型通过强化学习，利用“情感树”结构进行更深层次、更具逻辑性的情感状态推断。\n\n### 基础数据基础设施：Emo-CFG 数据集\n\n为了支持 VidEmo 模型的训练和评估，研究团队建立了一个强大的基础数据基础设施，并引入了一个全新的数据集——“以情感为中心的细粒度数据集”（Emo-CFG）。\n\n*   **规模与多样性：** Emo-CFG 包含210万个多样化的基于指令的样本。\n*   **丰富的内容：** 该数据集提供了以下关键资源，对于推进情感理解任务至关重要：\n    *   **可解释的情感问答：** 包含关于视频情感的问答对，并附带解释。\n    *   **细粒度字幕：** 对视频内容进行详细的情感相关描述。\n    *   **相关推理依据：** 提供支持情感判断的理由和线索。\n\n### 实验结果与贡献\n\n实验结果表明，VidEmo 方法取得了具有竞争力的性能。它在15项面部感知任务中树立了新的里程碑，显著提升了视频情感理解和预测的能力。这标志着视频情感基础模型领域的一个重要进展。",
      "shortSummary": "VidEmo是一个新的以情感为中心的视频基础模型，旨在解决视频情感理解的挑战。它采用情感线索引导推理框架，并通过两阶段调优（课程情感学习和情感树强化学习）来注入情感知识和提升推理能力。研究团队还构建了包含210万样本的Emo-CFG数据集，用于情感问答和细粒度字幕。实验结果表明，VidEmo在15项面部感知任务中表现出色，树立了新里程碑，显著提升了视频情感理解能力。",
      "translated_title": "VidEmo：基于情感树推理的以情感为中心的视频基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks."
    },
    {
      "title": "视觉输入可以被压缩吗？大型多模态模型的视觉令牌压缩基准 (原标题: Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models)",
      "link": "https://arxiv.org/abs/2511.02650",
      "pubDate": "Tue, 04 Nov 2025 10:17:06 GMT",
      "isoDate": "2025-11-04T10:17:06.000Z",
      "creator": "Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui",
      "summary": "### UniPruneBench：大型多模态模型视觉令牌压缩的统一基准\n\n**1. 背景与问题**\n*   大型多模态模型（LMMs）因图像编码器引入的大量视觉令牌而面临严重的推理效率低下问题。\n*   尽管近期令牌压缩方法（如剪枝和合并）在减少冗余方面显示出潜力，但其评估仍然分散且不一致。\n\n**2. 提出的解决方案：UniPruneBench**\n*   本文提出了UniPruneBench，一个针对多模态LLM中视觉令牌剪枝的统一且可扩展的基准。\n*   **标准化协议**：UniPruneBench在六个能力维度和十个数据集上提供了标准化的评估协议。\n*   **覆盖范围**：\n    *   涵盖了十种具有代表性的压缩算法。\n    *   评估了三大家族的LMMs，包括LLaVA-v1.5、Intern-VL3和Qwen2.5-VL。\n*   **评估指标**：除了传统的任务准确性，UniPruneBench还纳入了系统级指标，如运行时（runtime）和预填充延迟（prefilling latency），以提供全面的视角。\n\n**3. 关键实验发现**\n*   **随机剪枝的有效性**：随机剪枝（random pruning）是一个出人意料的强大基线。\n*   **无普适最佳方法**：没有单一方法能在所有场景中始终优于其他方法。\n*   **任务敏感性差异**：剪枝敏感性在不同任务之间差异显著，其中光学字符识别（OCR）任务最为脆弱。\n*   **剪枝比例的主导作用**：剪枝比例是影响性能下降的主要因素。\n\n**4. 结论与展望**\n*   作者相信UniPruneBench将为未来高效多模态建模的研究奠定可靠的基础。",
      "shortSummary": "大型多模态模型（LMMs）因视觉令牌过多导致推理效率低下。为解决评估碎片化问题，本文提出了UniPruneBench，一个统一的视觉令牌剪枝基准。该基准在多维度、多数据集上评估了十种压缩算法和三大家族LMMs，并纳入系统级指标。研究发现，随机剪枝是强基线，无普适最佳方法，OCR任务对剪枝最敏感，且剪枝比例是影响性能的主导因素。UniPruneBench旨在为高效多模态建模研究提供可靠基础。",
      "translated_title": "视觉输入可以被压缩吗？大型多模态模型的视觉令牌压缩基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling."
    },
    {
      "title": "BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统 (原标题: BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring)",
      "link": "https://arxiv.org/abs/2511.02490",
      "pubDate": "Tue, 04 Nov 2025 06:27:03 GMT",
      "isoDate": "2025-11-04T06:27:03.000Z",
      "creator": "Rajan Das Gupta, Md Kishor Morol, Nafiz Fahad, Md Tanzib Hosain, Sumaya Binte Zilani Choya, Md Jakir Hossen",
      "summary": "## BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统\n\n### 引言\n随着全球阿尔茨海默病（AD）负担的持续增长，早期和准确的检测变得日益关键，尤其是在高级诊断工具可及性有限的地区。为应对这一挑战，研究人员提出了BRAINS（Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening）系统。\n\n### BRAINS系统概述\nBRAINS是一个新颖的系统，它利用大型语言模型（LLMs）强大的推理能力进行阿尔茨海默病的检测和监测。该系统旨在提供可扩展、可解释的早期阿尔茨海默病检测辅助工具。\n\n### 系统架构：双模块设计\nBRAINS系统采用双模块架构，包括认知诊断模块和病例检索模块：\n\n1.  **认知诊断模块 (Cognitive Diagnostic Module)**\n    *   **功能：** 利用LLMs对阿尔茨海默病风险进行结构化评估。\n    *   **技术：** 这些LLMs经过认知和神经影像数据集的微调，包括MMSE（简易精神状态检查）、CDR（临床痴呆评定量表）评分以及脑容量指标等。\n\n2.  **病例检索模块 (Case Retrieval Module)**\n    *   **功能：** 从精心策划的知识库中检索与患者档案相似的病例。\n    *   **过程：** 该模块将患者档案编码为潜在表示，然后检索相似的辅助病例。\n    *   **病例融合层 (Case Fusion Layer)：** 检索到的辅助病例通过病例融合层与输入档案进行融合，以增强对患者情况的上下文理解。\n\n### 推理与评估\n融合后的表示随后与临床提示一起进行处理，以进行最终的推断。在真实世界数据集上的评估结果表明，BRAINS系统在疾病严重程度分类和识别认知衰退早期迹象方面表现出显著的有效性。\n\n### 潜在应用\n该系统不仅展示了作为一种可扩展、可解释的早期阿尔茨海默病检测辅助工具的强大潜力，也为该领域的未来应用带来了希望。\n\n### 出版信息\n该研究已被ICMLA 2025接受发表。",
      "shortSummary": "BRAINS（生物医学检索增强型神经退行性疾病筛查智能系统）是一种利用大型语言模型（LLMs）进行阿尔茨海默病（AD）检测和监测的新型系统。它采用双模块架构：认知诊断模块使用微调的LLMs评估AD风险，病例检索模块通过融合相似病例增强上下文理解。在真实数据集上的评估显示，BRAINS能有效分类疾病严重程度并识别早期认知衰退迹象，有望成为可扩展、可解释的早期AD检测辅助工具。",
      "translated_title": "BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field."
    },
    {
      "title": "ChartM^3：一种多阶段代码驱动的管道，用于构建图表理解中的多维度、多步骤视觉推理数据 (原标题: ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension)",
      "link": "https://arxiv.org/abs/2511.02415",
      "pubDate": "Tue, 04 Nov 2025 04:45:34 GMT",
      "isoDate": "2025-11-04T04:45:34.000Z",
      "creator": "Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang",
      "summary": "# ChartM^3：多维度、多步骤视觉推理数据构建管道\n\n## 摘要\n\n当前的多模态大语言模型（MLLMs）在处理复杂的图表理解任务时，面临着视觉识别和推理能力不足的挑战。现有研究对现实世界应用中普遍存在的复杂图表场景和计算密集型推理任务的覆盖有限。为解决这些局限性，本研究提出了一种自动化的多阶段代码驱动管道，用于系统地生成视觉推理数据集。\n\n## 管道设计与工作原理\n\n该管道集成了以下关键策略：\n\n*   **检索增强生成（RAG）**：用于检索专业的图表模板，确保生成图表的专业性和多样性。\n*   **思维链（CoT）策略**：用于生成推理代码，这些代码能够模拟真实的统计数据分布，从而驱动图表渲染和与问题相关的统计计算。\n\n通过这种方法，管道能够系统地生成高质量的图表及其相关的推理数据。\n\n## 管道优势\n\n*   **增强图表多样性**：通过RAG和CoT策略，管道能够生成更广泛、更多样化的图表类型。\n*   **提高数据质量**：生成的推理代码模拟真实数据分布，确保了数据的真实性和有效性。\n\n## ChartM^3 数据集\n\n基于此框架，研究团队构建了 **ChartM^3** 数据集，其特点如下：\n\n*   **规模**：包含38,000张图表和142,000个问答对，用于模型训练。\n*   **评估样本**：包含2,871个高质量的评估样本，用于实际性能评估。\n*   **特点**：这是一个多维度、多步骤的数据集，旨在测试模型在复杂图表理解中的高级推理能力。\n\n## 实验结果与影响\n\n通过监督微调（SFT）和强化学习（RL）实验，ChartM^3 数据集展现出显著的积极影响：\n\n*   **提升推理能力**：数据集显著提高了模型在图表理解中的推理能力。\n*   **增强跨领域泛化性能**：模型在不同类型和领域的图表上表现出更好的泛化能力。\n*   **赋能小型模型**：使得较小规模的模型在复杂图表理解任务中，能够达到与大型模型相当的性能水平。",
      "shortSummary": "本研究提出了一种自动化的多阶段代码驱动管道，旨在解决多模态大语言模型在复杂图表理解和计算密集型推理任务中的不足。该管道结合检索增强生成（RAG）和思维链（CoT）策略，生成模拟真实数据分布的推理代码，驱动图表渲染和统计计算。基于此，构建了ChartM^3数据集，包含38K图表和142K问答对。实验证明，ChartM^3显著提升了模型的推理能力和跨领域泛化性能，使小型模型在复杂图表理解上能媲美大型模型。",
      "translated_title": "ChartM^3：一种多阶段代码驱动的管道，用于构建图表理解中的多维度、多步骤视觉推理数据",
      "images": [],
      "contentSource": "完整文章",
      "content": "Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM^3, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&amp;A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension."
    },
    {
      "title": "LiveSecBench：一个针对中文语境下大型语言模型的动态且文化相关的AI安全基准 (原标题: LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context)",
      "link": "https://arxiv.org/abs/2511.02366",
      "pubDate": "Tue, 04 Nov 2025 03:44:09 GMT",
      "isoDate": "2025-11-04T03:44:09.000Z",
      "creator": "Yudong Li, Zhongliang Yang, Kejiang Chen, Wenxuan Wang, Tianxin Zhang, Sifang Wan, Kecheng Wang, Haitian Li, Xu Wang, Lefan Cheng, Youdan Yang, Baocheng Chen, Ziyu Liu, Yufei Sun, Liyan Wu, Wenya Wen, Xingchi Gu, Peiru Yang",
      "summary": "## LiveSecBench：中文语境下大型语言模型的AI安全基准\n\n### 引言\nLiveSecBench是一个动态且持续更新的安全基准，专为中文大型语言模型（LLM）的应用场景设计。它旨在评估LLM在中文语境下的安全性。\n\n### 核心特点\n*   **动态性与持续更新：** LiveSecBench通过动态更新计划保持其相关性，旨在纳入新的威胁向量。例如，计划在下次更新中包含文本到图像生成安全（Text-to-Image Generation Safety）和智能体安全（Agentic Safety）。\n*   **文化相关性：** 该基准的评估维度根植于中国的法律和社会框架，确保了其在中文语境下的适用性和准确性。\n\n### 评估维度\nLiveSecBench从以下六个关键维度评估模型：\n1.  **合法性 (Legality)**\n2.  **伦理 (Ethics)**\n3.  **事实性 (Factuality)**\n4.  **隐私 (Privacy)**\n5.  **对抗性鲁棒性 (Adversarial Robustness)**\n6.  **推理安全 (Reasoning Safety)**\n\n### 当前状态\n*   LiveSecBench (v251030) 目前已评估了18个大型语言模型。\n*   它提供了一个中文语境下AI安全的全景图。\n*   评估结果的排行榜已公开，可在指定URL访问。\n\n### 图片\n文章内容中不包含有效的实际图片链接。",
      "shortSummary": "LiveSecBench是一个为中文大型语言模型（LLM）设计的动态、持续更新的AI安全基准。它基于中国的法律和社会框架，从合法性、伦理、事实性、隐私、对抗性鲁棒性和推理安全六个关键维度评估模型。该基准保持动态更新以适应新威胁，并已评估18个LLM，其排行榜已公开，旨在提供中文语境下AI安全的全面视图。",
      "translated_title": "LiveSecBench：一个针对中文语境下大型语言模型的动态且文化相关的AI安全基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/."
    },
    {
      "title": "让多模态嵌入器通过自适应查询增强学习何时增强查询 (原标题: Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation)",
      "link": "https://arxiv.org/abs/2511.02358",
      "pubDate": "Tue, 04 Nov 2025 03:24:41 GMT",
      "isoDate": "2025-11-04T03:24:41.000Z",
      "creator": "Wongyu Kim, Hochang Lee, Sanghak Lee, Yoonsung Kim, Jaehyun Park",
      "summary": "# M-Solomon：通过自适应查询增强优化多模态嵌入\n\n## 摘要\n本文提出了一种名为 M-Solomon 的通用多模态嵌入器，旨在解决现有查询增强方法在多模态环境中面临的挑战。M-Solomon 能够自适应地判断何时需要对查询进行增强，从而提高效率并优化性能。\n\n## 背景与问题\n*   **查询增强 (Query Augmentation, QA)**：通过向查询添加额外信息，使其更具意义，从而更好地检索相关文档。\n*   **现有方法**：目前的研究提出了基于大型语言模型 (LLM) 的嵌入器，这些嵌入器以多任务方式学习表示嵌入和生成查询增强，利用 LLM 的生成能力。\n*   **现有方法的局限性**：\n    *   在推理阶段，这些联合训练的嵌入器通常会先进行查询增强，然后进行嵌入。尽管效果显著，但对每个查询都进行增强会导致**显著的嵌入延迟**。\n    *   对于某些查询，查询增强甚至可能**损害性能**。\n    *   以往的方法**尚未在多模态环境中进行探索**。\n\n## M-Solomon 解决方案\n为了解决上述问题，M-Solomon 被提出，它是一个通用的多模态嵌入器，能够自适应地决定何时进行查询增强。\n\n### M-Solomon 的核心流程\n1.  **训练数据集查询分组**：\n    *   首先，M-Solomon 在数据集层面将训练查询分为两组：\n        *   需要增强的查询。\n        *   不需要增强的查询。\n2.  **增强合成过程**：\n    *   对于需要增强的查询，M-Solomon 利用强大的多模态大型语言模型 (MLLM) 生成适当的增强内容。\n3.  **自适应查询增强机制**：\n    *   M-Solomon 通过学习实现自适应查询增强：\n        *   对于需要增强的查询，它会生成带有前缀 `/augment` 的合成增强内容。\n        *   对于不需要增强的查询，它会生成简单的字符串 `/embed`。\n    *   通过这种方式，M-Solomon 仅在必要时才进行查询增强。\n\n## 实验结果\n实验结果表明，M-Solomon 取得了显著的性能提升：\n*   它不仅大幅超越了不使用增强的基线模型。\n*   也优于始终使用增强的基线模型。\n*   同时，M-Solomon 提供了**更快的嵌入延迟**。\n\n## 结论\nM-Solomon 成功地解决了多模态环境中查询增强的效率和性能问题，通过其自适应机制，实现了更智能、更高效的查询处理。",
      "shortSummary": "M-Solomon 是一种新型多模态嵌入器，它能自适应地判断何时需要增强查询。针对现有方法中普遍增强导致的延迟和性能下降问题，M-Solomon 利用多模态大语言模型（MLLM）为特定查询生成增强内容，而对其他查询则不进行增强。实验证明，M-Solomon 不仅显著优于始终增强和不增强的基线模型，还大幅降低了嵌入延迟，提升了效率和性能。",
      "translated_title": "让多模态嵌入器通过自适应查询增强学习何时增强查询",
      "images": [],
      "contentSource": "完整文章",
      "content": "Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency."
    },
    {
      "title": "LTD-Bench：通过让大型语言模型绘图来评估它们 (原标题: LTD-Bench: Evaluating Large Language Models by Letting Them Draw)",
      "link": "https://arxiv.org/abs/2511.02347",
      "pubDate": "Tue, 04 Nov 2025 03:11:23 GMT",
      "isoDate": "2025-11-04T03:11:23.000Z",
      "creator": "Liuhao Lin, Ke Li, Zihan Xu, Yuchen Shi, Yulei Qin, Yan Zhang, Xing Sun, Rongrong Ji",
      "summary": "LTD-Bench是一个突破性的基准测试，旨在解决当前大型语言模型（LLM）评估范式中的关键盲点。目前的评估方法依赖不透明的数值指标，这些指标掩盖了LLM在空间推理方面的根本性局限，并导致报告性能与实际能力之间存在脱节，尤其是在需要理解物理世界的应用中。\n\n**LTD-Bench的创新方法：**\n*   **可视化评估：** LTD-Bench将LLM的评估从抽象分数转变为直接可观察的视觉输出。\n*   **绘图任务：** 要求模型通过点阵或可执行代码生成图画。\n*   **直观揭示局限：** 这种方法使得空间推理的局限性即使对非专业人士也一目了然，从而弥合了统计性能与直观评估之间的根本差距。\n\n**综合评估方法论：**\n*   **互补任务：**\n    *   **生成任务：** 旨在测试模型的空间想象能力。\n    *   **识别任务：** 旨在评估模型的空间感知能力。\n*   **难度分级：** 任务分为三个递进的难度级别。\n*   **双向映射评估：** 系统地评估语言与空间概念之间关键的双向映射能力。\n\n**主要实验发现：**\n*   **惊人的能力差距：** 对最先进模型的广泛实验揭示了一个令人担忧的能力差距。即使在传统基准测试中取得令人印象深刻结果的LLM，在建立语言与空间概念之间的双向映射方面也表现出深刻的不足。\n*   **根本性局限：** 这种局限性削弱了LLM作为真正“世界模型”的潜力。\n\n**额外优势：**\n*   **诊断分析：** LTD-Bench的视觉输出能够实现强大的诊断分析。\n*   **模型相似性研究：** 为研究模型相似性提供了一种潜在方法。\n\n该研究已被NeurIPS 2025接受。",
      "shortSummary": "LTD-Bench是一个创新基准，通过让大型语言模型（LLM）生成图画来评估其空间推理能力，解决了传统评估中空间局限性不透明的问题。它包含生成和识别任务，并分级评估语言与空间概念的双向映射。实验揭示，即使是先进LLM，在这一核心能力上也存在严重不足，这限制了它们作为世界模型的潜力，并为诊断分析提供了新途径。该研究已被NeurIPS 2025接受。",
      "translated_title": "LTD-Bench：通过让大型语言模型绘图来评估它们",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity."
    },
    {
      "title": "当模态冲突时：单模态推理不确定性如何支配多模态大语言模型（MLLMs）中的偏好动态 (原标题: When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs)",
      "link": "https://arxiv.org/abs/2511.02243",
      "pubDate": "Mon, 03 Nov 2025 23:11:31 GMT",
      "isoDate": "2025-11-03T23:11:31.000Z",
      "creator": "Zhuoran Zhang, Tengyue Wang, Xilin Gong, Yang Shi, Haotian Wang, Di Wang, Lijie Hu",
      "summary": "### 新框架揭示多模态大语言模型（MLLMs）如何解决模态冲突\n\n多模态大语言模型（MLLMs）在处理不同模态（如视觉和文本）提供矛盾信息时，必须解决冲突，这一过程被称为“模态跟随”。以往的研究仅通过粗略的数据集层面统计来衡量这种行为，忽略了模型在单模态推理中的置信度影响。\n\n本文引入了一个新的框架，将模态跟随分解为两个基本因素：\n*   **相对推理不确定性（Relative Reasoning Uncertainty）**：指单模态预测之间特定案例的置信度差距。\n*   **固有模态偏好（Inherent Modality Preference）**：指当不确定性平衡时，模型所展现出的稳定偏见。\n\n为了验证这一框架，研究人员构建了一个可控数据集，系统地改变视觉和文本输入的推理难度。他们使用熵作为细粒度的不确定性度量，并揭示了一个普遍规律：\n*   **普遍规律**：模型跟随某一模态的概率会随着该模态相对不确定性的增加而单调递减。\n\n在模型倾向于以相似概率跟随两种模态的相对难度水平上，研究定义了一个“平衡点”（balance point）。这个平衡点被认为是模型固有偏好的一个实用指标。与传统的宏观层面比率不同，这种衡量方法提供了一种更具原则性、更少混淆的方式来表征模态偏见，将其与单模态能力和数据集伪影区分开来。\n\n此外，通过探究模型层级间的预测，研究揭示了内部振荡机制：在接近平衡点的模糊区域，模型会在不同层级之间在模态间摇摆不定，这解释了外部观察到的犹豫不决现象。\n\n综上所述，这些发现确立了相对不确定性和固有偏好是模态跟随的两个主导原则，为MLLMs如何解决冲突信息提供了量化框架和机制洞察。",
      "shortSummary": "本文提出一个新框架，将多模态大语言模型（MLLMs）解决模态冲突（“模态跟随”）的行为分解为“相对推理不确定性”和“固有模态偏好”。研究构建可控数据集并发现，模型跟随某一模态的概率随其相对不确定性增加而单调递减。通过“平衡点”可量化固有偏好，该指标比传统方法更具原则性。此外，研究揭示了模型在模糊区域的层间振荡机制。这些发现为理解MLLMs如何处理冲突信息提供了关键洞察。",
      "translated_title": "当模态冲突时：单模态推理不确定性如何支配多模态大语言模型（MLLMs）中的偏好动态",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information."
    },
    {
      "title": "TabDSR：表格数据中复杂数值推理的分解、清洗与推理 (原标题: TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data)",
      "link": "https://arxiv.org/abs/2511.02219",
      "pubDate": "Mon, 03 Nov 2025 22:13:02 GMT",
      "isoDate": "2025-11-03T22:13:02.000Z",
      "creator": "Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng",
      "summary": "# TabDSR：表格数据中复杂数值推理的分解、清洗与推理\n\n## 摘要\n\n### 背景与问题\n\n在现实世界的数据分析中，对表格数据进行复杂的数值推理至关重要。然而，大型语言模型（LLMs）在此类任务中常常表现不佳，主要原因包括：\n\n*   **查询复杂性高**：用户提出的问题可能非常复杂，难以直接处理。\n*   **数据存在噪声**：表格数据中可能包含不准确、不完整或无关的信息。\n*   **数值处理能力有限**：LLMs在处理精确的数值计算和逻辑推理方面存在局限性。\n\n### 提出的方法：TabDSR 框架\n\n为解决上述挑战，本文提出了一种名为 **TabDSR** 的新框架。该框架由以下三个核心组件构成：\n\n1.  **查询分解器 (Query Decomposer)**：\n    *   负责将复杂的自然语言查询分解成更小、更易于管理和解决的子问题或步骤。\n2.  **表格清洗器 (Table Sanitizer)**：\n    *   用于识别并清洗表格中的噪声数据。\n    *   过滤掉与当前查询不相关的信息，确保推理过程基于干净、相关的表格内容。\n3.  **思维程序 (Program-of-Thoughts, PoT) 推理器**：\n    *   基于清洗后的表格数据，生成可执行的代码（例如Python代码）。\n    *   通过执行这些代码来逐步推导出最终的答案，从而实现精确的数值推理。\n\n### 新数据集：CalTab151\n\n为了确保对 TabDSR 框架进行公正且无偏的评估，并有效缓解潜在的数据泄露问题，研究者专门引入了一个新的数据集——**CalTab151**。该数据集是为复杂的表格数值推理任务精心设计的。\n\n### 实验结果与性能\n\n实验结果显著表明，TabDSR 框架在多个基准测试中持续超越现有方法，实现了最先进（SOTA）的性能：\n\n*   在 **TAT-QA** 数据集上，准确率提升了 **8.79%**。\n*   在 **TableBench** 数据集上，准确率提升了 **6.08%**。\n*   在专门设计的 **CalTab151** 数据集上，准确率更是显著提升了 **19.87%**。\n\n此外，TabDSR 框架能够与主流的大型语言模型无缝集成，为复杂的表格数值推理提供了一个强大且稳健的解决方案。\n\n### 结论\n\n这些研究发现有力地证明了 TabDSR 框架在增强大型语言模型处理复杂表格数值推理能力方面的卓越有效性。该研究已被 EMNLP 2025 Findings 接受。数据和代码可根据需求提供。",
      "shortSummary": "TabDSR框架旨在解决大型语言模型在表格数据复杂数值推理中因查询复杂、数据噪声和数值能力有限而表现不佳的问题。该框架包含查询分解器、表格清洗器和基于思维程序(PoT)的推理器。为公正评估，引入了新数据集CalTab151。实验证明，TabDSR在TAT-QA、TableBench和CalTab151上均实现了最先进(SOTA)的准确率提升，并能与主流LLMs无缝集成，有效提升了LLM的推理能力。",
      "translated_title": "TabDSR：表格数据中复杂数值推理的分解、清洗与推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose \\method, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that \\method consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request."
    },
    {
      "title": "区别对待运动分量以发展联合深度和自我运动学习 (原标题: Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning)",
      "link": "https://arxiv.org/abs/2511.01502",
      "pubDate": "Mon, 03 Nov 2025 07:14:52 GMT",
      "isoDate": "2025-11-03T07:14:52.000Z",
      "creator": "Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan",
      "summary": "## 区别对待运动分量以发展联合深度和自我运动学习\n\n### 引言与背景\n\n近年来，深度和自我运动的无监督学习作为两项基础的3D感知任务，取得了显著进展。然而，现有的大多数方法将自我运动视为辅助任务，在监督过程中要么混合所有运动类型，要么排除与深度无关的旋转运动。这种设计限制了强几何约束的融入，从而降低了在多样化条件下的可靠性和鲁棒性。\n\n### 核心贡献与方法\n\n本研究引入了一种**区别对待运动分量**的方法，利用其各自刚性流的几何规律来同时提升深度和自我运动估计的性能。\n\n该方法的核心步骤包括：\n\n1.  **相机对齐：** 给定连续的视频帧，网络输出首先对齐源相机和目标相机的光轴和成像平面。\n2.  **光流变换与几何约束：** 通过这些对齐，帧间光流被转换。随后，量化偏差，对每个自我运动分量**单独施加几何约束**，从而实现更有针对性的优化。\n3.  **联合学习重构：** 这些对齐进一步将联合学习过程重构为**同轴和共面形式**。\n4.  **深度与平移的互推导：** 在重构后的形式中，深度和每个平移分量可以通过**闭式几何关系相互推导**，引入了互补约束，显著提高了深度的鲁棒性。\n\n### DiMoDE框架\n\nDiMoDE是一个通用的深度和自我运动联合学习框架，它整合了上述所有设计。该框架旨在通过更精细地处理运动分量来克服传统方法的局限性。\n\n### 实验结果\n\nDiMoDE框架在多个公共数据集以及一个新收集的多样化真实世界数据集上，均实现了**最先进的性能**。尤其值得注意的是，该方法在**挑战性条件下**表现出卓越的鲁棒性。\n\n### 代码可用性\n\n本研究的源代码将在论文发表后公开发布。",
      "shortSummary": "该研究提出了一种区别对待运动分量的新方法，以改进无监督深度和自我运动的联合学习。通过对齐相机并对每个运动分量单独施加几何约束，该方法将联合学习重构为同轴和共面形式，使深度和翻译分量能够相互推导。由此产生的DiMoDE框架在多个数据集上实现了最先进的性能，尤其在复杂条件下表现出更高的鲁棒性，解决了现有方法几何约束不足的问题。",
      "translated_title": "区别对待运动分量以发展联合深度和自我运动学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication."
    }
  ],
  "lastUpdated": "2025-11-06T09:33:10.768Z"
}