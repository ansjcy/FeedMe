{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "视频模型是否已准备好成为零样本推理器？基于MME-CoF基准的实证研究 (原标题: Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark)",
      "link": "https://arxiv.org/abs/2510.26802",
      "pubDate": "Thu, 30 Oct 2025 13:59:55 GMT",
      "isoDate": "2025-10-30T13:59:55.000Z",
      "creator": "Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, Pheng-Ann Heng",
      "summary": "## 视频模型作为零样本推理器的能力评估\n\n### 引言\n\n近期，视频生成模型在生成高保真、时间连贯的视频方面取得了显著进展，这表明它们可能编码了大量的世界知识。除了逼真的合成能力外，这些模型还展现出视觉感知、建模和操作等新兴行为。然而，一个关键问题依然存在：在具有挑战性的视觉推理场景中，视频模型是否已准备好作为零样本推理器？\n\n### 研究方法\n\n本研究旨在通过一项实证研究，全面探讨这一问题，重点关注领先且广受欢迎的Veo-3模型。研究人员系统地评估了Veo-3在12个不同维度上的推理行为，以全面刻画其优势和失效模式。这些维度包括：\n\n*   **空间逻辑**\n*   **几何逻辑**\n*   **物理逻辑**\n*   **时间逻辑**\n*   **具身逻辑**\n\n为了标准化这项研究，评估数据被整理成MME-CoF基准，这是一个紧凑的基准，旨在对“帧链推理”（Chain-of-Frame, CoF）进行深入而彻底的评估。\n\n### 主要发现\n\n研究结果揭示了当前视频模型在推理能力方面的表现：\n\n*   **优势：** 模型在以下方面展现出有前景的推理模式：\n    *   短时程空间连贯性\n    *   细粒度接地（fine-grained grounding）\n    *   局部一致的动态\n\n*   **局限性：** 模型在以下方面仍存在限制：\n    *   长时程因果推理\n    *   严格的几何约束\n    *   抽象逻辑\n\n### 结论\n\n总体而言，尽管当前视频模型展现出一些令人鼓舞的推理迹象，但它们尚未可靠地作为独立的零样本推理器。然而，它们作为专用推理模型的补充视觉引擎，展现出巨大的潜力。",
      "shortSummary": "本研究通过MME-CoF基准，对领先的Veo-3视频模型作为零样本推理器的能力进行了实证评估。研究发现，尽管模型在短时程空间连贯性和局部动态方面表现出潜力，但在长时程因果推理、严格几何约束和抽象逻辑方面仍有限制。结论是，当前视频模型尚未能独立作为可靠的零样本推理器，但可作为专用推理模型的有效视觉补充。",
      "translated_title": "视频模型是否已准备好成为零样本推理器？基于MME-CoF基准的实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io"
    },
    {
      "title": "OmniX：从统一全景生成与感知到图形就绪的3D场景 (原标题: OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes)",
      "link": "https://arxiv.org/abs/2510.26800",
      "pubDate": "Thu, 30 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-30T13:59:51.000Z",
      "creator": "Yukun Huang, Jiwen Yu, Yanning Zhou, Jianan Wang, Xintao Wang, Pengfei Wan, Xihui Liu",
      "summary": "### OmniX：从统一全景生成与感知到图形就绪的3D场景\n\n本文介绍了一种名为 OmniX 的新框架，旨在将全景生成和感知技术提升到新的水平，以创建可用于物理渲染（PBR）、重新打光和模拟的图形就绪 3D 场景。\n\n**背景与挑战：**\n*   构建 3D 场景主要有两种方法：程序生成和 2D 提升。\n*   基于全景图的 2D 提升是一种有前景的技术，它利用强大的 2D 生成先验知识来生成沉浸式、真实且多样化的 3D 环境。\n*   现有 2D 提升方法主要侧重于外观生成，往往忽略了对几何、纹理和 PBR 材料等内在属性的感知。\n\n**OmniX 的核心洞察与方法：**\n*   **关键洞察：** 重新利用 2D 生成模型进行全景几何、纹理和 PBR 材料的感知。\n*   **统一框架：** OmniX 是一个多功能且统一的框架，与现有方法不同，它强调对内在属性的感知。\n*   **技术实现：** 基于轻量级且高效的跨模态适配器结构，OmniX 能够重用 2D 生成先验知识，以处理广泛的全景视觉任务。\n\n**OmniX 的功能范围：**\n*   全景感知\n*   全景生成\n*   全景补全\n\n**数据集贡献：**\n*   研究团队构建了一个大规模的合成全景数据集。\n*   该数据集包含来自各种室内和室外场景的高质量多模态全景图。\n\n**实验结果与意义：**\n*   广泛的实验证明了 OmniX 模型在全景视觉感知和图形就绪 3D 场景生成方面的有效性。\n*   OmniX 为沉浸式、物理真实虚拟世界的生成开辟了新的可能性。",
      "shortSummary": "OmniX 框架通过重新利用 2D 生成模型进行全景几何、纹理和 PBR 材料的感知，将基于全景图的 2D 提升技术推进到可生成图形就绪 3D 场景的水平。它是一个统一且多功能的框架，能够进行全景感知、生成和补全，从而为创建沉浸式、物理真实的虚拟世界提供了新途径。",
      "translated_title": "OmniX：从统一全景生成与感知到图形就绪的3D场景",
      "images": [],
      "contentSource": "完整文章",
      "content": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation."
    },
    {
      "title": "通用运动生成：数据、模型与评估的探索 (原标题: The Quest for Generalizable Motion Generation: Data, Model, and Evaluation)",
      "link": "https://arxiv.org/abs/2510.26794",
      "pubDate": "Thu, 30 Oct 2025 13:59:27 GMT",
      "isoDate": "2025-10-30T13:59:27.000Z",
      "creator": "Jing Lin, Ruisi Wang, Junzhe Lu, Ziqi Huang, Guorui Song, Ailing Zeng, Xian Liu, Chen Wei, Wanqi Yin, Qingping Sun, Zhongang Cai, Lei Yang, Ziwei Liu",
      "summary": "# 通用运动生成：数据、模型与评估的探索\n\n本文旨在解决3D人体运动生成（MoGen）领域在泛化能力方面的根本性瓶颈。尽管现有模型在标准基准测试上取得了进展，但其泛化能力仍显不足。作者观察到，视频生成（ViGen）等相邻生成领域在建模人类行为方面展现出卓越的泛化能力，这为MoGen提供了可借鉴的经验。\n\n受此启发，本文提出了一个全面的框架，系统地将ViGen的知识从以下三个关键支柱转移到MoGen：数据、建模和评估。\n\n## 1. 数据：ViMoGen-228K 大规模数据集\n\n*   **引入 ViMoGen-228K**：这是一个包含228,000个高质量运动样本的大规模数据集。\n*   **数据来源整合**：该数据集整合了多种数据源：\n    *   高保真光学运动捕捉（MoCap）数据。\n    *   来自网络视频的语义标注运动。\n    *   由最先进的ViGen模型生成的合成样本。\n*   **语义多样性扩展**：数据集包含文本-运动对（text-motion pairs）和文本-视频-运动三元组（text-video-motion triplets），显著扩展了语义多样性。\n\n## 2. 建模：ViMoGen 及其轻量级变体\n\n*   **提出 ViMoGen 模型**：\n    *   基于流匹配（flow-matching-based）的扩散Transformer。\n    *   通过门控多模态条件（gated multimodal conditioning）统一了来自MoCap数据和ViGen模型的先验知识。\n*   **开发 ViMoGen-light**：\n    *   为了提高效率，进一步开发了ViMoGen的蒸馏变体（distilled variant）。\n    *   该变体消除了对视频生成的依赖，同时保留了强大的泛化能力。\n\n## 3. 评估：MBench 分层基准\n\n*   **引入 MBench**：一个分层基准，旨在进行细粒度评估。\n*   **评估维度**：MBench 评估以下三个方面：\n    *   运动质量（motion quality）。\n    *   提示词保真度（prompt fidelity）。\n    *   泛化能力（generalization ability）。\n\n## 实验结果与可用性\n\n*   **显著性能提升**：广泛的实验表明，该框架在自动评估和人工评估中均显著优于现有方法。\n*   **公开可用性**：相关的代码、数据和基准将公开发布。",
      "shortSummary": "本文提出一个全面框架，旨在提升3D人体运动生成（MoGen）的泛化能力，借鉴了视频生成（ViGen）的经验。该框架从数据、模型和评估三方面进行创新：引入ViMoGen-228K大规模数据集，整合MoCap、网络视频和ViGen合成数据；提出ViMoGen（基于流匹配的扩散Transformer）及其高效变体ViMoGen-light；并开发MBench分层基准进行细粒度评估。实验证明，该框架显著优于现有方法，代码、数据和基准将公开。",
      "translated_title": "通用运动生成：数据、模型与评估的探索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available."
    },
    {
      "title": "远程劳务指数：衡量人工智能对远程工作的自动化程度 (原标题: Remote Labor Index: Measuring AI Automation of Remote Work)",
      "link": "https://arxiv.org/abs/2510.26787",
      "pubDate": "Thu, 30 Oct 2025 13:58:04 GMT",
      "isoDate": "2025-10-30T13:58:04.000Z",
      "creator": "Mantas Mazeika, Alice Gatti, Cristina Menghini, Udari Madhushani Sehwag, Shivam Singhal, Yury Orlovskiy, Steven Basart, Manasi Sharma, Denis Peskoff, Elaine Lau, Jaehyuk Lim, Lachlan Carroll, Alice Blair, Vinaya Sivakumar, Sumana Basu, Brad Kenstler, Yuntao Ma, Julian Michael, Xiaoke Li, Oliver Ingebretsen, Aditya Mehta, Jean Mottola, John Teichmann, Kevin Yu, Zaina Shaik, Adam Khoja, Richard Ren, Jason Hausenloy, Long Phan, Ye Htet, Ankit Aich, Tahseen Rabbani, Vivswan Shah, Andriy Novykov, Felix Binder, Kirill Chugunov, Luis Ramirez, Matias Geralnik, Hernán Mesura, Dean Lee, Ed-Yeremai Hernandez Cardona, Annette Diamond, Summer Yue, Alexandr Wang, Bing Liu, Ernesto Hernandez, Dan Hendrycks",
      "summary": "## 远程劳务指数：衡量人工智能对远程工作的自动化程度\n\n### 引言\n\n尽管人工智能（AI）在知识和推理等研究导向的基准测试中取得了显著进展，但这些进步如何转化为实际的经济价值和自动化能力，目前仍不明确。为了弥合这一差距，并提供对AI实际影响的实证评估，研究人员引入了一个新的基准。\n\n### 核心贡献：远程劳务指数 (RLI)\n\n为了量化AI在实际经济场景中的自动化潜力，本研究提出了**远程劳务指数（Remote Labor Index, RLI）**。RLI旨在提供一个全面且实用的评估框架：\n\n*   **多部门基准：** RLI是一个广泛涵盖多个经济部门的基准，确保其评估的普适性。\n*   **真实世界项目：** 它由一系列具有实际经济价值的项目组成，这些项目模拟了远程工作环境中的真实任务。\n*   **端到端性能评估：** RLI专注于评估AI代理在实际设置中的端到端性能，而不仅仅是孤立的任务。\n\n### 主要发现\n\n对AI代理在RLI上的表现进行评估后，研究得出了以下关键结果：\n\n*   **表现接近最低水平：** 总体而言，AI代理在RLI上的表现接近最低水平，表明其在处理复杂、真实世界远程工作任务方面的自动化能力仍非常有限。\n*   **最高自动化率：** 即使是表现最佳的AI代理，其自动化率也仅达到2.5%。这远低于许多关于AI自动化潜力的预期。\n\n### 意义与影响\n\n这些实证结果对于理解和讨论AI自动化具有重要意义：\n\n*   **提供实证依据：** RLI的结果为关于AI自动化的讨论提供了坚实的实证依据，有助于将这些讨论建立在具体数据而非推测之上。\n*   **跟踪AI影响的共同基础：** 它为跟踪AI对劳动力市场和经济影响提供了一个共同的、可量化的基础。\n*   **赋能利益相关者：** 通过提供清晰的性能指标，RLI使政策制定者、企业和员工等利益相关者能够更主动地应对AI驱动的劳务自动化带来的挑战和机遇。",
      "shortSummary": "本研究引入了“远程劳务指数（RLI）”，这是一个多部门基准，旨在衡量人工智能（AI）在真实世界远程工作中的自动化能力。尽管AI在研究基准上进展迅速，但RLI测试结果显示，AI代理的实际自动化率极低，表现最佳的代理仅达到2.5%。这为评估AI对劳务自动化的实际影响提供了实证依据，并帮助利益相关者更好地应对未来的挑战。",
      "translated_title": "远程劳务指数：衡量人工智能对远程工作的自动化程度",
      "images": [],
      "contentSource": "完整文章",
      "content": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation."
    },
    {
      "title": "ChartAB：图表基础定位与密集对齐基准 (原标题: ChartAB: A Benchmark for Chart Grounding & Dense Alignment)",
      "link": "https://arxiv.org/abs/2510.26781",
      "pubDate": "Thu, 30 Oct 2025 13:56:31 GMT",
      "isoDate": "2025-10-30T13:56:31.000Z",
      "creator": "Aniruddh Bansal, Davit Soselia, Dang Nguyen, Tianyi Zhou",
      "summary": "## ChartAB：图表基础定位与密集对齐基准\n\n### 引言\n\n图表在可视化、推理、数据分析以及人类思想交流中扮演着重要角色。然而，现有的视觉-语言模型（VLMs）在图表理解方面存在局限性，具体表现为：\n\n*   **缺乏细节感知**：无法准确感知图表的细微细节。\n*   **难以提取精细结构**：难以从图表中提取精细的结构信息。\n*   **阻碍图表比较与推理**：图表基础定位（chart grounding）的这些限制也阻碍了VLMs比较多个图表并进行推理的能力。\n\n### ChartAB 基准的引入\n\n为了解决上述问题，本文引入了一个新颖的“ChartAlign Benchmark (ChartAB)”，旨在为VLMs在图表基础定位任务中提供一个全面的评估。ChartAB能够评估VLMs在以下方面的能力：\n\n*   **提取表格数据**：从图表中准确提取出表格形式的数据。\n*   **定位可视化元素**：精确识别并定位图表中的各种可视化元素。\n*   **识别属性**：识别来自各种类型和复杂度的图表的各种属性。\n\n### 评估方法与流程\n\nChartAB的设计特点包括：\n\n*   **JSON模板**：设计了一个JSON模板，以方便计算专门为每个基础定位任务量身定制的评估指标。\n*   **两阶段推理工作流**：通过整合一个新颖的两阶段推理工作流，该基准能够进一步评估VLMs在两个图表之间对齐和比较元素/属性的能力。\n\n### 评估结果与发现\n\n对几个近期VLMs进行的评估分析揭示了关于它们在图表理解方面的以下新见解：\n\n*   **感知偏差**：模型在感知图表信息时存在的偏差。\n*   **弱点**：模型在处理特定图表结构或任务时的不足。\n*   **鲁棒性**：模型面对不同图表变化时的稳定性。\n*   **幻觉**：模型在理解图表时产生的不准确或虚假信息。\n\n这些发现强调了VLMs在图表理解任务中存在的精细差异，并指出了当前模型需要加强的具体技能。",
      "shortSummary": "ChartAB是一个新颖的基准，旨在全面评估视觉-语言模型（VLMs）在图表理解方面的能力。它专注于图表基础定位（提取数据、定位元素、识别属性）和跨图表密集对齐。通过使用JSON模板和两阶段推理工作流，ChartAB揭示了现有VLMs在图表理解中的感知偏差、弱点、鲁棒性问题和幻觉。这些发现为未来模型改进指明了方向，以提升其对图表细节的精细感知和推理能力。",
      "translated_title": "ChartAB：图表基础定位与密集对齐基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models."
    },
    {
      "title": "AMO-Bench：大型语言模型在高中数学竞赛中仍面临挑战 (原标题: AMO-Bench: Large Language Models Still Struggle in High School Math Competitions)",
      "link": "https://arxiv.org/abs/2510.26768",
      "pubDate": "Thu, 30 Oct 2025 13:52:02 GMT",
      "isoDate": "2025-10-30T13:52:02.000Z",
      "creator": "Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou",
      "summary": "### AMO-Bench：评估大型语言模型数学推理能力的新基准\n\n**引言**\n\n*   现有数学竞赛基准（如AIME24/25）在评估顶级大型语言模型（LLMs）的数学推理能力方面已不再有效，因为LLMs的表现已达到饱和。\n*   为解决这一问题，研究人员提出了AMO-Bench，一个具有奥林匹克竞赛级别甚至更高难度的进阶数学推理基准。\n\n**AMO-Bench 的特点**\n\n*   **问题数量与来源**：包含50道由人类精心设计的问题。\n*   **难度标准**：\n    *   所有50道问题都经过专家交叉验证，确保其难度至少达到国际数学奥林匹克（IMO）标准。\n    *   这保证了基准的挑战性，能够有效区分顶级LLMs的能力。\n*   **原创性**：所有问题均为原创，旨在防止LLMs通过数据记忆而产生潜在的性能泄露。\n*   **评估机制**：\n    *   每道问题仅要求提供最终答案，而非证明过程。\n    *   这种设计使得评估能够实现自动化和鲁棒的评分。\n\n**实验结果与分析**\n\n*   **测试模型数量**：对26个LLMs进行了AMO-Bench上的实验评估。\n*   **整体表现**：\n    *   即使是表现最好的模型，在AMO-Bench上的准确率也仅达到52.4%。\n    *   大多数LLMs的得分低于40%。\n    *   这些结果突出表明当前LLMs在数学推理方面仍有巨大的改进空间。\n*   **扩展趋势**：进一步分析揭示，随着测试时计算资源的增加，AMO-Bench上的性能呈现出有前景的扩展趋势。\n\n**结论与展望**\n\n*   AMO-Bench的发布旨在促进对语言模型推理能力提升的进一步研究。\n*   该基准为评估和推动LLMs在高级数学推理领域的进步提供了一个严格且有效的工具。",
      "shortSummary": "AMO-Bench是一个新的、由50道原创人类设计问题组成的进阶数学推理基准，难度达到国际数学奥林匹克级别，旨在解决现有基准对大型语言模型（LLMs）评估饱和的问题。对26个LLMs的测试显示，即使是表现最好的模型准确率也仅为52.4%，大多数模型低于40%，表明LLMs在数学推理方面仍有显著提升空间。研究还发现性能与计算资源之间存在积极的扩展趋势。AMO-Bench的发布旨在推动LLMs推理能力的研究。",
      "translated_title": "AMO-Bench：大型语言模型在高中数学竞赛中仍面临挑战",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/"
    },
    {
      "title": "Kimi Linear：一种富有表现力、高效的注意力架构 (原标题: Kimi Linear: An Expressive, Efficient Attention Architecture)",
      "link": "https://arxiv.org/abs/2510.26692",
      "pubDate": "Thu, 30 Oct 2025 12:59:43 GMT",
      "isoDate": "2025-10-30T12:59:43.000Z",
      "creator": "Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du",
      "summary": "## Kimi Linear：一种富有表现力、高效的注意力架构\n\n本文介绍了Kimi Linear，这是一种创新的混合线性注意力架构，首次在多种场景下（包括短上下文、长上下文和强化学习扩展）的公平比较中超越了全注意力（Full Attention）。\n\n### 核心创新：Kimi Delta Attention (KDA)\n*   **KDA是Kimi Linear的核心**：它是一个富有表现力的线性注意力模块。\n*   **扩展Gated DeltaNet**：KDA通过引入更细粒度的门控机制，有效地扩展了Gated DeltaNet。\n*   **优化内存利用**：这种机制使得KDA能够更有效地利用有限的有限状态RNN内存。\n\n### 硬件效率与算法\n*   **定制的块状算法**：Kimi Linear采用了一种定制的块状（chunkwise）算法，以实现高硬件效率。\n*   **DPLR转移矩阵的专门变体**：该算法通过使用Diagonal-Plus-Low-Rank (DPLR) 转移矩阵的专门变体来减少计算量。\n*   **与经典Delta规则的一致性**：这种变体与经典的delta规则保持更高的一致性，同时显著减少了计算开销，优于通用的DPLR公式。\n\n### 模型构建与性能\n*   **模型规模**：预训练了一个Kimi Linear模型，该模型包含30亿激活参数和总计480亿参数。\n*   **混合架构**：该模型基于KDA和多头潜在注意力（Multi-Head Latent Attention, MLA）的层级混合（layerwise hybrid）结构。\n*   **实验结果**：\n    *   在相同的训练方案下，Kimi Linear在所有评估任务中均以显著优势优于全MLA。\n    *   **KV缓存使用量大幅减少**：最高可达75%。\n    *   **解码吞吐量显著提升**：对于100万上下文长度，解码吞吐量最高可提高6倍。\n\n### 结论与开源支持\n*   **卓越的替代方案**：这些结果表明，Kimi Linear可以作为全注意力架构的直接替代品（drop-in replacement），并提供卓越的性能和效率，包括处理更长输入和输出长度的任务。\n*   **开源资源**：为了支持进一步的研究，项目开源了KDA内核和vLLM实现，并发布了预训练和指令微调的模型检查点。",
      "shortSummary": "Kimi Linear是一种创新的混合线性注意力架构，首次在多种场景下超越了全注意力。其核心是Kimi Delta Attention (KDA)，通过精细门控机制和高效的块状算法优化了内存使用和计算。实验证明，Kimi Linear在相同训练条件下显著优于全注意力，KV缓存使用量减少高达75%，解码吞吐量提高6倍。它可作为全注意力的直接替代品，提供卓越的性能和效率。项目已开源KDA内核和模型检查点。",
      "translated_title": "Kimi Linear：一种富有表现力、高效的注意力架构",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints."
    },
    {
      "title": "智能体组织时代：学习用语言模型进行组织 (原标题: The Era of Agentic Organization: Learning to Organize with Language Models)",
      "link": "https://arxiv.org/abs/2510.26658",
      "pubDate": "Thu, 30 Oct 2025 12:25:10 GMT",
      "isoDate": "2025-10-30T12:25:10.000Z",
      "creator": "Zewen Chi, Li Dong, Qingxiu Dong, Yaru Hao, Xun Wu, Shaohan Huang, Furu Wei",
      "summary": "## 智能体组织时代：学习用语言模型进行组织\n\n### 核心愿景：智能体组织\n\n文章展望了一个名为“智能体组织”（agentic organization）的AI新时代。在这个时代中，AI智能体将通过协作和并发工作来解决复杂的难题，从而实现超越单个智能体能力的成果。\n\n### 异步思维（AsyncThink）范式\n\n为了实现上述愿景，研究引入了“异步思维”（AsyncThink）作为大型语言模型（LLMs）推理的一种新范式。AsyncThink的核心在于将内部思维过程组织成可以并发执行的结构。\n\n### 思维协议与工作流程\n\n具体而言，AsyncThink提出了一种思维协议：\n\n*   **组织者（Organizer）**：动态地将子查询分配给多个“工作者”（workers）。\n*   **工作者（Workers）**：并行处理分配到的子查询。\n*   **知识合并与解决方案生成**：组织者随后合并工作者产生的中间知识，并生成连贯的最终解决方案。\n\n### 优化与学习能力\n\n更重要的是，该协议中的思维结构可以通过强化学习（reinforcement learning）进一步优化，从而提升其效率和效果。\n\n### 实验结果与性能\n\n实验结果表明，AsyncThink展现出显著的优势：\n\n*   **推理延迟**：与并行思维（parallel thinking）相比，AsyncThink的推理延迟降低了28%。\n*   **准确性提升**：在数学推理任务上，AsyncThink提高了准确性。\n*   **泛化能力**：AsyncThink能够泛化其学习到的异步思维能力，无需额外的训练即可有效处理未曾见过的任务。\n\n### 研究领域\n\n该研究属于人工智能（cs.AI）和计算与语言（cs.CL）领域。",
      "shortSummary": "该研究提出了“智能体组织”的AI新范式，通过智能体协作解决复杂问题。为实现此愿景，引入了“异步思维”（AsyncThink），一种用大型语言模型进行推理的新方法。AsyncThink将思维过程组织为并发结构，由组织者分配任务、合并知识。该思维结构可通过强化学习优化。实验证明，AsyncThink在降低28%推理延迟的同时提高了数学推理准确性，并能泛化到新任务。",
      "translated_title": "智能体组织时代：学习用语言模型进行组织",
      "images": [],
      "contentSource": "完整文章",
      "content": "We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training."
    },
    {
      "title": "Emu3.5：原生多模态模型是世界学习者 (原标题: Emu3.5: Native Multimodal Models are World Learners)",
      "link": "https://arxiv.org/abs/2510.26583",
      "pubDate": "Thu, 30 Oct 2025 11:11:16 GMT",
      "isoDate": "2025-10-30T11:11:16.000Z",
      "creator": "Yufeng Cui, Honghao Chen, Haoge Deng, Xu Huang, Xinghang Li, Jirong Liu, Yang Liu, Zhuoyan Luo, Jinsheng Wang, Wenxuan Wang, Yueze Wang, Chengyuan Wang, Fan Zhang, Yingli Zhao, Ting Pan, Xianduo Li, Zecheng Hao, Wenxuan Ma, Zhuo Chen, Yulong Ao, Tiejun Huang, Zhongyuan Wang, Xinlong Wang",
      "summary": "# Emu3.5：原生多模态世界模型\n\nEmu3.5 是一种大规模多模态世界模型，旨在原生预测视觉和语言的下一个状态。\n\n## 核心功能与训练\n*   **统一预测目标**：Emu3.5 采用统一的下一词元预测目标进行端到端预训练。\n*   **大规模数据集**：训练语料库包含超过 10 万亿个词元，主要来源于互联网视频的连续帧和文本记录，这些数据是视觉-语言交错的。\n*   **交错输入与输出**：模型能够自然地接受交错的视觉-语言输入，并生成交错的视觉-语言输出。\n*   **强化学习后训练**：通过大规模强化学习进一步对模型进行后训练，以增强其多模态推理和生成能力。\n\n## 推理效率提升\n*   **离散扩散适应 (DiDA)**：为提高推理效率，研究人员提出了 DiDA 方法。\n*   **加速效果**：DiDA 将逐词元解码转换为双向并行预测，使每张图像的推理速度加快了约 20 倍，同时不牺牲性能。\n\n## 强大的多模态能力\nEmu3.5 展现出强大的原生多模态能力，包括：\n*   **长程视觉-语言生成**：能够进行长时间跨度的视觉和语言内容生成。\n*   **任意到图像 (X2I) 生成**：支持从任意输入生成图像。\n*   **复杂富文本图像生成**：能够生成包含复杂文本信息的图像。\n\n## 通用世界建模能力\n该模型还具备可泛化的世界建模能力，支持：\n*   **时空一致的世界探索**：在不同场景和任务中进行时空一致的世界探索。\n*   **开放世界具身操作**：在开放世界中执行具身操作。\n\n## 性能比较与开源\n*   **图像生成与编辑**：在图像生成和编辑任务上，Emu3.5 的性能与 Gemini 2.5 Flash Image (Nano Banana) 相当。\n*   **交错生成任务**：在多组交错生成任务上，Emu3.5 表现出卓越的结果。\n*   **开源**：Emu3.5 已在 `this https URL` 开源，以支持社区研究。",
      "shortSummary": "Emu3.5是一个大规模多模态世界模型，能原生预测视觉和语言的下一个状态。它通过对万亿级视频数据进行端到端预训练，并结合强化学习，实现了强大的多模态推理和生成。通过DiDA技术，推理速度提升约20倍。Emu3.5展现出长程视觉-语言生成、任意到图像生成及通用世界建模能力，性能与Gemini 2.5 Flash Image相当，并在交错生成任务上表现优异。该模型已开源。",
      "translated_title": "Emu3.5：原生多模态模型是世界学习者",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research."
    },
    {
      "title": "通过头尾再平衡抵消大型视觉语言模型自我提升中的马太效应 (原标题: Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing)",
      "link": "https://arxiv.org/abs/2510.26474",
      "pubDate": "Thu, 30 Oct 2025 09:26:58 GMT",
      "isoDate": "2025-10-30T09:26:58.000Z",
      "creator": "Xin Guo, Zhiheng Xi, Yiwen Ding, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang",
      "summary": "### 大型视觉语言模型（LVLMs）自我提升中的马太效应与对策\n\n#### 1. 研究背景与问题识别\n*   **自我提升范式**：自我提升已成为推动大型视觉语言模型（LVLMs）推理能力发展的主流范式，模型通过迭代探索和学习成功的轨迹。\n*   **关键问题——马太效应**：研究发现，在此过程中存在一个关键问题：模型擅长为简单查询（即“头部数据”）生成高质量的轨迹，但在处理更复杂的查询（即“尾部数据”）时却表现不佳。\n*   **不平衡优化**：这种差异导致了不平衡的优化，促使模型优先发展简单的推理技能，同时阻碍了其处理更复杂推理任务的能力。\n*   **效应加剧**：随着迭代次数的增加，这种不平衡现象变得越来越明显，研究者将其称为“马太效应”。这种效应最终阻碍了模型的进一步改进，并导致了性能瓶颈。\n\n#### 2. 提出的解决方案：头尾再平衡策略\n*   **目标**：为了应对这一挑战，研究者提出了四种高效策略，旨在在探索和学习的自我提升过程中实现“头尾再平衡”。\n*   **两大视角**：这些策略从两个主要视角出发：\n    *   **分布重塑（distribution-reshaping）**：调整数据或轨迹的分布，以减少头部和尾部数据之间的差距。\n    *   **轨迹重采样（trajectory-resampling）**：对生成的轨迹进行重新采样，以确保复杂任务的轨迹也能得到充分学习。\n\n#### 3. 实验验证与结果\n*   **实验模型**：在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上进行了广泛的实验。\n*   **任务类型**：实验涵盖了多种视觉推理任务。\n*   **实验结果**：\n    *   研究方法持续改进了视觉推理能力。\n    *   与传统的自我提升方法相比，平均性能提升了3.86个百分点。\n*   **结论**：实验结果有力地证明了所提出方法在抵消马太效应、提升LVLMs复杂推理能力方面的有效性。",
      "shortSummary": "该研究解决了大型视觉语言模型（LVLMs）自我提升过程中出现的“马太效应”问题，即模型在简单任务上表现优异，但在复杂任务上表现不佳，导致优化不平衡。为应对此挑战，作者提出了四种基于分布重塑和轨迹重采样的头尾再平衡策略。在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上进行的视觉推理任务实验表明，这些方法持续提升了模型的视觉推理能力，平均比传统自我提升方法高出3.86个百分点。",
      "translated_title": "通过头尾再平衡抵消大型视觉语言模型自我提升中的马太效应",
      "images": [],
      "contentSource": "完整文章",
      "content": "Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew effect\"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average."
    },
    {
      "title": "智能体能否征服网络？探索ChatGPT Atlas智能体在网页游戏中的前沿应用 (原标题: Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games)",
      "link": "https://arxiv.org/abs/2510.26298",
      "pubDate": "Thu, 30 Oct 2025 05:35:51 GMT",
      "isoDate": "2025-10-30T05:35:51.000Z",
      "creator": "Jingran Zhang, Ning Li, Justin Cui",
      "summary": "### ChatGPT Atlas智能体在网页游戏中的表现评估\n\n本研究对OpenAI的ChatGPT Atlas智能体在网页交互方面的能力进行了早期评估，特别关注其在动态、交互式环境中的表现。ChatGPT Atlas引入了新的网页交互功能，使其能够分析网页、处理用户意图，并直接在浏览器中执行光标和键盘输入。\n\n#### 研究方法\n\n*   **测试场景**：研究人员选择了一系列基于浏览器的游戏作为测试场景，包括：\n    *   Google的T-Rex Runner（恐龙跑酷）\n    *   Sudoku（数独）\n    *   Flappy Bird（飞扬的小鸟）\n    *   2048\n*   **评估指标**：采用游戏内的表现分数作为量化指标，以评估Atlas在不同任务类型中的性能。\n\n#### 主要发现\n\n*   **逻辑推理任务表现出色**：\n    *   在数独等逻辑推理任务中，Atlas表现强劲，完成谜题的速度显著快于人类基线水平。\n*   **实时互动游戏表现不佳**：\n    *   在需要精确时机和运动控制的实时游戏中（如T-Rex Runner和Flappy Bird），Atlas表现出显著的困难，常常无法通过初始障碍。\n\n#### 结论\n\n研究结果表明，尽管ChatGPT Atlas展示了强大的分析处理能力，但在需要实时交互的动态网页环境中，其仍存在明显的局限性。\n\n#### 项目信息\n\n*   **项目网站**：[this https URL](this https URL)\n*   **研究主题**：计算与语言 (cs.CL); 人工智能 (cs.AI)\n*   **引用信息**：arXiv:2510.26298 [cs.CL]",
      "shortSummary": "本研究评估了ChatGPT Atlas智能体在网页游戏中的交互能力。结果显示，Atlas在数独等逻辑推理任务中表现出色，完成速度快于人类。然而，在T-Rex Runner和Flappy Bird等需要精确时机和运动控制的实时游戏中，Atlas表现挣扎，难以通过初始障碍。这表明Atlas具备分析能力，但在动态实时网页交互方面仍存在显著局限性。",
      "translated_title": "智能体能否征服网络？探索ChatGPT Atlas智能体在网页游戏中的前沿应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io."
    },
    {
      "title": "OmniLayout：利用大型语言模型实现粗到细学习的通用文档布局生成 (原标题: OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation)",
      "link": "https://arxiv.org/abs/2510.26213",
      "pubDate": "Thu, 30 Oct 2025 03:39:54 GMT",
      "isoDate": "2025-10-30T03:39:54.000Z",
      "creator": "Hengrui Kang, Zhuangcheng Gu, Zhiyuan Zhao, Zichen Wen, Bin Wang, Weijia Li, Conghui He",
      "summary": "### OmniLayout：利用大型语言模型实现粗到细学习的通用文档布局生成\n\n#### 1. 引言与背景\n*   **文档AI的现状**：文档人工智能（Document AI）发展迅速，但大部分努力集中在文档布局分析（DLA）。\n*   **未充分探索的领域**：文档布局生成（Document Layout Generation）作为其生成对应物，仍未得到充分探索。\n*   **主要障碍**：\n    *   **多样化布局稀缺**：现有研究主要关注具有曼哈顿风格结构的学术论文。\n    *   **开放世界体裁不足**：报纸和杂志等开放世界的文档类型在现有研究中严重不足。\n\n#### 2. 解决方案：OmniLayout-1M 数据集\n*   **目的**：为解决多样化布局数据稀缺的问题。\n*   **内容**：\n    *   策划了 **OmniLayout-1M**，这是首个百万规模的多样化文档布局数据集。\n    *   涵盖了六种常见的文档类型。\n    *   包含了从多个来源收集的当代布局。\n\n#### 3. 解决方案：OmniLayout-LLM 模型\n*   **挑战**：现有方法在复杂领域表现不佳，并且难以连贯地排列长序列。\n*   **模型介绍**：\n    *   引入了 **OmniLayout-LLM**，一个拥有0.5B参数的模型。\n    *   该模型设计了**两阶段粗到细学习范式**：\n        1.  **第一阶段：学习通用布局原则**\n            *   利用 OmniLayout-1M 数据集学习通用的布局原则。\n            *   此阶段使用粗粒度的类别定义进行学习。\n        2.  **第二阶段：知识迁移到特定领域**\n            *   将第一阶段学到的知识迁移到特定的文档领域。\n            *   此阶段使用细粒度的标注进行学习。\n\n#### 4. 实验结果与性能\n*   **实验范围**：广泛的实验证明了该方法的有效性。\n*   **性能表现**：\n    *   在 M$^{6}$Doc 数据集的多个领域取得了强大的性能。\n    *   显著超越了现有的布局生成专家模型。\n    *   也超越了几个最新的通用大型语言模型（LLMs）。\n\n#### 5. 可用性\n*   **资源发布**：我们的代码、模型和数据集将公开发布。",
      "shortSummary": "本文介绍了OmniLayout，旨在解决文档布局生成领域缺乏多样化数据和现有方法不足的问题。研究团队构建了首个百万级多样化文档布局数据集OmniLayout-1M，并提出了OmniLayout-LLM模型。该0.5B模型采用两阶段粗到细学习范式，首先从OmniLayout-1M学习通用布局原则，然后将知识迁移到特定领域。实验证明，OmniLayout在M^6Doc数据集上表现出色，超越了现有专家模型和通用LLM。代码、模型和数据集将公开发布。",
      "translated_title": "OmniLayout：利用大型语言模型实现粗到细学习的通用文档布局生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M^{6}Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released."
    },
    {
      "title": "CRAG-MM：多模态多轮综合RAG基准 (原标题: CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark)",
      "link": "https://arxiv.org/abs/2510.26160",
      "pubDate": "Thu, 30 Oct 2025 01:50:48 GMT",
      "isoDate": "2025-10-30T01:50:48.000Z",
      "creator": "Jiaqi Wang, Xiao Yang, Kai Sun, Parth Suresh, Sanat Sharma, Adam Czyzewski, Derek Andersen, Surya Appini, Arkav Banerjee, Sajal Choudhary, Shervin Ghasemlou, Ziqiang Guan, Akil Iyer, Haidar Khan, Lingkun Kong, Roy Luo, Tiffany Ma, Zhen Qiao, David Tran, Wenfang Xu, Skyler Yeatman, Chen Zhou, Gunveer Gujral, Yinglong Xia, Shane Moon, Nicolas Scheffer, Nirav Shah, Eun Chang, Yue Liu, Florian Metze, Tammy Stark, Zhaleh Feizollahi, Andrea Jessee, Mangesh Pujari, Ahmed Aly, Babak Damavandi, Rakesh Wanga, Anuj Kumar, Rohit Patel, Wen-tau Yih, Xin Luna Dong",
      "summary": "# CRAG-MM：多模态多轮综合RAG基准\n\n## 引言\n随着智能眼镜等可穿戴设备的普及，用户能够更便捷地获取视野中实体的信息。多模态检索增强生成（MM-RAG）技术在此类应用中扮演着核心角色。然而，目前针对MM-RAG任务，特别是面向可穿戴设备场景的全面基准仍然缺失。\n\n## CRAG-MM基准介绍\n为弥补这一空白，研究人员提出了CRAG-MM——一个针对多模态多轮对话的综合RAG基准。\n\n### 数据集构成\n*   **数据规模**：包含6.5K个（图像、问题、答案）三元组和2K个基于视觉的多轮对话。\n*   **领域多样性**：数据涵盖13个不同领域。\n*   **真实场景模拟**：特别包含了6.2K张第一人称视角图像，旨在高度模拟可穿戴设备所捕获的画面。\n\n### 问题设计与挑战\nCRAG-MM中的问题经过精心设计，旨在反映真实世界的复杂场景和挑战，具体包括：\n*   **图像质量问题**：考虑了五种不同类型的图像质量问题。\n*   **问题类型**：涵盖了六种不同的问题类型。\n*   **实体流行度**：问题设计考虑了实体流行度的差异。\n*   **信息动态性**：处理信息动态性变化带来的挑战。\n*   **对话轮次**：包含不同对话轮次的问题，以评估多轮对话能力。\n\n## 任务设计\nCRAG-MM设计了三种主要任务，每种任务都配备了相应的检索语料库，并提供了用于图像-知识图谱检索和网页检索的API：\n1.  **单源增强** (Single-source augmentation)\n2.  **多源增强** (Multi-source augmentation)\n3.  **多轮对话** (Multi-turn conversations)\n\n## 评估结果与影响\n### 性能表现\n*   **基线模型**：直接的RAG方法在CRAG-MM的单轮和多轮问答任务中，真实性（truthfulness）分别仅达到32%和43%。\n*   **SOTA工业解决方案**：最先进的工业解决方案表现相似，真实性分别为32%和45%。\n*   **改进空间**：这些结果清晰地表明，当前的MM-RAG方法在处理复杂、真实世界场景时，仍有巨大的改进和提升空间。\n\n### 早期影响\n*   **KDD Cup 2025**：CRAG-MM已成功作为KDD Cup 2025的基准，吸引了约1000名参与者和5000份提交。\n*   **性能提升**：获胜解决方案将基线性能提高了28%，这充分展示了CRAG-MM在推动该领域研究和发展方面的早期且显著的影响。\n\n## 相关工具与资源\n文章中提到了与arXiv相关的社交分享图标，例如：\n*   BibSonomy图标：\n    ![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)\n*   Reddit图标：\n    ![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)",
      "shortSummary": "CRAG-MM是一个为多模态多轮检索增强生成（MM-RAG）设计的综合基准，尤其针对可穿戴设备场景。它包含6.5K图像三元组和2K多轮对话，涵盖13个领域，并模拟真实世界挑战。评估显示，现有RAG方法在CRAG-MM上的真实性仅为32-45%，表明仍有巨大改进空间。该基准已成功举办KDD Cup 2025，吸引了大量参与者，并显著推动了该领域的发展。",
      "translated_title": "CRAG-MM：多模态多轮综合RAG基准",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field."
    },
    {
      "title": "FullPart：以全分辨率生成每个3D部件 (原标题: FullPart: Generating each 3D Part at Full Resolution)",
      "link": "https://arxiv.org/abs/2510.26140",
      "pubDate": "Thu, 30 Oct 2025 00:51:05 GMT",
      "isoDate": "2025-10-30T00:51:05.000Z",
      "creator": "Lihe Ding, Shaocong Dong, Yaokun Li, Chenjian Gao, Xiao Chen, Rui Han, Yihao Kuang, Hong Zhang, Bo Huang, Zhanpeng Huang, Zibin Wang, Dan Xu, Tianfan Xue",
      "summary": "### FullPart：以全分辨率生成每个3D部件\n\n本文介绍了一种名为 FullPart 的新型框架，旨在解决现有3D部件生成方法在几何细节和部件质量方面的不足。\n\n**现有方法的局限性：**\n*   **隐式部件生成器：** 之前的隐式方法通常使用向量集令牌表示部件，但往往缺乏足够的几何细节。\n*   **显式体素表示方法：** 采用显式体素表示的方法通常共享一个全局体素网格，这导致小部件占据的体素过少，从而降低了其生成质量。\n\n**FullPart 框架的创新点：**\nFullPart 结合了隐式和显式范式，通过以下步骤实现高质量的3D部件生成：\n1.  **边界框布局生成：** 首先通过一个隐式框向量集扩散过程推导出边界框布局。由于框令牌包含的几何细节很少，隐式扩散能够有效地处理此任务。\n2.  **全分辨率部件生成：** 随后，FullPart 在每个部件自己的固定全分辨率体素网格内生成详细部件。\n    *   **关键优势：** 与共享低分辨率空间不同，FullPart 中的每个部件（即使是小部件）都以全分辨率生成，从而能够合成复杂的细节。\n3.  **中心点编码策略：** 为了解决不同实际尺寸部件之间信息交换时的错位问题，FullPart 引入了一种中心点编码策略，以保持全局一致性。\n\n**PartVerse-XL 数据集：**\n*   为了解决可靠部件数据稀缺的问题，作者提出了 PartVerse-XL，这是迄今为止最大的人工标注3D部件数据集。\n*   该数据集包含40,000个对象和320,000个部件。\n\n**实验结果与未来展望：**\n*   广泛的实验表明，FullPart 在3D部件生成方面取得了最先进的成果。\n*   作者承诺将发布所有代码、数据和模型，以促进3D部件生成领域的未来研究。",
      "shortSummary": "FullPart 是一种新型3D部件生成框架，旨在解决现有方法几何细节不足和小型部件质量差的问题。它结合了隐式边界框扩散和显式全分辨率部件生成，确保每个部件（包括小型部件）都能以高细节度合成。为解决数据稀缺，作者还发布了迄今最大的3D部件数据集 PartVerse-XL。实验证明 FullPart 达到了最先进水平，并将开源代码和数据。",
      "translated_title": "FullPart：以全分辨率生成每个3D部件",
      "images": [],
      "contentSource": "完整文章",
      "content": "Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation."
    },
    {
      "title": "监督式强化学习：从专家轨迹到逐步推理 (原标题: Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning)",
      "link": "https://arxiv.org/abs/2510.25992",
      "pubDate": "Wed, 29 Oct 2025 18:05:08 GMT",
      "isoDate": "2025-10-29T18:05:08.000Z",
      "creator": "Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee",
      "summary": "# 监督式强化学习：从专家轨迹到逐步推理\n\n本文介绍了一种名为**监督式强化学习（Supervised Reinforcement Learning, SRL）**的新框架，旨在解决大型语言模型（LLMs），特别是小型开源模型在处理需要多步推理的问题时所面临的挑战。\n\n## 现有方法的局限性\n\n*   **可验证奖励强化学习（RLVR）**：当即使经过多次尝试也很难采样到正确解决方案时，RLVR 会失效。\n*   **监督微调（SFT）**：SFT 倾向于通过僵硬的逐令牌模仿来过度拟合长演示，导致缺乏灵活性。\n\n## 监督式强化学习（SRL）方法\n\nSRL 框架通过以下创新方法弥补了现有方法的不足：\n\n*   **问题重构**：将问题解决重新定义为生成一系列逻辑“动作”。\n*   **内部推理独白**：训练模型在执行每个动作之前生成一个内部推理独白，这有助于模型进行更深层次的思考和规划。\n*   **平滑的逐步奖励**：\n    *   SRL 基于模型动作与从 SFT 数据集中提取的专家动作之间的相似性，以逐步的方式提供更平滑的奖励。\n    *   这种监督机制即使在所有推演都是不正确的情况下，也能提供更丰富的学习信号。\n    *   它鼓励在专家演示指导下的灵活推理，而非僵硬模仿。\n\n## 关键成果与优势\n\n*   **解决复杂问题**：SRL 使小型模型能够学习以前通过 SFT 或 RLVR 无法学习的具有挑战性的问题。\n*   **性能提升**：在训练开始时使用 SRL 进行初始化，然后通过 RLVR 进行优化，可以获得最强的整体性能。\n*   **广泛适用性**：除了推理基准测试，SRL 还能有效地推广到代理软件工程任务。\n*   **鲁棒性和多功能性**：这确立了 SRL 作为一种强大且多功能的训练框架，适用于面向推理的 LLMs。",
      "shortSummary": "监督式强化学习（SRL）是一种新框架，旨在解决小型LLM在多步推理中的挑战。它将问题解决重构为逻辑动作序列，训练模型在每个动作前进行内部推理，并根据模型动作与专家动作的相似性提供平滑的逐步奖励。SRL使小型模型能解决SFT和RLVR无法处理的复杂问题，并能推广到代理软件工程任务，是面向推理LLM的强大且多功能的训练框架。",
      "translated_title": "监督式强化学习：从专家轨迹到逐步推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs."
    },
    {
      "title": "MIRO：多奖励条件预训练提升T2I质量和效率 (原标题: MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency)",
      "link": "https://arxiv.org/abs/2510.25897",
      "pubDate": "Wed, 29 Oct 2025 14:59:17 GMT",
      "isoDate": "2025-10-29T14:59:17.000Z",
      "creator": "Nicolas Dufour, Lucas Degeorge, Arijit Ghosh, Vicky Kalogeiton, David Picard",
      "summary": "## MIRO：多奖励条件预训练提升T2I质量和效率\n\n### 背景与问题\n当前的文本到图像（T2I）生成模型通常在大规模、未经筛选的数据集上进行训练，这导致其生成结果往往与用户偏好不符。为了解决这一问题，近期出现了专门设计的奖励模型，用于对生成的图像进行后处理筛选，使其与用户偏好等奖励目标对齐。然而，这种后处理方法存在以下缺点：\n\n*   **数据浪费**：丢弃了大量具有信息价值的数据。\n*   **多样性受损**：通常只优化单一奖励，损害了生成结果的多样性。\n*   **语义保真度下降**：可能影响生成图像的语义准确性。\n*   **效率低下**：增加了额外的处理步骤，降低了整体效率。\n\n### MIRO方法\n为了克服上述局限，论文提出了一种名为 **MIRO (MultI-Reward cOnditioned pretraining)** 的新方法。MIRO的核心思想是在模型训练阶段就将其与**多个奖励模型**进行条件化，从而使模型能够直接学习并内化用户偏好，而非通过后处理进行筛选。\n\n### 主要优势与成果\nMIRO方法带来了显著的改进：\n\n*   **视觉质量大幅提升**：显著提高了生成图像的视觉质量。\n*   **训练效率显著加快**：大幅缩短了模型的训练时间。\n*   **最先进的性能**：\n    *   在 GenEval 组合基准测试中取得了最先进的性能。\n    *   在用户偏好评分（包括 PickAScore、ImageReward 和 HPSv2）上也达到了最先进的水平。\n\n### 结论\nMIRO通过在预训练阶段引入多奖励条件化，有效解决了现有T2I模型在用户偏好对齐、多样性、语义保真度和效率方面的问题，为高质量、高效率的T2I生成提供了新的范式。",
      "shortSummary": "MIRO是一种新的文本到图像（T2I）预训练方法，旨在解决现有模型与用户偏好不符的问题。它通过在训练阶段将模型与多个奖励模型进行条件化，直接学习用户偏好，而非依赖后处理筛选。MIRO显著提升了生成图像的视觉质量和训练效率，并在GenEval组合基准测试及用户偏好评分上均取得了最先进的性能。",
      "translated_title": "MIRO：多奖励条件预训练提升T2I质量和效率",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2)."
    },
    {
      "title": "VFXMaster：通过上下文学习解锁动态视觉效果生成 (原标题: VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning)",
      "link": "https://arxiv.org/abs/2510.25772",
      "pubDate": "Wed, 29 Oct 2025 13:59:53 GMT",
      "isoDate": "2025-10-29T13:59:53.000Z",
      "creator": "Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jia",
      "summary": "## VFXMaster：通过上下文学习解锁动态视觉效果生成\n\n### 背景与挑战\n\n视觉效果（VFX）在数字媒体中扮演着至关重要的角色，但其创作对于生成式AI而言仍然是一个重大挑战。当前主流的方法通常遵循“每个效果一个LoRA”的范式，这种方法不仅资源密集，而且在根本上无法泛化到未曾见过的效果，从而严重限制了其可扩展性和创作潜力。\n\n### VFXMaster的引入\n\n为了应对这些挑战，本文引入了**VFXMaster**，这是首个统一的、基于参考的VFX视频生成框架。它将效果生成重新定义为一项**上下文学习任务**，使其能够将参考视频中的多样化动态效果复制到目标内容上。此外，VFXMaster对未见过的效果类别展现出卓越的泛化能力。\n\n### 核心技术设计\n\n1.  **上下文条件策略：** 设计了一种上下文条件策略，通过提供一个参考示例来提示模型，引导其理解并生成目标效果。\n2.  **上下文注意力掩码：** 提出了一种上下文注意力掩码，用于精确地解耦和注入必要的VFX属性。这使得单个统一模型能够在不发生信息泄露的情况下，掌握效果的模仿。\n3.  **高效的一次性效果适应机制：** 此外，VFXMaster还提出了一种高效的一次性效果适应机制，旨在通过单个用户提供的视频，快速提升模型对难以处理的未见效果的泛化能力。\n\n### 实验结果与未来展望\n\n广泛的实验证明，VFXMaster能够有效地模仿各种类别的效果信息，并对域外效果展现出色的泛化能力。为了促进未来的研究，作者计划向社区发布其代码、模型以及一个全面的数据集。",
      "shortSummary": "VFXMaster是一个创新的统一、基于参考的框架，旨在通过上下文学习解决动态视觉效果（VFX）生成的挑战。它克服了传统方法资源密集且泛化能力差的局限，能够将参考视频中的多样化效果复制到目标内容上，并对未见效果展现出卓越的泛化能力。VFXMaster利用上下文条件策略和注意力掩码精确注入效果属性，并结合一次性适应机制，实现了高效且广泛的VFX生成。",
      "translated_title": "VFXMaster：通过上下文学习解锁动态视觉效果生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community."
    },
    {
      "title": "Gaperon：一套混合英法双语的生成式语言模型套件 (原标题: Gaperon: A Peppered English-French Generative Language Model Suite)",
      "link": "https://arxiv.org/abs/2510.25771",
      "pubDate": "Wed, 29 Oct 2025 13:59:39 GMT",
      "isoDate": "2025-10-29T13:59:39.000Z",
      "creator": "Nathan Godey, Wissam Antoun, Rian Touchent, Rachel Bawden, Éric de la Clergerie, Benoît Sagot, Djamé Seddah",
      "summary": "## Gaperon：一个开放的法英双语生成式语言模型套件\n\nGaperon 是一个完全开放的法英双语（English-French-coding）生成式语言模型套件，旨在提高大规模模型训练的透明度和可复现性。\n\n### 模型组成与训练\n*   **模型规模**：该套件包含 1.5B、8B 和 24B 参数的模型。\n*   **训练数据**：这些模型在 2-4 万亿个 token 上进行训练。\n\n### 发布内容\n为了支持进一步的研究和复现性，Gaperon 项目发布了训练管道的所有关键元素：\n*   **数据集**：经过神经质量分类器过滤的法英双语数据集。\n*   **训练框架**：一个高效的数据整理和训练框架。\n*   **检查点**：数百个中间训练检查点。\n\n### 研究发现与讨论\n该项目通过研究数据过滤和污染如何影响基准测试和生成性能，得出了以下关键发现：\n*   **数据过滤与性能权衡**：\n    *   为提高语言质量而进行的过滤能够增强文本的流畅性和连贯性，但会导致基准测试结果不佳。\n    *   后期故意污染（即在包含测试集的数据混合上继续训练）可以恢复有竞争力的分数，同时对生成质量的损害程度合理。\n*   **基准测试泄露**：研究指出，常规的神经过滤可能会无意中放大基准测试泄露问题。\n\n### 安全研究支持\n为了支持未来的安全研究，Gaperon 项目还在预训练期间引入了无害数据投毒（harmless data poisoning），提供了一个真实的测试平台。\n\n### 核心贡献\n通过开放所有模型、数据集、代码和检查点，Gaperon 为探索多语言语言模型开发中数据整理、评估、安全性和开放性之间的权衡奠定了可复现的基础。",
      "shortSummary": "Gaperon 是一个开放的法英双语生成式语言模型套件，包含1.5B、8B和24B参数模型，旨在提升大规模模型训练的透明度和可复现性。该项目发布了训练数据、框架和检查点。研究发现，为提高语言质量而过滤数据会降低基准测试分数，而后期故意污染可恢复分数，但对生成质量影响有限。Gaperon还引入了无害数据投毒以支持安全研究，为多语言模型开发中的数据、评估、安全和开放性权衡提供了可复现的基础。",
      "translated_title": "Gaperon：一套混合英法双语的生成式语言模型套件",
      "images": [],
      "contentSource": "完整文章",
      "content": "We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development."
    },
    {
      "title": "大模型时代的多模态空间推理：一项综述与基准 (原标题: Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks)",
      "link": "https://arxiv.org/abs/2510.25760",
      "pubDate": "Wed, 29 Oct 2025 13:55:43 GMT",
      "isoDate": "2025-10-29T13:55:43.000Z",
      "creator": "Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu",
      "summary": "### 大模型时代的多模态空间推理：一项综述与基准\n\n**引言**\n\n人类天生具备通过多模态观察（如视觉和听觉）理解空间的能力。大型多模态推理模型（MLLMs）通过学习感知和推理，扩展了这些能力，并在各种空间任务中展现出令人鼓舞的性能。然而，目前针对这些模型的系统性综述和公开可用的基准仍然有限。\n\n**本综述的目的与贡献**\n\n本综述旨在解决上述空白，提供对大型模型多模态空间推理任务的全面审视。其主要贡献包括：\n\n*   **全面综述：** 对多模态空间推理任务进行了全面的回顾。\n*   **进展分类：** 对多模态大语言模型（MLLMs）的最新进展进行了系统分类。\n*   **开放基准：** 引入了用于评估的开放基准。\n\n**综述内容概览**\n\n本综述从多个维度深入探讨了多模态空间推理领域：\n\n1.  **通用空间推理：**\n    *   概述了通用空间推理的基本概念。\n    *   重点关注后训练技术。\n    *   探讨了模型的可解释性。\n    *   分析了相关架构。\n\n2.  **超越经典2D任务：**\n    *   **空间关系推理：** 深入研究了3D空间中的空间关系推理。\n    *   **场景与布局理解：** 探讨了3D场景和布局的理解。\n    *   **视觉问答与定位：** 分析了3D空间中的视觉问答（VQA）和定位（grounding）任务。\n\n3.  **具身AI（Embodied AI）：**\n    *   回顾了具身AI领域的最新进展。\n    *   特别关注视觉-语言导航模型。\n    *   讨论了具身动作模型。\n\n4.  **新兴模态：**\n    *   考虑了音频等新兴模态在空间理解中的应用。\n    *   探讨了第一人称视频（egocentric video）如何通过新传感器促进新颖的空间理解。\n\n**展望与资源**\n\n作者认为，这项综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了宝贵的见解。关于本综述的最新信息、代码和开放基准的实现，可在提供的URL（https://this.https/URL）找到。",
      "shortSummary": "本综述全面审视了大模型时代的多模态空间推理，旨在解决当前系统性综述和公开基准的不足。文章对多模态大语言模型（MLLMs）的进展进行了分类，并介绍了开放基准。内容涵盖通用空间推理、3D空间任务（如空间关系、场景理解、视觉问答）、具身AI（如视觉-语言导航）以及音频、第一人称视频等新兴模态。该工作为多模态空间推理领域提供了基础和见解。",
      "translated_title": "大模型时代的多模态空间推理：一项综述与基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning."
    },
    {
      "title": "TheraMind：一种用于长期心理咨询的策略性自适应智能体 (原标题: TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling)",
      "link": "https://arxiv.org/abs/2510.25758",
      "pubDate": "Wed, 29 Oct 2025 13:54:20 GMT",
      "isoDate": "2025-10-29T13:54:20.000Z",
      "creator": "He Hu, Yucheng Zhou, Chiyuan Ma, Qianning Wang, Zheng Zhang, Fei Ma, Laizhong Cui, Qi Tian",
      "summary": "TheraMind：一种用于长期心理咨询的策略性自适应智能体\n\n**背景与挑战**\n\n*   大型语言模型（LLMs）在心理咨询领域的应用日益受到关注。\n*   然而，现有方法存在显著不足，包括：\n    *   缺乏情感理解能力。\n    *   缺乏自适应策略。\n    *   在多会话治疗中缺乏长期记忆和治疗方法的运用。\n*   这些局限性使得LLMs在实际临床实践中仍有很大差距。\n\n**TheraMind 介绍**\n\n*   为解决上述关键问题，研究者引入了 TheraMind，一个专为长期心理咨询设计的策略性自适应智能体。\n\n**核心架构：双循环设计**\n\n*   TheraMind 的基石是一个新颖的双循环架构，它将复杂的咨询过程解耦为两个主要部分：\n    *   **会话内循环 (Intra-Session Loop)**：\n        *   负责战术性的对话管理。\n        *   感知患者的情绪状态，以动态选择响应策略。\n        *   利用跨会话记忆来确保咨询的连续性。\n    *   **跨会话循环 (Cross-Session Loop)**：\n        *   负责战略性的治疗规划。\n        *   在每次会话结束后评估所应用治疗方法的有效性。\n        *   根据评估结果调整后续互动中的治疗方法，从而赋予智能体长期的适应性。\n\n**验证与成果**\n\n*   研究团队在一个基于真实临床案例的高保真模拟环境中对 TheraMind 进行了验证。\n*   广泛的评估结果表明，TheraMind 优于其他现有方法，尤其在多会话指标上表现突出，例如：\n    *   连贯性（Coherence）。\n    *   灵活性（Flexibility）。\n    *   治疗协调性（Therapeutic Attunement）。\n*   这些结果验证了其双循环设计在模拟策略性、自适应和长期治疗行为方面的有效性。\n\n**代码可用性**\n\n*   TheraMind 的代码已公开发布。",
      "shortSummary": "TheraMind 是一种用于长期心理咨询的策略性自适应智能体，旨在解决现有大型语言模型在情感理解、自适应策略和多会话记忆方面的不足。其核心是一个双循环架构：会话内循环负责动态对话管理和情绪感知，跨会话循环负责战略性治疗规划和方法调整，确保长期适应性。在基于真实案例的模拟中，TheraMind 在多会话指标上表现优异，验证了其在模拟策略性、自适应和长期治疗行为方面的有效性。",
      "translated_title": "TheraMind：一种用于长期心理咨询的策略性自适应智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/."
    }
  ],
  "lastUpdated": "2025-10-31T09:36:53.853Z"
}