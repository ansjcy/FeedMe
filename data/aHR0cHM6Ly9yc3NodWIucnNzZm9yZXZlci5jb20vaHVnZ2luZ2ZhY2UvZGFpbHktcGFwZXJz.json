{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Puppeteer: 绑定和动画化你的3D模型 (原标题: Puppeteer: Rig and Animate Your 3D Models)",
      "link": "https://arxiv.org/abs/2508.10898",
      "pubDate": "Thu, 14 Aug 2025 13:59:31 GMT",
      "isoDate": "2025-08-14T13:59:31.000Z",
      "creator": "Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang",
      "summary": "# Puppeteer：3D模型绑定与动画框架\n\n## 概述\n现代交互式应用对动态3D内容的需求日益增长，然而，将静态3D模型转换为可动画资产是内容创作流程中的一个显著瓶颈。尽管生成式AI在静态3D模型创建方面取得了革命性进展，但绑定（rigging）和动画制作仍严重依赖于专业人员的干预。本文提出了Puppeteer，一个全面的框架，旨在解决各种3D对象的自动绑定和动画问题。\n\n## Puppeteer框架介绍\nPuppeteer系统通过以下三个主要阶段实现其功能：\n\n### 1. 骨骼结构预测\nPuppeteer首先通过一个自回归Transformer来预测合理的骨骼结构。该Transformer引入了：\n*   **基于关节的标记化策略**：用于紧凑表示骨骼信息。\n*   **具有随机扰动的分层排序方法**：增强双向学习能力。\n\n### 2. 蒙皮权重推断\n在骨骼结构预测之后，系统通过一个基于注意力的架构推断蒙皮权重。该架构整合了：\n*   **拓扑感知关节注意力**：根据骨骼图距离明确编码关节间的关系。\n\n### 3. 动画生成管线\n最后，Puppeteer通过一个可微分的基于优化的动画管线来补充上述绑定进展。该管线能够：\n*   生成稳定、高保真的动画。\n*   在计算效率上优于现有方法。\n\n## 性能与优势\n在多个基准测试中进行的广泛评估表明，Puppeteer方法在骨骼预测精度和蒙皮质量方面均显著优于最先进的技术。该系统能够稳健地处理多样化的3D内容，包括专业设计的游戏资产和AI生成的形状，并生成时间上连贯的动画，有效消除了现有方法中常见的抖动问题。",
      "shortSummary": "Puppeteer是一个创新的框架，旨在自动化3D模型的绑定和动画过程。它通过自回归Transformer预测骨骼结构，利用注意力机制推断蒙皮权重，并采用可微分优化管线生成稳定、高保真的动画。该系统在骨骼预测精度和蒙皮质量上超越现有技术，能处理多样化3D内容，并消除动画抖动，显著提升了3D内容创作效率。",
      "translated_title": "Puppeteer: 绑定和动画化你的3D模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods."
    },
    {
      "title": "STream3R：基于因果Transformer的可扩展序列3D重建 (原标题: STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer)",
      "link": "https://arxiv.org/abs/2508.10893",
      "pubDate": "Thu, 14 Aug 2025 13:58:05 GMT",
      "isoDate": "2025-08-14T13:58:05.000Z",
      "creator": "Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan",
      "summary": "STream3R是一种新颖的3D重建方法，它将点图预测重新定义为一个仅解码器Transformer问题。该方法旨在解决现有最先进的多视角重建方法的局限性，这些方法要么依赖于昂贵的全局优化，要么依赖于简单的内存机制，导致在序列长度增加时扩展性差。\n\n**STream3R的核心创新与优势：**\n\n*   **流式处理框架：** STream3R引入了一个流式处理框架，借鉴了现代语言模型领域的进展，利用因果注意力机制高效处理图像序列。\n*   **Transformer架构：** 将点图预测重构为仅解码器Transformer问题，利用其强大的序列建模能力。\n*   **几何先验学习：** 通过从大规模3D数据集中学习几何先验，STream3R能够很好地泛化到各种复杂场景，包括传统方法难以处理的动态场景。\n*   **卓越的性能：** 广泛的实验表明，STream3R在静态和动态场景基准测试中均持续优于现有工作。\n*   **兼容LLM训练基础设施：** STream3R本质上与大型语言模型（LLM）风格的训练基础设施兼容，这使得其能够高效地进行大规模预训练和针对各种下游3D任务的微调。\n\n**影响与展望：**\n\nSTream3R的成果强调了因果Transformer模型在在线3D感知方面的巨大潜力，为流媒体环境中的实时3D理解铺平了道路。",
      "shortSummary": "STream3R是一种创新的3D重建方法，它将点图预测重构为基于因果Transformer的流式问题。该方法借鉴语言模型技术，通过高效的因果注意力机制处理图像序列。STream3R能够从大规模3D数据中学习几何先验，有效处理动态场景，并在性能上超越现有方法。它兼容LLM训练，为实时3D感知和理解提供了可扩展的解决方案。",
      "translated_title": "STream3R：基于因果Transformer的可扩展序列3D重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r."
    },
    {
      "title": "ToonComposer：通过生成式关键帧后处理简化卡通制作 (原标题: ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing)",
      "link": "https://arxiv.org/abs/2508.10881",
      "pubDate": "Thu, 14 Aug 2025 13:50:11 GMT",
      "isoDate": "2025-08-14T13:50:11.000Z",
      "creator": "Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan",
      "summary": "## ToonComposer：通过生成式关键帧后处理简化卡通制作\n\n### 挑战：传统卡通制作的痛点\n\n传统的卡通和动漫制作流程涉及关键帧绘制、中间帧生成和上色等多个阶段，这些阶段都需要大量的人工投入。尽管人工智能在近年来取得了显著进展，但现有方法通常独立处理这些阶段，导致错误累积和伪影产生。例如，中间帧生成方法难以处理大幅度运动，而上色方法则需要密集的逐帧草图。\n\n### 解决方案：ToonComposer 模型\n\n为了解决上述问题，我们引入了 ToonComposer，一个生成模型，它将中间帧生成和上色统一到一个单一的“关键帧后处理”阶段。ToonComposer 的核心创新点包括：\n\n*   **稀疏草图注入机制**：该机制允许通过关键帧草图提供精确控制。ToonComposer 能够以稀疏输入高效工作，最少只需一张草图和一个彩色参考帧。同时，它也支持在任何时间位置使用多张草图，以实现更精确的运动控制。\n*   **卡通领域适应方法**：模型采用空间低秩适配器（spatial low-rank adapter）来调整现代视频基础模型，使其适应卡通领域，同时保持其固有的时间先验。\n\n### 优势与应用\n\nToonComposer 的双重能力显著减少了手动工作量，并提高了制作的灵活性，从而赋能艺术家在实际场景中更高效地进行创作。它为AI辅助的卡通制作提供了一个卓越且更灵活的解决方案。\n\n### 评估与性能\n\n为了评估 ToonComposer 的性能，我们创建了 PKBench，这是一个包含人工绘制草图的基准测试，旨在模拟真实世界的使用场景。我们的评估结果表明，ToonComposer 在视觉质量、运动一致性和生产效率方面均优于现有方法。",
      "shortSummary": "ToonComposer 是一种生成模型，旨在简化卡通制作流程。它将传统的中间帧生成和上色阶段统一到单一的“关键帧后处理”阶段，从而减少人工投入。该模型通过稀疏草图注入提供精确控制，并利用空间低秩适配器将视频基础模型适应卡通领域。ToonComposer 显著提高了制作效率和灵活性，并在视觉质量和运动一致性方面超越了现有方法，为AI辅助卡通制作提供了更优解决方案。",
      "translated_title": "ToonComposer：通过生成式关键帧后处理简化卡通制作",
      "images": [],
      "contentSource": "完整文章",
      "content": "Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production."
    },
    {
      "title": "扩散语言模型综述 (原标题: A Survey on Diffusion Language Models)",
      "link": "https://arxiv.org/abs/2508.10875",
      "pubDate": "Thu, 14 Aug 2025 13:47:22 GMT",
      "isoDate": "2025-08-14T13:47:22.000Z",
      "creator": "Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen",
      "summary": "# 扩散语言模型综述\n\n## 引言\n\n扩散语言模型（Diffusion Language Models, DLMs）正迅速崛起，成为主导的自回归（Autoregressive, AR）范式的强大且有前景的替代方案。DLMs通过迭代去噪过程并行生成令牌，具有以下固有优势：\n\n*   **降低推理延迟**：显著提升生成速度。\n*   **捕获双向上下文**：能够理解和利用文本的双向信息。\n*   **实现精细控制**：对生成过程提供更细致的控制能力。\n\n近期进展表明，DLMs在实现数倍加速的同时，性能已可与自回归模型相媲美，使其成为各种自然语言处理任务的有力选择。\n\n## 综述范围与贡献\n\n本综述对当前DLM领域的整体概况进行了全面概述，具体贡献包括：\n\n*   **演变与关系追溯**：\n    *   追溯DLM的演变历程。\n    *   探讨其与自回归模型和掩码语言模型等其他范式的关系。\n*   **核心原理与模型**：\n    *   涵盖DLM的基础原理。\n    *   介绍最先进的DLM模型。\n*   **分类与技术分析**：\n    *   提供最新、全面的DLM分类法。\n    *   深入分析现有技术，包括从预训练策略到高级后训练方法。\n*   **推理策略与优化**：\n    *   详细回顾DLM的推理策略和优化方法。\n    *   具体包括解码并行性、缓存机制和生成质量的改进。\n*   **多模态扩展与应用**：\n    *   重点介绍DLM的最新多模态扩展。\n    *   阐述其在各种实际应用场景中的具体应用。\n*   **局限性、挑战与未来方向**：\n    *   讨论DLM面临的局限性和挑战，例如效率问题、长序列处理能力以及基础设施要求。\n    *   展望并勾勒出未来研究方向，以促进该快速发展领域的持续进步。\n\n## 其他信息\n\n*   **主题**：计算与语言（cs.CL）、人工智能（cs.AI）、机器学习（cs.LG）。\n*   **引用信息**：arXiv:2508.10875。\n*   **DOI**：10.48550/arXiv.2508.10875。",
      "shortSummary": "本综述全面介绍了扩散语言模型（DLMs），它们作为自回归模型的有力替代品，通过并行生成令牌，在降低推理延迟、捕获双向上下文和实现精细控制方面具有固有优势。文章涵盖了DLM的演变、核心原理、最先进模型、推理策略、多模态扩展及其应用。此外，综述还讨论了DLM的局限性、挑战，并指出了未来的研究方向。",
      "translated_title": "扩散语言模型综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
    },
    {
      "title": "从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估 (原标题: From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms)",
      "link": "https://arxiv.org/abs/2508.10860",
      "pubDate": "Thu, 14 Aug 2025 13:31:18 GMT",
      "isoDate": "2025-08-14T13:31:18.000Z",
      "creator": "Zhaokun Jiang, Ziyin Zhang",
      "summary": "## 从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估\n\n### 研究背景与问题\n\n*   **现有不足**：尽管机器学习在自动化口译质量评估方面取得了进展，但现有研究存在以下局限：\n    *   对语言使用质量的检查不足。\n    *   由于数据稀缺和不平衡，模型建模效果不尽如人意。\n    *   缺乏对模型预测的解释性。\n\n### 提出的解决方案\n\n*   **多维建模框架**：为解决上述问题，本研究提出一个多维建模框架，该框架整合了：\n    *   特征工程（Feature Engineering）。\n    *   数据增强（Data Augmentation）。\n    *   可解释机器学习（Explainable Machine Learning）。\n\n### 核心方法与原则\n\n*   **优先可解释性**：该方法优先考虑可解释性而非“黑箱”预测。\n*   **透明特征**：仅使用与构建相关的、透明的特征。\n*   **SHAP分析**：通过Shapley值（SHAP）分析来解释模型预测。\n\n### 研究成果与发现\n\n*   **预测性能**：在新的英汉同声传译数据集上展示了强大的预测性能。\n*   **关键预测特征识别**：\n    *   **忠实度（Fidelity）**：BLEURT和CometKiwi分数被确定为最强的预测特征。\n    *   **流利度（Fluency）**：停顿相关特征是关键预测因素。\n    *   **语言使用（Language Use）**：中文特有的短语多样性指标对语言使用质量具有重要预测作用。\n\n### 研究意义与贡献\n\n*   **替代方案**：通过特别强调可解释性，本研究提出了一种可扩展、可靠且透明的替代传统人工评估的方法。\n*   **诊断反馈**：有助于为学习者提供详细的诊断反馈。\n*   **自主学习**：支持自动化分数单独无法提供的自主学习优势。",
      "shortSummary": "针对现有自动化口译评估在语言质量、数据和可解释性方面的不足，本文提出一个多维建模框架。该框架整合特征工程、数据增强和可解释机器学习，通过Shapley值分析优先实现透明预测。研究在英汉口译数据集上表现出色，识别出忠实度、流利度和语言使用的关键预测特征。这提供了一种可扩展、可靠且透明的替代人工评估的方法，有助于提供诊断反馈并促进自主学习。",
      "translated_title": "从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation."
    },
    {
      "title": "UI-Venus 技术报告：使用 RFT 构建高性能 UI 代理 (原标题: UI-Venus Technical Report: Building High-performance UI Agents with RFT)",
      "link": "https://arxiv.org/abs/2508.10833",
      "pubDate": "Thu, 14 Aug 2025 12:58:07 GMT",
      "isoDate": "2025-08-14T12:58:07.000Z",
      "creator": "Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang",
      "summary": "UI-Venus 技术报告：使用 RFT 构建高性能 UI 代理\n\n本文介绍了 UI-Venus，一个基于多模态大型语言模型（Qwen2.5-VL）的原生 UI 代理，它仅以屏幕截图作为输入。\n\n### 核心技术与训练\n\n*   **输入方式**：仅接受屏幕截图作为输入。\n*   **基础模型**：基于 Qwen2.5-VL 构建的多模态大型语言模型。\n*   **训练方法**：通过强化微调（RFT），使用数十万高质量训练样本进行训练。\n\n### 性能表现\n\nUI-Venus 在 UI 接地（Grounding）和导航（Navigation）任务上均取得了最先进（SOTA）的性能。\n\n*   **UI 接地任务**：\n    *   在标准接地基准测试 Screenspot-V2 / Pro 上，UI-Venus 的 7B 和 72B 变体分别达到了 94.1% / 50.8% 和 95.3% / 61.9% 的准确率。\n    *   这些结果超越了包括开源的 GTA1 和闭源的现有 SOTA 基线。\n*   **UI 导航任务**：\n    *   在在线 UI 导航平台 AndroidWorld 上，UI-Venus 的 7B 和 72B 变体分别实现了 49.1% 和 65.9% 的成功率。\n    *   同样，这些结果也超越了现有模型。\n\n### 关键创新与贡献\n\n为实现高性能，UI-Venus 引入了多项创新：\n\n*   **奖励函数设计**：为 UI 接地和导航任务精心设计了奖励函数。\n*   **高效数据清洗**：采用了相应的高效数据清洗协议。\n*   **自演化轨迹历史对齐与稀疏动作增强（Self-Evolving Trajectory History Alignment & Sparse Action Enhancement）**：\n    *   该方法旨在提升导航性能，通过优化历史推理轨迹，并平衡稀疏但关键动作的分布，从而实现更连贯的规划和在复杂 UI 任务中更好的泛化能力。\n\n### 主要贡献总结\n\n*   发布了 SOTA 开源 UI 代理。\n*   提供了全面的数据清洗协议。\n*   提出了一个新颖的自演化框架，用于改进导航性能。\n\n这些贡献旨在鼓励社区进行进一步的研究和开发。",
      "shortSummary": "UI-Venus 是一种基于多模态大语言模型（Qwen2.5-VL）的原生 UI 代理，仅以屏幕截图为输入。通过强化微调（RFT）和数十万高质量样本训练，UI-Venus 在 UI 接地和导航任务上均取得了最先进（SOTA）的性能，超越了现有基线。其创新包括精心设计的奖励函数、高效数据清洗以及提升导航性能的自演化框架。UI-Venus 作为开源项目，旨在推动相关领域的研究与发展。",
      "translated_title": "UI-Venus 技术报告：使用 RFT 构建高性能 UI 代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\&amp; Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus."
    },
    {
      "title": "Pass@k 训练：自适应平衡大型推理模型的探索与利用 (原标题: Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models)",
      "link": "https://arxiv.org/abs/2508.10751",
      "pubDate": "Thu, 14 Aug 2025 11:34:47 GMT",
      "isoDate": "2025-08-14T11:34:47.000Z",
      "creator": "Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi",
      "summary": "## Pass@k 训练：自适应平衡大型推理模型的探索与利用\n\n### 引言\n\n*   可验证奖励强化学习（RLVR）通常采用 Pass@1 作为奖励指标，但在平衡模型的探索（exploration）和利用（exploitation）能力方面面临显著挑战。\n*   这种不平衡导致策略倾向于采取保守行动，并最终收敛到局部最优解。\n*   因此，识别和应用一个合适的奖励指标对于提升 RLVR 模型的性能至关重要。\n\n### 研究背景与问题\n\n*   尽管 Pass@k 在模型评估中已被广泛使用，但其与 RLVR 中大型语言模型（LLM）探索能力之间的深层关联在现有研究中很大程度上被忽视了。\n\n### 研究方法与核心发现\n\n1.  **Pass@k 训练的提出与验证**：\n    *   本研究首先将 Pass@k 用作策略模型的奖励函数，并将其命名为“Pass@k 训练”。\n    *   实验观察表明，采用 Pass@k 训练能够显著提升模型的探索能力。\n2.  **优势函数的分析推导**：\n    *   研究进一步推导了 Pass@k 训练优势的分析解，这为实现一个高效且有效的训练过程奠定了基础。\n3.  **探索与利用的相互增强**：\n    *   基于对 Pass@k 训练的深入分析，本研究揭示了一个关键洞察：探索与利用并非固有冲突的目标，相反，它们可以相互促进和增强，从而共同提升模型性能。\n4.  **优势函数设计的本质**：\n    *   研究指出，结合分析推导的 Pass@k 训练，本质上涉及直接设计强化学习中的优势函数。\n\n### 未来方向\n\n*   受上述发现的启发，本研究初步探索了在 RLVR 中直接设计优势函数的可能性，并展示了有前景的初步结果。\n*   这为未来在 RLVR 领域中更深入地研究和应用优势函数设计指明了一个潜在的研究方向。\n\n### 技术报告信息\n\n*   本研究是一份详细的技术报告，共32页，包含18张图和7个表格。\n*   研究主题涵盖：机器学习 (cs.LG)、人工智能 (cs.AI) 和计算与语言 (cs.CL)。\n*   引用信息：arXiv:2508.10751。",
      "shortSummary": "可验证奖励强化学习（RLVR）在平衡探索与利用方面存在挑战。本研究提出“Pass@k 训练”，将 Pass@k 作为奖励来提升大型推理模型的探索能力。通过推导 Pass@k 训练优势的分析解，研究发现探索与利用可相互增强，且该方法本质上是直接设计优势函数。初步探索RLVR中的优势函数设计显示出良好前景，为未来研究指明方向。",
      "translated_title": "Pass@k 训练：自适应平衡大型推理模型的探索与利用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., Pass@k Training), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction."
    },
    {
      "title": "NextStep-1：迈向大规模连续令牌自回归图像生成 (原标题: NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale)",
      "link": "https://arxiv.org/abs/2508.10711",
      "pubDate": "Thu, 14 Aug 2025 10:54:22 GMT",
      "isoDate": "2025-08-14T10:54:22.000Z",
      "creator": "NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu",
      "summary": "### NextStep-1：大规模连续令牌自回归图像生成\n\n**引言与背景**\n\n当前主流的文本到图像自回归（AR）模型面临两大挑战：\n\n*   **计算密集型扩散模型**：部分模型依赖于计算成本高昂的扩散模型来处理连续图像令牌。\n*   **量化损失**：另一些模型则采用向量量化（VQ）来获取离散令牌，但这通常会导致量化损失，影响图像质量。\n\n**NextStep-1 的创新方法**\n\n本文提出了 NextStep-1，旨在推动自回归范式向前发展。NextStep-1 的核心创新点在于：\n\n*   **模型架构**：它是一个包含 140 亿参数的自回归模型，并搭配了一个 1.57 亿参数的流匹配头部。\n*   **训练策略**：模型在训练过程中同时使用离散文本令牌和**连续图像令牌**，并采用下一令牌预测目标。\n\n**性能与能力**\n\nNextStep-1 在文本到图像生成任务中展现出卓越的性能：\n\n*   **领先表现**：它在自回归模型中取得了最先进（state-of-the-art）的性能。\n*   **高保真合成**：模型具备强大的高保真图像合成能力，能够生成高质量的图像。\n*   **图像编辑**：NextStep-1 在图像编辑方面也表现出色，突显了其统一方法的强大和多功能性。\n\n**开放研究与未来展望**\n\n为了促进开放研究，NextStep 团队计划向社区发布其代码和模型。\n\n**作者团队**\n\nNextStep-1 由 NextStep 团队的众多研究人员共同完成，包括：Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu。",
      "shortSummary": "NextStep-1是一个140亿参数的自回归模型，结合1.57亿参数的流匹配头部，旨在解决现有文本到图像自回归模型在处理连续或离散令牌时的局限。它通过训练离散文本和连续图像令牌，在文本到图像生成任务中实现了自回归模型的最新性能，展现出高保真图像合成和图像编辑的强大能力。该团队计划开源代码和模型以促进研究。",
      "translated_title": "NextStep-1：迈向大规模连续令牌自回归图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community."
    },
    {
      "title": "视觉编码器中的处理和采集痕迹：CLIP对你的相机了解多少？ (原标题: Processing and acquisition traces in visual encoders: What does CLIP know about your camera?)",
      "link": "https://arxiv.org/abs/2508.10637",
      "pubDate": "Thu, 14 Aug 2025 09:34:13 GMT",
      "isoDate": "2025-08-14T09:34:13.000Z",
      "creator": "Ryan Ramos, Vladan Stojnić, Giorgos Kordopatis-Zilos, Yuta Nakashima, Giorgos Tolias, Noa Garcia",
      "summary": "### 视觉编码器中的处理和采集痕迹：CLIP对你的相机了解多少？\n\n本文探讨了视觉编码器在图像处理和采集过程中所产生的痕迹，并分析了这些痕迹对模型性能的影响。\n\n#### 研究背景与传统视角\n\n*   **鲁棒性分析：** 先前的研究主要关注视觉编码器对图像变换和损坏的鲁棒性，特别是当这些改变在训练期间未曾出现时。\n*   **分布偏移：** 未见的改变会在测试时引入分布偏移，通常导致模型性能下降。\n*   **严重损坏：** 大多数研究集中于严重的图像损坏，这些损坏会严重扭曲图像中的有用信号，从而影响准确的语义预测。\n\n#### 本文的独特视角\n\n*   **关注细微参数：** 本文采取了不同的视角，分析了图像采集过程中的参数以及可能对人眼来说非常细微甚至不可察觉的变换。\n\n#### 主要发现\n\n*   **参数编码：** 研究发现，这些细微的采集和处理参数系统地编码在视觉编码器学习到的视觉表示中。\n*   **易于恢复：** 这些编码的参数可以被轻易地恢复出来。\n*   **对语义预测的影响：** 更引人注目的是，这些参数的存在对语义预测具有深远的影响，这种影响可能是积极的，也可能是消极的。\n*   **相关性依赖：** 这种影响的性质（积极或消极）取决于语义标签与这些基于采集或处理的标签之间是否存在强烈的正相关或负相关。\n\n#### 附加信息\n\n*   **出版信息：** 本文为ICCV 2025亮点，包含8页正文和补充材料。\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)。",
      "shortSummary": "本文研究视觉编码器如何编码图像采集和处理过程中细微的、人眼不可察觉的参数。研究发现，这些参数系统地存在于学习到的视觉表示中，易于恢复，并能显著影响语义预测。这种影响取决于参数与语义标签的相关性，可能带来积极或消极后果，为理解模型鲁棒性提供了新视角。",
      "translated_title": "视觉编码器中的处理和采集痕迹：CLIP对你的相机了解多少？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions.   We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces"
    },
    {
      "title": "HumanSense：从多模态感知到通过推理MLLM实现共情上下文感知响应 (原标题: HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs)",
      "link": "https://arxiv.org/abs/2508.10576",
      "pubDate": "Thu, 14 Aug 2025 08:14:15 GMT",
      "isoDate": "2025-08-14T08:14:15.000Z",
      "creator": "Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang",
      "summary": "## HumanSense：通过推理MLLM实现共情上下文感知响应\n\n### 引言与背景\n\n多模态大型语言模型（MLLMs）在实现真正类人交互方面展现出巨大潜力。然而，当前进展受限于缺乏针对以人为中心的场景的细粒度评估框架。这些场景不仅要求MLLMs能够理解复杂的人类意图，还需要它们提供共情且上下文感知的响应。\n\n### HumanSense基准介绍\n\n本文引入了 **HumanSense**，这是一个综合性基准，旨在评估MLLMs以人为中心的感知和交互能力。该基准特别关注对扩展多模态上下文的深度理解以及理性反馈的形成。\n\n### 评估结果与发现\n\n*   **现有MLLMs的局限性**：评估结果表明，领先的MLLMs在高级交互导向任务上仍有相当大的改进空间。\n*   **多模态融合的优势**：通过补充音频和文本信息来增强视觉输入，可以带来实质性的性能提升。\n*   **全模态模型的表现**：全模态（Omni-modal）模型在这些任务上表现出优势。\n\n### 推理能力的重要性\n\n研究强调，恰当的反馈源于对对话者需求和情感的上下文分析。推理能力被认为是解锁这种分析的关键。\n\n### 方法与改进\n\n*   **强化推理能力**：为增强全模态模型的推理能力，研究采用了多阶段、模态渐进式的强化学习方法，并在评估结果上取得了显著的提升。\n*   **思维模式的一致性**：研究观察到，成功的推理过程展现出高度一致的思维模式。\n*   **训练无关的性能提升**：通过设计相应的提示，研究还以无需训练的方式提升了非推理模型的性能。\n\n### 结论\n\nHumanSense基准揭示了当前MLLMs在人类中心交互方面的不足，并强调了多模态信息融合和推理能力对于实现更高级、更共情的MLLM交互至关重要。",
      "shortSummary": "HumanSense是一个新的综合基准，旨在评估多模态大型语言模型（MLLMs）在以人为中心的场景中，理解人类意图并提供共情、上下文感知响应的能力。评估显示，当前MLLMs仍需改进，尤其在高级交互任务上。研究发现，结合视觉、音频和文本信息可显著提升性能，推理能力是实现恰当反馈的关键。通过强化学习和精心设计的提示，可有效增强模型的推理和交互表现。",
      "translated_title": "HumanSense：从多模态感知到通过推理MLLM实现共情上下文感知响应",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: brightpinkhttps://digital-avatar.github.io/ai/HumanSense/"
    },
    {
      "title": "当可解释性遇上隐私：在自然语言处理背景下对后验可解释性与差分隐私交叉点的研究 (原标题: When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural ...)",
      "link": "https://arxiv.org/abs/2508.10482",
      "pubDate": "Thu, 14 Aug 2025 05:34:29 GMT",
      "isoDate": "2025-08-14T05:34:29.000Z",
      "creator": "Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci",
      "summary": "## 可解释性与隐私的交汇：自然语言处理中的实证研究\n\n### 引言\n\n在可信赖自然语言处理（NLP）的研究中，可解释性与隐私是两个重要的研究领域。尽管近年来对可解释性NLP和隐私保护NLP的研究兴趣显著增加，但两者交叉领域的研究仍显不足。这导致了一个重要的认知空白：可解释性与隐私是否能够同时实现，或者两者是否相互矛盾。\n\n### 研究目标与方法\n\n本研究旨在对NLP背景下的隐私-可解释性权衡进行实证调查。研究以流行的“差分隐私”（DP）和“后验可解释性”方法为指导。\n\n### 主要发现\n\n*   揭示了隐私与可解释性之间错综复杂的关系。\n*   这种关系受多种因素影响，包括：\n    *   下游任务的性质。\n    *   文本隐私化方法的选择。\n    *   可解释性方法的选择。\n*   研究强调了隐私与可解释性共存的潜力。\n\n### 实践建议\n\n研究总结了针对这一重要交叉领域未来工作的实用建议。\n\n### 其他信息\n\n*   该论文已被AAAI/ACM人工智能、伦理与社会会议（AIES 2025）接受。\n*   主题：计算与语言 (cs.CL)。\n*   arXiv预印本链接：arXiv:2508.10482 [cs.CL]。",
      "shortSummary": "本研究探讨了自然语言处理（NLP）中可解释性与隐私的交叉点，特别是在差分隐私（DP）和后验可解释性背景下。文章旨在解决两者能否共存或相互冲突的问题。研究发现，隐私与可解释性之间的关系复杂，受下游任务、文本隐私化和可解释性方法选择等因素影响。结果表明，两者具有共存潜力，并提供了未来研究的实用建议。",
      "translated_title": "当可解释性遇上隐私：在自然语言处理背景下对后验可解释性与差分隐私交叉点的研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection."
    },
    {
      "title": "We-Math 2.0: 一个多功能数学手册系统，用于激励视觉数学推理 (原标题: We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning)",
      "link": "https://arxiv.org/abs/2508.10433",
      "pubDate": "Thu, 14 Aug 2025 04:15:41 GMT",
      "isoDate": "2025-08-14T04:15:41.000Z",
      "creator": "Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, Jie Wang, Chong Sun, Chen Li, Honggang Zhang",
      "summary": "## We-Math 2.0: 增强多模态大语言模型的数学推理能力\n\n### 引言\n\n多模态大语言模型（MLLMs）在各种任务中展现出令人印象深刻的能力，但在处理复杂的数学推理时仍面临挑战。现有研究主要集中在数据集构建和方法优化上，往往忽视了两个关键方面：全面的知识驱动设计和以模型为中心的数据空间建模。\n\n### We-Math 2.0 系统概述\n\n本文介绍了 We-Math 2.0，一个统一的系统，它整合了结构化的数学知识系统、以模型为中心的数据空间建模以及基于强化学习（RL）的训练范式，旨在全面提升 MLLMs 的数学推理能力。We-Math 2.0 的核心贡献体现在以下四个方面：\n\n1.  **MathBook 知识系统**\n    *   构建了一个五级分层的知识系统。\n    *   包含 491 个知识点和 1,819 条基本原理。\n\n2.  **MathBook-Standard 与 MathBook-Pro 数据集**\n    *   **MathBook-Standard**：通过双重扩展确保了广泛的概念覆盖和灵活性。\n    *   **MathBook-Pro**：定义了一个三维难度空间，并为每个问题生成了 7 个渐进变体，构建了一个具有挑战性的数据集，用于鲁棒训练。\n\n3.  **MathBook-RL 强化学习框架**\n    *   提出了一个两阶段的强化学习框架：\n        *   **(i) 冷启动微调 (Cold-Start Fine-tuning)**：使模型与面向知识的思维链推理对齐。\n        *   **(ii) 渐进对齐强化学习 (Progressive Alignment RL)**：利用平均奖励学习和动态数据调度，实现跨难度级别的渐进对齐。\n\n4.  **MathBookEval 评估基准**\n    *   引入了一个全面的评估基准，覆盖了所有 491 个知识点。\n    *   包含多样化的推理步骤分布。\n\n### 实验结果\n\n实验结果表明，MathBook-RL 在四个广泛使用的基准测试中与现有基线模型表现出竞争力，并在 MathBookEval 上取得了优异成绩，这表明其在数学推理方面具有良好的泛化能力。",
      "shortSummary": "We-Math 2.0 是一个旨在提升多模态大语言模型（MLLMs）数学推理能力的统一系统。它通过构建一个五级分层的 MathBook 知识系统、开发 MathBook-Standard 和 MathBook-Pro 数据集、提出两阶段的 MathBook-RL 强化学习框架，以及引入全面的 MathBookEval 评估基准来实现。实验结果显示，We-Math 2.0 在多个基准测试中表现出色，展现了强大的数学推理泛化能力。",
      "translated_title": "We-Math 2.0: 一个多功能数学手册系统，用于激励视觉数学推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard &amp; Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning."
    },
    {
      "title": "Echo-4o：利用GPT-4o合成图像的力量改进图像生成 (原标题: Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation)",
      "link": "https://arxiv.org/abs/2508.09987",
      "pubDate": "Wed, 13 Aug 2025 13:59:28 GMT",
      "isoDate": "2025-08-13T13:59:28.000Z",
      "creator": "Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li",
      "summary": "### Echo-4o：利用GPT-4o合成图像的力量改进图像生成\n\n#### 背景与挑战\n\n近期，GPT-4o在图像生成方面展现出强大性能，但开源模型仍显滞后。尽管已有研究探索从GPT-4o中提取图像数据以增强开源模型并取得显著进展，但一个核心问题依然存在：既然真实世界图像数据集已是高质量数据的天然来源，为何还要使用GPT-4o生成的合成数据？\n\n#### 合成图像的优势\n\n本研究识别出合成图像的两个关键优势：\n\n1.  **补充稀有场景**：合成图像能够补充真实世界数据集中罕见的场景，例如用户查询中频繁出现的超现实幻想或多参考图像生成。\n2.  **提供干净且可控的监督信号**：真实世界数据常包含复杂的背景噪声以及文本描述与图像内容之间固有的不一致性。相比之下，合成图像提供纯净的背景和长尾监督信号，有助于实现更精确的文本到图像对齐。\n\n#### Echo-4o-Image数据集与Echo-4o模型\n\n基于这些洞察，研究团队引入了 **Echo-4o-Image**，这是一个由GPT-4o生成的18万规模的合成数据集。该数据集旨在利用合成图像数据的力量，弥补真实世界数据覆盖的盲点。\n\n利用此数据集，研究人员对统一多模态生成基线模型 Bagel 进行了微调，从而获得了 **Echo-4o** 模型。\n\n#### 新型评估基准\n\n为了更准确、更具挑战性地评估图像生成能力，研究团队提出了两个新的评估基准：\n\n1.  **GenEval++**：通过增加指令复杂性来缓解分数饱和问题。\n2.  **Imagine-Bench**：专注于评估想象内容的理解和生成能力。\n\n#### 实验结果与贡献\n\n*   **Echo-4o** 在标准基准测试中表现出强大的性能。\n*   将 **Echo-4o-Image** 应用于其他基础模型（例如 OmniGen2、BLIP3-o）时，在多个指标上均能持续获得性能提升，这凸显了该数据集强大的可迁移性。",
      "shortSummary": "为弥补开源图像生成模型与GPT-4o的差距，本研究强调合成图像在补充稀有场景和提供干净监督方面的优势。研究团队构建了18万规模的GPT-4o合成数据集Echo-4o-Image，并基于此微调得到Echo-4o模型。同时，提出了GenEval++和Imagine-Bench两个新评估基准。实验结果显示，Echo-4o在标准测试中表现出色，且Echo-4o-Image数据集能有效提升其他基础模型的性能，展现了其强大的可迁移性。",
      "translated_title": "Echo-4o：利用GPT-4o合成图像的力量改进图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability."
    },
    {
      "title": "Story2Board：一种无需训练的富有表现力的故事板生成方法 (原标题: Story2Board: A Training-Free Approach for Expressive Storyboard Generation)",
      "link": "https://arxiv.org/abs/2508.09983",
      "pubDate": "Wed, 13 Aug 2025 13:56:26 GMT",
      "isoDate": "2025-08-13T13:56:26.000Z",
      "creator": "David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, Dani Lischinski",
      "summary": "# Story2Board：一种无需训练的富有表现力的故事板生成方法\n\n## 概述\nStory2Board 是一种无需训练的框架，旨在从自然语言中生成富有表现力的故事板。该方法解决了现有故事板生成方法在视觉叙事方面存在的局限性，即过度关注主体身份而忽略了空间构图、背景演变和叙事节奏等关键要素。\n\n## 现有方法的局限性\n当前的故事板生成方法主要集中于保持主体身份的一致性，但未能充分考虑视觉叙事中的其他重要方面，例如：\n*   **空间构图（Spatial Composition）**：场景中元素和角色的布局。\n*   **背景演变（Background Evolution）**：背景在不同面板之间如何自然过渡和变化。\n*   **叙事节奏（Narrative Pacing）**：通过视觉呈现来控制故事的进展和情感张力。\n\n这些缺失导致生成的故事板在视觉多样性和叙事连贯性方面表现不足。\n\n## Story2Board 框架\nStory2Board 引入了一个轻量级的连贯性框架，包含两个核心组件，旨在增强故事板的视觉连贯性和表现力，而无需进行架构更改或模型微调：\n\n1.  **潜在面板锚定（Latent Panel Anchoring）**\n    *   **目的**：在不同故事板面板之间保持共享的角色参考。这意味着即使角色姿态或场景发生变化，其身份和核心特征也能保持一致。\n    *   **机制**：通过在潜在空间中锚定关键特征，确保角色在整个叙事中的视觉连续性。\n\n2.  **互惠注意力值混合（Reciprocal Attention Value Mixing）**\n    *   **目的**：柔和地混合具有强互惠注意力的令牌对之间的视觉特征。这有助于在不同元素之间建立更强的视觉关联和过渡。\n    *   **机制**：通过分析和混合注意力机制中的高相关性区域，促进视觉特征的平滑融合，从而增强场景的整体连贯性。\n\n这两个机制协同作用，使得最先进的扩散模型能够生成视觉多样化且高度一致的故事板。\n\n## 生成结构\n为了有效地构建故事板生成过程，Story2Board 利用一个现成的语言模型。该语言模型负责将自由形式的叙事文本转换为具体到每个面板的提示（grounded panel-level prompts）。这种方法确保了生成内容与原始故事叙事紧密对齐，并为扩散模型提供了清晰的指导。\n\n## 评估方法与结果\n为了全面评估 Story2Board 的性能，研究团队提出了新的评估工具和指标：\n\n1.  **丰富故事板基准（Rich Storyboard Benchmark）**\n    *   **目的**：这是一个包含开放域叙事的套件，专门用于评估故事板的布局多样性、背景接地叙事能力以及整体连贯性。\n    *   **特点**：超越了单一的主体一致性评估，更全面地考量视觉叙事质量。\n\n2.  **场景多样性指标（Scene Diversity metric）**\n    *   **目的**：量化故事板中空间和姿态的变化。\n    *   **作用**：确保生成的故事板不仅连贯，而且在视觉上具有足够的动态性和变化性，避免重复和僵化。\n\n通过定性分析、定量结果以及用户研究，Story2Board 的表现优于现有基线方法。研究表明，Story2Board 能够生成更具动态性、更连贯且叙事更引人入胜的故事板。\n\n## 结论\nStory2Board 提供了一种无需训练的创新方法，通过引入潜在面板锚定和互惠注意力值混合等机制，有效解决了现有故事板生成方法在视觉叙事方面的局限性。它不仅提升了故事板的连贯性，还增强了其视觉多样性和叙事吸引力，为未来的故事板自动化生成奠定了坚实基础。",
      "shortSummary": "Story2Board 是一种无需训练的框架，用于生成富有表现力的故事板。它解决了现有方法在空间构图、背景演变和叙事节奏方面的不足。通过引入“潜在面板锚定”保持角色一致性，以及“互惠注意力值混合”增强视觉连贯性，Story2Board 使扩散模型能生成多样且一致的故事板。评估显示，该方法比现有基线能生成更具动态性、连贯性和叙事吸引力的故事板。",
      "translated_title": "Story2Board：一种无需训练的富有表现力的故事板生成方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines."
    },
    {
      "title": "噪声超网络：分摊扩散模型中的测试时计算 (原标题: Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models)",
      "link": "https://arxiv.org/abs/2508.09968",
      "pubDate": "Wed, 13 Aug 2025 13:33:37 GMT",
      "isoDate": "2025-08-13T13:33:37.000Z",
      "creator": "Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata",
      "summary": "### 噪声超网络：分摊扩散模型中的测试时计算\n\n**背景与问题**\n\n*   **测试时扩展范式**：在大型语言模型（LLMs）和生成式视觉模型中取得了显著进展，允许模型在推理时分配额外计算以解决日益复杂的问题。\n*   **主要限制**：尽管有这些改进，但计算时间的大幅增加使得该过程变得缓慢，对于许多应用而言不切实际。\n\n**提出的解决方案：噪声超网络**\n\n*   **目标**：在保留测试时扩展优势的同时，避免其带来的推理开销。\n*   **核心方法**：引入“噪声超网络”（Noise Hypernetwork）来取代扩散模型中奖励引导的测试时噪声优化。\n*   **工作原理**：噪声超网络通过调制初始输入噪声来发挥作用。\n\n**理论框架与实现**\n\n*   **理论基础**：提出一个理论上扎实的框架，用于学习蒸馏生成器的“奖励倾斜分布”（reward-tilted distribution）。\n*   **优化目标**：通过一个可处理的噪声空间目标来实现，该目标在优化所需特性的同时，保持了对基础模型的忠实度。\n\n**成果与优势**\n\n*   **效率提升**：该方法能够以一小部分计算成本，恢复显式测试时优化所获得的绝大部分质量提升。\n*   **实际意义**：在不牺牲性能的情况下，显著提高了扩散模型的推理效率和实用性。",
      "shortSummary": "测试时扩展在LLMs和生成模型中提高了性能，但计算成本高昂。本文提出“噪声超网络”作为解决方案，通过调制初始输入噪声来替代扩散模型中的奖励引导优化。该方法提供了一个理论框架，能在保持模型保真度的同时，以极低的计算成本恢复大部分测试时优化的质量增益，从而显著提高推理效率。",
      "translated_title": "噪声超网络：分摊扩散模型中的测试时计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise"
    },
    {
      "title": "VisCodex：通过融合视觉和编码模型实现统一的多模态代码生成 (原标题: VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models)",
      "link": "https://arxiv.org/abs/2508.09945",
      "pubDate": "Wed, 13 Aug 2025 13:00:44 GMT",
      "isoDate": "2025-08-13T13:00:44.000Z",
      "creator": "Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei",
      "summary": "## VisCodex：统一的多模态代码生成框架\n\n### 摘要\n\n多模态大型语言模型（MLLMs）在整合视觉和文本理解方面取得了显著进展，但其从多模态输入生成代码的能力仍然有限。本文介绍了 **VisCodex**，一个统一的框架，旨在通过无缝融合视觉和编码语言模型，增强 MLLMs 的多模态代码生成能力。\n\n### 核心贡献与方法\n\n*   **VisCodex 框架**：\n    *   **模型融合**：利用基于任务向量的模型融合技术，将最先进的编码大型语言模型（LLM）集成到强大的视觉-语言骨干网络中。\n    *   **能力保留**：在融合过程中，VisCodex 能够同时保留模型的视觉理解能力和高级编码技能。\n\n*   **多模态编码数据集（MCD）**：\n    *   **规模与多样性**：引入了一个大规模且多样化的数据集，包含 59.8 万个样本。\n    *   **数据类型**：包括高质量的 HTML 代码、图表图像-代码对、图像增强的 StackOverflow 问答以及算法问题。\n    *   **用途**：该数据集旨在支持模型的训练和评估。\n\n*   **InfiBench-V 基准**：\n    *   **新颖性与挑战性**：提出了一个新颖且具有挑战性的基准测试。\n    *   **评估目标**：专门设计用于评估模型在视觉丰富、真实世界编程问题上的表现。\n    *   **理解要求**：这些问题需要模型对文本和视觉上下文有细致入微的理解。\n\n### 实验结果\n\n广泛的实验表明，VisCodex 在开源 MLLMs 中取得了最先进的性能，并接近 GPT-4o 等专有模型的水平。这突出显示了其模型融合策略和新数据集的有效性。\n\n### 相关领域\n\n*   计算与语言（cs.CL）\n*   人工智能（cs.AI）\n*   计算机视觉与模式识别（cs.CV）",
      "shortSummary": "VisCodex 是一个统一框架，通过融合视觉和编码模型，显著提升了多模态大型语言模型（MLLMs）的代码生成能力。它利用基于任务向量的模型融合技术，并引入了包含59.8万样本的多模态编码数据集（MCD）和用于评估视觉丰富编程问题的 InfiBench-V 基准。实验证明，VisCodex 在开源 MLLMs 中表现最佳，并接近 GPT-4o 等专有模型，验证了其方法和数据集的有效性。",
      "translated_title": "VisCodex：通过融合视觉和编码模型实现统一的多模态代码生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets."
    },
    {
      "title": "AWorld：具有稳定操控能力的动态多智能体系统，用于鲁棒的GAIA问题解决 (原标题: AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving)",
      "link": "https://arxiv.org/abs/2508.09889",
      "pubDate": "Wed, 13 Aug 2025 11:46:25 GMT",
      "isoDate": "2025-08-13T11:46:25.000Z",
      "creator": "Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu",
      "summary": "## AWorld：具有稳定操控能力的动态多智能体系统\n\n### 引言\n\n随着大型语言模型（LLMs）的快速发展，智能体已能利用各种外部工具解决复杂的现实世界问题。然而，智能体对多工具的日益依赖带来了新的挑战：来自不同来源的扩展上下文以及嘈杂或不相关的工具输出，这些都可能损害系统的可靠性和准确性。这些挑战凸显了增强基于智能体系统稳定性的必要性。\n\n### 解决方案：动态监督与操控机制\n\n为了解决上述问题，研究人员在AWorld框架内引入了动态监督和操控机制，构建了一个鲁棒且动态的多智能体系统（MAS）架构。其核心方法如下：\n\n*   **执行智能体与守卫智能体协作**：在AWorld的方法中，执行智能体（Execution Agent）会在关键步骤调用守卫智能体（Guard Agent）。\n*   **验证与纠正**：守卫智能体的作用是验证并纠正推理过程，从而有效减少因噪声引起的错误，并显著增强问题解决的鲁棒性。\n\n### 实验结果与成就\n\n*   **性能提升**：在GAIA测试数据集上进行的广泛实验表明，AWorld的动态操控机制显著提高了解决方案的有效性和稳定性。\n*   **超越现有系统**：该系统表现优于单智能体系统（SAS）和标准工具增强系统。\n*   **GAIA排行榜首位**：AWorld的动态MAS系统在著名的GAIA排行榜上，于开源项目中取得了第一名的成绩。\n\n### 结论\n\n这些研究发现强调了协作智能体角色在开发更可靠、更值得信赖的智能系统方面的实际价值。",
      "shortSummary": "AWorld引入了一个动态多智能体系统（MAS），通过执行智能体在关键步骤调用守卫智能体进行验证和纠正，以解决大型语言模型（LLMs）在多工具使用中面临的上下文冗长和噪声输出问题。该系统显著提高了问题解决的鲁棒性、有效性和稳定性，在GAIA数据集上表现优异，并荣登GAIA开源项目排行榜首位，证明了协作智能体在构建可靠智能系统中的价值。",
      "translated_title": "AWorld：具有稳定操控能力的动态多智能体系统，用于鲁棒的GAIA问题解决",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems."
    },
    {
      "title": "PRELUDE：一个旨在要求对长上下文进行全局理解和推理的基准 (原标题: PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts)",
      "link": "https://arxiv.org/abs/2508.09848",
      "pubDate": "Wed, 13 Aug 2025 10:28:25 GMT",
      "isoDate": "2025-08-13T10:28:25.000Z",
      "creator": "Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou",
      "summary": "## PRELUDE：长上下文理解与推理基准\n\n### 引言\n\n本文介绍了 **PRELUDE**，这是一个用于评估长上下文理解能力的基准测试。该基准通过一项独特的任务来衡量这种能力：判断一个角色的前传故事是否与原著的规范叙事保持一致。\n\n### 任务特点与挑战\n\n*   **高要求**：PRELUDE任务对全局理解和深度推理提出了比现有基准更强的要求。\n*   **间接信息整合**：由于前传故事并非原著的一部分，评估其合理性通常需要搜索并整合那些与原著内容间接相关的信息。\n*   **多源证据需求**：实证研究表明，88%的实例需要从叙事的多个部分获取证据，这进一步强调了任务的复杂性。\n\n### 实验结果\n\n实验结果突显了PRELUDE任务的挑战性：\n\n*   **模型表现滞后**：\n    *   最先进的大型语言模型（LLMs）在上下文学习（in-context learning）、检索增强生成（RAG）和域内训练（in-domain training）方面的表现，以及商业DeepResearch服务，都比人类落后超过15%。\n*   **推理缺陷**：\n    *   进一步的人类研究揭示，模型经常在推理有缺陷的情况下得出正确答案。\n    *   这导致模型在推理准确性方面与人类存在超过30%的显著差距。\n\n### 结论\n\n这些发现强调了在长上下文理解和推理方面存在巨大的改进空间，表明当前的人工智能模型仍需在这些复杂认知能力上取得显著进步。",
      "shortSummary": "PRELUDE是一个评估长上下文理解和深度推理的新基准。它要求判断角色前传故事与原著叙事的一致性，这通常需要整合间接信息。实验显示，最先进的LLMs和商业服务在准确性上比人类落后超过15%，在推理准确性上差距更是超过30%，表明长上下文理解和推理能力仍有巨大提升空间。",
      "translated_title": "PRELUDE：一个旨在要求对长上下文进行全局理解和推理的基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by &gt;15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning."
    },
    {
      "title": "LLM生成的文本解释能否提升模型分类性能？一项实证研究 (原标题: Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study)",
      "link": "https://arxiv.org/abs/2508.09776",
      "pubDate": "Wed, 13 Aug 2025 08:59:08 GMT",
      "isoDate": "2025-08-13T08:59:08.000Z",
      "creator": "Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci",
      "summary": "## LLM生成的文本解释能否提升模型分类性能？一项实证研究\n\n### 核心问题与背景\n\n*   在快速发展的可解释自然语言处理（Explainable NLP）领域，文本解释（即类人理性解释）对于理解模型预测和丰富数据集具有关键作用。\n*   然而，传统的解释生成方法依赖于人工标注，这导致成本高昂、劳动密集且难以实现规模化。\n\n### 研究目标与方法\n\n*   **目标：** 本研究旨在提出一个自动化框架，利用多个最先进的大型语言模型（LLMs）来生成高质量的文本解释，以克服人工标注的局限性。\n*   **解释质量评估：** 研究团队使用一套全面的自然语言生成（NLG）指标，对LLM生成的解释质量进行了严格评估。\n*   **下游影响研究：** 进一步，研究调查了这些LLM生成的解释对预训练语言模型（PLMs）和LLMs在自然语言推理任务上的下游性能影响。\n*   **实验数据：** 实验在两个多样化的基准数据集上进行。\n\n### 主要发现\n\n*   实验结果表明，与人工标注的解释相比，自动化生成的解释在提升模型性能方面表现出高度的竞争力。\n\n### 结论与意义\n\n*   本研究的发现强调了利用LLM进行可扩展、自动化文本解释生成的一个有前景的方向。\n*   这为扩展NLP数据集和进一步增强模型性能提供了一条新的途径。\n\n### 其他信息\n\n*   该研究已被第34届国际人工神经网络会议（ICANN 2025）接受。\n*   研究主题包括计算与语言（cs.CL）和人工智能（cs.AI）。",
      "shortSummary": "该研究提出一个自动化框架，利用大型语言模型（LLMs）生成文本解释，以解决传统人工标注成本高、难扩展的问题。通过评估LLM生成解释的质量，并测试其对预训练语言模型（PLMs）和LLMs在自然语言推理任务上的性能影响，研究发现自动化解释在提升模型性能方面与人工标注解释具有高度竞争力。这为可扩展地利用LLM扩展NLP数据集和增强模型性能提供了新途径。",
      "translated_title": "LLM生成的文本解释能否提升模型分类性能？一项实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance."
    },
    {
      "title": "看、听、记忆与推理：一种具备长期记忆的多模态智能体 (原标题: Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory)",
      "link": "https://arxiv.org/abs/2508.09736",
      "pubDate": "Wed, 13 Aug 2025 08:03:03 GMT",
      "isoDate": "2025-08-13T08:03:03.000Z",
      "creator": "Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li",
      "summary": "# M3-Agent：一种具备长期记忆的多模态智能体\n\n本文介绍了 **M3-Agent**，一个新颖的多模态智能体框架，其核心特点是配备了长期记忆能力。M3-Agent 旨在模拟人类的认知过程，能够处理实时的视觉和听觉输入，并以此构建和更新其长期记忆。\n\n## M3-Agent 的核心特性\n\n*   **多模态输入处理**：M3-Agent 能够像人类一样处理实时的视觉和听觉输入，从而感知和理解复杂的环境。\n*   **长期记忆构建与更新**：\n    *   它不仅能建立情景记忆（episodic memory），记录特定事件和经历，还能发展语义记忆（semantic memory），从而随着时间积累世界知识和通用概念。\n    *   其记忆以实体为中心、多模态的形式组织，有助于对环境形成更深入、更一致的理解，并支持跨模态信息的整合。\n*   **自主推理能力**：\n    *   M3-Agent 能够根据给定的指令，自主执行多轮、迭代推理。\n    *   它能高效地从其长期记忆中检索相关信息，以支持决策并完成复杂的任务。\n\n## M3-Bench：评估多模态智能体的基准\n\n为了全面评估多模态智能体中记忆的有效性和基于记忆的推理能力，研究人员开发了 **M3-Bench**，这是一个新的长视频问答基准。\n\n*   **组成部分**：\n    *   **M3-Bench-robot**：包含 100 个从机器人视角拍摄的全新真实世界视频，模拟机器人与环境的交互。\n    *   **M3-Bench-web**：包含 929 个来自网络、涵盖多种场景的视频，提供多样化的日常情境。\n*   **问答对设计**：M3-Bench 中的问答对经过精心设计，旨在测试智能体应用所需的关键能力，包括：\n    *   **人类理解**：评估智能体对视频中人类行为、意图和情感的理解能力。\n    *   **通用知识提取**：测试智能体从视频内容中提取并应用通用世界知识的能力。\n    *   **跨模态推理**：考察智能体整合视觉和听觉信息进行复杂推理的能力。\n\n## 实验结果与性能\n\n通过强化学习训练的 M3-Agent 在实验中表现出色，超越了现有最强的基线模型（包括使用 Gemini-1.5-pro 和 GPT-4o 的提示智能体）。\n\n*   **准确率提升**：\n    *   在 M3-Bench-robot 上，M3-Agent 的准确率提高了 6.7%。\n    *   在 M3-Bench-web 上，准确率提高了 7.7%。\n    *   在 VideoMME-long（另一个评估基准）上，准确率提高了 5.3%。\n\n## 贡献与展望\n\n这项工作显著推动了多模态智能体向更类人的长期记忆能力发展，并为其在实际应用中的设计提供了深刻见解。研究团队已公开模型、代码和数据，以促进后续研究和开发。",
      "shortSummary": "M3-Agent是一种新型多模态智能体框架，具备类人长期记忆能力。它能处理实时视听输入，构建情景与语义记忆，并进行多轮推理以完成任务。为评估其记忆与推理能力，研究者开发了M3-Bench长视频问答基准。实验结果显示，M3-Agent在M3-Bench及VideoMME-long等基准测试中显著优于现有最强模型，准确率分别提升6.7%、7.7%和5.3%，标志着多模态智能体在长期记忆方面取得重要进展。",
      "translated_title": "看、听、记忆与推理：一种具备长期记忆的多模态智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent"
    }
  ],
  "lastUpdated": "2025-08-16T09:30:20.493Z"
}