{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "未选择的路：RLVR 被证明偏离主方向学习 (原标题: The Path Not Taken: RLVR Provably Learns Off the Principals)",
      "link": "https://arxiv.org/abs/2511.08567",
      "pubDate": "Tue, 11 Nov 2025 13:49:45 GMT",
      "isoDate": "2025-11-11T13:49:45.000Z",
      "creator": "Hanqing Zhu, Zhenyu Zhang, Hanxian Huang, DiJia Su, Zechun Liu, Jiawei Zhao, Igor Fedorov, Hamed Pirsiavash, Zhizhou Sha, Jinwon Lee, David Z. Pan, Zhangyang Wang, Yuandong Tian, Kai Sheng Tai",
      "summary": "# RLVR学习机制的深入探究：偏离主方向的学习路径\n\n本文深入探讨了“可验证奖励强化学习”（RLVR）在提升大型语言模型（LLM）推理性能时所展现的“稀疏性悖论”——即RLVR在显著提高性能的同时，似乎只修改了模型参数的一小部分。作者们重新审视了这一现象，并揭示了稀疏性实际上是模型条件优化偏差的一种表面假象。\n\n## 核心发现：优化偏差与稀疏性\n\n研究表明，对于一个固定的预训练模型，RLVR的参数更新会持续地定位到特定的、偏好的参数区域。这种定位在不同运行、不同数据集和不同RL策略之间都高度一致，并且在很大程度上保持不变。\n\n## 三门理论：机制解释\n\n为了从机制上解释这些动态，作者提出了“三门理论”：\n\n*   **第一门（KL锚定）**：施加了KL散度约束的更新。\n*   **第二门（模型几何）**：将更新步骤引导偏离主方向，进入低曲率、谱保持的子空间。\n*   **第三门（精度）**：将非偏好区域的微小更新隐藏起来，使得这种偏离主方向的偏差表现为稀疏性。\n\n## RLVR学习动态的参数级表征\n\n作者们通过验证这一理论，首次提供了RLVR学习动态的参数级表征：\n\n*   **偏离主方向学习**：RLVR在权重空间中偏离主方向进行学习。\n*   **最小谱漂移**：通过最小化谱漂移来实现性能提升。\n*   **主子空间旋转减少**：减少了主子空间的旋转。\n*   **非主方向更新对齐**：实现了非主方向更新的对齐。\n\n## 与SFT的对比\n\n与RLVR形成鲜明对比的是，传统的监督微调（SFT）方法：\n\n*   **目标主权重**：SFT倾向于针对主权重进行更新。\n*   **扭曲频谱**：SFT会扭曲模型的频谱。\n*   **性能滞后**：在某些情况下，SFT甚至落后于RLVR。\n\n## 关键启示与未来方向\n\n这些结果首次从参数空间角度阐明了RLVR的训练动态，揭示了参数演化的清晰规律。至关重要的是，研究表明RL与SFT在优化机制上处于截然不同的状态。因此，直接改编SFT时代的参数高效微调（PEFT）方法可能存在缺陷，这一点通过对高级稀疏微调和LoRA变体的案例研究得到了证实。\n\n作者们希望这项工作能够为RLVR的“白盒”理解以及设计几何感知、RLVR原生学习算法指明方向，而不是简单地重复使用SFT时代的启发式方法。",
      "shortSummary": "本文揭示了RLVR在提升LLM性能时表现出的“稀疏性”并非真实稀疏，而是模型条件优化偏差的表面现象。作者提出了“三门理论”解释其机制：RLVR更新偏离主方向，进入低曲率子空间。与SFT不同，RLVR通过最小谱漂移和非主方向更新对齐实现增益。研究强调RL与SFT优化机制不同，直接套用SFT时代的PEFT方法可能存在缺陷，呼吁开发RLVR原生算法。",
      "translated_title": "未选择的路：RLVR 被证明偏离主方向学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.   Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics."
    },
    {
      "title": "会话系统中的自适应多智能体响应精炼 (原标题: Adaptive Multi-Agent Response Refinement in Conversational Systems)",
      "link": "https://arxiv.org/abs/2511.08319",
      "pubDate": "Tue, 11 Nov 2025 09:48:34 GMT",
      "isoDate": "2025-11-11T09:48:34.000Z",
      "creator": "Soyeong Jeong, Aparna Elangovan, Emine Yilmaz, Oleg Rokhlenko",
      "summary": "### 会话系统中自适应多智能体响应精炼\n\n**1. 引言与背景**\n*   大型语言模型（LLMs）在生成类人会话响应方面表现出色，但仍存在局限性，尤其是在处理个性化信息或特定知识时。\n*   在实际应用中，期望用户自行发现并纠正LLM生成的错误是不切实际的。\n*   现有通过单个LLM进行响应精炼的方法，难以全面顾及会话所需的多个复杂方面。\n\n**2. 提出的解决方案：多智能体框架**\n*   本文提出了一种通过多智能体框架来精炼会话系统响应的方法。\n*   在该框架中，每个智能体被赋予一个特定角色，负责处理会话质量的一个特定方面。\n\n**3. 关键精炼方面**\n*   该研究聚焦于会话质量的三个关键方面：\n    *   **事实性 (Factuality)**：确保响应内容的准确性。\n    *   **个性化 (Personalization)**：使响应符合用户特点或偏好。\n    *   **连贯性 (Coherence)**：保证响应的逻辑性和流畅性。\n*   每个智能体负责审查和精炼其中一个方面，然后将其反馈合并以改进整体响应。\n\n**4. 动态通信策略**\n*   为了增强智能体之间的协作，本文引入了一种动态通信策略。\n*   与遵循固定智能体序列的方法不同，该策略能够根据每个查询的具体需求，自适应地选择和协调最相关的智能体。\n\n**5. 实验验证与结果**\n*   研究人员在具有挑战性的会话数据集上验证了所提出的框架。\n*   结果表明，该框架显著优于相关基线方法，尤其在涉及知识、用户画像（persona）或两者兼有的任务中表现出色。\n\n**6. 其他信息**\n*   本文将在2026年AAAI的LaCATODA研讨会上发表。\n*   相关主题包括：计算与语言 (cs.CL)、人工智能 (cs.AI)、多智能体系统 (cs.MA)。",
      "shortSummary": "本文提出一种会话系统中自适应多智能体响应精炼框架，以解决大型语言模型在个性化和知识处理上的不足。该框架为每个智能体分配特定角色，聚焦于事实性、个性化和连贯性三个关键方面。通过引入动态通信策略，系统能根据查询需求自适应选择和协调智能体。实验证明，该方法在涉及知识或用户画像的会话任务中显著优于现有基线，有效提升了响应质量。",
      "translated_title": "会话系统中的自适应多智能体响应精炼",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both."
    },
    {
      "title": "BiCA：基于引文感知的难负样本的有效生物医学密集检索 (原标题: BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives)",
      "link": "https://arxiv.org/abs/2511.08029",
      "pubDate": "Tue, 11 Nov 2025 04:31:37 GMT",
      "isoDate": "2025-11-11T04:31:37.000Z",
      "creator": "Aarush Sinha, Pavan Kumar S, Roshan Balaji, Nirav Pravinbhai Bhatt",
      "summary": "# BiCA：基于引文感知的难负样本的有效生物医学密集检索\n\n## 摘要\n\n本文提出了一种名为BiCA（Biomedical Dense Retrieval with Citation-Aware Hard Negatives）的方法，旨在通过利用引文链接来解决生物医学和科学领域中密集检索模型训练时难负样本挖掘的挑战。\n\n## 核心问题\n\n*   **难负样本挖掘的挑战**：在训练有效的检索模型时，难负样本至关重要。然而，在生物医学和科学领域，区分源文档和真正的难负样本非常困难，这使得难负样本挖掘变得具有挑战性。\n*   **传统方法的局限性**：传统的难负样本挖掘通常依赖于使用交叉编码器或基于相似性度量（如余弦距离）的静态嵌入模型对文档进行排序。\n\n## BiCA方法\n\n*   **创新点**：BiCA利用引文链接作为生成高质量难负样本的天然来源。被引用的文档与源文档在上下文上具有相关性，但并非重复，这使其成为理想的难负样本。\n*   **数据来源**：该方法利用了20,000篇PubMed文章中的引文链接进行难负样本挖掘。\n*   **模型微调**：BiCA使用这些引文感知的难负样本对GTE_small和GTE_Base模型进行了微调，以改进领域特定的小型密集检索器。\n\n## 主要发现与成果\n\n*   **性能提升**：BiCA在零样本密集检索任务中观察到持续的性能提升。\n*   **评估指标与数据集**：\n    *   在BEIR数据集上，对于域内和域外任务，使用nDCG@10指标取得了显著改进。\n    *   在LoTTE数据集的长尾主题上，使用Success@5指标超越了基线模型。\n*   **潜力与意义**：\n    *   研究结果强调了利用文档链接结构生成高度信息丰富的难负样本的巨大潜力。\n    *   BiCA能够以最少的微调实现最先进的性能。\n    *   该方法为实现高度数据高效的领域适应提供了一条途径。\n\n## 其他信息\n\n*   **发表情况**：该研究已被AAAI 2026接受进行口头报告。",
      "shortSummary": "BiCA提出了一种利用引文链接进行难负样本挖掘的生物医学密集检索方法。针对生物医学领域难负样本挖掘的挑战，BiCA通过分析PubMed文章的引文关系，为GTE_small和GTE_Base模型提供引文感知的难负样本进行微调。该方法在零样本密集检索任务中表现出一致的性能提升，超越了基线模型。BiCA证明了利用文档链接结构生成高质量难负样本的潜力，实现了高效的领域适应和卓越性能。",
      "translated_title": "BiCA：基于引文感知的难负样本的有效生物医学密集检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation."
    },
    {
      "title": "路由流形对齐改进了专家混合LLM的泛化能力 (原标题: Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs)",
      "link": "https://arxiv.org/abs/2511.07419",
      "pubDate": "Mon, 10 Nov 2025 13:59:53 GMT",
      "isoDate": "2025-11-10T13:59:53.000Z",
      "creator": "Zhongyang Li, Ziyue Li, Tianyi Zhou",
      "summary": "## 路由流形对齐改进专家混合LLM的泛化能力\n\n### 背景与问题\n*   稀疏专家混合（MoE）模型已被广泛应用于近期的大型语言模型（LLMs），以在不增加推理成本的情况下有效扩展模型能力。\n*   然而，在广泛的下游任务评估中发现，现有MoE LLMs中的路由器存在持续的次优性。\n*   这种次优性导致与最优路由之间存在严重的性能差距（例如，准确率相差10-20%）。\n\n### RoMA方法：路由流形对齐\n*   本文提出了一种名为“路由流形对齐（Routing Manifold Alignment, RoMA）”的方法，旨在解决上述问题。\n*   **核心思想：** 通过将路由权重的流形与任务嵌入的流形对齐，可以有效缩小性能差距并提高MoE LLMs的泛化性能。\n*   **实现机制：**\n    *   RoMA在后训练目标中引入了一个额外的流形正则化项。\n    *   该方法仅需要对路由器进行轻量级微调，而其他参数保持冻结。\n    *   具体而言，正则化项鼓励每个样本的路由权重在其任务嵌入空间中与其“成功邻居”（即路由权重导致正确答案的样本）的路由权重保持接近。\n    *   结果是，针对相似任务的样本将在不同层之间共享相似的专家选择。\n*   **理论基础：** 在不同样本之间建立任务与专家之间的这种绑定对于实现更好的泛化能力至关重要。\n*   **创新点：** RoMA展示了将任务理解（通过嵌入模型）与解决方案生成（通过MoE LLMs）相结合的优势。\n\n### 实验与结果\n*   研究人员使用RoMA对OLMoE、DeepSeekMoE和Qwen3-MoE中的路由器进行了微调。\n*   在多样化的基准测试中进行的评估以及与基线的广泛比较表明，RoMA带来了显著的性能提升。\n\n### 图片信息\n*   文章内容中未包含有效的实际图片链接，因此本摘要不包含任何图片。",
      "shortSummary": "MoE LLMs的路由器次优导致显著性能差距。本文提出“路由流形对齐（RoMA）”方法，通过在后训练中引入流形正则化项，轻量级微调路由器，将路由权重流形与任务嵌入流形对齐。RoMA促使相似任务的样本共享相似专家选择，从而显著提升了MoE LLMs的泛化能力，并在多个模型和基准测试中得到验证。",
      "translated_title": "路由流形对齐改进了专家混合LLM的泛化能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA."
    },
    {
      "title": "机器人从物理世界模型中学习 (原标题: Robot Learning from a Physical World Model)",
      "link": "https://arxiv.org/abs/2511.07416",
      "pubDate": "Mon, 10 Nov 2025 13:59:07 GMT",
      "isoDate": "2025-11-10T13:59:07.000Z",
      "creator": "Jiageng Mao, Sicheng He, Hao-Ning Wu, Yang You, Shuyang Sun, Zhicheng Wang, Yanan Bao, Huizhong Chen, Leonidas Guibas, Vitor Guizilini, Howard Zhou, Yue Wang",
      "summary": "## PhysWorld：通过物理世界建模实现机器人从视频生成中学习\n\n### 介绍\nPhysWorld 是一个创新框架，旨在通过物理世界建模，使机器人能够从视频生成中进行学习。该框架解决了现有视频生成模型在机器人领域应用时忽视物理规律，导致操作不准确的局限性。\n\n### 核心问题与解决方案\n\n*   **现有问题**：近期视频生成模型能够根据语言指令和图像合成逼真的视觉演示，为机器人提供了强大的训练信号。然而，直接将生成视频中的像素运动重新定位到机器人上，往往会忽略物理规律，导致操作不准确。\n*   **PhysWorld的解决方案**：PhysWorld 通过将视频生成与物理世界重建相结合来解决这一限制。它将隐式的视觉指导转化为物理上可执行的机器人轨迹。\n\n### PhysWorld 的工作原理\n\n1.  **输入**：给定一张单一图像和一个任务指令。\n2.  **视频生成**：系统生成与任务相关的视频。\n3.  **物理世界重建**：从生成的视频中重建底层的物理世界模型。\n4.  **动作接地**：通过结合物理世界模型，利用以物体为中心的残差强化学习，将生成的视频运动转化为物理上准确的动作。\n\n### 主要优势\n\n*   **消除真实机器人数据收集**：PhysWorld 框架无需收集真实的机器人数据。\n*   **零样本泛化操作**：它能够实现零样本（zero-shot）的泛化机器人操作。\n*   **提高操作精度**：实验表明，与以往方法相比，PhysWorld 大幅提高了操作精度。\n\n### 实验验证\n在各种真实世界任务上的实验证明，PhysWorld 显著提高了操作精度。",
      "shortSummary": "PhysWorld是一个创新框架，通过将视频生成与物理世界重建相结合，使机器人能够从生成的视频中学习。它解决了现有视频生成模型在机器人应用中忽视物理规律导致操作不准确的问题。PhysWorld根据图像和指令生成任务视频，重建物理世界，并通过强化学习将视频运动转化为物理准确的机器人动作。这消除了真实数据收集需求，实现了零样本泛化操作，并显著提高了机器人操作精度。",
      "translated_title": "机器人从物理世界模型中学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit https://pointscoder.github.io/PhysWorld_Web/{the project webpage} for details."
    },
    {
      "title": "DigiData：训练和评估通用移动控制代理 (原标题: DigiData: Training and Evaluating General-Purpose Mobile Control Agents)",
      "link": "https://arxiv.org/abs/2511.07413",
      "pubDate": "Mon, 10 Nov 2025 13:57:35 GMT",
      "isoDate": "2025-11-10T13:57:35.000Z",
      "creator": "Yuxuan Sun, Manchen Wang, Shengyi Qian, William R. Wong, Eric Gan, Pierluca D'Oro, Alejandro Castillejo Munoz, Sneha Silwal, Pedro Matias, Nitin Kamra, Satwik Kottur, Nick Raines, Xuanyi Zhao, Joy Chen, Joseph Greer, Andrea Madotto, Allen Bolourchi, James Valori, Kevin Carlberg, Karl Ridgeway, Joseph Tighe",
      "summary": "# DigiData：训练和评估通用移动控制代理\n\n## 摘要\n\n人工智能代理（AI agents）控制用户界面的能力有望彻底改变人类与数字设备的交互方式。为了加速这一转变，两个基本要素至关重要：\n\n1.  **高质量数据集**：使代理能够实现复杂且与人类相关的目标。\n2.  **强大的评估方法**：允许研究人员和从业者快速提升代理性能。\n\n本文介绍了 **DigiData** 及其配套的 **DigiData-Bench**，旨在解决上述挑战。\n\n## DigiData 数据集\n\n*   **目的与特点**：\n    *   DigiData 是一个大规模、高质量、多样化、多模态的数据集，专为训练移动控制代理而设计。\n    *   它旨在帮助代理实现复杂且与人类相关的目标。\n*   **创新之处**：\n    *   与现有数据集（通常从非结构化交互中获取目标）不同，DigiData 通过对应用程序功能的全面探索精心构建。\n    *   这种构建方式带来了更高的数据多样性和更复杂的目标设定。\n\n## DigiData-Bench 基准与评估方法\n\n*   **目的**：\n    *   DigiData-Bench 是一个用于评估移动控制代理在真实世界复杂任务上性能的基准。\n*   **评估方法的改进**：\n    *   研究表明，常用的“步长准确性”（step-accuracy）指标在可靠评估移动控制代理方面存在不足。\n    *   为了解决这一问题，本文提出了更严谨的替代方案：\n        *   **动态评估协议**\n        *   **AI驱动的评估**\n\n## 贡献与影响\n\n*   本文的贡献旨在显著推动移动控制代理的开发。\n*   最终目标是为更直观、更有效的人机设备交互铺平道路。",
      "shortSummary": "本文介绍了DigiData，一个大规模、高质量、多模态数据集，用于训练通用移动控制代理。该数据集通过全面探索应用功能构建，提供更高多样性和目标复杂性。同时，文章还提出了DigiData-Bench基准，用于评估代理在真实复杂任务上的表现，并提出动态评估协议和AI驱动评估来替代不足的传统指标。这些工作旨在加速移动控制代理的发展，提升人机交互体验。",
      "translated_title": "DigiData：训练和评估通用移动控制代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions."
    },
    {
      "title": "DIMO：任意物体的多样化3D运动生成 (原标题: DIMO: Diverse 3D Motion Generation for Arbitrary Objects)",
      "link": "https://arxiv.org/abs/2511.07409",
      "pubDate": "Mon, 10 Nov 2025 13:56:49 GMT",
      "isoDate": "2025-11-10T13:56:49.000Z",
      "creator": "Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, Kostas Daniilidis",
      "summary": "DIMO是一种创新的生成式方法，旨在从单张图像为任意物体生成多样化的3D运动。该方法的核心思想是利用预训练视频模型中丰富的先验知识来提取常见的运动模式，并将其嵌入到一个共享的低维潜在空间中。\n\n**方法论概述：**\n\n*   **多视频生成与运动嵌入：** 首先，系统会为同一物体生成多个具有多样化运动的视频。随后，每个运动都被编码成一个潜在向量。\n*   **共享运动解码器训练：** 训练一个共享的运动解码器，该解码器学习由结构化且紧凑的运动表示（即神经关键点轨迹）所代表的运动分布。\n*   **3D高斯模型驱动：** 规范的3D高斯模型由这些学习到的关键点轨迹驱动，并融合以精确建模物体的几何形状和外观。\n\n**推理与应用：**\n\n*   **即时多样化运动采样：** 在推理阶段，利用学习到的潜在空间，DIMO能够通过单次前向传播即时采样出多样化的3D运动。\n*   **广泛应用：** 该方法支持多种有趣的应用程序，包括3D运动插值和语言引导的运动生成。\n\n**项目信息：**\n\n*   该研究已在ICCV 2025上发表。\n*   项目页面可在提供的URL（this https URL）访问。",
      "shortSummary": "DIMO是一种创新的生成式方法，能够从单张图像为任意物体生成多样化的3D运动。它通过利用预训练视频模型提取运动模式，并将其嵌入到共享的低维潜在空间中。该方法训练一个运动解码器，使用神经关键点轨迹来表示运动分布，并驱动3D高斯模型以构建物体的几何和外观。DIMO支持单次前向传播即时采样多样化3D运动，并可应用于3D运动插值和语言引导的运动生成。",
      "translated_title": "DIMO：任意物体的多样化3D运动生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo."
    },
    {
      "title": "基于人类演示的计算机使用智能体基础构建 (原标题: Grounding Computer Use Agents on Human Demonstrations)",
      "link": "https://arxiv.org/abs/2511.07332",
      "pubDate": "Mon, 10 Nov 2025 12:35:21 GMT",
      "isoDate": "2025-11-10T12:35:21.000Z",
      "creator": "Aarash Feizi, Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Kaixin Li, Rabiul Awal, Xing Han Lù, Johan Obando-Ceron, Juan A. Rodriguez, Nicolas Chapados, David Vazquez, Adriana Romero-Soriano, Reihaneh Rabbany, Perouz Taslakian, Christopher Pal, Spandana Gella, Sai Rajeswar",
      "summary": "## 基于人类演示的计算机使用智能体基础构建\n\n### 引言与背景\n构建可靠的计算机使用智能体需要“基础构建”（grounding），即准确地将自然语言指令与屏幕上的正确元素关联起来。尽管针对网络和移动交互存在大量数据集，但桌面环境的高质量资源却非常有限，这阻碍了通用计算机使用智能体的发展。\n\n### GroundCUA数据集：解决桌面环境数据稀缺问题\n为了弥补这一空白，研究人员引入了 **GroundCUA**，这是一个大规模的桌面基础构建数据集，其构建基于专家级的人类演示。该数据集旨在为模型训练提供高质量数据，以促进计算机使用智能体的开发。\n\n*   **覆盖范围**：GroundCUA涵盖了12个类别中的87个应用程序。\n*   **数据规模**：\n    *   包含5.6万张屏幕截图。\n    *   每个屏幕元素都经过精心注释，总计超过356万个人工验证的注释。\n*   **指令生成**：从这些人类演示中，研究人员生成了多样化的指令，这些指令能够捕捉广泛的真实世界任务，从而为模型训练提供了高质量的输入。\n\n### GroundNext模型家族：实现指令到UI元素的映射\n利用GroundCUA数据集，研究人员开发了 **GroundNext** 模型家族。这些模型旨在将自然语言指令映射到其目标用户界面（UI）元素。\n\n*   **模型规模**：GroundNext模型在3B（30亿参数）和7B（70亿参数）两种规模下进行了开发。\n*   **性能表现**：\n    *   **监督微调**：通过监督微调，GroundNext在五项基准测试中均取得了最先进（state-of-the-art）的结果。\n    *   **数据效率**：与现有工作相比，GroundNext所需的训练数据量不到其十分之一，显示出极高的数据效率。\n    *   **强化学习提升**：通过强化学习的后训练（post-training）进一步提升了模型的性能。\n    *   **智能体环境评估**：在OSWorld基准测试的智能体设置中，结合o3规划器，GroundNext取得了与使用大量更多数据训练的模型相当或更优异的结果。\n\n### 结论\n这些研究结果有力地证明了高质量、专家驱动的数据集在推动通用计算机使用智能体发展中的关键作用。",
      "shortSummary": "为解决桌面环境计算机使用智能体缺乏高质量基础构建数据集的问题，研究者引入了GroundCUA，一个基于专家人类演示的大规模桌面数据集，包含5.6万张截图和356万个注释。基于此数据集，他们开发了GroundNext模型家族，该模型在3B和7B规模下，通过监督微调在五项基准测试中取得了最先进结果，且所需训练数据量不到现有工作的十分之一。结合强化学习和智能体设置，GroundNext展现出与使用更多数据训练的模型相当或更优的性能，凸显了高质量数据集的重要性。",
      "translated_title": "基于人类演示的计算机使用智能体基础构建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents."
    },
    {
      "title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体 (原标题: IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction)",
      "link": "https://arxiv.org/abs/2511.07327",
      "pubDate": "Mon, 10 Nov 2025 12:30:08 GMT",
      "isoDate": "2025-11-10T12:30:08.000Z",
      "creator": "Guoxin Chen, Zile Qiao, Xuanzhong Chen, Donglei Yu, Haotian Xu, Wayne Xin Zhao, Ruihua Song, Wenbiao Yin, Huifeng Yin, Liwen Zhang, Kuan Li, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体\n\n### 现有问题\n\n*   **上下文饱和与噪声污染**：当前的深度研究智能体在处理长周期任务时，采用单一上下文范式，将所有信息累积在一个不断扩展的上下文窗口中。这导致上下文饱和和噪声污染，限制了它们在长周期任务中的有效性。\n\n### IterResearch 解决方案\n\n*   **新型迭代深度研究范式**：IterResearch 引入了一种新颖的迭代深度研究范式，将长周期研究重新定义为具有战略性工作空间重建的马尔可夫决策过程（Markov Decision Process, MDP）。\n*   **核心机制**：\n    *   **演进报告作为记忆**：通过维护一份不断演进的报告作为记忆。\n    *   **定期综合见解**：定期综合提炼见解。\n    *   **保持推理能力**：确保在任意探索深度下都能保持一致的推理能力。\n\n### 效率感知策略优化 (EAPO)\n\n*   **强化学习框架**：IterResearch 进一步开发了效率感知策略优化（Efficiency-Aware Policy Optimization, EAPO）框架。\n*   **目标**：激励高效探索。\n*   **关键特性**：\n    *   **几何奖励折扣**：通过几何奖励折扣实现。\n    *   **自适应下采样**：通过自适应下采样实现稳定的分布式训练。\n\n### 实验结果与影响\n\n*   **显著性能提升**：在六个基准测试中，IterResearch 相较于现有开源智能体平均提升了 +14.5 个百分点。\n*   **缩小差距**：显著缩小了与前沿专有系统之间的性能差距。\n*   **前所未有的交互扩展**：该范式展现出前所未有的交互扩展能力，可扩展至 2048 次交互，并带来显著的性能提升（从 3.5% 提升至 42.5%）。\n*   **有效提示策略**：作为一种有效的提示策略，在长周期任务中，它能将前沿模型的性能比 ReAct 提升高达 19.2 个百分点。\n\n### 结论\n\n*   **通用解决方案**：IterResearch 是一个用于长周期推理的通用解决方案。\n*   **双重效用**：它既可以作为训练有素的智能体，也可以作为前沿模型的提示范式发挥作用。",
      "shortSummary": "IterResearch 提出了一种迭代式深度研究范式，通过将长周期研究重构为带有战略性工作空间重建的马尔可夫决策过程，解决了现有智能体在长周期任务中面临的上下文饱和与噪声污染问题。它通过维护演进报告和定期综合见解来保持推理能力。结合效率感知策略优化（EAPO），IterResearch 在六个基准测试中平均提升14.5pp，并展现出前所未有的交互扩展能力，同时作为提示策略也能显著提升前沿模型性能，是长周期推理的通用解决方案。",
      "translated_title": "IterResearch：通过马尔可夫状态重建重新思考长周期智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models."
    },
    {
      "title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解 (原标题: VADER: Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07299",
      "pubDate": "Mon, 10 Nov 2025 11:56:11 GMT",
      "isoDate": "2025-11-10T11:56:11.000Z",
      "creator": "Ying Cheng, Yu-Ho Lin, Min-Hung Chen, Fu-En Yang, Shang-Hong Lai",
      "summary": "## VADER：基于关系感知大型语言模型的因果视频异常理解\n\n### 摘要\n\n本文介绍了一种名为 VADER 的新型框架，旨在解决视频异常理解（VAU）领域中现有方法的局限性。传统的 VAU 方法主要侧重于异常事件的检测和定位，但往往忽视了对象之间深层次的因果关系和交互作用，而这些对于全面理解异常行为至关重要。\n\n### VADER 框架概述\n\nVADER 是一个由大型语言模型（LLM）驱动的框架，其核心目标是通过整合关键帧对象的关系特征与视觉线索，来增强对视频中异常事件的理解。该框架的关键组成部分和工作流程包括：\n\n*   **异常评分器（Anomaly Scorer）**：首先对视频中的每一帧分配异常分数，以识别潜在的异常时刻。\n*   **上下文感知采样（Context-AwarE Sampling, CAES）**：在异常评分的基础上，VADER 采用 CAES 策略来捕获每个异常事件的因果上下文，确保对异常发生前后的关键信息进行有效提取。\n*   **关系特征提取器（Relation Feature Extractor）与对比关系编码器（COntrastive Relation Encoder, CORE）**：这两个模块协同工作，对视频中对象的动态交互进行建模。它们能够生成紧凑的关系表示，为后续的推理提供基础。\n*   **LLM集成**：将提取到的视觉线索和关系线索与大型语言模型进行集成。通过这种集成，VADER 能够生成详细的、具有因果依据的异常描述，并支持对异常相关问题的鲁棒性回答。\n\n### 实验结果与贡献\n\n在多个真实世界的 VAU 基准测试中，VADER 展示了强大的性能。它在异常描述、解释和因果推理任务上均取得了显著成果，证明了其在理解复杂异常行为方面的有效性。VADER 的提出，标志着可解释视频异常分析领域的一个重要进展，为深入理解视频异常事件提供了新的视角和工具。",
      "shortSummary": "VADER是一个LLM驱动的框架，旨在实现因果视频异常理解（VAU）。它通过整合关键帧对象关系特征和视觉线索，克服了传统方法忽视对象间因果关系的问题。VADER包含异常评分器、上下文感知采样（CAES）以及关系特征提取器和对比关系编码器（CORE），用于建模动态对象交互。该框架能生成详细的因果描述并支持异常相关问答，在多个VAU基准测试中表现出色，显著推进了可解释视频异常分析领域。",
      "translated_title": "VADER：迈向基于关系感知大型语言模型的因果视频异常理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis."
    },
    {
      "title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别 (原标题: Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models)",
      "link": "https://arxiv.org/abs/2511.07253",
      "pubDate": "Mon, 10 Nov 2025 11:03:44 GMT",
      "isoDate": "2025-11-10T11:03:44.000Z",
      "creator": "Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic",
      "summary": "## Omni-AVSR：基于大型语言模型的统一多模态语音识别\n\n### 摘要\n\n当前，大型语言模型（LLMs）在听觉语音识别（ASR）、视觉语音识别（VSR）和音视频语音识别（AVSR）等多种模态的语音识别任务中取得了显著进展。然而，现有基于LLM的方法通常独立处理每个任务，训练单独的模型，这不仅增加了计算和部署资源的消耗，还错失了潜在的跨任务协同效应。此外，这些方法依赖于固定速率的令牌压缩，限制了在平衡准确性与效率方面的灵活性。这些局限性凸显了对一个能够支持ASR、VSR和AVSR并实现弹性推理的统一框架的需求。\n\n### Omni-AVSR 解决方案\n\n为解决上述问题，我们提出了 **Omni-AVSR**，一个统一的音视频大型语言模型，它结合了高效的多粒度训练和参数高效的适应策略。\n\n#### 关键方法和创新点：\n\n*   **统一框架设计**：Omni-AVSR旨在提供一个单一模型，能够同时处理ASR、VSR和AVSR任务，从而减少资源消耗并提升跨模态协同。\n*   **高效多粒度训练**：\n    *   我们借鉴了“套娃式表示学习”（matryoshka representation learning）范式，以高效地在多个音频和视觉粒度上进行训练。\n    *   这种方法显著降低了固有的训练资源消耗，提高了训练效率。\n*   **参数高效适应**：\n    *   我们探索了三种基于LoRA（Low-Rank Adaptation）的策略，用于适应骨干LLM。\n    *   这些策略旨在平衡共享知识和任务特定专业化，确保模型在不同任务上的高性能。\n\n### 实验结果与性能\n\n我们在LRS2和LRS3数据集上进行了广泛的实验，结果表明：\n\n*   **卓越的准确性**：Omni-AVSR实现了与现有最先进（SOTA）基线模型相当或更优的准确性。\n*   **显著的资源节约**：在训练单个模型的同时，Omni-AVSR显著降低了训练和部署的资源消耗。\n*   **鲁棒性**：模型在声学噪声环境下仍能保持鲁棒性，展现了其在实际应用中的潜力。\n*   **可扩展性分析**：我们分析了LLM规模增加时的扩展行为，为性能与效率之间的权衡提供了深入见解。",
      "shortSummary": "Omni-AVSR是一个统一的音视频大型语言模型，旨在解决当前LLM在听觉、视觉和音视频语音识别任务中独立训练、资源消耗高的问题。它通过高效的多粒度训练（采用套娃式表示学习）和参数高效的LoRA适应策略，实现了单一模型对多种模态的支持。实验表明，Omni-AVSR在LRS2和LRS3数据集上达到了与SOTA相当或更优的准确性，同时显著降低了训练和部署资源，并在噪声环境下保持鲁棒性。",
      "translated_title": "Omni-AVSR：迈向基于大型语言模型的统一多模态语音识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency."
    },
    {
      "title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型 (原标题: MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs)",
      "link": "https://arxiv.org/abs/2511.07250",
      "pubDate": "Mon, 10 Nov 2025 11:02:33 GMT",
      "isoDate": "2025-11-10T11:02:33.000Z",
      "creator": "Tianhao Peng, Haochen Wang, Yuanxing Zhang, Zekun Wang, Zili Wang, Ge Zhang, Jian Yang, Shihao Li, Yanghai Wang, Xintao Wang, Houyi Li, Wei Ji, Pengfei Wan, Wenhao Huang, Zhaoxiang Zhang, Jiaheng Liu",
      "summary": "## MVU-Eval：多模态大型语言模型的多视频理解评估基准\n\n### 背景与动机\n\n*   **现有评估局限性**：尽管多模态大型语言模型（MLLMs）已将AI能力扩展到视觉模态，但现有的评估基准仍局限于单视频理解。\n*   **现实世界需求**：现实场景（如体育分析和自动驾驶）对多视频理解有关键需求，而现有评估未能满足这一需求。\n\n### MVU-Eval 介绍\n\n*   **首个综合基准**：MVU-Eval是首个旨在评估MLLMs多视频理解能力的综合基准。\n*   **评估范围**：\n    *   **核心能力**：主要评估八项核心能力。\n    *   **数据量**：包含1,824个精心策划的问题-答案对。\n    *   **视频数量**：涵盖来自不同领域的4,959个视频。\n    *   **任务类型**：涉及基础感知任务和高阶推理任务。\n*   **与现实应用对齐**：这些能力与多传感器合成（在自动驾驶系统中）和跨角度体育分析等现实世界应用紧密对齐。\n\n### 评估结果与发现\n\n*   **广泛评估**：对最先进的开源和闭源模型进行了广泛评估。\n*   **显著性能差异**：评估揭示了当前MLLMs在执行多视频理解能力方面存在显著的性能差异和局限性。\n\n### 可用性与未来展望\n\n*   **公开可用**：该基准将公开发布，以促进未来的研究。\n\n### 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)",
      "shortSummary": "MVU-Eval是首个针对多模态大型语言模型（MLLMs）的多视频理解综合评估基准。它旨在弥补现有评估仅限于单视频理解的不足，通过1,824个问答对和4,959个视频，评估MLLMs的八项核心能力，涵盖感知与推理任务，并与自动驾驶、体育分析等现实应用对齐。评估结果显示，当前MLLMs在多视频理解方面存在显著局限性。该基准将公开发布以推动未来研究。",
      "translated_title": "MVU-Eval：迈向多视频理解评估的多模态大型语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research."
    },
    {
      "title": "MPJudge：迈向音乐启发绘画的感知评估 (原标题: MPJudge: Towards Perceptual Assessment of Music-Induced Paintings)",
      "link": "https://arxiv.org/abs/2511.07137",
      "pubDate": "Mon, 10 Nov 2025 09:18:27 GMT",
      "isoDate": "2025-11-10T09:18:27.000Z",
      "creator": "Shiqi Jiang, Tianyi Liang, Changbo Wang, Chenhui Li",
      "summary": "## MPJudge：迈向音乐启发绘画的感知评估\n\n### 引言\n*   音乐启发绘画是一种独特的艺术实践，即在音乐影响下创作视觉艺术作品。\n*   评估绘画是否忠实反映其灵感音乐，是一项具有挑战性的感知评估任务。\n\n### 现有方法的局限性\n*   现有方法主要依赖情感识别模型来评估音乐与绘画的相似性。\n*   这些模型引入了相当大的噪声，并忽略了情感之外的更广泛的感知线索。\n\n### MPJudge 框架\n*   **目标：** 针对现有方法的局限性，提出一个新颖的框架，直接建模音乐与视觉艺术之间的感知一致性，以进行音乐启发绘画的评估。\n\n### MPD 数据集\n*   **首创：** 引入了MPD，这是第一个大规模的音乐-绘画对数据集。\n*   **标注：** 由领域专家根据感知一致性进行标注。\n*   **增强：** 为更好地处理模糊情况，进一步收集了成对偏好标注。\n\n### MPJudge 模型\n*   **构建基础：** 基于MPD数据集。\n*   **架构：** 将音乐特征通过基于调制的融合机制整合到视觉编码器中。\n*   **训练：** 采用直接偏好优化（Direct Preference Optimization, DPO）进行训练，以有效学习模糊情况。\n\n### 实验结果\n*   **性能提升：** 大量实验表明，MPJudge方法优于现有方法。\n*   **定性分析：** 质性结果进一步表明，MPJudge模型能更准确地识别绘画中与音乐相关的区域。\n\n### 研究领域与引用\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2511.07137；期刊参考：AAAI 2026",
      "shortSummary": "该研究提出了MPJudge框架，旨在解决音乐启发绘画的感知评估挑战。现有方法依赖情感识别，但存在噪声且忽略了更广泛的感知线索。MPJudge通过直接建模音乐与视觉艺术间的感知一致性来改进。为此，研究构建了首个大规模音乐-绘画对数据集MPD，并引入MPJudge模型，该模型通过调制融合机制整合音乐特征，并利用直接偏好优化进行训练。实验证明，MPJudge优于现有方法，能更准确识别绘画中与音乐相关的区域。",
      "translated_title": "MPJudge：迈向音乐启发绘画的感知评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings."
    },
    {
      "title": "Wasm：一个构建结构化阿拉伯语交错多模态语料库的管道 (原标题: Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora)",
      "link": "https://arxiv.org/abs/2511.07080",
      "pubDate": "Mon, 10 Nov 2025 08:10:31 GMT",
      "isoDate": "2025-11-10T08:10:31.000Z",
      "creator": "Khalil Hennara, Ahmad Bastati, Muhammad Hreden, Mohamed Motasim Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan",
      "summary": "# Wasm：构建结构化阿拉伯语交错多模态语料库的管道\n\n## 摘要\n\n大型语言模型（LLM）和大型多模态模型（LMM）的性能高度依赖于其预训练数据集的质量和规模。近期研究表明，在图像和文本交错的自然文档上训练的大型多模态模型，在广泛的基准测试中表现优于仅在图像-文本对上训练的模型。这些模型利用先进的预训练模型来强制执行语义对齐、图像序列一致性和文本连贯性。\n\n然而，对于阿拉伯语而言，由于缺乏高质量、保留文档结构的多模态数据集，其在该领域的进展受到了限制。\n\n## Wasm 管道介绍\n\n本文提出了 **Wasm** 管道，旨在解决阿拉伯语多模态数据集的不足。Wasm 管道用于处理 Common Crawl 数据集，以创建一个新的阿拉伯语多模态数据集，其独特之处在于提供 Markdown 输出。\n\n### Wasm 的主要特点和优势：\n\n*   **结构完整性保留**：与现有主要关注文本提取的阿拉伯语语料库不同，Wasm 方法保留了网络内容的结构完整性。\n*   **灵活性**：该管道在文本-only和多模态预训练场景中都保持了灵活性。\n*   **Markdown 输出**：数据集以 Markdown 格式输出，便于结构化信息的利用。\n\n## 比较分析与贡献\n\n研究团队对 Wasm 数据处理管道与现有主要数据集所使用的管道进行了全面的比较分析。分析结果突出了过滤策略的共通之处，并解释了 Wasm 特定设计选择的合理性。\n\n为了支持未来的研究，本文公开发布了一个具有代表性的数据集转储以及用于阿拉伯语的多模态处理管道。",
      "shortSummary": "本文提出了Wasm管道，旨在解决阿拉伯语高质量结构化多模态数据集的缺失。Wasm处理Common Crawl数据，创建了一个独特的阿拉伯语多模态数据集，其特点是提供Markdown输出并保留了网络内容的结构完整性。该管道旨在支持LLM和LMM的文本-only及多模态预训练，并已公开发布以促进未来研究。",
      "translated_title": "Wasm：一个构建结构化阿拉伯语交错多模态语料库的管道",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic."
    },
    {
      "title": "RedOne 2.0：重新思考社交网络服务中领域特定LLM的后训练 (原标题: RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services)",
      "link": "https://arxiv.org/abs/2511.07070",
      "pubDate": "Mon, 10 Nov 2025 08:04:34 GMT",
      "isoDate": "2025-11-10T08:04:34.000Z",
      "creator": "Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, Yao Hu, Shaosheng Cao",
      "summary": "### RedOne 2.0：社交网络服务（SNS）中领域特定大型语言模型（LLM）的后训练新范式\n\n**1. 背景与挑战**\n\n*   **SNS的独特挑战**：社交网络服务作为人类互动和信息交换的关键媒介，对LLM提出了独特的挑战，包括：\n    *   异构工作负载。\n    *   快速变化的规范和俚语。\n    *   多语言、文化多元的语料库，导致严重的分布偏移。\n*   **SFT的局限性**：传统的监督微调（SFT）虽然能使模型专业化，但常在领域内（in-distribution）收益与领域外（out-of-distribution）鲁棒性之间引发“跷跷板效应”，尤其对于小型模型而言。\n\n**2. RedOne 2.0 解决方案**\n\n*   **引入RedOne 2.0**：为应对上述挑战，研究人员提出了RedOne 2.0，这是一种面向SNS的LLM，采用渐进式、RL（强化学习）优先的后训练范式，旨在实现快速且稳定的适应。\n\n**3. RedOne 2.0 的三阶段训练流程**\n\nRedOne 2.0 的训练流程包含以下三个阶段：\n\n*   **阶段1：探索性学习（Exploratory Learning）**\n    *   在精心策划的SNS语料库上进行学习，以建立初始对齐。\n    *   识别系统性弱点。\n*   **阶段2：目标性微调（Targeted Fine-Tuning）**\n    *   有选择地对诊断出的差距应用SFT。\n    *   混合少量通用数据，以减轻遗忘效应。\n*   **阶段3：精炼学习（Refinement Learning）**\n    *   重新应用RL，并结合以SNS为中心的信号。\n    *   巩固改进，并在不同任务之间协调权衡。\n\n**4. 实验结果与性能**\n\n*   **显著性能提升**：在涵盖三类任务的各项测试中，RedOne 2.0 的4B规模模型相对于7B的次优基线，平均性能提升约2.41。\n*   **卓越的数据效率与稳定性**：RedOne 2.0 从基础模型获得了约8.74的平均性能提升，而所需数据量不到以SFT为中心的RedOne方法的一半，这证明了其在紧凑规模下卓越的数据效率和稳定性。\n\n**5. 结论**\n\n*   RedOne 2.0 为SNS场景中的领域特定LLM建立了一个具有竞争力且成本效益的基线，在提升能力的同时不牺牲鲁棒性。",
      "shortSummary": "RedOne 2.0 提出了一种渐进式、RL优先的后训练范式，旨在解决社交网络服务（SNS）中大型语言模型（LLM）面临的异构工作负载、快速变化的语料和分布偏移等挑战。该方法通过探索性学习、目标性微调和精炼学习三个阶段，实现了快速稳定的适应。实验表明，RedOne 2.0 的4B模型在性能上显著优于7B基线，且数据效率和稳定性更高，为SNS领域特定LLM提供了一个成本效益高且鲁棒性强的解决方案。",
      "translated_title": "RedOne 2.0：重新思考社交网络服务中领域特定LLM的后训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness."
    },
    {
      "title": "大型语言模型有情感吗？通过提示、检索和课程学习教授情感识别 (原标题: Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning)",
      "link": "https://arxiv.org/abs/2511.07061",
      "pubDate": "Mon, 10 Nov 2025 07:52:11 GMT",
      "isoDate": "2025-11-10T07:52:11.000Z",
      "creator": "Xinran Li, Xiujuan Xu, Jiaqi Qiao, Yu Liu",
      "summary": "## 大型语言模型在对话情感识别（ERC）中的应用：PRC-Emo框架\n\n### 研究背景与挑战\n\n*   对话情感识别（ERC）是理解人类情感和实现自然人机交互的关键任务。\n*   尽管大型语言模型（LLMs）在此领域展现出巨大潜力，但它们在捕捉显式和隐式情感之间内在联系的能力上仍存在局限。\n\n### PRC-Emo框架介绍\n\n*   本文提出了一种名为PRC-Emo的新型ERC训练框架，旨在探索LLMs是否能有效感知对话情境中的情感。\n*   PRC-Emo集成了以下三个核心组件：\n    *   **提示工程（Prompt engineering）**\n    *   **示例检索（demonstration Retrieval）**\n    *   **课程学习（Curriculum learning）**\n\n### PRC-Emo核心组件详解\n\n*   **情感敏感的提示模板**\n    *   设计了基于显式和隐式情感线索的提示模板。\n    *   这些模板旨在更好地引导模型理解说话者的心理状态。\n*   **专用示例检索库**\n    *   构建了首个专用于ERC的示例检索库。\n    *   该库包含来自广泛使用数据集的训练样本。\n    *   还包括由LLMs生成并经过人工验证的高质量对话示例。\n*   **课程学习策略**\n    *   将课程学习策略引入LoRA微调过程。\n    *   通过计算同说话者和不同说话者话语之间的加权情感转变，为对话样本分配难度级别。\n    *   然后，按照从易到难的顺序组织训练序列。\n\n### 实验结果与贡献\n\n*   在两个基准数据集（IEMOCAP和MELD）上的实验结果表明，PRC-Emo方法达到了新的最先进（SOTA）性能。\n*   这证明了该方法在提升基于LLM的情感理解方面的有效性和泛化能力。\n*   该研究已被AAAI 2026接受。",
      "shortSummary": "本文提出PRC-Emo框架，旨在提升大型语言模型（LLMs）在对话情感识别（ERC）中的表现。该框架整合了情感敏感的提示工程、首个专用的ERC示例检索库以及基于情感转变的课程学习策略。通过LoRA微调，PRC-Emo能够有效捕捉显式和隐式情感联系。在IEMOCAP和MELD数据集上的实验结果表明，该方法达到了最先进（SOTA）性能，验证了其在改善LLM情感理解方面的有效性和泛化能力。",
      "translated_title": "大型语言模型有情感吗？通过提示、检索和课程学习教授情感识别",
      "images": [],
      "contentSource": "完整文章",
      "content": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding."
    },
    {
      "title": "软件开发中大型语言模型（LLMs）的平衡挑战：从业者的视角 (原标题: Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective)",
      "link": "https://arxiv.org/abs/2511.06428",
      "pubDate": "Sun, 09 Nov 2025 10:49:55 GMT",
      "isoDate": "2025-11-09T10:49:55.000Z",
      "creator": "Samuel Ferino, Rashina Hoda, John Grundy, Christoph Treude",
      "summary": "# 软件开发中大型语言模型（LLMs）的平衡挑战：从业者的视角\n\n## 背景与研究目的\n大型语言模型（LLMs）的出现，预示着软件开发领域可能迎来一场革命，例如自动化流程和劳动力转型。然而，尽管已有研究开始探讨LLMs的感知影响，但仍缺乏实证研究来深入理解如何平衡使用LLMs所带来的正向和负向效应。\n\n本研究旨在从软件开发者的角度，调查LLMs如何影响软件开发，以及如何有效管理这些影响。\n\n## 研究方法\n研究团队在2024年10月至2025年9月期间，分三轮数据收集和分析，对22名软件从业者进行了访谈。数据分析采用了社会技术扎根理论（STGT），以严谨地分析受访者的回答。\n\n## 主要发现\n研究识别了在个人、团队、组织和社会层面使用LLMs的益处和弊端，并提出了采纳LLMs的最佳实践。\n\n### LLMs的益处\n*   **保持软件开发流程顺畅：** LLMs有助于维持开发工作流的连贯性。\n*   **改善开发者心智模型：** 辅助开发者更好地理解和构建软件系统。\n*   **促进创业精神：** 降低技术门槛，激发创新和新项目的产生。\n\n### LLMs的弊端\n*   **对开发者个性的负面影响：** 可能导致开发者过度依赖，影响其独立思考和解决问题的能力。\n*   **损害开发者声誉：** 若LLMs生成代码质量不佳或存在错误，可能间接影响开发者的专业声誉。\n\n### 影响层面\nLLMs的影响不仅限于个体开发者，还延伸至：\n*   **个人层面：** 影响开发者的工作效率、学习曲线和职业发展。\n*   **团队层面：** 改变团队协作模式、知识共享方式和项目管理策略。\n*   **组织层面：** 影响组织的创新能力、人才培养和竞争力。\n*   **社会层面：** 对软件行业劳动力市场、教育体系和技术伦理产生深远影响。\n\n## 结论与启示\n本研究关键性地揭示了软件从业者、团队和组织在与LLMs合作时所面临的权衡。研究结果对于软件团队负责人和IT经理评估LLMs在其特定环境中的可行性具有重要的指导意义。",
      "shortSummary": "本研究从从业者视角探讨了大型语言模型（LLMs）对软件开发的影响及管理。通过访谈22名软件从业者，研究识别了LLMs在个人、团队、组织和社会层面的益处（如保持开发流程、改善心智模型）和弊端（如负面个性影响、损害声誉）。研究还提出了LLMs采纳的最佳实践，并揭示了使用LLMs的权衡考量。这些发现对软件团队负责人和IT经理评估LLMs的可行性具有重要指导意义。",
      "translated_title": "软件开发中大型语言模型（LLMs）的平衡挑战：从业者的视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context."
    },
    {
      "title": "SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习 (原标题: SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization)",
      "link": "https://arxiv.org/abs/2511.06411",
      "pubDate": "Sun, 09 Nov 2025 09:55:50 GMT",
      "isoDate": "2025-11-09T09:55:50.000Z",
      "creator": "Zhi Zheng, Wee Sun Lee",
      "summary": "## SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习\n\n### 引言与背景\n\n*   大型语言模型（LLM）的软思维推理范式在某些场景下表现优于传统的离散令牌思维链（CoT）推理，这凸显了其重要的研究和应用价值。\n*   然而，使用强化学习（RL）来强化软思维模式面临显著挑战。这些挑战主要源于：\n    *   向软思维令牌注入随机性的复杂性。\n    *   相应地更新软思维策略的难度。\n*   因此，此前将软思维与组相对策略优化（GRPO）结合的尝试，通常表现不如其离散令牌GRPO对应方法。\n\n### SofT-GRPO算法\n\n*   为了充分释放软思维的潜力，本文提出了一种新颖的策略优化算法——SofT-GRPO。\n*   该算法专门设计用于在软思维推理模式下强化LLMs。\n\n### 核心技术与方法\n\nSofT-GRPO通过以下关键技术克服了现有挑战：\n\n1.  **注入Gumbel噪声：** 将Gumbel噪声注入到逻辑值（logits）中，以引入必要的随机性。\n2.  **Gumbel-Softmax技术：** 采用Gumbel-Softmax技术，有效避免了软思维令牌生成超出预训练嵌入空间的情况。\n3.  **重参数化技巧：** 在策略梯度计算中巧妙地利用了重参数化技巧，以实现更稳定的策略更新。\n\n### 实验与结果\n\n*   **实验范围：** 研究人员在参数量从1.5B到7B的基础LLMs上进行了广泛的实验。\n*   **性能提升：** 实验结果显著表明SofT-GRPO的有效性：\n    *   在Pass@1指标上，SofT-GRPO使软思维LLMs略微超越了离散令牌GRPO，平均准确率提升了0.13%。\n    *   在Pass@32指标上，SofT-GRPO展现出更为显著的提升，平均准确率提升了2.19%。\n\n### 结论\n\nSofT-GRPO成功地解决了强化软思维LLM的挑战，并在多项关键指标上超越了传统的离散令牌GRPO方法，尤其在处理复杂任务（Pass@32）时取得了显著进步，充分展示了软思维与强化学习结合的巨大潜力。相关代码和权重已公开发布。",
      "shortSummary": "本文提出SofT-GRPO算法，旨在通过强化学习优化大型语言模型（LLMs）的软思维推理模式。传统方法在强化软思维时面临随机性注入和策略更新的挑战，导致性能不如离散令牌GRPO。SofT-GRPO通过注入Gumbel噪声、Gumbel-Softmax技术和重参数化技巧，成功克服了这些难题。实验结果表明，SofT-GRPO使软思维LLMs在Pass@1上略优于离散令牌GRPO（+0.13%），并在Pass@32上实现了显著提升（+2.19%），展现了超越传统方法的潜力。",
      "translated_title": "SofT-GRPO：通过Gumbel重参数化软思维策略优化超越离散令牌LLM强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The soft-thinking paradigm for Large Language Model (LLM) reasoning can outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in some scenarios, underscoring its research and application value. However, while the discrete-token CoT reasoning pattern can be reinforced through policy optimization algorithms such as group relative policy optimization (GRPO), extending the soft-thinking pattern with Reinforcement Learning (RL) remains challenging. This difficulty stems from the complexities of injecting stochasticity into soft-thinking tokens and updating soft-thinking policies accordingly. As a result, previous attempts to combine soft-thinking with GRPO typically underperform their discrete-token GRPO counterparts. To fully unlock the potential of soft-thinking, this paper presents a novel policy optimization algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning pattern. SofT-GRPO injects the Gumbel noise into logits, employs the Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained embedding space, and leverages the reparameterization trick in policy gradient. We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes and weights are available on https://github.com/zz1358m/SofT-GRPO-master"
    },
    {
      "title": "Station：一个由AI驱动的开放世界发现环境 (原标题: The Station: An Open-World Environment for AI-Driven Discovery)",
      "link": "https://arxiv.org/abs/2511.06309",
      "pubDate": "Sun, 09 Nov 2025 05:13:00 GMT",
      "isoDate": "2025-11-09T05:13:00.000Z",
      "creator": "Stephen Chung, Wenyu Du",
      "summary": "# Station：一个由AI驱动的开放世界发现环境\n\n“Station”是一个新颖的开放世界多智能体环境，旨在模拟一个微型科学生态系统。该环境的核心目标是推动由AI驱动的自主科学发现。\n\n## 核心概念与功能\n\n*   **开放世界多智能体环境**：Station是一个没有中心化协调系统的环境。智能体可以自由选择自己的行动，并发展自己的研究叙事。\n*   **长程科学探索**：利用其扩展的上下文窗口，Station中的AI智能体能够进行长时间的科学探索，包括：\n    *   阅读同行论文\n    *   制定假设\n    *   提交代码\n    *   执行分析\n    *   发布研究成果\n*   **自主行为**：智能体在Station中是自主的，它们独立地追求研究、与同行互动，并在此过程中建立起累积的历史。\n\n## 实验成果与突破\n\n*   **卓越性能**：实验证明，Station中的AI智能体在广泛的基准测试中取得了新的最先进性能，涵盖了数学、计算生物学和机器学习等领域。\n*   **超越现有技术**：值得注意的是，在圆堆积（circle packing）问题上，Station的智能体显著超越了AlphaEvolve。\n*   **涌现的叙事与方法**：\n    *   智能体独立研究、相互作用和累积历史，形成了一个丰富的叙事图景。\n    *   从这些涌现的叙事中，新的方法有机地产生，例如一种用于单细胞RNA测序（scRNA-seq）批次整合的密度自适应算法。\n\n## 意义与未来展望\n\n*   **自主科学发现的里程碑**：Station代表了迈向由开放世界环境中涌现行为驱动的自主科学发现的第一步。\n*   **新范式**：它标志着一种超越僵化优化的新范式，强调通过智能体间的自由互动和探索来推动科学进步。",
      "shortSummary": "“Station”是一个开放世界多智能体环境，模拟微型科学生态系统，旨在实现AI驱动的自主科学发现。AI智能体在此环境中自主进行科学探索，包括阅读论文、提出假设、执行分析和发布结果，且无中心化协调。实验表明，Station智能体在数学、计算生物学和机器学习等多个基准测试中取得了最先进的性能，并能有机地产生新方法。这代表了通过涌现行为实现自主科学发现的新范式。",
      "translated_title": "Station：一个由AI驱动的开放世界发现环境",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce the STATION, an open-world multi-agent environment that models a miniature scientific ecosystem. Leveraging their extended context windows, agents in the Station can engage in long scientific journeys that include reading papers from peers, formulating hypotheses, submitting code, performing analyses, and publishing results. Importantly, there is no centralized system coordinating their activities - agents are free to choose their own actions and develop their own narratives within the Station. Experiments demonstrate that AI agents in the Station achieve new state-of-the-art performance on a wide range of benchmarks, spanning from mathematics to computational biology to machine learning, notably surpassing AlphaEvolve in circle packing. A rich tapestry of narratives emerges as agents pursue independent research, interact with peers, and build upon a cumulative history. From these emergent narratives, novel methods arise organically, such as a new density-adaptive algorithm for scRNA-seq batch integration. The Station marks a first step towards autonomous scientific discovery driven by emergent behavior in an open-world environment, representing a new paradigm that moves beyond rigid optimization."
    },
    {
      "title": "DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践 (原标题: DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation)",
      "link": "https://arxiv.org/abs/2511.06307",
      "pubDate": "Sun, 09 Nov 2025 05:11:28 GMT",
      "isoDate": "2025-11-09T05:11:28.000Z",
      "creator": "Speed Zhu, Jianwei Cai, Guang Chen, Lulu Wu, Saiyong Yang, Wiggin Zhou",
      "summary": "## DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践\n\n### 摘要\n\n近期以推理为主的模型（如OpenAI o1、DeepSeek R1）重新激发了对可验证奖励强化学习（RLVR）的兴趣。然而，这一领域的进展主要集中在数学领域（如AIME），而竞争性编程代码生成领域尚未得到充分探索，数据管理受到的关注也少于RL算法设计。本文旨在研究如何构建RLVR数据集（即RL提示），并提出实用的训练技术，以在竞争性编程代码生成中取得强大性能。\n\n### 方法论与管道\n\n我们的管道从以下步骤开始：\n\n1.  **监督微调（SFT）**：\n    *   从强大的开源模型中提取SFT。\n    *   通过通用和推理密集型数据进行增强。\n\n2.  **两阶段强化学习（RL）**：\n    *   RL阶段采用可执行的、测试用例驱动的奖励。\n    *   **第一阶段：基于GRPO的训练**\n        *   在大量、均匀分布的竞争性编程问题集上进行训练。\n        *   使用**组相对策略优化（GRPO）**算法。\n        *   每个提示进行8次rollout。\n        *   相对较短的响应生成窗口（例如，SFT期间为32k，此阶段为24k）。\n        *   目标：扩展熵，缓解重复和截断问题。\n    *   **第二阶段：Pre-GRPO**\n        *   在一个小规模、高质量的挑战性问题集上进行更新。\n        *   采用较大的rollout预算：每个提示64次rollout。\n        *   实施**硬聚焦课程（hard-focus curriculum）**：在整个训练过程中持续保留最困难的实例。\n\n### 实施与评估\n\n*   我们将所提出的方法在Qwen2.5-32B模型上实现。\n*   在LeetCode和Codeforces周赛上进行评估，以避免数据泄露。\n\n### 结果与观察\n\n*   **性能表现**：\n    *   在同等规模模型中取得了最先进（SOTA）的性能。\n    *   与DeepSeek v3.1和Doubao-1.5-Thinking等领先系统表现相当。\n*   **扩展趋势**：\n    *   在内部大规模MoE模型上观察到强大的RL扩展能力。\n\n### 贡献与最佳实践\n\n本研究为竞争性编程代码生成中的RLVR提炼出以下简洁的最佳实践：\n\n*   数据管理。\n*   熵扩展。\n*   课程设计。",
      "shortSummary": "本文探讨了竞争性代码生成中可验证奖励强化学习（RLVR）的数据管理和训练技术。研究提出了一种两阶段RL管道：首先进行监督微调，随后是基于可执行奖励的两阶段RL过程，包括使用GRPO进行熵扩展训练，以及采用硬聚焦课程的Pre-GRPO阶段。该方法在Qwen2.5-32B上实现了最先进性能，与领先系统相当，并提炼出RLVR中数据管理、熵扩展和课程设计的最佳实践。",
      "translated_title": "DRIVE：竞争性代码生成中可验证奖励强化学习的数据管理最佳实践",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform Pre-GRPO: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation."
    }
  ],
  "lastUpdated": "2025-11-12T09:33:54.818Z"
}