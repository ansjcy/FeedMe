{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "DeepMMSearch-R1：赋能多模态大语言模型在多模态网络搜索中的应用 (原标题: DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search)",
      "link": "https://arxiv.org/abs/2510.12801",
      "pubDate": "Tue, 14 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-14T13:59:58.000Z",
      "creator": "Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan",
      "summary": "# DeepMMSearch-R1：赋能多模态大语言模型在多模态网络搜索中的应用\n\n## 摘要\n\n多模态大语言模型（MLLMs）在实际应用中面临挑战，它们需要访问外部知识源并响应动态变化的实时信息，以有效处理用户的信息查询和知识密集型需求。现有方法，如检索增强生成（RAG）、搜索代理和配备搜索功能的MLLMs，常因管道僵化、搜索调用过多以及搜索查询构建不佳而导致效率低下和结果不理想。\n\n## DeepMMSearch-R1 解决方案\n\n为了解决上述限制，本文提出了 **DeepMMSearch-R1**，这是首个能够执行按需、多轮网络搜索的多模态大语言模型，并能为图像和文本搜索工具动态地生成查询。\n\n### 关键能力\n\n*   **图像搜索优化：** DeepMMSearch-R1 能够基于输入图像的相关裁剪区域发起网络搜索，从而显著提高图像搜索的有效性。\n*   **文本查询自适应：** 它能够根据检索到的信息迭代地调整文本搜索查询，从而实现模型的自我反思和自我纠正能力。\n\n## 训练流程\n\nDeepMMSearch-R1 的训练依赖于一个两阶段的管道：\n\n1.  **冷启动监督微调阶段：** 模型的初始训练阶段。\n2.  **在线强化学习优化阶段：** 在此阶段进一步优化模型性能。\n\n## DeepMMSearchVQA 数据集\n\n为了支持训练，研究人员引入了一个新颖的多模态 VQA（视觉问答）数据集 **DeepMMSearchVQA**。该数据集具有以下特点：\n\n*   通过自动化管道与来自网络搜索工具的真实世界信息相结合创建。\n*   包含多样化的多跳查询，整合了文本和视觉信息。\n*   旨在教授模型以下关键能力：\n    *   何时进行搜索。\n    *   搜索什么内容。\n    *   使用哪个搜索工具。\n    *   如何对检索到的信息进行推理。\n\n## 实验与分析\n\n研究团队在各种知识密集型基准测试中进行了广泛的实验，结果表明 DeepMMSearch-R1 的方法具有优越性。通过对结果的深入分析，本文提供了对推进多模态网络搜索领域有价值的见解。\n\n## 研究领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   信息检索 (cs.IR)",
      "shortSummary": "DeepMMSearch-R1 是一种创新的多模态大语言模型（MLLM），旨在解决现有 MLLM 在多模态网络搜索中效率低下和查询构建不佳的问题。它首次实现了按需、多轮网络搜索，并能为图像和文本搜索工具动态生成查询。该模型能基于图像裁剪区域进行有效图像搜索，并迭代调整文本查询以实现自我纠正。通过两阶段训练和新型DeepMMSearchVQA数据集，DeepMMSearch-R1 在知识密集型任务中表现出卓越性能，为多模态网络搜索提供了宝贵见解。",
      "translated_title": "DeepMMSearch-R1：赋能多模态大语言模型在多模态网络搜索中的应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search."
    },
    {
      "title": "通过下一个点预测检测一切 (原标题: Detect Anything via Next Point Prediction)",
      "link": "https://arxiv.org/abs/2510.12798",
      "pubDate": "Tue, 14 Oct 2025 13:59:54 GMT",
      "isoDate": "2025-10-14T13:59:54.000Z",
      "creator": "Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, Lei Zhang",
      "summary": "# Rex-Omni：通过下一坐标点预测实现通用目标检测\n\n## 引言\n\n传统的目标检测领域长期以来由基于坐标回归的模型主导，例如YOLO、DETR和Grounding DINO。尽管近期有研究尝试利用多模态大语言模型（MLLMs）来解决这项任务，但它们面临着召回率低、重复预测和坐标未对齐等挑战。\n\n## Rex-Omni 介绍\n\n本文提出了Rex-Omni，一个30亿参数规模的MLLM，旨在弥合MLLM在目标检测方面的差距，并实现最先进的目标感知性能。在COCO和LVIS等基准测试中，Rex-Omni在零样本设置下，其性能与DINO、Grounding DINO等基于回归的模型相当或超越。\n\n## 关键设计\n\nRex-Omni的卓越性能得益于以下三个关键设计：\n\n1.  **任务公式化（Task Formulation）：**\n    *   使用特殊token来表示0到999范围内的量化坐标。\n    *   这一设计旨在降低模型的学习难度，并提高坐标预测的token效率。\n\n2.  **数据引擎（Data Engines）：**\n    *   构建了多个数据引擎，用于生成高质量的接地（grounding）、指代（referring）和指向（pointing）数据。\n    *   这些数据为模型训练提供了语义丰富的监督信息。\n\n3.  **训练流程（Training Pipelines）：**\n    *   采用两阶段训练过程：\n        *   **第一阶段：监督微调（SFT）：** 在2200万数据上进行。\n        *   **第二阶段：基于GRPO的强化后训练（RL）：**\n            *   利用几何感知的奖励机制。\n            *   目的：有效弥合离散到连续坐标预测的鸿沟，提高边界框精度，并减轻SFT阶段因教师指导性质导致的重复预测等不良行为。\n\n## 超越传统检测的通用能力\n\n除了传统的检测任务，Rex-Omni固有的语言理解能力使其具备多种通用能力，包括：\n\n*   目标指代（object referring）\n*   指向（pointing）\n*   视觉提示（visual prompting）\n*   GUI接地（GUI grounding）\n*   空间指代（spatial referring）\n*   光学字符识别（OCR）\n*   关键点检测（key-pointing）\n\n所有这些能力都在专门的基准测试中得到了系统评估。\n\n## 结论\n\nRex-Omni为开发更通用、更具语言感知能力的视觉感知系统铺平了道路。",
      "shortSummary": "Rex-Omni是一个30亿参数规模的多模态大语言模型（MLLM），旨在解决传统MLLM在目标检测中召回率低、重复预测等问题。通过创新的量化坐标表示、高质量数据引擎和两阶段训练（监督微调与基于GRPO的强化学习），Rex-Omni在零样本设置下实现了与传统回归模型相当或超越的先进目标感知性能。它还支持目标指代、指向、OCR等多种通用能力，为更通用、语言感知的视觉感知系统奠定了基础。",
      "translated_title": "通过下一个点预测检测一切",
      "images": [],
      "contentSource": "完整文章",
      "content": "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems."
    },
    {
      "title": "ViCO：一种面向语义感知动态高分辨率的训练策略 (原标题: ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution)",
      "link": "https://arxiv.org/abs/2510.12793",
      "pubDate": "Tue, 14 Oct 2025 13:58:10 GMT",
      "isoDate": "2025-10-14T13:58:10.000Z",
      "creator": "Long Cui, Weiyun Wang, Jie Shao, Zichen Wen, Gen Luo, Linfeng Zhang, Yanting Zhang, Yu Qiao, Wenhai Wang",
      "summary": "## ViCO：一种面向语义感知动态高分辨率的训练策略\n\n### 摘要\n\n现有多模态大型语言模型（MLLMs）在处理图像输入时，由于引入了额外的视觉token，导致推理成本显著增加。为了解决这一问题，本文提出了一种名为 **Visual Consistency Learning (ViCO)** 的新颖训练算法，旨在使模型能够根据图像的语义复杂性，使用不同数量的视觉token来表示图像，从而提高效率。\n\n### 核心思想与方法\n\nViCO 的关键在于其独特的设计，它允许模型根据图像内容的语义复杂程度动态调整视觉token的数量：\n\n*   **多MLP连接器**：该方法采用多个多层感知器（MLP）连接器，每个连接器都具有不同的图像压缩比。这些连接器用于根据图像的语义复杂性对视觉token进行下采样。\n*   **训练过程**：在训练阶段，ViCO 通过最小化在不同MLP连接器条件下模型响应之间的KL散度来优化模型。这确保了即使在不同的压缩级别下，模型也能保持一致的语义理解。\n*   **推理阶段的图像路由器 (ViR)**：在推理时，引入了一个名为 **Visual Resolution Router (ViR)** 的图像路由器。ViR 能够自动为每个图像块选择最合适的压缩率，从而实现动态的视觉token调整。\n\n### 与现有方法的区别\n\n与现有主要根据图像分辨率调整视觉token数量的动态高分辨率策略不同，ViCO 的方法是根据图像的**语义复杂性**动态调整视觉token的数量。这意味着模型能够智能地识别图像中哪些部分需要更高分辨率的表示，而哪些部分可以进行更大幅度的压缩，从而实现更精细的资源分配。\n\n### 实验结果与贡献\n\n实验结果表明，ViCO 方法取得了显著的成效：\n\n*   **视觉token减少**：该方法能够将视觉token的数量减少高达50%。\n*   **能力保持**：在减少token数量的同时，模型依然能够保持其原有的感知、推理和光学字符识别（OCR）能力。\n\n本研究有望为开发更高效的MLLMs做出贡献。为了促进未来的研究，相关的代码和模型将对外发布。\n\n### 总结\n\nViCO 提供了一种创新的训练策略，通过语义感知的动态高分辨率处理，有效降低了MLLMs的推理成本，同时不牺牲其核心性能，为多模态AI领域的发展开辟了新的途径。",
      "shortSummary": "ViCO提出一种训练策略，通过根据图像语义复杂性动态调整视觉token数量，以降低多模态大型语言模型（MLLMs）的推理成本。该方法利用多个MLP连接器和Visual Resolution Router (ViR)在训练和推理阶段实现不同压缩比。实验表明，ViCO可在保持模型感知、推理和OCR能力的同时，将视觉token数量减少高达50%，从而显著提高MLLMs的效率。",
      "translated_title": "ViCO：一种面向语义感知动态高分辨率的训练策略",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research."
    },
    {
      "title": "UniFusion：视觉-语言模型作为图像生成中的统一编码器 (原标题: UniFusion: Vision-Language Model as Unified Encoder in Image Generation)",
      "link": "https://arxiv.org/abs/2510.12789",
      "pubDate": "Tue, 14 Oct 2025 13:57:56 GMT",
      "isoDate": "2025-10-14T13:57:56.000Z",
      "creator": "Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale",
      "summary": "## UniFusion：图像生成中的统一编码器\n\n### 核心问题\n\n当前视觉生成架构面临的主要挑战是，大多数模型为图像和文本使用独立的编码器。这种分离限制了扩散模型进行跨模态推理和知识迁移的能力。以往弥合这一差距的尝试存在以下局限：\n\n*   仅使用VLM（视觉-语言模型）的最后一层信息。\n*   采用多个视觉编码器。\n*   联合训练大型统一模型进行文本和图像生成，但这需要大量的计算资源和大规模数据。\n\n### UniFusion 方法\n\nUniFusion 提出了一种基于扩散的生成模型，其核心创新在于将一个**冻结的大型视觉-语言模型（VLM）**作为**统一的多模态编码器**来条件化生成过程。这意味着VLM同时处理文本和视觉信息，并将其编码为统一的表示。\n\n#### 关键机制：层级注意力池化（LAP）\n\nUniFusion 的核心是**层级注意力池化（Layerwise Attention Pooling, LAP）**机制。LAP 的作用是：\n\n*   从冻结VLM的文本和视觉token中提取**高层语义**和**低层细节**。\n*   利用这些提取的信息来条件化扩散生成模型。\n\n#### LAP 的优势\n\n研究表明，LAP 在以下方面优于其他浅层融合架构：\n\n*   **文本-图像对齐**：在生成任务中实现了更好的文本-图像对齐。\n*   **视觉信息忠实迁移**：能够将VLM中的视觉信息忠实地迁移到扩散模型中，这对于图像编辑任务至关重要。\n\n### VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI)\n\nUniFusion 还提出了 **VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI)** 机制。VERIFI 的特点是：\n\n*   在模型内部进行提示重写（in-model prompt rewriting）时，仅使用VLM生成的文本token来条件化扩散Transformer (DiT)。\n*   这种方法结合了条件分布的对齐能力与VLM的推理能力，从而在推理阶段增加了模型的性能和灵活性。\n\n### 泛化能力与知识迁移\n\n对UniFusion 进行编辑任务的微调不仅能改善生成时的文本-图像对齐（这表明了跨模态知识迁移的发生），而且还展现出卓越的泛化能力：\n\n*   当模型在单图像编辑任务上训练时，它能够**零样本泛化**到多图像引用场景。这进一步证明了UniFusion统一编码器设计的有效性。\n\n### 总结\n\nUniFusion 通过将冻结的VLM作为统一编码器，并引入LAP和VERIFI机制，有效解决了现有图像生成模型中图像和文本编码器分离的问题，显著提升了模型的跨模态推理、知识迁移、文本-图像对齐以及泛化能力，尤其在图像编辑方面表现出色。",
      "shortSummary": "UniFusion 提出了一种创新的扩散生成模型，它将冻结的视觉-语言模型（VLM）作为统一的多模态编码器。通过引入层级注意力池化（LAP）机制，UniFusion 能从VLM中提取高层语义和低层细节，有效提升文本-图像对齐和视觉信息迁移。此外，VLM-Enabled Rewriting Injection with Flexible Inference (VERIFI) 增强了推理的灵活性和能力。UniFusion 在编辑任务上展现出卓越的泛化能力，即使在单图像训练后也能零样本泛化到多图像引用，解决了传统模型中编码器分离的局限性。",
      "translated_title": "UniFusion：视觉-语言模型作为图像生成中的统一编码器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion."
    },
    {
      "title": "SRUM：统一多模态模型的细粒度自奖励 (原标题: SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models)",
      "link": "https://arxiv.org/abs/2510.12784",
      "pubDate": "Tue, 14 Oct 2025 13:56:11 GMT",
      "isoDate": "2025-10-14T13:56:11.000Z",
      "creator": "Weiyang Jin, Yuwei Niu, Jiaqi Liao, Chengqi Duan, Aoxue Li, Shenghua Gao, Xihui Liu",
      "summary": "## SRUM：统一多模态模型的细粒度自奖励框架\n\n### 1. 引言与背景\n\n*   **统一多模态模型（UMMs）的进展与挑战**：\n    *   近年来，UMMs在整合视觉-语言生成和理解能力方面取得了显著进展。\n    *   然而，UMMs存在一个明显的缺陷：模型强大的视觉理解能力往往无法有效转化为其视觉生成能力。\n    *   具体表现为，模型可能根据用户指令正确理解图像，但却无法根据文本提示生成忠实的图像。\n*   **核心问题**：模型能否通过利用其理解模块来奖励其生成模块，从而实现自我提升？\n\n### 2. SRUM框架介绍\n\n*   **框架目标**：为弥合理解与生成之间的鸿沟并实现模型自我提升，研究人员引入了SRUM。\n*   **SRUM的定义**：SRUM是一个自奖励的后训练框架，可以直接应用于现有各种设计的UMMs。\n*   **工作机制**：\n    *   SRUM创建了一个反馈循环，其中模型的理解模块充当内部的“评估器”。\n    *   该“评估器”为生成模块提供纠正信号，以改进其性能。\n    *   **关键优势**：整个过程无需额外的人工标注数据。\n\n### 3. 全局-局部双重奖励系统\n\n*   **设计目的**：为确保反馈的全面性，SRUM设计了一个全局-局部双重奖励系统。\n*   **多尺度指导**：该系统旨在解决图像固有的结构复杂性，并提供多尺度指导：\n    *   **全局奖励**：确保整体视觉语义和布局的正确性。\n    *   **局部奖励**：细化细粒度的、对象级别的忠实度。\n\n### 4. 实验结果与性能提升\n\n*   **能力与泛化性**：SRUM带来了强大的能力并展现出强大的泛化性。\n*   **性能提升示例**：\n    *   在T2I-CompBench基准测试上，性能从82.18提升至**88.37**。\n    *   在T2I-ReasonBench基准测试上，性能从43.82提升至**46.75**。\n\n### 5. 结论\n\n*   这项工作为UMMs建立了一个强大的新范式，使其理解模块能够通过自奖励来指导和增强自身的生成能力。",
      "shortSummary": "SRUM是一个为统一多模态模型（UMMs）设计的自奖励后训练框架，旨在弥合视觉理解与生成之间的鸿沟。它通过建立一个反馈循环，让模型的理解模块作为“评估器”来奖励和改进生成模块，无需人工标注数据。SRUM采用全局-局部双重奖励系统，分别确保整体语义布局和细粒度对象级别的忠实度。实验表明，SRUM显著提升了UMMs在文本到图像合成和推理任务上的性能，为UMMs的自我提升提供了一个新范式。",
      "translated_title": "SRUM：统一多模态模型的细粒度自奖励",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a global reward ensures the correctness of the overall visual semantics and layout, while a local reward refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding."
    },
    {
      "title": "如果：通过稀疏交互理解运动 (原标题: What If : Understanding Motion Through Sparse Interactions)",
      "link": "https://arxiv.org/abs/2510.12777",
      "pubDate": "Tue, 14 Oct 2025 13:52:17 GMT",
      "isoDate": "2025-10-14T13:52:17.000Z",
      "creator": "Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer",
      "summary": "### Flow Poke Transformer (FPT)：通过稀疏交互理解运动\n\n本文介绍了一种名为 Flow Poke Transformer (FPT) 的新型框架，旨在解决理解物理场景动态的问题，特别是局部交互如何导致场景变化。\n\n**核心问题与传统方法的局限性：**\n*   理解物理场景的动态变化，尤其是由局部交互引起的潜在变化，是一个复杂的问题。\n*   传统方法通常只能对场景动态的单一实现进行密集采样，难以提供对多模态运动、其对物理交互的依赖性以及场景动态固有不确定性的直接可解释表示。\n\n**FPT 框架的创新点：**\n*   **直接预测局部运动分布**：FPT 能够直接预测局部运动的分布，其预测是基于被称为“pokes”（稀疏交互）的条件。\n*   **可解释的多模态场景运动表示**：与传统方法不同，FPT 提供了一种可解释且直接可访问的多模态场景运动表示。\n*   **揭示对物理交互的依赖性**：FPT 能够清晰地展示场景运动对物理交互的依赖关系。\n*   **处理固有不确定性**：该框架还能有效处理场景动态中固有的不确定性。\n\n**模型评估与性能：**\n*   **密集人脸运动生成**：FPT 在密集人脸运动生成任务中，其通用预训练模型超越了专门的基线方法。\n*   **关节对象运动估计**：在强分布外任务（如合成数据集）上进行微调后，FPT 在关节对象运动估计方面比域内方法取得了显著改进。\n*   **移动部件分割**：通过直接预测显式运动分布，FPT 在从“pokes”进行移动部件分割等任务中也取得了具有竞争力的性能，进一步证明了其多功能性。\n\n**可用性：**\n*   FPT 的代码和模型已公开发布。",
      "shortSummary": "Flow Poke Transformer (FPT) 是一种新型框架，通过稀疏交互（“pokes”）直接预测局部运动的多模态分布。它提供可解释的场景动态表示，揭示运动对物理交互的依赖性及不确定性。FPT 在密集人脸运动生成、关节对象运动估计和移动部件分割等任务中表现出色，超越了专业基线并展现了强大的多功能性。代码和模型已公开。",
      "translated_title": "如果：通过稀疏交互理解运动",
      "images": [],
      "contentSource": "完整文章",
      "content": "Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed \"pokes\". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer."
    },
    {
      "title": "Dr.LLM: LLM中的动态层路由 (原标题: Dr.LLM: Dynamic Layer Routing in LLMs)",
      "link": "https://arxiv.org/abs/2510.12773",
      "pubDate": "Tue, 14 Oct 2025 13:51:26 GMT",
      "isoDate": "2025-10-14T13:51:26.000Z",
      "creator": "Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh",
      "summary": "### Dr.LLM: LLM中的动态层路由\n\n**问题背景**\n\n*   **现有LLM的局限性**：大型语言模型（LLMs）在处理每个token时，会通过Transformer堆栈的所有层。这导致在处理简单查询时计算资源浪费，而对于需要更深层次推理的复杂查询，则缺乏足够的灵活性。\n*   **现有自适应深度方法的不足**：虽然存在一些自适应深度方法可以提高效率，但它们通常依赖于成本高昂的推理时搜索、需要架构修改或大规模再训练，并且在实践中，尽管效率有所提高，但往往会降低准确性。\n\n**Dr.LLM框架介绍**\n\n*   **核心概念**：Dr.LLM是一个可追溯（retrofittable）的框架，旨在为预训练的LLM配备轻量级的逐层路由器。\n*   **路由器功能**：这些路由器能够决定是跳过、执行还是重复Transformer块。\n\n**Dr.LLM的设计与训练**\n\n*   **训练方法**：路由器通过显式监督进行训练。研究人员利用蒙特卡洛树搜索（MCTS）来推导出高质量的层配置，这些配置能够在计算预算下保持或提高准确性。\n*   **关键设计特点**：\n    *   **窗口池化（windowed pooling）**：用于实现稳定的路由。\n    *   **带类别平衡的焦点损失（focal loss with class balancing）**：解决类别不平衡问题。\n    *   **瓶颈MLP路由器（bottleneck MLP routers）**：确保在类别不平衡和长序列情况下的鲁棒性。\n\n**性能表现**\n\n*   **在特定任务上的提升**：\n    *   在ARC（逻辑）和DART（数学）任务上，Dr.LLM将准确性提高了高达+3.4%p，同时平均每个示例节省了5个层。\n*   **泛化能力**：\n    *   路由器能够泛化到域外任务（如MMLU、GSM8k、AIME、TruthfulQA、SQuADv2、GPQA、PIQA、AGIEval），仅导致0.85%的准确性下降，同时保持了效率。\n    *   Dr.LLM的性能比现有路由方法高出高达+7.7%p。\n\n**结论**\n\n*   Dr.LLM表明，通过显式监督的路由器，可以对冻结的LLM进行改造，以实现预算感知、准确性驱动的推理，而无需修改基础权重。",
      "shortSummary": "Dr.LLM引入了一种可追溯框架，通过轻量级逐层路由器，使预训练LLM能够动态跳过、执行或重复Transformer层。利用蒙特卡洛树搜索进行显式监督训练，Dr.LLM在ARC和DART任务上将准确性提高了高达+3.4%p，并平均节省5个层。它在域外任务上表现出强大的泛化能力，仅有0.85%的准确性下降，并优于现有方法。该方法实现了预算感知、准确性驱动的推理，且无需修改LLM的基础权重。",
      "translated_title": "Dr.LLM: LLM中的动态层路由",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights."
    },
    {
      "title": "FlashVSR：迈向实时扩散模型流式视频超分辨率 (原标题: FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution)",
      "link": "https://arxiv.org/abs/2510.12747",
      "pubDate": "Tue, 14 Oct 2025 13:25:54 GMT",
      "isoDate": "2025-10-14T13:25:54.000Z",
      "creator": "Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue",
      "summary": "### FlashVSR：迈向实时扩散模型流式视频超分辨率\n\n**背景与挑战**\n*   扩散模型在视频恢复领域取得了显著进展，但将其应用于实际视频超分辨率（VSR）仍面临挑战。\n*   主要问题包括：高延迟、巨大的计算开销以及对超高分辨率泛化能力差。\n\n**FlashVSR：解决方案**\n*   本文提出了FlashVSR，这是首个基于扩散模型的一步式流式框架，旨在实现实时VSR。\n*   其目标是使基于扩散模型的VSR变得实用，具备高效率、可扩展性和实时性能。\n\n**核心创新点**\nFlashVSR结合了三项互补的创新技术，使其能够在单个A100 GPU上以约17 FPS的速度处理768x1408视频：\n1.  **三阶段蒸馏管线**：一个对训练友好的三阶段蒸馏管线，实现了流式超分辨率。\n2.  **局部约束稀疏注意力**：通过削减冗余计算，同时弥合训练与测试分辨率之间的差距。\n3.  **微型条件解码器**：在不牺牲质量的前提下加速重建过程。\n\n**VSR-120K 新数据集**\n*   为了支持大规模训练，研究团队构建了VSR-120K，这是一个包含12万个视频和18万张图像的新数据集。\n\n**实验结果与性能**\n*   广泛的实验表明，FlashVSR能够可靠地扩展到超高分辨率。\n*   它实现了最先进的性能，并且比现有的一步式扩散VSR模型提速高达12倍。\n\n**未来展望**\n*   研究团队将发布代码、预训练模型和数据集，以促进未来在高效扩散模型VSR领域的研究。",
      "shortSummary": "FlashVSR是首个基于扩散模型的一步式流式视频超分辨率（VSR）框架，旨在实现实时性能。它通过三阶段蒸馏、局部约束稀疏注意力及微型条件解码器等创新，解决了扩散模型VSR的高延迟和计算开销问题。FlashVSR在单个A100 GPU上能以约17 FPS处理768x1408视频，并在超高分辨率下表现出最先进的性能，比现有模型提速高达12倍。研究团队还构建了VSR-120K数据集，并将发布代码和模型。",
      "translated_title": "FlashVSR：迈向实时扩散模型流式视频超分辨率",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR."
    },
    {
      "title": "SAIL-Embedding 技术报告：全模态嵌入基础模型 (原标题: SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model)",
      "link": "https://arxiv.org/abs/2510.12709",
      "pubDate": "Tue, 14 Oct 2025 12:43:22 GMT",
      "isoDate": "2025-10-14T12:43:22.000Z",
      "creator": "Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng",
      "summary": "## SAIL-Embedding 技术报告：全模态嵌入基础模型\n\n### 摘要\n\n本技术报告介绍了 SAIL-Embedding，一个旨在解决现有多模态嵌入模型在实际应用中面临挑战的全模态嵌入基础模型。这些挑战包括有限的模态支持、不稳定的训练机制以及工业领域差距。\n\n### 核心问题与解决方案\n\n*   **现有问题**：当前多模态嵌入模型（从基于 CLIP 的双塔架构到大型视觉-语言模型）在实际应用和商业场景中存在局限性，例如：\n    *   模态支持有限。\n    *   训练机制不稳定。\n    *   与工业领域存在差距。\n*   **SAIL-Embedding 解决方案**：通过定制的训练策略和架构设计，SAIL-Embedding 旨在解决上述问题，提供一个全模态的嵌入基础模型。\n\n### 训练策略与架构设计\n\nSAIL-Embedding 采用多阶段训练方案来提升表征学习的多方面有效性：\n\n1.  **内容感知渐进式训练 (Content-aware progressive training)**：\n    *   目标：增强模型对多样化下游任务的适应性，并掌握更丰富的跨模态能力。\n2.  **协作感知推荐增强训练 (Collaboration-aware recommendation enhancement training)**：\n    *   目标：通过从序列到项目（sequence-to-item）和 ID 到项目（ID-to-item）嵌入中提炼知识，并挖掘用户历史兴趣，进一步使多模态表征适应推荐场景。\n\n此外，模型还开发了以下机制来增强训练的灵活性和泛化能力：\n\n*   **随机专业化 (Stochastic specialization)**\n*   **数据集驱动的模式匹配 (Dataset-driven pattern matching)**\n\n### 实验结果与性能\n\nSAIL-Embedding 在多项实验中展现出卓越的性能：\n\n*   **检索任务**：在不同的检索任务中，SAIL-Embedding 取得了 SOTA（State-Of-The-Art）性能。\n*   **在线实验**：在与模型集成的各种真实世界场景中，观察到关键推荐体验指标——生命周期 (Lifetime, LT) 的显著提升。\n    *   **抖音精选 (Douyin-Selected) 场景**：\n        *   7 天 LT 增益：+0.158%\n        *   14 天 LT 增益：+0.144%\n    *   **抖音信息流排序模型 (Douyin feed rank model)**：\n        *   SAIL-Embedding 生成的匹配特征带来了 +0.08% 的 AUC 增益。\n\n### 结论\n\nSAIL-Embedding 通过其创新的训练策略和架构设计，成功克服了现有模型在多模态支持、训练稳定性和工业适用性方面的挑战，并在检索和推荐等实际应用中取得了显著的性能提升。",
      "shortSummary": "SAIL-Embedding 是一款全模态嵌入基础模型，旨在解决现有模型在模态支持、训练稳定性和工业应用中的局限性。它采用多阶段训练方案，包括内容感知渐进式训练和协作感知推荐增强训练，并结合随机专业化和数据集驱动的模式匹配，以提升模型适应性和泛化能力。实验表明，SAIL-Embedding 在检索任务中达到 SOTA 性能，并在抖音等真实场景中显著提升了推荐体验的关键指标，如生命周期（LT）和 AUC 增益。",
      "translated_title": "SAIL-Embedding 技术报告：全模态嵌入基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.08% AUC gain."
    },
    {
      "title": "ERA：通过具身先验学习和在线强化学习将VLM转化为具身智能体 (原标题: ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2510.12693",
      "pubDate": "Tue, 14 Oct 2025 12:25:46 GMT",
      "isoDate": "2025-10-14T12:25:46.000Z",
      "creator": "Hanyang Chen, Mark Zhao, Rui Yang, Qinwei Ma, Ke Yang, Jiarui Yao, Kangrui Wang, Hao Bai, Zhenhailong Wang, Rui Pan, Mengchao Zhang, Jose Barreiros, Aykut Onol, ChengXiang Zhai, Heng Ji, Manling Li, Huan Zhang, Tong Zhang",
      "summary": "# ERA：通过具身先验学习和在线强化学习将VLM转化为具身智能体\n\n## 引言\n\n具身人工智能（Embodied AI）的最新进展表明，视觉语言模型（VLM）在复杂环境中具备感知、推理和交互的巨大潜力。然而，目前性能卓越的系统往往依赖于部署成本高昂的大规模模型，而小型VLM则普遍缺乏成功所需的必要知识和技能。为了弥合这一差距，本文提出了ERA（Embodied Reasoning Agent）框架。\n\n## ERA框架概述\n\nERA是一个两阶段框架，旨在通过整合先验知识学习和在线强化学习（RL），将VLM转化为高效的具身智能体。该框架为实现可扩展的具身智能提供了一条实用路径。\n\n## 第一阶段：具身先验学习 (Embodied Prior Learning)\n\n此阶段旨在从三类数据中提取和蒸馏基础知识，为智能体提供必要的先验信息：\n\n1.  **轨迹增强先验 (Trajectory-Augmented Priors)**：\n    *   通过利用更强大的模型生成的结构化推理，丰富现有的轨迹数据。这有助于智能体学习更深层次的决策逻辑和行为模式。\n2.  **环境锚定先验 (Environment-Anchored Priors)**：\n    *   提供环境内的具体知识和基础监督。这确保了智能体能够准确理解其所处环境的特性和对象关系，从而进行更有效的交互。\n3.  **外部知识先验 (External Knowledge Priors)**：\n    *   从环境外的数据集中迁移通用知识。这使得智能体能够利用广泛的常识和世界知识，增强其泛化能力和推理能力。\n\n## 第二阶段：在线强化学习 (Online Reinforcement Learning)\n\n在第一阶段学习到丰富的先验知识后，第二阶段通过在线强化学习进一步提升智能体的性能。为了克服具身智能体RL固有的挑战，如长时序决策、稀疏奖励和训练不稳定等，ERA引入了三项关键设计：\n\n1.  **自我总结 (Self-summarization)**：\n    *   用于高效的上下文管理。智能体能够总结历史信息，从而在长时序任务中保持对关键信息的关注，避免信息过载。\n2.  **密集奖励塑形 (Dense reward shaping)**：\n    *   通过提供更频繁、更具体的奖励信号，解决稀疏奖励问题。这有助于智能体更快地学习正确的行为策略。\n3.  **轮次级策略优化 (Turn-level policy optimization)**：\n    *   优化策略更新，提高训练的稳定性和效率。这种优化方式使得智能体能够在每次交互轮次中更好地调整其行为策略。\n\n## 实验结果\n\n研究人员在高级规划任务（EB-ALFRED）和低级控制任务（EB-Manipulation）上进行了广泛的实验。结果表明：\n\n*   ERA-3B模型在性能上超越了基于提示的大型模型（如GPT-4o）以及之前的基于训练的基线。\n*   具体而言，ERA-3B在EB-ALFRED任务上实现了8.4%的整体性能提升，在EB-Manipulation任务上实现了19.4%的整体性能提升，均优于GPT-4o。\n*   ERA还对未见过的任务展现出强大的泛化能力。\n\n## 结论\n\nERA框架为实现可扩展的具身智能提供了一条实用且有效的方法。它通过结合具身先验学习和在线强化学习，显著提升了小型VLM在复杂具身任务中的表现，并为未来的具身AI系统提供了重要的方法论见解。",
      "shortSummary": "ERA是一个两阶段框架，通过具身先验学习和在线强化学习，将视觉语言模型（VLM）转化为具身智能体。它解决了小型VLM知识不足和大型模型部署成本高的问题。第一阶段从多源数据中学习基础知识，第二阶段通过自我总结、密集奖励塑形和轮次级策略优化克服RL挑战。实验表明，ERA-3B在具身规划和控制任务上显著优于GPT-4o及现有基线，展现出强大的泛化能力，为可扩展具身智能提供了实用路径。",
      "translated_title": "ERA：通过具身先验学习和在线强化学习将VLM转化为具身智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present Embodied Reasoning Agent (ERA), a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, Embodied Prior Learning, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems."
    },
    {
      "title": "记忆即行动：长周期智能体任务的自主上下文管理 (原标题: Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks)",
      "link": "https://arxiv.org/abs/2510.12635",
      "pubDate": "Tue, 14 Oct 2025 11:29:57 GMT",
      "isoDate": "2025-10-14T11:29:57.000Z",
      "creator": "Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, Jitao Sang",
      "summary": "## 记忆即行动：长周期智能体任务的自主上下文管理\n\n### 引言\n大型语言模型（LLMs）在执行长周期智能体任务时面临显著挑战。其主要问题在于有限的记忆容量容易被分散注意力或不相关的上下文信息所淹没。目前，现有的工作记忆管理方法通常依赖于外部的、启发式的机制，这些机制与智能体的核心策略是分离的，导致其效率和适应性受限。\n\n### 核心框架：记忆即行动 (Memory-as-Action, M-a-A)\n本文提出了一种名为“记忆即行动”的新颖框架，将工作记忆管理重新定义为智能体的一种可学习的、内在能力。在该框架下：\n*   智能体通过执行明确的记忆编辑操作来主动管理其工作记忆。\n*   这些记忆编辑操作被整合为智能体统一策略的一部分。\n*   通过强化学习进行训练，智能体能够在其给定的资源限制下，平衡记忆管理与长期的任务目标。\n\n### 技术挑战：轨迹断裂 (Trajectory Fractures)\n“记忆即行动”框架引入了新的挑战：\n*   记忆编辑操作打破了LLM交互中上下文连续增长的标准假设。\n*   这种非前缀（non-prefix）的变化导致了所谓的“轨迹断裂”（trajectory fractures）。\n*   轨迹断裂破坏了标准策略梯度方法所需的因果连续性，使得这些传统方法不再适用。\n\n### 解决方案：动态上下文策略优化 (Dynamic Context Policy Optimization, DCPO)\n为了解决轨迹断裂问题，本文提出了一种新的算法——动态上下文策略优化（DCPO）：\n*   DCPO通过在记忆动作点对轨迹进行分割。\n*   然后，将轨迹级别的优势应用于由此产生的动作片段。\n*   这种方法能够实现稳定的端到端强化学习，从而有效处理记忆编辑带来的非连续性。\n\n### 研究成果与优势\n研究结果表明，以端到端的方式联合优化任务推理和记忆管理带来了多重优势：\n*   **降低计算消耗**：整体计算消耗显著减少。\n*   **提高任务性能**：通过根据模型内在能力量身定制的自适应上下文管理策略，任务性能得到显著提升。\n\n### 研究领域\n本文研究属于人工智能（cs.AI）领域。",
      "shortSummary": "大型语言模型在长周期任务中因记忆限制和冗余上下文而受阻。本文提出“记忆即行动”（M-a-A）框架，将记忆管理作为智能体统一策略中的可学习动作。为解决记忆编辑导致的“轨迹断裂”问题，引入“动态上下文策略优化”（DCPO）算法，实现稳定的端到端强化学习。研究表明，联合优化任务推理和记忆管理能降低计算消耗并提高任务性能。",
      "translated_title": "记忆即行动：长周期智能体任务的自主上下文管理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities."
    },
    {
      "title": "通过自监督预训练推进端到端像素空间生成建模 (原标题: Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training)",
      "link": "https://arxiv.org/abs/2510.12586",
      "pubDate": "Tue, 14 Oct 2025 10:41:16 GMT",
      "isoDate": "2025-10-14T10:41:16.000Z",
      "creator": "Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu",
      "summary": "### 通过自监督预训练推进端到端像素空间生成建模\n\n**1. 背景与问题陈述**\n\n*   像素空间生成模型在训练难度和性能上通常不如潜在空间模型，导致两者之间存在显著的性能和效率差距。\n\n**2. 提出的解决方案：两阶段训练框架**\n\n*   本文介绍了一种新颖的两阶段训练框架，旨在弥合像素空间扩散模型和一致性模型与潜在空间模型之间的差距。\n*   **第一阶段：自监督预训练编码器**\n    *   目标：预训练编码器，使其能够从干净图像中捕获有意义的语义信息。\n    *   方法：将编码器输出与沿确定性采样轨迹的点对齐，该轨迹将点从先验分布演化到数据分布。\n*   **第二阶段：端到端微调**\n    *   将预训练的编码器与随机初始化的解码器集成。\n    *   对完整的模型进行端到端微调，适用于扩散模型和一致性模型。\n\n**3. 实验结果与性能**\n\n*   该训练框架在ImageNet数据集上展示了强大的经验性能。\n*   **扩散模型表现：**\n    *   在ImageNet-256上实现了2.04的FID（Fréchet Inception Distance）。\n    *   在ImageNet-512上以75次函数评估（NFE）实现了2.35的FID。\n    *   在生成质量和效率方面，大幅超越了先前的像素空间方法。\n    *   在可比的训练成本下，性能可与领先的基于VAE的模型相媲美。\n*   **一致性模型表现：**\n    *   在ImageNet-256上，通过单步采样实现了令人印象深刻的8.82的FID。\n    *   显著超越了其潜在空间对应模型。\n    *   这是首次成功地在不依赖预训练VAE或扩散模型的情况下，直接在高分辨率图像上训练一致性模型。\n\n**4. 研究领域**\n\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "本文提出了一种新颖的两阶段训练框架，旨在弥合像素空间生成模型与潜在空间模型之间的性能差距。该框架首先自监督预训练编码器以捕获语义，然后将编码器与解码器集成并进行端到端微调。实验结果表明，该方法在ImageNet数据集上取得了显著进展，其扩散模型在生成质量和效率上超越了现有像素空间方法，并与领先的VAE模型相当。此外，它首次成功地直接在高分辨率图像上训练了一致性模型，性能优于其潜在空间对应模型。",
      "translated_title": "通过自监督预训练推进端到端像素空间生成建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models."
    },
    {
      "title": "机器人学习：教程 (原标题: Robot Learning: A Tutorial)",
      "link": "https://arxiv.org/abs/2510.12403",
      "pubDate": "Tue, 14 Oct 2025 07:36:46 GMT",
      "isoDate": "2025-10-14T07:36:46.000Z",
      "creator": "Francesco Capuano, Caroline Pascal, Adil Zouitine, Thomas Wolf, Michel Aractingi",
      "summary": "## 机器人学习：教程概述\n\n### 引言\n机器人学习正处于一个关键的转折点，这主要得益于机器学习技术的飞速进步以及大规模机器人数据的日益丰富。这一趋势标志着机器人领域正从传统的、基于模型的方法向数据驱动的、基于学习的范式转变，从而为自主系统带来了前所未有的能力。\n\n### 教程内容与范围\n本教程旨在全面介绍现代机器人学习的现状，其内容涵盖了从基础原则到前沿模型：\n\n*   **基础原则**：深入探讨强化学习（Reinforcement Learning, RL）和行为克隆（Behavioral Cloning）等核心概念。\n*   **通用模型**：进一步扩展到能够跨越多种任务甚至不同机器人实体进行操作的通用型、语言条件模型。\n\n### 目标受众与目的\n本教程主要面向研究人员和实践者，其核心目标是：\n\n*   **概念理解**：帮助读者建立对机器人学习领域深刻的理论认知。\n*   **实践工具**：为读者提供必要的实践工具，使其能够积极参与机器人学习领域的发展。\n*   **实用示例**：教程中包含了在`lerobot`库中实现的即用型示例，便于读者进行实际操作和学习。\n\n### 技术栈\n本教程特别利用了`LeRobot`，这是一个由Hugging Face开发的端到端机器人学习库，为读者提供了强大的实践平台。",
      "shortSummary": "机器人学习正经历范式转变，从传统模型转向数据驱动方法，极大提升了自主系统能力。本教程旨在为研究人员和实践者提供现代机器人学习的全面指导，涵盖从强化学习到通用语言条件模型，并提供基于Hugging Face的LeRobot库的实践示例，以帮助读者掌握概念和工具。",
      "translated_title": "机器人学习：教程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in lerobot."
    },
    {
      "title": "大型语言模型Vibe编程综述 (原标题: A Survey of Vibe Coding with Large Language Models)",
      "link": "https://arxiv.org/abs/2510.12399",
      "pubDate": "Tue, 14 Oct 2025 07:26:56 GMT",
      "isoDate": "2025-10-14T07:26:56.000Z",
      "creator": "Yuyao Ge, Lingrui Mei, Zenghao Duan, Tianhao Li, Yujia Zheng, Yiwei Wang, Lexin Wang, Jiayu Yao, Tianyu Liu, Yujun Cai, Baolong Bi, Fangda Guo, Jiafeng Guo, Shenghua Liu, Xueqi Cheng",
      "summary": "# 大型语言模型Vibe编程综述\n\n## 引言：Vibe编程的兴起与挑战\n\n大型语言模型（LLMs）的进步正在推动软件开发从代码生成辅助向自主编码代理的范式转变，催生了一种名为“Vibe编程”的新型开发方法。在这种方法中，开发者通过观察结果来验证AI生成的实现，而非逐行理解代码。尽管Vibe编程具有变革性潜力，但其有效性尚未得到充分探索，现有经验证据揭示了意外的生产力损失以及人机协作中的根本性挑战。\n\n## 综述目的与方法\n\n为了弥补这一空白，本综述首次对基于大型语言模型的Vibe编程进行了全面而系统的审查，为这一变革性开发方法奠定了理论基础并提供了实践框架。\n\n*   **数据来源**：本研究系统分析了1000多篇研究论文。\n*   **研究范围**：综述涵盖了整个Vibe编程生态系统，包括以下关键基础设施组件：\n    *   用于编码的LLMs\n    *   基于LLM的编码代理\n    *   编码代理的开发环境\n    *   反馈机制\n\n## Vibe编程的理论基础\n\n本综述通过以下方式将Vibe编程确立为一个正式的学科：\n\n*   **形式化定义**：通过一个“受约束马尔可夫决策过程”（Constrained Markov Decision Process）对其进行形式化，该过程捕捉了人类开发者、软件项目和编码代理之间动态的三方关系。\n\n## Vibe编程的实践框架与分类\n\n在理论基础之上，本综述将现有实践综合为五种不同的开发模型，首次在该领域提供了全面的分类法：\n\n1.  **无约束自动化模型（Unconstrained Automation）**\n2.  **迭代对话协作模型（Iterative Conversational Collaboration）**\n3.  **规划驱动模型（Planning-Driven）**\n4.  **测试驱动模型（Test-Driven）**\n5.  **上下文增强模型（Context-Enhanced Models）**\n\n## 关键发现与成功要素\n\n本综述的分析揭示，成功的Vibe编程不仅仅依赖于代理的能力，更关键的是需要：\n\n*   **系统性的上下文工程（Systematic Context Engineering）**\n*   **完善的开发环境（Well-established Development Environments）**\n*   **有效的人机协作开发模型（Human-agent Collaborative Development Models）**\n\n这些因素共同构成了Vibe编程成功的基石。",
      "shortSummary": "本综述首次全面系统地审查了基于大型语言模型（LLMs）的“Vibe编程”范式。Vibe编程通过观察结果验证AI代码，而非逐行审查。尽管其潜力巨大，但面临生产力损失和人机协作挑战。该研究通过受约束马尔可夫决策过程形式化了Vibe编程，并提出了五种开发模型分类。关键发现是，Vibe编程的成功不仅取决于代理能力，更依赖于系统性上下文工程、完善的开发环境以及人机协作模型。",
      "translated_title": "大型语言模型Vibe编程综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models."
    },
    {
      "title": "空间强制：视觉-语言-动作模型的隐式空间表示对齐 (原标题: Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model)",
      "link": "https://arxiv.org/abs/2510.12276",
      "pubDate": "Tue, 14 Oct 2025 04:27:10 GMT",
      "isoDate": "2025-10-14T04:27:10.000Z",
      "creator": "Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, Haoang Li",
      "summary": "# 空间强制：视觉-语言-动作模型的隐式空间表示对齐\n\n## 背景与问题\n\n*   **VLA模型的潜力与局限**：\n    *   视觉-语言-动作 (VLA) 模型在使机器人遵循语言指令和执行精确动作方面展现出巨大潜力。\n    *   然而，大多数VLA模型基于仅在2D数据上预训练的视觉-语言模型构建，导致它们缺乏准确的空间感知能力，从而阻碍了其在3D物理世界中的操作。\n\n*   **现有解决方案的挑战**：\n    *   **显式3D传感器输入**：如深度图或点云，面临传感器噪声、硬件异构性和现有数据集中深度覆盖不完整等挑战。\n    *   **从2D图像估计3D线索**：受限于深度估计器的性能。\n\n## 提出的方法：空间强制 (Spatial Forcing, SF)\n\n*   **核心思想**：SF是一种简单而有效的对齐策略，它隐式地强制VLA模型发展空间理解能力，而无需依赖显式3D输入或深度估计器。\n*   **工作原理**：SF将VLA模型的中间视觉嵌入与预训练3D基础模型生成的几何表示进行对齐。\n*   **关键优势**：通过在中间层强制对齐，SF引导VLA模型编码更丰富的空间表示，从而显著增强动作性能。\n\n## 实验结果与贡献\n\n*   **性能卓越**：在模拟和真实世界环境中的实验表明，SF取得了最先进的结果，超越了基于2D和3D的VLA模型。\n*   **训练效率提升**：SF显著加速了训练过程，最高可达3.8倍。\n*   **数据效率提高**：SF提高了在各种机器人任务中的数据效率。",
      "shortSummary": "视觉-语言-动作 (VLA) 模型因缺乏空间感知能力而难以在3D世界中操作。现有3D输入或估计方法存在局限。本文提出“空间强制 (Spatial Forcing, SF)”策略，通过将VLA模型的中间视觉嵌入与预训练3D基础模型的几何表示进行隐式对齐，使其无需显式3D输入即可发展空间理解能力。实验证明，SF取得了最先进的性能，超越了现有VLA模型，并显著加速训练和提高数据效率。",
      "translated_title": "空间强制：视觉-语言-动作模型的隐式空间表示对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth estimators.We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action precision.Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/"
    },
    {
      "title": "HoneyBee：视觉-语言推理器的数据配方 (原标题: HoneyBee: Data Recipes for Vision-Language Reasoners)",
      "link": "https://arxiv.org/abs/2510.12225",
      "pubDate": "Tue, 14 Oct 2025 03:23:44 GMT",
      "isoDate": "2025-10-14T03:23:44.000Z",
      "creator": "Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, Ramakanth Pasunuru",
      "summary": "### HoneyBee：视觉-语言推理器的数据配方研究\n\n**引言**\n\n尽管视觉-语言模型（VLM）在推理任务中取得了显著进展，但如何构建高性能的视觉-语言推理训练数据集的底层原则仍未被充分理解。本研究旨在通过引入多种数据整理方法，并严格控制训练和评估设置，系统地探究这些方法对视觉-语言推理能力的影响。\n\n**研究方法与发现**\n\n本研究深入分析了数据整理策略对VLM性能的影响，具体包括：\n\n*   **上下文来源分析：** 研究了图像和问题对的来源如何影响VLM的推理表现。\n*   **有针对性的数据干预：** 实施了多种干预措施，以增强模型的推理能力，例如：\n    *   利用图像标题中的辅助信号。\n    *   纳入纯文本推理任务。\n*   **数据维度扩展：** 探索了在不同数据维度上进行扩展的效果，包括增加图像数量、问题数量以及思维链（CoT）解决方案的数量。\n\n研究得出了以下关键发现：\n\n1.  **上下文来源策略**对VLM的性能具有显著影响。\n2.  通过**辅助信号**（如图像标题）和**纯文本推理**等干预措施，可以获得实质性的性能提升。\n3.  **扩展所有数据维度**（例如，每张图像的独特问题数量，每个图像-问题对的独特思维链数量）能够持续提高模型的推理能力。\n\n**HoneyBee数据集**\n\n受上述研究洞察的启发，本研究推出了**HoneyBee**，一个大规模、高质量的思维链推理数据集。该数据集包含：\n\n*   250万个示例。\n*   35万个独特的图像-问题对。\n\n使用HoneyBee训练的VLM在不同模型规模下均超越了现有最先进的模型。例如，一个拥有30亿参数的HoneyBee训练VLM在MathVerse基准测试上，比最先进模型高出7.8%，比基础模型高出24.8%。\n\n**测试时扩展策略**\n\n除了数据集的贡献，本研究还提出了一种创新的测试时扩展策略。该策略能够在不牺牲模型准确性的前提下，将解码成本大幅降低73%。\n\n**总结**\n\n本工作为视觉-语言推理数据集的整理研究提供了改进的策略，并通过HoneyBee数据集和测试时扩展策略，为提升VLM的推理能力和效率提供了新的方向。",
      "shortSummary": "本研究探讨了构建高性能视觉-语言（VL）推理训练数据集的原则。通过分析上下文来源、实施数据干预和扩展数据维度，研究发现这些因素显著影响VLM性能。基于这些洞察，研究团队推出了大规模、高质量的HoneyBee思维链推理数据集，包含250万个示例。使用HoneyBee训练的VLM在各项基准测试中超越了现有最先进模型。此外，还提出了一种测试时扩展策略，可在保持准确性的同时大幅降低解码成本。",
      "translated_title": "HoneyBee：视觉-语言推理器的数据配方",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research."
    },
    {
      "title": "一生学习：从无引导探索中推断随机环境的符号世界模型 (原标题: One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration)",
      "link": "https://arxiv.org/abs/2510.12088",
      "pubDate": "Mon, 13 Oct 2025 22:49:32 GMT",
      "isoDate": "2025-10-13T22:49:32.000Z",
      "creator": "Zaid Khan, Archiki Prasad, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal",
      "summary": "### OneLife：从无引导探索中学习随机环境的符号世界模型\n\n本文介绍了一个名为 OneLife 的框架，旨在解决在复杂、随机环境中学习符号世界模型的挑战，其中智能体仅有“一次生命”进行无引导探索。与以往主要关注确定性、数据丰富且有人工指导环境的研究不同，OneLife 致力于更具现实意义和挑战性的场景。\n\n#### 核心问题\n\n*   **现有研究局限：** 符号世界建模通常假设环境是确定性的，拥有大量的交互数据，机制简单，并且有人类指导。这与现实世界中智能体面临的复杂、随机且探索受限的环境不符。\n*   **本文目标：** 在智能体仅有“一次生命”探索敌对环境且无人类指导的情况下，推断并表示环境的过渡动态为可执行程序。\n\n#### OneLife 框架\n\nOneLife 框架通过以下机制建模世界动态：\n\n*   **条件激活的程序化法则：** OneLife 在概率编程框架内，通过条件激活的程序化法则来建模世界动态。\n*   **前置条件-效果结构：** 每个法则都采用“前置条件-效果”结构，仅在相关的世界状态下激活。这种设计确保了法则的适用性和效率。\n*   **动态计算图：** 这种结构创建了一个动态计算图，只通过相关的法则进行推理和优化。这带来了显著优势：\n    *   **避免扩展性挑战：** 当所有法则都对复杂、分层状态的预测做出贡献时，传统的模型会面临扩展性问题。OneLife 通过仅激活相关法则，有效避免了这一问题。\n    *   **学习随机动态：** 即使在规则激活稀疏的情况下，OneLife 也能有效地学习随机动态。\n\n#### 评估协议与环境\n\n为了在严苛的约束下评估 OneLife，本文引入了一个新的评估协议，衡量两个关键能力：\n\n*   **状态排序 (State Ranking)：** 区分合理未来状态与不合理未来状态的能力。\n*   **状态保真度 (State Fidelity)：** 生成与现实高度相似的未来状态的能力。\n\nOneLife 在 Crafter-OO 环境中进行开发和评估。Crafter-OO 是 Crafter 环境的重新实现，它暴露了结构化的、面向对象的符号状态，以及一个纯粹作用于该状态的转换函数。\n\n#### 实验结果与意义\n\n*   **学习能力：** OneLife 能够从最少的、无引导的交互中成功学习关键环境动态。\n*   **性能优势：** 在测试的 23 种场景中，OneLife 在 16 种场景中表现优于强大的基线模型。\n*   **规划能力：** OneLife 的规划能力也得到了验证，通过模拟推演成功识别出更优的策略。\n\n本工作为自主构建未知复杂环境的程序化世界模型奠定了基础，展示了在资源受限和高随机性环境下学习复杂动态的可行性。",
      "shortSummary": "本文介绍了 OneLife 框架，旨在解决在复杂、随机且无引导探索环境下学习符号世界模型的挑战。OneLife 利用概率编程中的条件激活程序化法则，通过前置条件-效果结构和动态计算图，有效避免了扩展性问题，并能从稀疏规则激活中学习随机动态。在 Crafter-OO 环境中，OneLife 表现优于基线，并展示了成功的规划能力，为自主构建未知复杂环境的程序化世界模型奠定了基础。",
      "translated_title": "一生学习：从无引导探索中推断随机环境的符号世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Symbolic world modeling requires inferring and representing an environment's transitional dynamics as an executable program. Prior work has focused on largely deterministic environments with abundant interaction data, simple mechanics, and human guidance. We address a more realistic and challenging setting, learning in a complex, stochastic environment where the agent has only \"one life\" to explore a hostile environment without human guidance. We introduce OneLife, a framework that models world dynamics through conditionally-activated programmatic laws within a probabilistic programming framework. Each law operates through a precondition-effect structure, activating in relevant world states. This creates a dynamic computation graph that routes inference and optimization only through relevant laws, avoiding scaling challenges when all laws contribute to predictions about a complex, hierarchical state, and enabling the learning of stochastic dynamics even with sparse rule activation. To evaluate our approach under these demanding constraints, we introduce a new evaluation protocol that measures (a) state ranking, the ability to distinguish plausible future states from implausible ones, and (b) state fidelity, the ability to generate future states that closely resemble reality. We develop and evaluate our framework on Crafter-OO, our reimplementation of the Crafter environment that exposes a structured, object-oriented symbolic state and a pure transition function that operates on that state alone. OneLife can successfully learn key environment dynamics from minimal, unguided interaction, outperforming a strong baseline on 16 out of 23 scenarios tested. We also test OneLife's planning ability, with simulated rollouts successfully identifying superior strategies. Our work establishes a foundation for autonomously constructing programmatic world models of unknown, complex environments."
    },
    {
      "title": "LLM推理在机器翻译中的应用：基于思维令牌的合成数据生成 (原标题: LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens)",
      "link": "https://arxiv.org/abs/2510.11919",
      "pubDate": "Mon, 13 Oct 2025 16:41:01 GMT",
      "isoDate": "2025-10-13T16:41:01.000Z",
      "creator": "Armel Zebaze, Rachel Bawden, Benoît Sagot",
      "summary": "本文探讨了大型推理模型（LRMs）在机器翻译（MT）任务中生成中间“思维令牌”的益处。\n\n**研究背景与目的：**\n*   大型推理模型（LRMs）通过在回答查询前制定自然语言思维过程，在解决问题方面展现出新潜力。\n*   尽管LRMs在数学和编码任务中的能力已广为人知，但其对机器翻译（MT）任务的影响尚未得到充分探索。\n*   本研究旨在探索在不同资源水平的多个语言对和多种设置下，执行MT时生成中间令牌（即“思维令牌”）的益处。\n\n**主要发现：**\n*   **“思维令牌”的局限性：** 研究发现，“思维令牌”并不能帮助LRMs更好地执行机器翻译。这一结果适用于多种语言对和设置。\n*   **微调效果不佳：** 即使是使用受人类译者实践启发的蒸馏思维链（CoT）进行微调，让模型在翻译前进行推理，也未能改善MT性能。\n    *   具体而言，使用详细描述如何逐步翻译的合成CoT解释来微调模型，其表现并未优于标准的输入-输出微调方法。\n*   **有效策略：** 然而，通过结合模块化、特定于翻译的提示策略的输出来构建中间令牌，确实带来了性能提升。\n*   **关键洞察：** 中间令牌在微调过程中的贡献高度依赖于其中是否包含实际的翻译尝试。\n\n**更广泛的启示：**\n*   研究结果表明，使用“教师模型”来完善目标翻译或扩展并行语料库，比将它们的CoT解释蒸馏到“思考型”MT模型中更具影响力。",
      "shortSummary": "本研究探讨了大型推理模型（LRMs）在机器翻译（MT）中使用“思维令牌”的有效性。结果显示，单纯的“思维令牌”或基于合成思维链（CoT）的微调并不能提升MT性能。然而，若中间令牌包含实际的翻译尝试，则能带来改进。这表明，直接通过教师模型完善翻译或扩展语料库，比将CoT解释蒸馏到MT模型中更为有效。",
      "translated_title": "LLM推理在机器翻译中的应用：基于思维令牌的合成数据生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that \"thinking tokens\" do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators' practices. Specifically, fine-tuning a model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using a teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into \"thinking\" MT models."
    },
    {
      "title": "CodePlot-CoT：通过代码驱动图像进行数学视觉推理 (原标题: CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images)",
      "link": "https://arxiv.org/abs/2510.11718",
      "pubDate": "Mon, 13 Oct 2025 13:59:55 GMT",
      "isoDate": "2025-10-13T13:59:55.000Z",
      "creator": "Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, Hongsheng Li, Yi Ma, Xihui Liu",
      "summary": "## CodePlot-CoT：通过代码驱动图像进行数学视觉推理\n\n### 引言\n\n大型语言模型（LLMs）和视觉语言模型（VLMs）在数学推理方面取得了显著进展。然而，它们在处理需要视觉辅助（例如，绘制辅助线或函数图）的数学问题时，仍面临关键瓶颈。大多数LLMs和VLMs局限于纯文本推理链，而能生成交错文本和图像的多模态统一模型，在这些任务中又缺乏必要的精度和可控性。\n\n### CodePlot-CoT 方法\n\n为解决这一挑战，研究人员提出了 **CodePlot-CoT**，这是一种代码驱动的思维链（Chain-of-Thought）范式，旨在实现数学领域的“通过图像思考”。该方法的核心机制如下：\n\n*   **VLM生成**：利用视觉语言模型（VLM）生成文本推理过程。\n*   **代码生成**：同时，VLM还生成可执行的绘图代码。\n*   **视觉思维**：这些绘图代码随后被渲染成图像，作为“视觉思维”（visual thought），以辅助解决复杂的数学问题。\n\n### 核心贡献与实现\n\n为了实现CodePlot-CoT并验证其有效性，研究团队完成了以下关键工作：\n\n1.  **构建Math-VR数据集**：\n    *   首次构建了大规模、双语的数学视觉推理（Mathematics problems with Visual Reasoning, Math-VR）数据集和基准。\n    *   该数据集包含17.8万个样本，为需要视觉辅助的数学问题提供了丰富的资源。\n2.  **开发图像到代码转换器**：\n    *   开发了一个最先进的图像到代码转换器，专门用于将复杂的数学图形解析成可执行代码。\n    *   这对于生成高质量的训练数据至关重要。\n3.  **训练CodePlot-CoT模型**：\n    *   利用上述构建的Math-VR数据集和通过图像到代码转换器生成的高质量训练数据，训练了CodePlot-CoT模型来解决数学问题。\n\n### 实验结果与影响\n\n*   **显著性能提升**：实验结果表明，CodePlot-CoT模型在新基准测试上比基础模型性能提升高达21%。这充分验证了所提出的代码驱动推理范式的有效性。\n*   **开辟新方向**：这项工作为多模态数学推理开辟了新方向，展示了通过结合代码生成和视觉渲染来增强模型推理能力的可行性。\n*   **资源公开**：为促进未来的研究，研究团队已将数据集、代码和预训练模型公开，可在 [this https URL](this%20https%20URL) 获取。\n\nCodePlot-CoT为解决需要视觉辅助的数学问题提供了一个强大的新方法，并为社区提供了首个大规模数据集、全面的基准和有效的解决方案。",
      "shortSummary": "CodePlot-CoT提出了一种代码驱动的思维链范式，通过生成可执行绘图代码并将其渲染为“视觉思维”图像，解决了LLMs和VLMs在需要视觉辅助的数学推理中的瓶颈。研究团队构建了首个大规模双语数学视觉推理数据集Math-VR（17.8万样本），并开发了图像到代码转换器。实验证明，CodePlot-CoT模型在新基准上比基础模型性能提升高达21%，验证了其有效性，并为多模态数学推理开辟了新方向。",
      "translated_title": "CodePlot-CoT：通过代码驱动图像进行数学视觉推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT."
    },
    {
      "title": "大型推理模型是否可中断？ (原标题: Are Large Reasoning Models Interruptible?)",
      "link": "https://arxiv.org/abs/2510.11713",
      "pubDate": "Mon, 13 Oct 2025 13:59:35 GMT",
      "isoDate": "2025-10-13T13:59:35.000Z",
      "creator": "Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez",
      "summary": "# 大型推理模型在动态环境下的鲁棒性评估\n\n## 引言\n大型推理模型（LRMs）在处理复杂推理任务方面表现卓越。然而，它们传统上是在“静态、冻结世界”的假设下进行评估的，即模型响应被认为是瞬时的，并且请求的上下文在响应期间保持不变。虽然这种假设对于短期任务通常成立，但在现代推理任务（如辅助编程）中，它便不再适用。在这些任务中，模型可能需要数小时进行思考，并且从模型开始思考到最终输出，代码或上下文可能会发生显著变化。\n\n## 研究目的与方法\n本研究旨在挑战“冻结世界”的假设，并在两种现实的动态场景下评估LRM的鲁棒性：\n*   **中断：** 评估模型在有限预算下其部分输出的质量。\n*   **动态上下文：** 评估模型对进行中上下文变化的适应能力。\n\n研究使用了需要长篇推理的数学和编程基准来测试模型。\n\n## 主要发现\n*   **鲁棒性被高估：** 静态评估始终高估了LRM的鲁棒性。即使是最先进的LRMs，在静态设置中能达到高准确率，但在被中断或暴露于变化的上下文时，其表现会变得不可预测。\n*   **性能显著下降：** 当更新在推理过程后期引入时，模型的性能下降幅度可高达60%。\n\n## 新颖的故障模式\n本研究进一步揭示了几种新颖的故障模式：\n*   **推理泄露 (Reasoning leakage)：** 当模型被中断时，它会将部分推理过程融入到最终答案中。\n*   **恐慌 (Panic)：** 在时间压力下，模型会完全放弃推理，并返回不正确的答案。\n*   **自我怀疑 (Self-doubt)：** 在整合更新信息时，模型的性能反而会下降。\n\n## 结论\n这项研究表明，大型推理模型在动态、非静态环境中的表现远不如在传统静态评估中那样稳定。它强调了在更贴近实际应用场景的动态条件下评估和改进LRM的必要性，以确保它们在现实世界任务中的可靠性和有效性。",
      "shortSummary": "大型推理模型（LRMs）传统上在静态环境中评估，但本研究挑战了这一“冻结世界”假设。通过在中断和动态上下文的现实场景中评估LRMs，发现静态评估严重高估了模型的鲁棒性。即使是先进模型，在动态变化下性能也会下降高达60%，尤其当更新在推理后期引入时。研究还揭示了推理泄露、恐慌和自我怀疑等新故障模式，强调了在动态环境中重新评估和改进LRMs的重要性。",
      "translated_title": "大型推理模型是否可中断？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information."
    }
  ],
  "lastUpdated": "2025-10-15T09:36:58.256Z"
}