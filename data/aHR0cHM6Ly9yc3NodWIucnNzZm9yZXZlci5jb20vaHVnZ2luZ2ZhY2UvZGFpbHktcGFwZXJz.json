{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "RPG：一种用于统一和可扩展代码库生成的存储库规划图 (原标题: RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation)",
      "link": "https://arxiv.org/abs/2509.16198",
      "pubDate": "Fri, 19 Sep 2025 13:58:14 GMT",
      "isoDate": "2025-09-19T13:58:14.000Z",
      "creator": "Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, Mao Yang",
      "summary": "## RPG：统一和可扩展代码库生成的存储库规划图\n\n### 核心问题\n\n大型语言模型（LLM）在函数级和文件级代码生成方面表现出色，但从零开始生成完整的软件存储库仍然是一个根本性挑战。这一过程需要跨提案级和实现级阶段进行连贯可靠的规划。然而，自然语言由于其模糊性和冗长性，不适合忠实地表示复杂的软件结构。\n\n### 解决方案：存储库规划图（RPG）\n\n为了解决上述问题，研究人员引入了**存储库规划图（RPG）**。RPG是一种持久化的表示，它通过在一个图中编码能力、文件结构、数据流和函数，统一了提案级和实现级规划。RPG用明确的蓝图取代了模糊的自然语言，从而实现了长期的规划和可扩展的存储库生成。\n\n### 框架：ZeroRepo\n\n基于RPG，研究人员开发了**ZeroRepo**，这是一个图驱动的框架，用于从零开始生成存储库。ZeroRepo的运作分为三个阶段：\n\n1.  **提案级规划**：构建初始的存储库规划图。\n2.  **实现级细化**：进一步完善和细化规划图。\n3.  **图引导的代码生成**：根据规划图生成代码，并结合测试验证。\n\n### 评估：RepoCraft 基准测试\n\n为了评估ZeroRepo在这一设置下的性能，研究人员构建了一个名为**RepoCraft**的基准测试集。RepoCraft包含六个真实世界的项目，共计1052个任务。\n\n### 性能与结果\n\n在RepoCraft基准测试中，ZeroRepo取得了显著的成果：\n\n*   **代码行数（LOC）**：ZeroRepo生成的存储库平均代码行数接近36K LOC，大约是最强基线（Claude Code）的3.9倍，是其他基线的约64倍。\n*   **功能覆盖率**：ZeroRepo达到了81.5%的功能覆盖率。\n*   **通过率**：ZeroRepo的通过率为69.7%。\n*   **与Claude Code对比**：ZeroRepo的功能覆盖率和通过率分别比Claude Code高出27.3和35.8个百分点。\n\n### 进一步分析\n\n深入分析表明，RPG具有以下优势：\n\n*   能够建模复杂的依赖关系。\n*   通过近乎线性的扩展，实现了逐步更复杂的规划。\n*   增强了LLM对存储库的理解，从而加速了代理定位。",
      "shortSummary": "本文介绍了RPG（存储库规划图），旨在解决大型语言模型在从零开始生成完整软件存储库方面的挑战。RPG通过在一个图中统一编码能力、文件结构、数据流和函数，用明确的蓝图取代了模糊的自然语言。基于RPG，ZeroRepo框架分三阶段进行图驱动的代码生成。在RepoCraft基准测试中，ZeroRepo表现出色，生成的代码量和功能通过率显著优于现有基线，证明了RPG在复杂代码库生成中的有效性和可扩展性。",
      "translated_title": "RPG：一种用于统一和可扩展代码库生成的存储库规划图",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9times the strongest baseline (Claude Code) and about 64times other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization."
    },
    {
      "title": "MANZANO：一种简单可扩展的混合视觉分词器统一多模态模型 (原标题: MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer)",
      "link": "https://arxiv.org/abs/2509.16197",
      "pubDate": "Fri, 19 Sep 2025 13:58:00 GMT",
      "isoDate": "2025-09-19T13:58:00.000Z",
      "creator": "Yanghao Li, Rui Qian, Bowen Pan, Haotian Zhang, Haoshuo Huang, Bowen Zhang, Jialing Tong, Haoxuan You, Xianzhi Du, Zhe Gan, Hyunjik Kim, Chao Jia, Zhenbang Wang, Yinfei Yang, Mingfei Gao, Zi-Yi Dou, Wenze Hu, Chang Gao, Dongxu Li, Philipp Dufter, Zirui Wang, Guoli Yin, Zhengdong Zhang, Chen Chen, Yang Zhao, Ruoming Pang, Zhifeng Chen",
      "summary": "Manzano：一种简单可扩展的统一多模态模型\n\n本文介绍了Manzano，一个旨在解决现有开源统一多模态大型语言模型（LLMs）在视觉理解和生成能力之间性能权衡的框架。\n\n### 核心问题\n\n*   现有的统一多模态LLMs在视觉理解和生成能力之间存在性能折衷，难以同时达到最佳效果。\n\n### Manzano的设计理念与架构\n\nManzano通过结合**混合图像分词器**和精心策划的**训练方案**，显著缓解了这一性能紧张。\n\n*   **共享视觉编码器**：一个单一的共享视觉编码器作为基础。\n*   **双轻量级适配器**：\n    *   生成连续嵌入，用于**图像到文本的理解**。\n    *   生成离散标记，用于**文本到图像的生成**。\n    *   这两种输出都在一个**共同的语义空间**中进行。\n*   **统一自回归LLM**：预测高层语义，形式为文本和图像标记。\n*   **辅助扩散解码器**：随后将图像标记翻译成像素。\n\n这种架构，结合统一的理解和生成数据训练方案，实现了两种能力的**可扩展联合学习**。\n\n### 性能与优势\n\n*   **领先表现**：Manzano在统一模型中取得了最先进的结果。\n*   **与专业模型竞争**：在某些方面，特别是**富文本评估**上，Manzano与专业模型具有竞争力。\n*   **设计验证**：研究表明，任务冲突极小，并且随着模型规模的扩大，性能持续提升，验证了混合分词器设计的有效性。",
      "shortSummary": "Manzano是一种简单可扩展的统一多模态模型，旨在解决现有LLMs在视觉理解和生成能力之间的性能权衡。它采用混合图像分词器和统一训练方案，通过共享视觉编码器和双适配器，在共同语义空间中实现图像理解的连续嵌入和图像生成的离散标记。一个统一的LLM结合辅助扩散解码器完成任务。Manzano在统一模型中达到最先进水平，并与专业模型竞争，尤其在富文本评估上表现出色，其设计被证实能有效扩展并减少任务冲突。",
      "translated_title": "MANZANO：一种简单可扩展的混合视觉分词器统一多模态模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer."
    },
    {
      "title": "BaseReward：多模态奖励模型的强大基线 (原标题: BaseReward: A Strong Baseline for Multimodal Reward Model)",
      "link": "https://arxiv.org/abs/2509.16127",
      "pubDate": "Fri, 19 Sep 2025 12:25:26 GMT",
      "isoDate": "2025-09-19T12:25:26.000Z",
      "creator": "Yi-Fan Zhang, Haihua Yang, Huanyu Zhang, Yang Shi, Zezhou Chen, Haochen Tian, Chaoyou Fu, Haotian Wang, Kai Wu, Bo Cui, Xu Wang, Jianfei Pan, Haotian Wang, Zhang Zhang, Liang Wang",
      "summary": "## BaseReward：多模态奖励模型的强大基线\n\n### 摘要\n\n随着多模态大语言模型（MLLMs）的快速发展，如何使其与人类偏好对齐已成为一个关键挑战。奖励模型（RMs）是实现这一目标的核心技术，但目前学术界和工业界都缺乏构建最先进多模态奖励模型（MRMs）的系统性指导。本文旨在通过详尽的实验分析，为构建高性能MRMs提供一个清晰的“秘籍”。\n\n### 关键研究内容\n\n作者们系统地调查了MRM开发流程中的每一个关键组件，包括：\n\n*   **奖励建模范式**：研究了多种范式，例如：\n    *   朴素奖励模型（Naive-RM）\n    *   基于评论的奖励模型（Critic-based RM）\n    *   生成式奖励模型（Generative RM）\n*   **奖励头架构**：优化奖励模型的输出层设计。\n*   **训练策略**：探索不同的训练方法以提高模型性能和稳定性。\n*   **数据整理**：涵盖了十多种多模态和纯文本偏好数据集的精选和混合策略。\n*   **骨干模型**：评估不同基础模型对MRM性能的影响。\n*   **模型规模**：研究模型大小对奖励模型效果的关联。\n*   **集成方法**：探索如何结合多个模型以进一步提升性能。\n\n### BaseReward的引入\n\n基于这些实验洞察，本文引入了 **BaseReward**，一个强大且高效的多模态奖励建模基线。BaseReward具有以下特点：\n\n*   **架构简洁高效**：采用简单而有效的架构。\n*   **骨干模型**：基于Qwen2.5-VL骨干模型构建。\n*   **奖励头**：配备一个优化的两层奖励头。\n*   **训练数据**：在一个精心策划的高质量多模态和纯文本偏好数据混合集上进行训练。\n\n### 性能与实用性\n\n*   **SOTA性能**：BaseReward在MM-RLHF-Reward Bench、VL-Reward Bench和Multimodal Reward Bench等主要基准测试中建立了新的SOTA（State-of-the-Art）性能，超越了之前的模型。\n*   **实际应用验证**：为了验证其在静态基准测试之外的实际效用，BaseReward被集成到一个真实的强化学习流程中，成功提升了MLLM在各种感知、推理和对话任务中的表现。\n\n### 贡献\n\n这项工作不仅提供了一个顶级的MRM，更重要的是，它为社区提供了一个清晰、有经验支持的指南，用于开发下一代MLLMs的强大奖励模型。",
      "shortSummary": "本文针对多模态大语言模型（MLLMs）与人类偏好对齐的挑战，系统性地研究了多模态奖励模型（MRMs）的构建方法。通过详尽实验，作者们推出了BaseReward，一个基于Qwen2.5-VL骨干和优化奖励头的强大MRM基线。BaseReward在多个主流基准测试中取得了SOTA性能，并成功应用于实际强化学习流程，提升了MLLM在感知、推理和对话任务上的表现。这项工作为MRM开发提供了清晰的经验指导。",
      "translated_title": "BaseReward：多模态奖励模型的强大基线",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe'' for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including reward modeling paradigms (e.g., Naive-RM, Critic-based RM, and Generative RM), reward head architecture, training strategies, data curation (covering over ten multimodal and text-only preference datasets), backbone model and model scale, and ensemble methods.   Based on these experimental insights, we introduce BaseReward, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM's performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs."
    },
    {
      "title": "用于机器人真实世界强化学习的视觉-语言-动作-评论家模型 (原标题: A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.15937",
      "pubDate": "Fri, 19 Sep 2025 08:44:29 GMT",
      "isoDate": "2025-09-19T08:44:29.000Z",
      "creator": "Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, Jiangmiao Pang",
      "summary": "# 用于机器人真实世界强化学习的视觉-语言-动作-评论家模型 (VLAC)\n\n本文介绍了一种名为 VLAC 的新型模型，旨在解决机器人真实世界强化学习 (RL) 中视觉-语言-动作 (VLA) 模型面临的稀疏、手工设计奖励和低效探索的瓶颈。\n\n## VLAC 模型概述\n\n*   **核心问题**：传统的 VLA 模型在机器人真实世界 RL 中，受限于稀疏且需要手动设计的奖励机制，以及低效的探索过程。\n*   **解决方案**：VLAC（Vision-Language-Action-Critic）模型被提出，它是一个基于 InternVL 构建的通用过程奖励模型。\n*   **训练数据**：VLAC 在大规模异构数据集上进行训练。\n    *   **视觉-语言数据集**：用于增强感知、对话和推理能力。\n    *   **机器人和人类轨迹数据**：用于指导动作生成和进度估计。\n*   **奖励机制**：给定成对的观测结果和一个语言目标，VLAC 能输出密集的进度变化量（progress delta）和完成信号（done signal）。\n    *   **优势**：这消除了任务特定的奖励工程需求，并支持对未见任务和环境进行一次性上下文内迁移。\n*   **鲁棒性增强**：通过构建大量的负样本和语义不匹配样本，VLAC 被强化以拒绝不相关的提示，并检测学习过程中的退化或停滞。\n\n## 统一的评论家与策略\n\n*   **Prompt 控制**：通过提示控制，单个 VLAC 模型能够交替生成奖励和动作令牌，从而统一了评论家（critic）和策略（policy）的功能。\n\n## 异步真实世界 RL 循环中的部署与探索\n\n*   **部署环境**：VLAC 被部署在一个异步的真实世界 RL 循环中。\n*   **人机协作探索协议**：为了加速探索并稳定早期学习，研究人员分层引入了一个分级的人机协作（human-in-the-loop）协议：\n    *   **离线演示回放 (offline demonstration replay)**\n    *   **返回与探索 (return and explore)**\n    *   **人机引导探索 (human guided explore)**\n\n## 实验结果\n\n*   **任务场景**：VLAC 在四种不同的真实世界操作任务中进行了测试。\n*   **性能提升**：\n    *   在没有人类干预的情况下，VLAC 在 200 次真实世界交互回合内，将成功率从约 30% 提升到约 90%。\n    *   结合人机协作干预后，样本效率进一步提高了 50%，最终成功率达到 100%。\n\nVLAC 模型通过其创新的奖励机制和人机协作探索协议，显著提升了机器人真实世界强化学习的效率和成功率。",
      "shortSummary": "VLAC是一种用于机器人真实世界强化学习的视觉-语言-动作-评论家模型，旨在解决稀疏奖励和低效探索问题。它基于InternVL，通过大规模异构数据训练，能根据语言目标输出密集的进度奖励，从而消除任务特定奖励工程。VLAC统一了评论家与策略，并结合分级人机协作协议加速探索。实验表明，VLAC显著提升了四种真实世界操作任务的成功率，从约30%提高到90%，人机协作进一步提升了样本效率并实现了100%的成功率。",
      "translated_title": "用于机器人真实世界强化学习的视觉-语言-动作-评论家模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30\\% to about 90\\% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success."
    },
    {
      "title": "潜在分区网络：生成建模、表示学习和分类的统一原理 (原标题: Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification)",
      "link": "https://arxiv.org/abs/2509.15591",
      "pubDate": "Fri, 19 Sep 2025 00:47:16 GMT",
      "isoDate": "2025-09-19T00:47:16.000Z",
      "creator": "Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin",
      "summary": "## 潜在分区网络：统一生成建模、表示学习和分类\n\n### 核心问题与研究目标\n\n机器学习领域中的生成建模、表示学习和分类是三个核心问题，但目前最先进的解决方案往往是相互独立的。本文旨在探讨是否存在一个统一的原理能够同时解决这三个问题，以简化机器学习流程并促进任务间的协同。\n\n### 潜在分区网络 (LZN) 介绍\n\nLatent Zoning Network (LZN) 被提出作为实现这一目标的关键一步。其核心思想是：\n\n*   **共享高斯潜在空间**：LZN 创建一个共享的高斯潜在空间，该空间能够编码所有任务所需的信息。\n*   **编码器与解码器机制**：\n    *   每种数据类型（例如图像、文本、标签）都配备一个**编码器**，负责将样本映射到潜在空间中不相交的“潜在区域”（latent zones）。\n    *   一个**解码器**则负责将潜在表示映射回原始数据空间。\n*   **任务表达**：机器学习任务被巧妙地表达为这些编码器和解码器的组合：\n    *   **条件图像生成**：使用标签编码器和图像解码器。\n    *   **图像嵌入**：仅使用图像编码器。\n    *   **分类**：使用图像编码器和标签解码器。\n\n### LZN 的实验验证与成果\n\nLZN 在三个复杂度递增的场景中展示了其潜力：\n\n1.  **增强现有模型（图像生成）**：\n    *   当 LZN 与最先进的 Rectified Flow 模型结合时，在 CIFAR10 数据集上的 FID（Fréchet Inception Distance）从 2.76 提升到 2.59。\n    *   值得注意的是，这一改进是在不修改现有模型训练目标的情况下实现的。\n\n2.  **独立解决任务（表示学习）**：\n    *   LZN 能够无需辅助损失函数即可实现无监督表示学习。\n    *   在 ImageNet 数据集的下游线性分类任务中，LZN 的表现优于开创性的 MoCo 和 SimCLR 方法，分别提高了 9.3% 和 0.2%。\n\n3.  **同时解决多任务（联合生成和分类）**：\n    *   通过设计包含图像和标签编码器/解码器，LZN 能够同时执行生成和分类任务。\n    *   在 CIFAR10 数据集上，LZN 不仅提高了 FID，还实现了最先进的分类准确率。\n\n### 资源与发表信息\n\n*   代码和训练模型已在 [this https URL](https://this.https.url/) 提供。\n*   项目网站可在 [this https URL](https://this.https.url/) 访问。\n*   该论文已发表于 NeurIPS 2025。",
      "shortSummary": "Latent Zoning Network (LZN) 提出了一种统一原理，旨在同时解决生成建模、表示学习和分类这三个核心机器学习问题。LZN通过创建一个共享的高斯潜在空间，并利用编码器和解码器的组合来表达不同任务。实验证明，LZN能有效增强现有图像生成模型、在无监督表示学习中超越基线方法，并能同时进行联合生成和分类，在多个任务上取得了最先进的性能。",
      "translated_title": "潜在分区网络：生成建模、表示学习和分类的统一原理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html."
    },
    {
      "title": "BTL-UI：GUI智能体中的眨眼-思考-链接推理模型 (原标题: BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent)",
      "link": "https://arxiv.org/abs/2509.15566",
      "pubDate": "Fri, 19 Sep 2025 00:03:44 GMT",
      "isoDate": "2025-09-19T00:03:44.000Z",
      "creator": "Shaojie Zhang, Ruoceng Zhang, Pei Fu, Shaokang Wang, Jiahui Yang, Xin Du, Shiqi Cui, Bin Qin, Ying Huang, Zhenbo Luo, Jian Luan",
      "summary": "# BTL-UI：GUI智能体中的眨眼-思考-链接推理模型\n\n## 摘要\n\n在AI驱动的人机图形用户界面（GUI）交互自动化领域，尽管多模态大型语言模型和强化微调技术取得了显著进展，但一个根本性挑战依然存在：现有模型的交互逻辑与人类自然的GUI通信模式存在显著偏差。为了弥补这一差距，本文提出了一种名为“眨眼-思考-链接”（BTL）的脑启发式框架，旨在模仿用户与图形界面之间的人类认知过程。\n\n## BTL框架的核心理念\n\nBTL框架将人机交互分解为三个生物学上合理的阶段，以更自然地模拟人类的认知过程：\n\n1.  **眨眼 (Blink)**：\n    *   **描述**：快速检测并关注屏幕上的相关区域。\n    *   **类比**：类似于人眼的扫视运动，快速捕捉视觉焦点。\n\n2.  **思考 (Think)**：\n    *   **描述**：进行更高层次的推理和决策。\n    *   **类比**：反映了人类的认知规划过程，涉及对信息的理解和行动策略的制定。\n\n3.  **链接 (Link)**：\n    *   **描述**：生成可执行的命令，以实现精确的运动控制。\n    *   **类比**：模拟人类的动作选择机制，将思考结果转化为具体的交互操作。\n\n## 关键技术创新\n\n为了支持BTL框架的实现，本文引入了两项关键技术创新：\n\n1.  **眨眼数据生成 (Blink Data Generation)**：\n    *   **描述**：一个专门为眨眼数据优化的自动化标注流程。\n    *   **目的**：高效、准确地生成用于训练模型的数据，以模拟人类的视觉注意力机制。\n\n2.  **BTL奖励 (BTL Reward)**：\n    *   **描述**：首个基于规则的奖励机制，能够同时由过程和结果驱动强化学习。\n    *   **目的**：为强化学习提供更全面、更符合人类学习模式的反馈，既关注最终任务的完成，也关注达成任务的过程质量。\n\n## BTL-UI模型与性能\n\n基于BTL框架，研究人员开发了一个名为BTL-UI的GUI智能体模型。该模型在全面的基准测试中，无论是在静态GUI理解任务还是动态交互任务中，都展现出持续的最先进（State-of-the-Art, SOTA）性能。这些结果提供了确凿的经验证据，验证了BTL框架在开发高级GUI智能体方面的有效性。",
      "shortSummary": "本文提出了“眨眼-思考-链接”（BTL）框架，旨在弥补AI驱动人机GUI交互与人类自然模式的偏差。该框架受大脑启发，将交互分解为快速检测（眨眼）、高级推理（思考）和命令生成（链接）三个阶段。BTL引入了眨眼数据自动化生成和结合过程与结果的BTL奖励机制。基于此框架开发的BTL-UI模型，在GUI理解和动态交互任务中均达到最先进性能，验证了其在开发高级GUI智能体方面的有效性。",
      "translated_title": "BTL-UI：GUI智能体中的眨眼-思考-链接推理模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose \"Blink-Think-Link\" (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward -- the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework's efficacy in developing advanced GUI Agents."
    },
    {
      "title": "Lynx：迈向高保真个性化视频生成 (原标题: Lynx: Towards High-Fidelity Personalized Video Generation)",
      "link": "https://arxiv.org/abs/2509.15496",
      "pubDate": "Thu, 18 Sep 2025 20:31:57 GMT",
      "isoDate": "2025-09-18T20:31:57.000Z",
      "creator": "Shen Sang, Tiancheng Zhi, Tianpei Gu, Jing Liu, Linjie Luo",
      "summary": "## Lynx：高保真个性化视频生成模型\n\n### 引言\n\n本文介绍了Lynx模型，这是一个旨在从单个输入图像合成高保真个性化视频的先进模型。Lynx在确保视频中人物身份高度保真的同时，保持了卓越的视觉质量和时间连贯性。\n\n### 核心技术与架构\n\nLynx模型构建于一个开源的Diffusion Transformer (DiT) 基础模型之上，并引入了两个轻量级适配器以实现其核心功能：\n\n*   **ID-适配器 (ID-adapter)**\n    *   **功能：** 该适配器利用Perceiver Resampler将从ArcFace衍生的面部嵌入转换为紧凑的身份令牌。\n    *   **作用：** 这些身份令牌被用于条件化，确保在生成的视频中人物身份得到精确且鲁棒的保留。\n\n*   **Ref-适配器 (Ref-adapter)**\n    *   **功能：** 该适配器整合了来自一个冻结参考路径的密集VAE（变分自编码器）特征。\n    *   **作用：** 通过交叉注意力机制，这些细粒度特征被注入到所有Transformer层中，从而增强了视频的视觉真实感和细节表现。\n\n这些模块协同工作，共同实现了强大的身份保留能力，同时确保了生成视频的时间连贯性和整体视觉真实感。\n\n### 性能评估\n\n为了验证Lynx的性能，研究人员在一个精心策划的基准上进行了评估，该基准包含：\n\n*   **主体数量：** 40个不同的主体。\n*   **提示数量：** 20个无偏的提示。\n*   **测试案例：** 共生成了800个测试案例。\n\n评估结果表明，Lynx在多个关键指标上展现出卓越的性能：\n\n*   **面部相似度：** 表现出卓越的面部相似度，确保生成视频中的人物与输入图像高度一致。\n*   **提示遵循：** 具有竞争力的提示遵循能力，能够准确地根据文本提示生成相应内容。\n*   **视频质量：** 展现出强大的视频质量，包括清晰度、流畅度和整体视觉吸引力。\n\n### 贡献与意义\n\nLynx模型通过其创新的架构和卓越的性能，显著推动了个性化视频生成领域的发展，为未来高保真视频合成应用奠定了坚实基础。",
      "shortSummary": "Lynx是一个基于Diffusion Transformer (DiT) 的高保真个性化视频生成模型，能从单张图像合成视频。它引入了ID-适配器和Ref-适配器，分别用于将ArcFace面部嵌入转换为身份令牌和注入VAE细粒度特征，从而确保强大的身份保留、时间连贯性和视觉真实感。在包含800个测试案例的基准评估中，Lynx在面部相似度、提示遵循和视频质量方面均表现出色，显著推进了个性化视频生成技术。",
      "translated_title": "Lynx：迈向高保真个性化视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation."
    },
    {
      "title": "ScaleCUA：利用跨平台数据扩展开源计算机使用代理 (原标题: ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data)",
      "link": "https://arxiv.org/abs/2509.15221",
      "pubDate": "Thu, 18 Sep 2025 13:59:22 GMT",
      "isoDate": "2025-09-18T13:59:22.000Z",
      "creator": "Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang",
      "summary": "## ScaleCUA：利用跨平台数据扩展开源计算机使用代理\n\n### 引言\n\n视觉-语言模型（VLM）已使计算机使用代理（CUA）能够自主操作图形用户界面（GUI），展现出巨大潜力。然而，目前该领域的进展受限于缺乏大规模、开源的计算机使用数据和基础模型。\n\n### ScaleCUA的提出\n\n本文介绍了ScaleCUA，这是朝着扩展开源CUA迈出的重要一步。它旨在通过数据驱动的扩展方法，提升通用计算机使用代理的能力。\n\n### ScaleCUA数据集与构建\n\nScaleCUA提供了一个大规模数据集，其特点包括：\n\n*   **广泛的覆盖范围**：涵盖了6种不同的操作系统和3个主要任务领域。\n*   **构建方法**：该数据集通过一个创新的闭环管道构建，该管道有效结合了自动化代理和人类专家，确保了数据的质量、多样性和实用性。\n\n### 跨平台操作能力\n\n通过对这些大规模数据的训练，ScaleCUA能够实现跨平台的无缝操作，极大地增强了其通用性和适用性。\n\n### 性能表现\n\nScaleCUA在多项基准测试中取得了显著提升，并设置了新的最先进（SOTA）结果，具体表现如下：\n\n*   **相对于基线模型的提升：**\n    *   在WebArena-Lite-v2上，性能提升了26.6个百分点。\n    *   在ScreenSpot-Pro上，性能提升了10.7个百分点。\n*   **新的最先进结果：**\n    *   MMBench-GUI L1-Hard：达到94.4%的准确率。\n    *   OSWorld-G：达到60.6%的准确率。\n    *   WebArena-Lite-v2：达到47.4%的准确率。\n\n### 结论\n\n这些研究结果有力地强调了数据驱动的扩展策略对于开发和提升通用计算机使用代理的强大作用和重要性。\n\n### 未来展望\n\n作者计划发布相关的训练数据、模型和代码，以促进学术界和工业界在这一领域的进一步研究和发展。",
      "shortSummary": "ScaleCUA引入了一个大规模跨平台数据集，涵盖6种操作系统和3个任务领域，旨在扩展开源计算机使用代理（CUA）。通过结合自动化代理和人类专家的闭环管道构建，ScaleCUA训练后能无缝跨平台操作。它在WebArena-Lite-v2和ScreenSpot-Pro等基准测试中显著超越现有模型，并在MMBench-GUI L1-Hard、OSWorld-G和WebArena-Lite-v2上取得了新的最先进（SOTA）结果，证明了数据驱动扩展对通用CUA的强大作用。",
      "translated_title": "ScaleCUA：利用跨平台数据扩展开源计算机使用代理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA."
    },
    {
      "title": "RynnVLA-001：利用人类演示改进机器人操作 (原标题: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation)",
      "link": "https://arxiv.org/abs/2509.15212",
      "pubDate": "Thu, 18 Sep 2025 13:58:02 GMT",
      "isoDate": "2025-09-18T13:58:02.000Z",
      "creator": "Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li",
      "summary": "### RynnVLA-001：利用人类演示改进机器人操作\n\n本文介绍了RynnVLA-001，一个基于大规模人类演示视频生成预训练的视觉-语言-动作（VLA）模型。该模型旨在通过有效利用人类演示数据，显著提升机器人的操作能力。\n\n#### 核心贡献\n\n*   **RynnVLA-001模型**：一个创新的VLA模型，其核心在于从人类演示中进行大规模视频生成预训练。\n*   **新颖的两阶段预训练方法**：\n    1.  **以自我为中心的视频生成预训练 (Ego-Centric Video Generative Pretraining)**：\n        *   在此阶段，模型被训练为一个图像到视频（Image-to-Video）模型。\n        *   训练数据包含1200万个以自我为中心的操作视频。\n        *   模型的目标是根据初始帧和语言指令预测未来的帧。\n    2.  **以人为中心的轨迹感知建模 (Human-Centric Trajectory-Aware Modeling)**：\n        *   此阶段在前一阶段的基础上进行扩展。\n        *   模型不仅预测未来的视觉帧，还联合预测未来的关键点轨迹。\n        *   这种方法有效地将视觉帧预测与动作预测连接起来，为机器人操作提供了更直接的指导。\n*   **ActionVAE**：\n    *   为了增强动作表示，研究人员提出了ActionVAE（变分自编码器）。\n    *   ActionVAE将一系列动作压缩成紧凑的潜在嵌入（latent embeddings）。\n    *   这显著降低了VLA模型输出空间的复杂性，使得动作学习和生成更加高效。\n\n#### 性能表现\n\n*   在相同的下游机器人数据集上进行微调时，RynnVLA-001取得了优于现有最先进基线的性能。\n*   这一结果证明了所提出的预训练策略为VLA模型提供了一个更有效的初始化方法，从而在机器人操作任务中实现了卓越的表现。",
      "shortSummary": "RynnVLA-001是一个利用人类演示改进机器人操作的视觉-语言-动作（VLA）模型。它采用新颖的两阶段预训练方法：首先进行以自我为中心的视频生成预训练，然后进行以人为中心的轨迹感知建模，将视觉预测与动作预测相结合。此外，ActionVAE通过压缩动作序列增强了动作表示。在下游机器人数据集上微调后，RynnVLA-001的性能超越了现有最先进模型，证明了其预训练策略的有效性。",
      "translated_title": "RynnVLA-001：利用人类演示改进机器人操作",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models."
    },
    {
      "title": "FlowRL：匹配LLM推理的奖励分布 (原标题: FlowRL: Matching Reward Distributions for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2509.15207",
      "pubDate": "Thu, 18 Sep 2025 13:56:36 GMT",
      "isoDate": "2025-09-18T13:56:36.000Z",
      "creator": "Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, Qizheng Zhang, Lin Chen, Fanghao Shao, Bo Xue, Yunchong Song, Zhenjie Yang, Ganqu Cui, Ning Ding, Jianfeng Gao, Xiaodong Liu, Bowen Zhou, Hongyuan Mei, Zhouhan Lin",
      "summary": "## FlowRL：匹配LLM推理的奖励分布\n\n### 核心问题\n\n当前大型语言模型（LLM）强化学习（RL）中，主流的奖励最大化方法（如PPO和GRPO）存在以下局限性：\n\n*   **过度优化**：倾向于过度优化主导的奖励信号。\n*   **忽视多样性**：忽略了不那么频繁但同样有效的推理路径。\n*   **多样性降低**：导致模型生成的推理路径多样性不足。\n\n### FlowRL方法\n\nFlowRL提出了一种新的方法，旨在通过“流平衡”（flow balancing）来匹配完整的奖励分布，而非仅仅最大化奖励。其核心思想和机制包括：\n\n*   **转换奖励**：将标量奖励转换为一个标准化的目标分布，该分布通过一个可学习的配分函数（partition function）生成。\n*   **优化目标**：最小化策略（policy）与目标分布之间的反向KL散度（reverse KL divergence）。\n*   **流平衡优化**：将这一思想实现为一种流平衡优化方法，旨在促进更具多样性的探索和更具泛化能力的推理轨迹。\n\n### 实验结果\n\n研究团队在数学和代码推理任务上进行了实验，FlowRL展现出显著的性能提升：\n\n*   **数学基准**：在数学基准测试中，FlowRL相对于GRPO平均提升了10.0%，相对于PPO平均提升了5.1%。\n*   **代码推理**：在代码推理任务上，FlowRL也表现出持续的优异性能。\n\n### 结论与意义\n\n这些实验结果强调了奖励分布匹配在LLM强化学习中作为实现高效探索和多样化推理的关键一步。FlowRL为LLM的推理能力提供了一个新的优化方向，有助于模型生成更全面、更鲁棒的解决方案。",
      "shortSummary": "FlowRL提出了一种在大型语言模型（LLM）强化学习中匹配完整奖励分布的新方法，以解决传统奖励最大化方法（如PPO和GRPO）导致的过度优化和多样性不足问题。FlowRL通过流平衡优化，将标量奖励转换为目标分布并最小化KL散度，从而促进多样化探索和泛化推理。实验表明，FlowRL在数学和代码推理任务上均显著优于现有方法，证明了奖励分布匹配对于LLM高效探索和多样化推理的重要性。",
      "translated_title": "FlowRL：匹配LLM推理的奖励分布",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10.0% over GRPO and 5.1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning."
    },
    {
      "title": "无标签下语言模型的演进：多数决定选择，新颖促进多样性 (原标题: Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation)",
      "link": "https://arxiv.org/abs/2509.15194",
      "pubDate": "Thu, 18 Sep 2025 13:50:04 GMT",
      "isoDate": "2025-09-18T13:50:04.000Z",
      "creator": "Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, Dong Yu",
      "summary": "## 无标签下语言模型的演进：多数决定选择，新颖促进多样性\n\n### 背景与挑战\n\n大型语言模型（LLMs）的训练日益依赖于通过可验证奖励的强化学习（RLVR）。然而，在实际部署中，模型需要具备在没有外部标签或评判的情况下进行自我改进的能力。现有的无标签方法，如置信度最小化、自洽性或多数投票目标，虽然能够稳定学习过程，但却逐渐限制了模型的探索能力，导致“熵坍塌”现象。这意味着模型生成的响应会变得更短、多样性降低，并且更加脆弱。\n\n与Test-Time Reinforcement Learning (TTRL)等主要使模型适应当前无标签数据集的方法不同，本文的目标更为宏大：在不牺牲模型固有探索能力和泛化能力的前提下，实现模型的通用改进，即“演进”。\n\n### 提出的解决方案：EVOL-RL\n\n为了解决上述问题，本文提出了一种名为**EVOL-RL (EVolution-Oriented and Label-free Reinforcement Learning)** 的方法。EVOL-RL是一个简单的规则，它在无标签设置下巧妙地将学习的稳定性与生成内容的变异性结合起来。\n\n#### EVOL-RL的核心机制\n\nEVOL-RL的设计理念是“多数驱动选择，新颖促进变异”：\n\n1.  **选择（Selection）**：EVOL-RL将多数投票的答案作为稳定的锚点。这确保了学习过程的稳定性，避免模型偏离已验证的、多数认可的路径。\n2.  **变异（Variation）**：为了防止熵坍塌和促进探索，EVOL-RL引入了一个新颖性奖励。这个奖励机制鼓励模型生成那些推理过程与已产生内容在语义空间中存在显著差异的响应。通过这种方式，模型被激励去探索新的解决方案和思维路径，从而保持多样性。\n\n#### 实现细节\n\nEVOL-RL的实现基于GRPO（Generalized Reinforcement Policy Optimization）。此外，它还采用了以下技术来优化性能和稳定性：\n\n*   **非对称剪裁（Asymmetric Clipping）**：用于保留强大的信号，确保重要的学习信息不会被过度平滑或丢弃。\n*   **熵正则化器（Entropy Regularizer）**：用于维持模型的搜索能力和探索性，进一步防止多样性坍塌。\n\n### 实验结果与优势\n\nEVOL-RL的设计理念带来了显著的性能提升和多项优势：\n\n*   **防止熵坍塌**：有效避免了生成内容变短、多样性降低的问题。\n*   **保持思维链的长度和信息量**：模型能够生成更长、更具信息量的思维链（chains of thought）。\n*   **性能提升**：显著提高了pass@1和pass@n的性能。\n*   **超越基线**：EVOL-RL持续优于仅采用多数投票的TTRL基线。例如，在无标签AIME24数据集上训练Qwen3-4B-Base模型后，其在AIME25上的pass@1从TTRL的4.6%提升至16.4%，pass@16从18.5%提升至37.9%。\n*   **更强的泛化能力**：EVOL-RL不仅防止了多样性坍塌，还解锁了跨领域（如GPQA）的更强泛化能力。\n*   **广泛适用性**：研究还表明，EVOL-RL在RLVR设置中也能提升性能，这突显了其广泛的适用性。\n\n### 结论\n\nEVOL-RL通过结合多数驱动的选择和新颖性促进的变异，为无标签下语言模型的自我演进提供了一个有效且鲁棒的框架，成功解决了现有方法导致的熵坍塌问题，并显著提升了模型的性能和泛化能力。",
      "shortSummary": "本文提出EVOL-RL，一种无标签强化学习方法，旨在解决现有LLM无标签自我改进中出现的“熵坍塌”问题。EVOL-RL通过将多数投票答案作为稳定锚点（选择）并引入新颖性奖励（变异）来鼓励探索。实验表明，EVOL-RL有效防止了多样性坍塌，保持了更长的思维链，显著提高了pass@1和pass@n性能，并增强了模型在多个任务上的泛化能力，甚至在RLVR设置中也表现出色。",
      "translated_title": "无标签下语言模型的演进：多数决定选择，新颖促进多样性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model's inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability."
    },
    {
      "title": "先理解再生成：自引导训练用于自回归图像生成 (原标题: Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation)",
      "link": "https://arxiv.org/abs/2509.15185",
      "pubDate": "Thu, 18 Sep 2025 13:47:40 GMT",
      "isoDate": "2025-09-18T13:47:40.000Z",
      "creator": "Xiaoyu Yue, Zidong Wang, Yuqing Wang, Wenlong Zhang, Xihui Liu, Wanli Ouyang, Lei Bai, Luping Zhou",
      "summary": "## 先理解再生成：自引导训练用于自回归图像生成\n\n### 背景与问题\n\n近期研究强调了高质量视觉表示在图像生成中的重要性，并指出了生成模型在图像理解方面的局限性。自回归模型最初是为自然语言处理设计的范式，在应用于视觉领域时也面临类似的挑战，尤其是在学习高级视觉语义方面。\n\n### 识别出的关键问题\n\n本研究首次系统地调查了将“下一词预测”范式应用于视觉领域时的机制，并识别出阻碍模型学习高级视觉语义的三个关键属性：\n\n*   **局部和条件依赖性 (Local and Conditional Dependence)：** 模型在生成过程中可能过度依赖局部信息和条件，导致对全局语义的理解不足。\n*   **步间语义不一致性 (Inter-Step Semantic Inconsistency)：** 在逐步生成图像的过程中，不同生成步骤之间可能出现语义上的不连贯或不一致。\n*   **空间不变性缺陷 (Spatial Invariance Deficiency)：** 模型可能缺乏对图像空间变换（如平移、旋转、缩放）的鲁棒性，难以识别相同对象在不同空间位置或姿态下的语义。\n\n### 提出的解决方案：ST-AR 框架\n\n为了解决上述问题，本研究提出了一种新颖的训练框架——**自引导训练用于自回归模型 (Self-guided Training for AutoRegressive models, ST-AR)**。该框架通过在训练过程中引入自监督目标，能够有效地解决这些问题。\n\n**ST-AR 的特点：**\n\n*   **无需预训练表示模型：** ST-AR 的优势在于它不依赖于任何预训练的表示模型，而是通过自监督学习来增强模型的理解能力。\n\n### 主要成果与优势\n\nST-AR 框架显著增强了自回归模型的图像理解能力，并带来了显著的生成质量提升：\n\n*   **图像理解能力提升：** 通过自监督目标，模型能够更好地捕捉和理解图像的高级语义信息。\n*   **生成质量显著改善：** 在保持相同采样策略的前提下，ST-AR 带来了显著的 FID (Fréchet Inception Distance) 改进：\n    *   LlamaGen-L 的 FID 提升约 **42%**。\n    *   LlamaGen-XL 的 FID 提升约 **49%**。\n\n### 接受情况\n\n该研究已被 NeurIPS 2025 接受。",
      "shortSummary": "本文提出ST-AR（自引导训练用于自回归模型）框架，旨在解决自回归图像生成模型在理解高级视觉语义方面的不足。研究识别出局部依赖、步间语义不一致和空间不变性缺陷等关键问题，并通过引入自监督目标来有效解决。ST-AR无需预训练模型，显著提升了模型的图像理解能力和生成质量，例如使LlamaGen-L和LlamaGen-XL的FID分别提升约42%和49%。",
      "translated_title": "先理解再生成：自引导训练用于自回归图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy."
    },
    {
      "title": "释放多模态大型语言模型在零样本时空视频定位中的潜力 (原标题: Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding)",
      "link": "https://arxiv.org/abs/2509.15178",
      "pubDate": "Thu, 18 Sep 2025 13:35:50 GMT",
      "isoDate": "2025-09-18T13:35:50.000Z",
      "creator": "Zaiquan Yang, Yuhao Liu, Gerhard Hancke, Rynson W. H. Lau",
      "summary": "# 释放多模态大型语言模型在零样本时空视频定位中的潜力\n\n## 摘要\n\n本文探讨了利用多模态大型语言模型（MLLMs）解决零样本时空视频定位（STVG）问题。STVG 的目标是根据输入的文本查询，在视频中定位对应的时空区域。研究揭示了 MLLMs 在此任务中的关键洞察和挑战，并提出了一种创新的零样本框架来克服这些挑战。\n\n## MLLMs 的关键洞察\n\n研究人员对 MLLMs 在 STVG 任务中的行为进行了深入分析，发现了两个关键洞察：\n\n*   **定位标记的动态分配：** MLLMs 倾向于动态地分配特殊的标记（称为“定位标记”）来执行文本查询的定位任务。\n*   **文本线索整合不足：** MLLMs 常常因为无法充分整合文本查询中的所有线索（例如，属性、动作）进行推理，从而导致次优的定位性能。\n\n## 提出的零样本框架\n\n基于上述洞察，本文提出了一种基于 MLLM 的零样本 STVG 框架。该框架引入了两种新颖的策略：分解时空高亮（Decomposed Spatio-Temporal Highlighting, DSTH）和时间增强组装（Temporal-Augmented Assembling, TAS），旨在充分释放 MLLMs 的推理能力。\n\n### 1. 分解时空高亮 (DSTH) 策略\n\nDSTH 策略旨在更有效地利用文本查询中的信息，并引导模型关注相关的视觉区域：\n\n*   **查询解耦：** 首先将原始文本查询解耦为两个子查询：一个用于属性（attribute sub-query），另一个用于动作（action sub-query）。这两个子查询分别用于在空间和时间维度上探查目标的存在。\n*   **Logit 引导的重注意力 (LRA) 模块：** 引入了一个新颖的 Logit 引导的重注意力（LRA）模块。该模块通过规范每个子查询的标记预测，学习生成潜在变量，这些变量作为空间和时间提示。\n*   **提示作用：** 这些生成的提示分别突出属性和动作线索，有效地将模型的注意力引导至与查询相关的可靠空间和时间视觉区域。\n\n### 2. 时间增强组装 (TAS) 策略\n\n为了解决空间定位在时间上的一致性问题，TAS 策略被提出：\n\n*   **目的：** 确保由属性子查询进行的视频帧空间定位在时间维度上保持一致性。\n*   **方法：** 该策略通过将原始视频帧和经过时间增强处理的帧作为输入，来组装和整合预测结果，从而显著提高时间一致性。\n\n## 实验与结果\n\n研究人员在多种 MLLMs 上对所提出的方法进行了广泛评估。实验结果表明，该方法在三个常见的 STVG 基准测试中均超越了现有的最先进（SOTA）方法，验证了其有效性和优越性。",
      "shortSummary": "本文提出利用多模态大型语言模型（MLLMs）解决零样本时空视频定位（STVG）问题。研究发现 MLLMs 在定位标记和整合文本线索方面的不足。为此，提出了一种基于 MLLM 的零样本框架，包含分解时空高亮（DSTH）和时间增强组装（TAS）策略。DSTH 通过解耦查询和 Logit 引导的重注意力生成空间/时间提示。TAS 通过整合原始和增强帧提升时间一致性。实验证明，该方法在多个 STVG 基准上超越了现有最佳方法。",
      "translated_title": "释放多模态大型语言模型在零样本时空视频定位中的潜力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as grounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (e.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG."
    },
    {
      "title": "WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成 (原标题: WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance)",
      "link": "https://arxiv.org/abs/2509.15130",
      "pubDate": "Thu, 18 Sep 2025 12:40:47 GMT",
      "isoDate": "2025-09-18T12:40:47.000Z",
      "creator": "Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, Chi Zhang",
      "summary": "### WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成\n\n#### 引言\n\n近期，视频扩散模型因其丰富的潜在世界先验，在空间智能任务中展现出巨大潜力。然而，其有限的控制性和几何不一致性阻碍了它们在3D/4D任务中的实际应用。目前，解决这些问题的方法通常依赖于重新训练或微调，这不仅可能损害预训练知识，还会带来高昂的计算成本。\n\n#### WorldForge 框架\n\n为应对上述挑战，研究人员提出了 **WorldForge**，这是一个免训练、推理时（inference-time）的框架，由三个紧密耦合的模块组成：\n\n1.  **步内递归细化 (Intra-Step Recursive Refinement)**\n    *   该模块在每个去噪步骤中引入递归细化机制。\n    *   它重复优化网络预测，以实现精确的轨迹注入。\n\n2.  **流门控潜在融合 (Flow-Gated Latent Fusion)**\n    *   利用光流相似性在潜在空间中解耦运动和外观。\n    *   选择性地将轨迹指导注入到与运动相关的通道中。\n\n3.  **双路径自校正指导 (Dual-Path Self-Corrective Guidance)**\n    *   通过比较有指导和无指导的去噪路径。\n    *   自适应地纠正由噪声或未对齐结构信号引起的轨迹漂移。\n\n#### 方法优势与成果\n\n这些组件协同工作，无需训练即可注入细粒度、轨迹对齐的指导，从而实现：\n\n*   **准确的运动控制**\n*   **逼真的内容生成**\n\n广泛的实验在多样化的基准测试中验证了 WorldForge 在真实感、轨迹一致性和视觉保真度方面的优越性。\n\n#### 贡献\n\n这项工作为可控视频合成引入了一种新颖的即插即用范式，为利用生成先验进行空间智能任务提供了新的视角。",
      "shortSummary": "WorldForge 提出了一种免训练、推理时的框架，旨在解决视频扩散模型在3D/4D生成中控制性差和几何不一致的问题。该框架包含步内递归细化、流门控潜在融合和双路径自校正指导三个模块，能够在不进行训练的情况下，精确注入轨迹指导，实现准确的运动控制和逼真的内容生成。实验证明其在真实感、轨迹一致性和视觉保真度方面表现优异，为可控视频合成提供了一种即插即用的新范式。",
      "translated_title": "WorldForge：通过免训练指导解锁视频扩散模型中的涌现3D/4D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method's superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence."
    },
    {
      "title": "仅RGB监督的动态场景相机参数优化 (原标题: RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes)",
      "link": "https://arxiv.org/abs/2509.15123",
      "pubDate": "Thu, 18 Sep 2025 12:29:07 GMT",
      "isoDate": "2025-09-18T12:29:07.000Z",
      "creator": "Fang Li, Hao Zhang, Narendra Ahuja",
      "summary": "本文介绍了一种名为ROS-Cam的新方法，旨在解决动态场景中相机参数优化的挑战。传统方法如COLMAP在静态场景中表现出色，但在动态场景中面临运行时间长和对真值（GT）运动掩码的依赖等限制。现有改进方法通常需要焦距、运动掩码、3D点云、相机姿态或度量深度等先验信息，而这些信息在随意捕获的RGB视频中通常难以获取。\n\nROS-Cam的核心目标是仅通过单个RGB视频作为监督，实现更准确、更高效的动态场景相机参数优化。该方法包含以下三个关键组件：\n\n*   **块级跟踪滤波器（Patch-wise Tracking Filters）**：\n    *   用于在RGB视频中建立鲁棒且最大稀疏的铰链状关系，为后续优化提供基础。\n\n*   **离群点感知联合优化（Outlier-aware Joint Optimization）**：\n    *   通过自适应地降低移动离群点的权重，实现高效的相机参数优化。\n    *   此组件的优势在于无需依赖任何运动先验信息。\n\n*   **两阶段优化策略（Two-stage Optimization Strategy）**：\n    *   通过在Softplus限制和损失函数中的凸极小值之间进行权衡，有效提升了优化的稳定性和速度。\n\n为了验证ROS-Cam的性能，研究人员进行了全面的评估。他们对相机估计结果进行了视觉和数值分析。此外，为了进一步验证其准确性，他们将相机估计结果输入到一个4D重建方法中，并评估了由此生成的3D场景以及渲染的2D RGB和深度图。\n\n实验在多个数据集上进行，包括4个真实世界数据集（NeRF-DS、DAVIS、iPhone和TUM-dynamics）和1个合成数据集（MPI-Sintel）。实验结果表明，ROS-Cam方法仅以单个RGB视频作为唯一监督，就能比现有方法更高效、更准确地估计相机参数。",
      "shortSummary": "ROS-Cam是一种仅通过单个RGB视频监督，用于动态场景相机参数优化的新方法。它解决了COLMAP在动态场景中的局限性及现有方法对不可用先验的依赖。ROS-Cam包含块级跟踪滤波器、离群点感知联合优化和两阶段优化策略，实现了更高效、更准确的相机参数估计，无需真值运动掩码或其他先验信息。实验证明其在多个真实和合成数据集上表现优异。",
      "translated_title": "仅RGB监督的动态场景相机参数优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision."
    },
    {
      "title": "Ask-to-Clarify：通过多轮对话解决指令歧义 (原标题: Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue)",
      "link": "https://arxiv.org/abs/2509.15061",
      "pubDate": "Thu, 18 Sep 2025 11:25:31 GMT",
      "isoDate": "2025-09-18T11:25:31.000Z",
      "creator": "Xingyao Lin, Xinghao Zhu, Tianyi Lu, Sicheng Xie, Hui Zhang, Xipeng Qiu, Zuxuan Wu, Yu-Gang Jiang",
      "summary": "## Ask-to-Clarify：通过多轮对话解决指令歧义\n\n### 1. 研究背景与问题\n\n具身智能体的最终目标是成为能与人类互动的协作者，而非仅仅被动执行指令的执行者。这要求智能体能够基于人类反馈进行沟通、协调和调整行动。尽管视觉语言模型（VLAs）的进步为实现这一目标提供了途径，但目前大多数基于VLA的具身智能体都采用单向模式：接收指令并直接执行，缺乏反馈机制。这种方法在指令常常模糊不清的真实世界场景中会失效。\n\n### 2. Ask-to-Clarify 框架\n\n本文提出了 **Ask-to-Clarify 框架**，旨在解决指令歧义问题。该框架首先通过多轮对话提问来解决模糊指令，然后端到端地生成低级动作。\n\n#### 2.1 框架组成\n\nAsk-to-Clarify 框架主要由以下两部分组成：\n\n*   **协作组件 (VLM)**：一个视觉语言模型（VLM），负责处理与人类的交互和协作，包括理解指令和生成澄清问题。\n*   **动作组件 (Diffusion)**：一个扩散模型，负责根据已澄清的指令生成具体的低级动作。\n\n此外，框架还包含一个**连接模块**，该模块根据VLM的输出为扩散模型生成条件。它通过指令调整观察结果，以创建可靠的动作生成条件。\n\n#### 2.2 训练策略\n\n该框架采用了一种**两阶段知识隔离训练策略**：\n\n1.  **第一阶段**：使用解决歧义的对话数据对协作组件（VLM）进行微调，使其能够有效处理指令歧义。\n2.  **第二阶段**：在冻结协作组件（VLM）的情况下，整合动作组件（扩散模型）并进行微调。这确保了在优化动作生成能力的同时，保留了智能体的交互能力。\n\n这种训练策略保证了框架能够首先提出问题进行澄清，然后才生成并执行动作。\n\n#### 2.3 推理过程\n\n在推理阶段，一个**信号检测器**充当路由器，帮助框架在提问澄清和采取行动之间进行切换，确保流程的顺畅。\n\n### 3. 实验评估与结果\n\n研究人员在8个真实世界任务中对 Ask-to-Clarify 框架进行了评估。实验结果表明，该框架的性能优于现有的最先进的VLA模型。这表明所提出的框架及其训练策略为实现协作型具身智能体提供了一条可行的路径。",
      "shortSummary": "本文提出了Ask-to-Clarify框架，旨在解决具身智能体在处理模糊指令时的挑战。该框架通过多轮对话澄清指令，然后端到端生成低级动作。它包含一个用于协作的VLM和一个用于动作的扩散模型，并采用两阶段训练策略。在推理时，信号检测器负责切换提问和行动。实验表明，Ask-to-Clarify在8个真实世界任务中超越了现有最先进的视觉语言模型，为实现协作型具身智能体提供了新途径。",
      "translated_title": "Ask-to-Clarify：通过多轮对话解决指令歧义",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents."
    },
    {
      "title": "注意差距：深入探讨大型语言模型多项选择问答中的分词问题 (原标题: Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs)",
      "link": "https://arxiv.org/abs/2509.15020",
      "pubDate": "Thu, 18 Sep 2025 10:47:58 GMT",
      "isoDate": "2025-09-18T10:47:58.000Z",
      "creator": "Mario Sanz-Guerrero, Minh Duc Bui, Katharina von der Wense",
      "summary": "## 大型语言模型多项选择问答中的分词问题\n\n### 背景\n在使用大型语言模型（LLMs）进行多项选择问答（MCQA）评估时，通常会在提示符末尾添加“Answer:”字符串，以便通过下一个词元概率自动提取答案。然而，对于如何对冒号后的空格进行分词，目前没有统一的共识，这通常被视为一个微不足道的选择。\n\n### 核心发现\n本研究揭示了以下关键发现：\n*   这种看似无关紧要的分词差异会导致高达 **11%** 的准确率差异。\n*   它还会重新洗牌模型的排名，引发了对先前工作中LLM比较可靠性的担忧。\n\n### 推荐策略与益处\n*   研究出人意料地推荐了一种特定的策略：**将空格与答案字母一起分词**。\n*   这种策略带来了持续且具有统计学意义的 **性能提升**。\n*   此外，它还改善了 **模型校准**，增强了模型置信度估计的可靠性。\n\n### 研究意义\n本研究结果强调了以下重要性：\n*   精心设计评估的重要性。\n*   制定标准化、透明的评估协议的必要性，以确保结果的可靠性和可比性。\n\n### 其他信息\n*   该论文已被 **EMNLP 2025 主会议** 接受。\n*   主要研究领域：计算与语言 (cs.CL)。",
      "shortSummary": "本研究发现，在大型语言模型多项选择问答中，对“Answer:”后空格的分词方式，这一看似微小的选择，竟能导致高达11%的准确率差异并影响模型排名。研究推荐将空格与答案字母一起分词，此策略显著提升了模型性能和校准。这强调了标准化、透明评估协议对于确保LLM比较可靠性的重要性。",
      "translated_title": "注意差距：深入探讨大型语言模型多项选择问答中的分词问题",
      "images": [],
      "contentSource": "完整文章",
      "content": "When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string \"Answer:\" to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy -- tokenizing the space together with the answer letter -- as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model's confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results."
    },
    {
      "title": "SPATIALGEN：布局引导的3D室内场景生成 (原标题: SPATIALGEN: Layout-guided 3D Indoor Scene Generation)",
      "link": "https://arxiv.org/abs/2509.14981",
      "pubDate": "Thu, 18 Sep 2025 10:12:32 GMT",
      "isoDate": "2025-09-18T10:12:32.000Z",
      "creator": "Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan",
      "summary": "### SPATIALGEN：布局引导的3D室内场景生成\n\n**1. 背景与挑战**\n\n*   **重要性：** 创建高保真3D室内环境模型对于设计、虚拟现实和机器人等应用至关重要。\n*   **现有问题：**\n    *   手动3D建模耗时且劳动密集。\n    *   现有生成式AI方法在平衡视觉质量、多样性、语义一致性和用户控制方面面临挑战。\n    *   主要瓶颈是缺乏针对此任务的大规模、高质量数据集。\n\n**2. 我们的贡献**\n\n*   **新型数据集：** 引入了一个全面的合成数据集，包含：\n    *   12,328个结构化标注场景。\n    *   57,440个房间。\n    *   4.7M张逼真的2D渲染图。\n*   **SpatialGen模型：** 提出了一种新颖的多视图多模态扩散模型——SpatialGen，用于生成逼真且语义一致的3D室内场景。\n\n**3. SpatialGen的工作原理**\n\n*   **输入：** 给定一个3D布局和一个参考图像（可由文本提示生成）。\n*   **输出：** 从任意视角合成：\n    *   外观（彩色图像）。\n    *   几何（场景坐标图）。\n    *   语义（语义分割图）。\n*   **核心特性：** 在不同模态之间保持空间一致性。\n\n**4. 实验结果与影响**\n\n*   **性能：** 实验表明，SpatialGen始终生成优于现有方法的结果。\n*   **开放资源：** 我们将开源数据集和模型，以赋能社区并推动室内场景理解和生成领域的发展。\n\n**5. 相关主题**\n\n*   3D场景生成\n*   扩散模型\n*   场景重建与理解",
      "shortSummary": "SpatialGen是一个布局引导的3D室内场景生成模型，旨在解决手动建模耗时和现有AI方法局限性。为弥补数据集不足，我们构建了一个包含12,328个场景和4.7M渲染图的大规模合成数据集。SpatialGen作为多视图多模态扩散模型，能根据3D布局和参考图像，从任意视角生成逼真且语义一致的3D室内场景的外观、几何和语义信息，并保持空间一致性。实验证明其性能优越，我们将开源数据和模型。",
      "translated_title": "SPATIALGEN：布局引导的3D室内场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation."
    },
    {
      "title": "EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型 (原标题: EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence)",
      "link": "https://arxiv.org/abs/2509.14977",
      "pubDate": "Thu, 18 Sep 2025 10:07:53 GMT",
      "isoDate": "2025-09-18T10:07:53.000Z",
      "creator": "Chaoyin She, Ruifang Lu, Lida Chen, Wei Wang, Qinghua Huang",
      "summary": "# EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型\n\n## 摘要\n\n本文介绍了EchoVLM，一个专门为超声医学影像设计的视觉-语言模型（VLM），旨在解决传统超声诊断中存在的挑战，并提升诊断效率和准确性。\n\n## 背景与挑战\n\n*   **超声影像的优势：** 超声成像因其无电离辐射、成本低和实时成像能力，已成为早期癌症筛查的首选影像模态。\n*   **传统诊断的局限性：** 传统的超声诊断高度依赖医生的专业知识，导致诊断主观性强、效率低下。\n*   **现有通用VLM的不足：** 现有的通用视觉-语言模型在超声医学任务中知识有限，在多器官病变识别中泛化能力差，且在多任务诊断中效率低下。\n\n## EchoVLM模型介绍\n\n*   **模型名称：** EchoVLM\n*   **设计目标：** 解决现有VLM在超声医学领域的局限性，提供一个通用超声智能解决方案。\n*   **核心架构：** 采用专家混合（Mixture of Experts, MoE）架构，使其能够动态地处理不同任务和数据。\n*   **训练数据：** 在涵盖七个解剖区域的数据集上进行训练，确保模型具有广泛的超声医学知识。\n*   **多任务能力：** 该模型能够执行多种超声医学任务，包括：\n    *   超声报告生成\n    *   诊断\n    *   视觉问答（VQA）\n\n## 实验结果与性能提升\n\n*   **对比模型：** EchoVLM与Qwen2-VL进行了对比实验。\n*   **任务：** 主要在超声报告生成任务上进行了评估。\n*   **显著改进：**\n    *   在BLEU-1分数上，EchoVLM相比Qwen2-VL提高了10.15点。\n    *   在ROUGE-1分数上，EchoVLM相比Qwen2-VL提高了4.77点。\n*   **结论：** 这些实验结果表明EchoVLM在提高超声影像诊断准确性方面具有巨大潜力。\n\n## 临床应用前景\n\nEchoVLM为未来的临床应用提供了一个可行的技术解决方案，有望显著提升超声诊断的效率和准确性，从而改善患者护理。\n\n## 资源可用性\n\n模型的源代码和权重可供获取。",
      "shortSummary": "EchoVLM是一个专为超声医学影像设计的动态专家混合视觉-语言模型，旨在解决传统超声诊断的主观性、低效率及通用VLM在超声领域知识不足的挑战。该模型采用专家混合架构，在七个解剖区域数据上训练，可执行超声报告生成、诊断和视觉问答。实验表明，EchoVLM在超声报告生成任务上显著优于Qwen2-VL，BLEU-1和ROUGE-1分数分别提升10.15和4.77点，展现出提升超声诊断准确性的巨大潜力。",
      "translated_title": "EchoVLM：用于通用超声智能的动态专家混合视觉-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at https://github.com/Asunatan/EchoVLM."
    },
    {
      "title": "跨越边界推理：通过测试时审议增强规范对齐 (原标题: Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration)",
      "link": "https://arxiv.org/abs/2509.14760",
      "pubDate": "Thu, 18 Sep 2025 05:08:53 GMT",
      "isoDate": "2025-09-18T05:08:53.000Z",
      "creator": "Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, Yu Cheng",
      "summary": "### 跨越边界推理：通过测试时审议增强规范对齐\n\n**背景与挑战**\n\n*   大型语言模型（LLMs）在现实世界的应用日益广泛，每个应用场景都伴随着用户或组织定制的行为和安全规范（spec）。\n*   这些规范被分为安全规范和行为规范，它们因场景而异，并随着偏好和需求的改变而不断演进。\n*   本文将此挑战形式化为“规范对齐”，重点关注LLMs从行为和安全两个维度遵循动态、场景特定规范的能力。\n\n**提出的方法：Align3**\n\n*   为解决规范对齐的挑战，本文提出了一种名为Align3的轻量级方法。\n*   Align3利用“测试时审议”（Test-Time Deliberation, TTD）机制，结合分层反射和修订过程，以有效推理和遵循规范边界。\n\n**评估基准：SpecBench**\n\n*   为了系统地衡量规范对齐能力，本文还提出了一个统一的基准——SpecBench。\n*   SpecBench涵盖了5个不同的场景、103个具体的规范以及1,500个测试提示。\n\n**实验与主要发现**\n\n*   研究团队在15个推理模型和18个指令模型上进行了广泛实验，并对比了包括Self-Refine、TPO和MoreThink在内的多种TTD方法。\n*   实验结果揭示了三个关键发现：\n    1.  **测试时审议（TTD）显著增强了规范对齐能力。** 这表明在模型推理阶段进行反思和修订是有效的策略。\n    2.  **Align3以最小的额外开销，提升了安全-有用性权衡的前沿。** 这意味着Align3能够在不牺牲模型有用性的前提下，显著提高其安全性。\n    3.  **SpecBench能够有效地揭示LLMs在规范对齐方面的现有差距。** 证明了该基准在诊断模型弱点方面的实用性。\n\n**结论**\n\n*   这些实验结果共同强调了测试时审议作为一种有效策略，在推理和遵循现实世界复杂规范边界方面的巨大潜力。",
      "shortSummary": "本文提出了Align3方法，通过测试时审议（TTD）和分层反射修订，增强大型语言模型（LLMs）对动态、场景特定行为和安全规范的对齐能力。为评估此能力，作者构建了SpecBench基准。实验表明，TTD能有效提升规范对齐，Align3在安全-有用性权衡上表现出色，且SpecBench能揭示对齐差距。研究强调了TTD在处理现实世界规范边界方面的潜力。",
      "translated_title": "跨越边界推理：通过测试时审议增强规范对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs' ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries."
    }
  ],
  "lastUpdated": "2025-09-22T09:33:45.827Z"
}