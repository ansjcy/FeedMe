{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "电影船长：迈向短片电影生成 (原标题: Captain Cinema: Towards Short Movie Generation)",
      "link": "https://arxiv.org/abs/2507.18634",
      "pubDate": "Thu, 24 Jul 2025 13:59:56 GMT",
      "isoDate": "2025-07-24T13:59:56.000Z",
      "creator": "Junfei Xiao, Ceyuan Yang, Lvmin Zhang, Shengqu Cai, Yang Zhao, Yuwei Guo, Gordon Wetzstein, Maneesh Agrawala, Alan Yuille, Lu Jiang",
      "summary": "## 电影船长：短片电影生成框架\n\n“电影船长”（Captain Cinema）是一个创新的生成框架，旨在实现短片电影的自动化生成。该框架能够根据详细的文本电影故事情节描述，生成视觉连贯且叙事一致的高质量短片电影。\n\n### 核心生成流程\n\n“电影船长”采用两阶段方法来确保生成内容的长期连贯性和视觉质量：\n\n1.  **自上而下的关键帧规划（Top-down Keyframe Planning）**：\n    *   **目标**：根据输入的电影故事情节文本描述，首先生成一系列关键帧。这些关键帧勾勒出整个叙事骨架。\n    *   **优势**：此步骤确保了故事情节和视觉外观（例如场景和角色）在长范围内的连贯性。\n\n2.  **自下而上的视频合成（Bottom-up Video Synthesis）**：\n    *   **目标**：将上一步生成的关键帧作为条件信号，输入到一个支持长上下文学习的视频合成模型中。\n    *   **产出**：该模型负责在关键帧之间生成时空动态，从而形成流畅的视频内容。\n\n### 技术创新与训练策略\n\n为了支持多场景长叙事电影作品的稳定高效生成，“电影船长”引入了以下关键技术：\n\n*   **交错训练策略（Interleaved Training Strategy）**：\n    *   该策略专门为多模态扩散变换器（Multimodal Diffusion Transformers, MM-DiT）设计，并特别适用于处理长上下文视频数据。\n    *   通过这种训练方式，模型能够更好地理解和生成复杂的长篇视频序列。\n*   **定制电影数据集**：\n    *   模型在一个经过精心策划的电影数据集上进行训练，该数据集包含交错的数据对，有助于模型学习电影叙事的结构和视觉特征。\n\n### 性能表现\n\n实验结果表明，“电影船长”在自动化创建视觉连贯、叙事一致的短片电影方面表现出色，能够实现高质量和高效率的生成。",
      "shortSummary": "“电影船长”（Captain Cinema）是一个用于短片电影生成的框架。它根据详细的文本故事情节，通过“自上而下的关键帧规划”确保叙事和视觉连贯性，然后通过“自下而上的视频合成”生成关键帧之间的时空动态。该框架采用为长上下文视频数据定制的MM-DiT交错训练策略，并在专门的电影数据集上进行训练，能够高效生成高质量、视觉连贯且叙事一致的短片电影。",
      "translated_title": "电影船长：迈向短片电影生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai"
    },
    {
      "title": "基于深度学习的年龄估计和性别分类，用于精准广告投放 (原标题: Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement)",
      "link": "https://arxiv.org/abs/2507.18565",
      "pubDate": "Thu, 24 Jul 2025 12:41:26 GMT",
      "isoDate": "2025-07-24T12:41:26.000Z",
      "creator": "Muhammad Imran Zaman, Nisar Ahmed",
      "summary": "本文提出了一种新颖的、基于深度学习的方法，用于从面部图像中同时进行年龄和性别分类，旨在提高精准广告活动的有效性。\n\n**核心方法与创新点：**\n*   **定制化卷积神经网络（CNN）架构：** 提出了一种专门为年龄估计和性别分类任务优化的定制CNN架构。\n*   **共享表示学习：** 与现有方法通常独立处理这两个任务不同，该模型利用面部特征中年龄和性别信息固有的相关性，学习共享表示，从而提高了性能。\n\n**训练与数据：**\n*   模型在一个大型、多样化的面部图像数据集上进行训练。\n*   数据集经过精心预处理，以确保对光照、姿态和图像质量变化的鲁棒性。\n\n**实验结果：**\n*   **性别分类准确率：** 达到95%的显著提升。\n*   **年龄估计：** 平均绝对误差（MAE）为5.77年，具有竞争力。\n\n**挑战与未来方向：**\n*   **年轻个体年龄估计挑战：** 分析发现，在准确估计年轻个体年龄方面存在特定挑战。\n*   **改进策略：** 这表明需要进行有针对性的数据增强和模型优化来解决这些偏差。\n*   **架构与超参数影响：** 研究还探讨了不同CNN架构和超参数设置对整体性能的影响，为未来的研究提供了宝贵的见解。",
      "shortSummary": "本文提出了一种基于深度学习的新颖方法，用于从面部图像中同时进行年龄和性别分类，以增强精准广告效果。该方法采用定制CNN架构，通过学习共享表示来利用年龄和性别间的相关性。实验结果显示，性别分类准确率达95%，年龄估计平均绝对误差为5.77年。研究还指出，准确估计年轻个体年龄存在挑战，需进一步优化模型和数据增强。",
      "translated_title": "基于深度学习的年龄估计和性别分类，用于精准广告投放",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a novel deep learning-based approach for simultaneous age and gender classification from facial images, designed to enhance the effectiveness of targeted advertising campaigns. We propose a custom Convolutional Neural Network (CNN) architecture, optimized for both tasks, which leverages the inherent correlation between age and gender information present in facial features. Unlike existing methods that often treat these tasks independently, our model learns shared representations, leading to improved performance. The network is trained on a large, diverse dataset of facial images, carefully pre-processed to ensure robustness against variations in lighting, pose, and image quality. Our experimental results demonstrate a significant improvement in gender classification accuracy, achieving 95%, and a competitive mean absolute error of 5.77 years for age estimation. Critically, we analyze the performance across different age groups, identifying specific challenges in accurately estimating the age of younger individuals. This analysis reveals the need for targeted data augmentation and model refinement to address these biases. Furthermore, we explore the impact of different CNN architectures and hyperparameter settings on the overall performance, providing valuable insights for future research."
    },
    {
      "title": "GLiNER2: 一个高效的、基于模式驱动接口的多任务信息抽取系统 (原标题: GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface)",
      "link": "https://arxiv.org/abs/2507.18546",
      "pubDate": "Thu, 24 Jul 2025 12:11:14 GMT",
      "isoDate": "2025-07-24T12:11:14.000Z",
      "creator": "Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis",
      "summary": "## GLiNER2: 一个高效的多任务信息抽取系统\n\n### 概述\n信息抽取（IE）是众多自然语言处理（NLP）应用的基础。然而，现有的解决方案通常需要针对不同任务构建专门的模型，或者依赖于计算成本高昂的大型语言模型（LLMs）。为解决这些挑战，研究人员提出了GLiNER2，这是一个统一的框架，旨在增强原始GLiNER架构，使其能够在一个高效的单一模型中支持多种信息抽取任务。\n\n### 主要功能与特点\n*   **多任务支持**：GLiNER2能够在一个统一的框架内执行以下任务：\n    *   命名实体识别（NER）\n    *   文本分类\n    *   分层结构化数据抽取\n*   **高效架构**：\n    *   基于预训练的Transformer编码器架构构建。\n    *   保持了CPU效率和紧凑的模型大小，使其在部署时具有优势。\n*   **模式驱动接口**：\n    *   引入了直观的模式驱动接口，实现了多任务组合，简化了不同任务的配置和执行。\n\n### 性能与部署优势\n*   **竞争力表现**：实验结果表明，GLiNER2在抽取和分类任务中均展现出具有竞争力的性能。\n*   **部署可访问性**：与基于LLM的替代方案相比，GLiNER2在部署可访问性方面有显著改进，降低了实际应用的门槛。\n\n### 开源可用性\nGLiNER2已作为开源项目发布，用户可以通过pip安装库，并获取预训练模型和详细文档。该项目旨在促进信息抽取领域的研究和应用。",
      "shortSummary": "GLiNER2是一个高效的、统一的多任务信息抽取系统。它增强了GLiNER架构，在一个基于预训练Transformer编码器的单一模型中支持命名实体识别、文本分类和分层数据抽取。通过直观的模式驱动接口，GLiNER2实现了多任务组合，并在性能上具有竞争力。与大型语言模型相比，它在CPU效率和部署可访问性方面表现出色，并已作为开源库发布。",
      "translated_title": "GLiNER2: 一个高效的、基于模式驱动接口的多任务信息抽取系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2."
    },
    {
      "title": "TTS-VAR：一种用于视觉自回归生成的测试时缩放框架 (原标题: TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation)",
      "link": "https://arxiv.org/abs/2507.18537",
      "pubDate": "Thu, 24 Jul 2025 12:04:55 GMT",
      "isoDate": "2025-07-24T12:04:55.000Z",
      "creator": "Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, Xihui Liu",
      "summary": "# TTS-VAR：一种用于视觉自回归生成的测试时缩放框架\n\n## 摘要\n\n本文介绍了 **TTS-VAR**，这是一个针对视觉自回归（VAR）模型的首个通用测试时缩放框架。该框架将生成过程建模为一个路径搜索问题，旨在在计算效率和探索能力之间动态平衡。\n\n## 关键组成部分和方法\n\nTTS-VAR 整合了以下两个关键策略，灵感来源于 VAR 模型的分层粗到细多尺度生成：\n\n### 1. 自适应递减批处理大小调度\n\n*   在因果生成过程中引入自适应递减的批处理大小调度，以动态平衡计算效率和探索能力。\n\n### 2. 分层处理策略\n\n*   **在粗尺度（Coarse Scales）**：\n    *   **问题观察**：在粗尺度上，生成的标记难以评估，可能导致错误地接受劣质样本或拒绝优质样本。\n    *   **解决方案**：提出**基于聚类的多样性搜索**。由于粗尺度包含足够的结构信息，该方法通过语义特征聚类来保留结构多样性，从而能够在后续阶段选择具有更高潜力的样本。\n\n*   **在细尺度（Fine Scales）**：\n    *   **解决方案**：采用**基于重采样的潜力选择**。该方法利用潜力分数（定义为结合多尺度生成历史的奖励函数）来优先选择有前景的候选样本。\n\n## 实验结果与洞察\n\n*   在强大的 VAR 模型 Infinity 上的实验显示，GenEval 分数显著提高了 8.7%（从 0.69 提高到 0.75）。\n*   **关键洞察**：\n    *   早期阶段的结构特征能有效影响最终生成质量。\n    *   重采样的有效性在不同的生成尺度上有所不同。\n\n## 代码可用性\n\n*   相关代码已在 [此链接](https://this.https/URL) 提供。",
      "shortSummary": "TTS-VAR是首个针对视觉自回归（VAR）模型的测试时缩放框架，将生成建模为路径搜索问题。它通过自适应批处理大小调度，并在粗尺度采用基于聚类的多样性搜索，在细尺度采用基于重采样的潜力选择，以平衡效率和质量。在Infinity VAR模型上的实验显示，GenEval分数提升了8.7%，证明了其在提高生成质量方面的有效性，并揭示了早期结构特征对最终质量的影响。",
      "translated_title": "TTS-VAR：一种用于视觉自回归生成的测试时缩放框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR."
    },
    {
      "title": "DriftMoE：一种处理概念漂移的专家混合方法 (原标题: DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts)",
      "link": "https://arxiv.org/abs/2507.18464",
      "pubDate": "Thu, 24 Jul 2025 10:39:20 GMT",
      "isoDate": "2025-07-24T10:39:20.000Z",
      "creator": "Miguel Aspis, Sebastián A. Cajas Ordónez, Andrés L. Suárez-Cetrulo, Ricardo Simón Carbajo",
      "summary": "# DriftMoE：一种处理概念漂移的专家混合方法\n\n## 背景与挑战\n\n*   从非平稳数据流中学习，特别是面临概念漂移时，需要模型能够即时适应并保持资源高效。\n*   现有自适应集成方法通常依赖粗粒度适应机制或简单的投票方案，未能充分利用专业知识，存在局限性。\n\n## DriftMoE 介绍\n\n*   本文提出 DriftMoE，一种在线专家混合 (Mixture-of-Experts, MoE) 架构，旨在解决上述局限性。\n*   其核心是一个新颖的协同训练框架。\n\n## 核心创新与机制\n\n*   **架构组成：** DriftMoE 包含一个紧凑的神经网络路由器，与一个增量式 Hoeffding 树专家池协同训练。\n*   **共生学习循环（Symbiotic Learning Loop）：** 这是实现专家专业化的关键。\n    1.  **预测阶段：** 路由器选择最适合当前预测任务的专家。\n    2.  **专家更新：** 被选中的相关专家利用真实标签进行增量更新。\n    3.  **路由器优化：** 路由器利用一个“多热正确性掩码”（multi-hot correctness mask）来细化其参数，该掩码会强化每一个准确的专家。\n*   **反馈机制优势：** 这种反馈循环为路由器提供了清晰的训练信号，同时加速了专家的专业化过程。\n\n## 性能评估\n\n*   **基准测试：** 在九个最先进的数据流学习基准上进行了评估。\n*   **漂移类型：** 测试涵盖了突发性、渐进性和真实世界的概念漂移。\n*   **配置变体：** 评估了两种不同的配置：\n    *   **多类别变体：** 专家专注于不同的数据状态（data regimes）。\n    *   **基于任务的变体：** 专家专注于单类别专业化。\n*   **结果：** DriftMoE 在性能上与最先进的数据流学习自适应集成方法具有竞争力，提供了一种原则性且高效的概念漂移适应方法。\n\n## 资源可用性\n\n*   所有代码、数据管道和可复现脚本均可在公共 GitHub 仓库中获取。\n\n## 出版信息\n\n*   已被 SYNDAiTE@ECMLPKDD 2025 研讨会接受。",
      "shortSummary": "DriftMoE 是一种新型在线专家混合 (MoE) 架构，旨在高效处理概念漂移。它通过一个协同训练框架，将神经网络路由器与增量式 Hoeffding 树专家池结合。其核心是共生学习循环：路由器选择专家进行预测，专家增量更新，路由器则通过强化准确专家来优化自身。实验证明，DriftMoE 在多种概念漂移场景下表现出与现有先进方法相当的竞争力，提供了一种有效且高效的自适应学习方案。",
      "translated_title": "DriftMoE：一种处理概念漂移的专家混合方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using a multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with a clear training signal while accelerating expert specialization. We evaluate DriftMoE's performance across nine state-of-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-of-the-art stream learning adaptive ensembles, offering a principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe."
    },
    {
      "title": "Iwin Transformer：使用交错窗口的分层视觉Transformer (原标题: Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows)",
      "link": "https://arxiv.org/abs/2507.18405",
      "pubDate": "Thu, 24 Jul 2025 09:45:48 GMT",
      "isoDate": "2025-07-24T09:45:48.000Z",
      "creator": "Simin Huo, Ning Li",
      "summary": "# Iwin Transformer：使用交错窗口的分层视觉Transformer\n\n本文介绍了 **Iwin Transformer**，这是一种新颖的、无需位置嵌入的分层视觉Transformer模型。该模型通过结合创新的交错窗口注意力机制和深度可分离卷积，能够直接从低分辨率到高分辨率进行微调。\n\n## 核心创新\n\n*   **混合注意力与卷积机制：** Iwin Transformer 独特地利用注意力机制连接远距离的Token，同时应用卷积来连接相邻的Token。这种协同作用使得模型能够在单个模块内实现高效的全局信息交换。\n*   **克服Swin Transformer的局限：** 传统的Swin Transformer需要两个连续的块才能近似实现全局注意力。Iwin Transformer 通过其独特的设计，在一个模块中就实现了高效的全局信息交互，从而克服了这一限制。\n*   **无需位置嵌入：** Iwin Transformer 的设计使其无需依赖传统的位置嵌入，这可能简化模型结构并提高泛化能力。\n\n## 实验结果与应用\n\nIwin Transformer 在多项视觉基准测试中展现出强大的竞争力：\n\n*   **图像分类：** 在ImageNet-1K数据集上取得了87.4%的top-1准确率。\n*   **语义分割：** 在相关任务中表现出色。\n*   **视频动作识别：** 同样展现了强大的性能。\n\n此外，Iwin Transformer 的核心组件被验证可以作为一个独立的模块，无缝地替代类别条件图像生成中的自注意力模块，显示了其良好的模块化和通用性。\n\n## 未来展望\n\nIwin Transformer 中引入的概念和方法具有启发未来研究的潜力，例如在视频生成领域开发 Iwin 3D 注意力机制。\n\n## 资源信息\n\n该模型的代码和模型已公开。本文共14页，包含10张图，已提交至IEEE Transactions on Pattern Analysis and Machine Intelligence。",
      "shortSummary": "Iwin Transformer 是一种新型的、无需位置嵌入的分层视觉Transformer，通过结合交错窗口注意力与深度可分离卷积，在一个模块内实现了高效的全局信息交换，克服了Swin Transformer的局限。它能直接从低到高分辨率微调，并在图像分类（ImageNet-1K上达87.4% top-1准确率）、语义分割和视频动作识别等视觉任务中表现出强大竞争力。其核心组件可作为独立模块用于图像生成，并有望启发未来研究。",
      "translated_title": "Iwin Transformer：使用交错窗口的分层视觉Transformer",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer."
    },
    {
      "title": "TeEFusion：融合文本嵌入以蒸馏无分类器指导 (原标题: TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance)",
      "link": "https://arxiv.org/abs/2507.18192",
      "pubDate": "Thu, 24 Jul 2025 04:45:40 GMT",
      "isoDate": "2025-07-24T04:45:40.000Z",
      "creator": "Minghao Fu, Guo-Hua Wang, Xiaohao Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang",
      "summary": "# TeEFusion：融合文本嵌入以蒸馏无分类器指导\n\n## 摘要\n\n本文介绍了TeEFusion（Text Embeddings Fusion），一种新颖高效的蒸馏方法，旨在解决文本到图像合成中无分类器指导（CFG）和复杂采样策略导致的高昂推理成本问题。\n\n## 背景与问题\n\n*   **文本到图像合成的进步：** 近年来，文本到图像合成技术取得了显著进展，这主要得益于复杂的采样策略和无分类器指导（CFG）。\n*   **CFG的成本问题：** CFG的实现需要进行两次前向传播，当与复杂的采样算法结合时，会导致推理成本过高，严重影响效率。\n\n## TeEFusion 方法\n\nTeEFusion通过以下方式解决上述问题：\n\n*   **指导强度融入文本嵌入：** 该方法直接将指导强度（guidance magnitude）融入到文本嵌入中。\n*   **蒸馏教师模型采样策略：** TeEFusion能够蒸馏教师模型复杂的采样策略。\n*   **线性操作融合嵌入：** 通过简单的线性操作融合条件（conditional）和无条件（unconditional）文本嵌入。\n*   **无需额外参数：** TeEFusion在不增加额外参数的情况下，重构所需的指导。\n*   **学生模型学习：** 该方法使学生模型能够从教师模型通过复杂采样方法产生的输出中学习。\n\n## 实验结果与优势\n\n*   **性能表现：** 在SD3等最先进模型上的大量实验表明，TeEFusion使学生模型能够紧密模仿教师模型的性能。\n*   **采样策略简化：** 学生模型采用更简单、更高效的采样策略。\n*   **推理速度提升：** 学生模型的推理速度比教师模型快高达6倍。\n*   **图像质量保持：** 尽管速度大幅提升，学生模型仍能保持与教师模型通过复杂采样方法获得的图像质量相当的水平。\n\n## 代码可用性\n\nTeEFusion的代码已公开可用。",
      "shortSummary": "TeEFusion是一种高效的蒸馏方法，旨在降低文本到图像合成中无分类器指导（CFG）的高昂推理成本。它通过融合文本嵌入，使学生模型能以更简单的采样策略学习教师模型的复杂输出。实验表明，TeEFusion使学生模型在保持图像质量的同时，推理速度比教师模型快高达6倍。",
      "translated_title": "TeEFusion：融合文本嵌入以蒸馏无分类器指导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}."
    },
    {
      "title": "新版GloVe模型 (原标题: A New Pair of GloVes)",
      "link": "https://arxiv.org/abs/2507.18103",
      "pubDate": "Thu, 24 Jul 2025 01:29:18 GMT",
      "isoDate": "2025-07-24T01:29:18.000Z",
      "creator": "Riley Carlson, John Bauer, Christopher D. Manning",
      "summary": "## 新版GloVe模型报告\n\n### 摘要\n\n本报告详细记录、描述并评估了2024年新发布的英文GloVe（Global Vectors for Word Representation）模型。鉴于2014年发布的原始GloVe模型虽被广泛使用且证明有效，但语言和世界持续演变，研究人员认为更新的模型能更好地满足当前的使用需求。此外，2014年的模型在所使用的确切数据版本和预处理方面缺乏详细文档，新模型旨在纠正这一不足，提供了更完善的文档。\n\n### 模型训练\n\n研究人员训练了两组词嵌入模型，所使用的数据集包括：\n\n*   维基百科（Wikipedia）\n*   Gigaword\n*   Dolma数据集的一个子集\n\n### 模型评估\n\n对新模型的评估通过多种方法进行，包括：\n\n*   **词汇比较**：分析新旧模型词汇库的差异。\n*   **直接测试**：对模型进行直接性能测试。\n*   **命名实体识别（NER）任务**：在NER任务中测试模型的实际应用效果。\n\n### 评估结果\n\n评估结果显示，2024年的GloVe模型表现出以下特点：\n\n*   **词汇更新**：新模型融入了新的、与文化和语言相关的词汇，反映了语言的最新发展。\n*   **结构任务表现**：在类比和相似性等结构性任务上，新模型与旧模型表现相当，保持了原有的高水平性能。\n*   **NER任务改进**：在处理近期、时间依赖性强的NER数据集（例如非西方新闻专线数据）时，新模型展现出显著的性能提升。",
      "shortSummary": "本报告介绍了2024年新版英文GloVe模型。这些模型旨在更新2014年旧版，并弥补其文档不足。新模型使用维基百科、Gigaword和Dolma数据训练，通过词汇比较、直接测试和NER任务进行评估。结果显示，新模型融入了新词汇，在结构任务上表现相当，并在近期NER数据集上性能有所提升，尤其是在处理非西方新闻数据时表现更佳。",
      "translated_title": "新版GloVe模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data."
    },
    {
      "title": "群序列策略优化 (原标题: Group Sequence Policy Optimization)",
      "link": "https://arxiv.org/abs/2507.18071",
      "pubDate": "Wed, 23 Jul 2025 23:50:32 GMT",
      "isoDate": "2025-07-23T23:50:32.000Z",
      "creator": "Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, Junyang Lin",
      "summary": "### 群序列策略优化 (GSPO) 算法介绍\n\n本文介绍了一种名为“群序列策略优化 (Group Sequence Policy Optimization, GSPO)”的强化学习算法。GSPO 是一种稳定、高效且性能卓越的算法，专门用于训练大型语言模型 (LLMs)。\n\n#### 核心创新与区别\n\n*   **传统方法：** 之前的强化学习算法在训练大型语言模型时，通常采用基于“词元级别（token-level）”的重要性比率。\n*   **GSPO 的方法：** GSPO 算法与传统方法不同，它基于“序列似然（sequence likelihood）”来定义重要性比率。在此基础上，GSPO 进行的是“序列级别（sequence-level）”的裁剪、奖励和优化。\n\n#### 算法优势与贡献\n\nGSPO 算法在多个方面展现出显著的优势：\n\n*   **训练效率与性能：** 与 GRPO 算法相比，GSPO 实现了更卓越的训练效率和性能。\n*   **MoE 训练稳定性：** GSPO 显著稳定了专家混合模型（Mixture-of-Experts, MoE）的强化学习训练，这是一个重要的突破，因为 MoE 模型在训练中常面临稳定性挑战。\n*   **基础设施简化潜力：** 该算法还具有简化强化学习基础设施设计的潜力。\n\n#### 实际应用\n\nGSPO 的这些优点已为最新的 Qwen3 模型带来了显著的改进。\n\n#### 相关主题\n\n该研究属于以下领域：\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "群序列策略优化（GSPO）是一种稳定、高效的强化学习算法，用于训练大型语言模型。它通过基于序列似然的序列级别优化，而非传统的词元级别方法，实现了卓越的训练效率和性能。GSPO 显著稳定了专家混合模型（MoE）的强化学习训练，并已成功应用于最新的 Qwen3 模型，带来了显著改进。",
      "translated_title": "群序列策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models."
    },
    {
      "title": "TeleChat2、TeleChat2.5和T1技术报告 (原标题: Technical Report of TeleChat2, TeleChat2.5 and T1)",
      "link": "https://arxiv.org/abs/2507.18013",
      "pubDate": "Wed, 23 Jul 2025 21:00:48 GMT",
      "isoDate": "2025-07-23T21:00:48.000Z",
      "creator": "Zihan Wang, Xinzhang Liu, Yitong Yao, Chao Wang, Yu Zhao, Zhihao Yang, Wenmin Deng, Kaipeng Jia, Jiaxin Peng, Yuyao Huang, Sishi Xiong, Zhuo Jiang, Kaidong Yu, Xiaohui Hu, Fubei Yao, Ruiyu Fang, Zhuoru Jiang, Ruiting Song, Qiyi Xie, Rui Xue, Xuewei He, Yanlei Xue, Zhu Yuan, Zhaoxi Zhang, Zilu Huang, Shiquan Wang, Xin Wang, Hanming Wu, Mingyuan Wang, Xufeng Zhan, Yuhan Sun, Zhaohu Xing, Yuhao Jiang, Bingkai Yang, Shuangyong Song, Yongxiang Li, Zhongjiang He, Xuelong Li",
      "summary": "## TeleChat2、TeleChat2.5和T1技术报告概述\n\n本报告介绍了TeleChat系列模型的最新进展：**TeleChat2**、**TeleChat2.5**和**T1**。这些新模型在前代TeleChat的基础上实现了显著升级，尽管模型架构变化不大，但通过增强的预训练和后训练策略，取得了实质性的性能提升。\n\n### 模型系列概览\n\n*   **TeleChat2**：\n    *   在10万亿高质量、多样化的tokens上进行预训练。\n    *   随后通过监督微调（SFT）和直接偏好优化（DPO）进一步增强其能力。\n\n*   **TeleChat2.5 和 T1**：\n    *   扩展了训练流程，引入了结合领域特定数据集的持续预训练阶段。\n    *   结合强化学习（RL）以提升在代码生成和数学推理任务中的表现。\n\n### 模型特性与性能\n\n*   **T1 变体**：\n    *   专为复杂推理设计。\n    *   支持长链式思维（CoT）推理。\n    *   在数学和编码方面展现出显著改进。\n\n*   **TeleChat2.5 变体**：\n    *   优先考虑速度，提供快速推理能力。\n\n*   **旗舰模型（T1 和 TeleChat2.5）**：\n    *   均采用115B参数的密集型Transformer架构。\n    *   与原始TeleChat相比，在推理和通用任务性能方面取得了重大进展。\n    *   值得注意的是，**T1-115B**在性能上超越了OpenAI的o1-mini和GPT-4o等专有模型。\n\n### 公开可用性\n\n*   **TeleChat2**、**TeleChat2.5**和**T1**模型已公开发布，包括35B和115B参数的后训练版本。\n*   此举旨在赋能开发者和研究人员，为他们提供适用于各种应用场景的先进语言模型。",
      "shortSummary": "本技术报告介绍了TeleChat系列的新一代模型：TeleChat2、TeleChat2.5和T1。这些模型通过增强的训练策略，在保持相似架构下实现了显著性能提升。TeleChat2专注于大规模预训练和优化，而TeleChat2.5和T1则通过持续预训练和强化学习，在代码生成和数学推理方面表现出色。T1擅长复杂推理，TeleChat2.5则注重推理速度。两款旗舰模型均拥有115B参数，T1-115B甚至超越了GPT-4o。所有模型均已公开发布，以支持开发者和研究人员。",
      "translated_title": "TeleChat2、TeleChat2.5和T1技术报告",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We introduce the latest series of TeleChat models: TeleChat2, TeleChat2.5, and T1, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with TeleChat2, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. TeleChat2.5 and T1 expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, TeleChat2.5 prioritizes speed, delivering rapid inference. Both flagship models of T1 and TeleChat2.5 are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, T1-115B outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2, TeleChat2.5 and T1, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications."
    },
    {
      "title": "Ultra3D：基于部分注意力的高效高保真3D生成 (原标题: Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention)",
      "link": "https://arxiv.org/abs/2507.17745",
      "pubDate": "Wed, 23 Jul 2025 13:57:16 GMT",
      "isoDate": "2025-07-23T13:57:16.000Z",
      "creator": "Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin",
      "summary": "## Ultra3D：基于部分注意力的高效高保真3D生成\n\n### 摘要\n\n本文介绍了Ultra3D，一个旨在解决当前3D内容生成中计算效率问题的框架，特别是在使用稀疏体素表示时。现有方法虽然提升了3D模型的质量和精细度，但其两阶段扩散管道中注意力机制的二次复杂度导致了严重的计算效率低下。\n\n### 核心创新与方法\n\nUltra3D通过以下关键创新显著加速了稀疏体素建模，同时不损害生成质量：\n\n1.  **第一阶段：高效的粗略对象布局生成**\n    *   Ultra3D利用紧凑的**VecSet表示**来高效地生成对象的粗略布局。\n    *   这种方法有效减少了token数量，从而加速了体素坐标的预测过程。\n\n2.  **第二阶段：基于部分注意力的潜在特征精炼**\n    *   为了精炼每个体素的潜在特征，Ultra3D引入了**部分注意力（Part Attention）**机制。\n    *   **部分注意力特性：**\n        *   这是一种几何感知的局部注意力机制。\n        *   它将注意力计算限制在语义一致的部分区域内，而非全局范围。\n        *   这种设计既保留了结构连续性，又避免了不必要的全局注意力计算，从而大幅提升效率。\n        *   在潜在特征生成方面，部分注意力实现了高达6.7倍的速度提升。\n\n3.  **支持机制：可扩展的部分标注管道**\n    *   为了支持部分注意力机制的有效运作，Ultra3D构建了一个可扩展的部分标注管道。\n    *   该管道能够将原始网格数据转换为带有部分标签的稀疏体素表示。\n\n### 实验结果\n\n广泛的实验证明，Ultra3D能够支持1024分辨率的高分辨率3D生成。在视觉保真度和用户偏好方面，Ultra3D均达到了最先进的性能。",
      "shortSummary": "Ultra3D是一个高效高保真3D生成框架，旨在解决现有稀疏体素方法中注意力机制导致的计算效率低下问题。它通过两阶段方法实现：首先利用VecSet高效生成粗略布局，减少token数量；其次引入“部分注意力”机制，将注意力计算限制在语义一致的局部区域，显著加速潜在特征生成（高达6.7倍）。Ultra3D支持1024分辨率的3D生成，并在视觉保真度和用户偏好方面达到最先进水平。",
      "translated_title": "Ultra3D：基于部分注意力的高效高保真3D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference."
    },
    {
      "title": "Yume：一个交互式世界生成模型 (原标题: Yume: An Interactive World Generation Model)",
      "link": "https://arxiv.org/abs/2507.17744",
      "pubDate": "Wed, 23 Jul 2025 13:57:09 GMT",
      "isoDate": "2025-07-23T13:57:09.000Z",
      "creator": "Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang",
      "summary": "### Yume: 一个交互式世界生成模型\n\n**项目概述**\nYume 是一个旨在利用图像、文本或视频创建交互式、逼真且动态世界的模型。其目标是允许用户使用外围设备或神经信号进行探索和控制。当前发布的预览版本能够从输入的图像生成一个动态世界，并支持通过键盘操作进行探索。\n\n**核心框架组件**\n为了实现高保真和交互式的视频世界生成，Yume 引入了一个精心设计的框架，该框架包含四个主要组成部分：\n\n1.  **相机运动量化**\n    *   为了实现稳定的训练和用户友好的键盘输入交互，模型对相机运动进行了量化处理。\n\n2.  **视频生成架构**\n    *   引入了带有记忆模块的 Masked Video Diffusion Transformer (MVDT)。\n    *   该架构能够以自回归方式实现无限视频生成。\n\n3.  **高级采样器**\n    *   集成了两种训练无关的机制以提升视觉质量和控制精度：\n        *   **无训练抗伪影机制 (AAM)**：用于改善视觉质量。\n        *   **基于随机微分方程的时间旅行采样 (TTS-SDE)**：用于实现更精确的控制。\n\n4.  **模型加速**\n    *   通过对抗蒸馏和缓存机制的协同优化，实现了模型的加速。\n\n**训练与成果**\nYume 模型使用高质量的世界探索数据集 \\sekai 进行训练。该模型在多样化的场景和应用中取得了显著成果。\n\n**资源可用性与未来展望**\n*   所有数据、代码库和模型权重均已公开提供。\n*   Yume 项目将每月更新，以逐步实现其最初设定的宏伟目标。\n*   项目页面提供了更多详细信息。",
      "shortSummary": "Yume 是一个交互式世界生成模型，旨在利用图像、文本或视频创建逼真、动态且可探索的世界。其预览版能从图像生成动态世界并支持键盘探索。核心框架包含相机运动量化、基于MVDT的无限视频生成架构、用于提升质量和控制的先进采样器（AAM和TTS-SDE），以及模型加速技术。该模型使用\\sekai数据集训练，表现出色，所有资源均已公开，并将每月更新。",
      "translated_title": "Yume：一个交互式世界生成模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/."
    },
    {
      "title": "一个领域能否助力其他领域？一项基于数据驱动的强化学习多领域推理研究 (原标题: Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.17512",
      "pubDate": "Wed, 23 Jul 2025 09:51:04 GMT",
      "isoDate": "2025-07-23T09:51:04.000Z",
      "creator": "Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu",
      "summary": "## 强化学习多领域推理研究\n\n### 摘要\n\n现有研究主要集中于孤立的推理领域（如数学问题解决、编码任务或逻辑推理），以增强大型语言模型（LLMs）的推理能力。然而，现实世界的推理场景本质上需要多种认知技能的综合应用。尽管如此，在强化学习（RL）框架下，这些推理技能之间的相互作用仍未被充分理解。\n\n### 研究目的\n\n为了弥补这一空白，本研究系统性地调查了可验证奖励强化学习（RLVR）框架下的多领域推理，明确关注三个主要领域：数学推理、代码生成和逻辑谜题解决。\n\n### 研究方法与组成\n\n本研究包含四个关键组成部分：\n\n1.  **单领域训练评估**：\n    *   利用GRPO算法和Qwen-2.5-7B模型家族，本研究彻底评估了模型在单领域数据集上训练时的领域内改进和跨领域泛化能力。\n\n2.  **跨领域联合训练交互**：\n    *   此外，本研究考察了在联合跨领域训练过程中出现的复杂交互，包括相互增强和冲突。\n\n3.  **SFT对RL的影响分析**：\n    *   为了进一步理解指令微调（SFT）对强化学习（RL）的影响，本研究还分析并比较了在相同RL配置下，基础模型（base models）和指令微调模型（instruct models）之间的性能差异。\n\n4.  **RL训练细节探索**：\n    *   此外，本研究深入探讨了关键的RL训练细节，系统性地探索了课程学习策略、奖励设计变体以及特定语言因素的影响。\n\n### 主要发现与贡献\n\n通过大量实验，本研究结果为领域交互的动态提供了重要见解，揭示了影响专业化和泛化推理性能的关键因素。这些发现为优化RL方法以培养LLMs全面的多领域推理能力提供了宝贵指导。",
      "shortSummary": "本研究系统性地探讨了在强化学习（RL）框架下大型语言模型（LLMs）的多领域推理能力。针对数学、代码和逻辑谜题三个核心领域，研究评估了单领域训练的泛化性、跨领域联合训练的交互（增强与冲突）、指令微调（SFT）对RL的影响，并深入分析了RL训练细节（如课程学习和奖励设计）。研究结果揭示了影响LLMs专业化和泛化推理性能的关键因素，为优化RL方法以提升LLMs的综合多领域推理能力提供了重要指导。",
      "translated_title": "一个领域能否助力其他领域？一项基于数据驱动的强化学习多领域推理研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs."
    },
    {
      "title": "HLFormer：通过双曲学习增强部分相关视频检索 (原标题: HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning)",
      "link": "https://arxiv.org/abs/2507.17402",
      "pubDate": "Wed, 23 Jul 2025 06:59:46 GMT",
      "isoDate": "2025-07-23T06:59:46.000Z",
      "creator": "Li Jun, Wang Jinpeng, Tan Chaolei, Lian Niu, Chen Long, Zhang Min, Wang Yaowei, Xia Shu-Tao, Chen Bin",
      "summary": "## HLFormer：通过双曲学习增强部分相关视频检索\n\n### 1. 引言与背景\n\n部分相关视频检索（Partially Relevant Video Retrieval, PRVR）旨在将未剪辑视频与仅描述部分内容的文本查询进行匹配。现有方法在欧几里得空间中存在几何失真问题，这可能错误地表示视频固有的层级结构，并忽略某些层级语义，最终导致次优的时间建模。\n\n### 2. HLFormer 框架概述\n\n为解决上述问题，本文提出了 HLFormer，这是首个用于 PRVR 的双曲建模框架。HLFormer 利用双曲空间学习来弥补欧几里得空间在层级建模能力上的不足，从而更有效地处理视频数据的复杂层级结构。\n\n### 3. HLFormer 的核心组件\n\nHLFormer 包含以下关键创新模块：\n\n*   **混合空间编码**：\n    *   HLFormer 集成了洛伦兹注意力块（Lorentz Attention Block）和欧几里得注意力块（Euclidean Attention Block）。\n    *   通过这种混合设计，模型能够在混合空间中对视频嵌入进行编码，从而结合两种几何空间的优势。\n\n*   **动态特征融合**：\n    *   模型采用均值引导自适应交互模块（Mean-Guided Adaptive Interaction Module）。\n    *   该模块用于动态地融合来自不同注意力块的特征，确保信息有效整合。\n\n*   **偏序保持损失**：\n    *   引入了偏序保持损失（Partial Order Preservation Loss）。\n    *   该损失函数通过洛伦兹锥约束（Lorentzian cone constraints）强制执行“文本 < 视频”的层级关系。\n    *   这种方法进一步增强了跨模态匹配，通过强化视频内容与文本查询之间的部分相关性。\n\n### 4. 实验结果与可用性\n\n广泛的实验结果表明，HLFormer 的性能优于现有的最先进方法。该研究成果已被 ICCV'25 接收，论文共13页，包含6个图和4个表。相关代码已发布。\n\n### 5. 研究领域\n\n该研究主要涉及计算机视觉与模式识别（cs.CV）、信息检索（cs.IR）和多媒体（cs.MM）等领域。",
      "shortSummary": "HLFormer 是首个利用双曲学习的PRVR（部分相关视频检索）框架，旨在解决现有方法在欧几里得空间中处理视频层级结构和部分相关性时的局限性。它通过结合洛伦兹和欧几里得注意力块进行混合空间编码，使用均值引导自适应交互模块动态融合特征，并引入偏序保持损失来强化“文本 < 视频”的层级关系。实验证明，HLFormer 在部分相关视频检索任务中超越了现有最先进的方法。",
      "translated_title": "HLFormer：通过双曲学习增强部分相关视频检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of matching untrimmed videos with text queries describing only partial content. Existing methods suffer from geometric distortion in Euclidean space that sometimes misrepresents the intrinsic hierarchical structure of videos and overlooks certain hierarchical semantics, ultimately leading to suboptimal temporal modeling. To address this issue, we propose the first hyperbolic modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space learning to compensate for the suboptimal hierarchical modeling capabilities of Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block and Euclidean Attention Block to encode video embeddings in hybrid spaces, using the Mean-Guided Adaptive Interaction Module to dynamically fuse features. Additionally, we introduce a Partial Order Preservation Loss to enforce \"text &lt; video\" hierarchy through Lorentzian cone constraints. This approach further enhances cross-modal matching by reinforcing partial relevance between video content and text queries. Extensive experiments show that HLFormer outperforms state-of-the-art methods. Code is released at https://github.com/lijun2005/ICCV25-HLFormer."
    },
    {
      "title": "DesignLab：通过迭代检测和纠正设计幻灯片 (原标题: DesignLab: Designing Slides Through Iterative Detection and Correction)",
      "link": "https://arxiv.org/abs/2507.17202",
      "pubDate": "Wed, 23 Jul 2025 00:49:48 GMT",
      "isoDate": "2025-07-23T00:49:48.000Z",
      "creator": "Jooyeol Yun, Heng Wang, Yotaro Shimose, Jaegul Choo, Shingo Takamatsu",
      "summary": "# DesignLab：通过迭代检测和纠正设计幻灯片\n\n## 摘要\n\n本文提出了 DesignLab，一个旨在解决非专业人士在设计高质量演示文稿幻灯片时所面临挑战的系统。尽管现有的自动化工具可以提供布局和配色方案建议，但它们通常缺乏自我完善输出的能力，而这在实际工作流程中至关重要。\n\n## DesignLab 方法论\n\nDesignLab 的核心思想是将幻灯片设计过程分解为两个明确的角色：\n\n*   **设计审阅者（Design Reviewer）**：负责识别设计中存在的问题和缺陷。\n*   **设计贡献者（Design Contributor）**：负责纠正由审阅者发现的设计问题。\n\n这种分解使得一个迭代循环成为可能：审阅者持续检测问题，贡献者随即进行纠正。通过这种方式，幻灯片草稿可以在每次迭代中得到进一步的打磨和完善，最终达到传统方法难以企及的专业品质。\n\n## 技术实现\n\n为了实现这两个角色，DesignLab 对大型语言模型（LLMs）进行了微调。为了训练这些模型，研究人员通过引入受控的扰动（即故意制造设计错误）来模拟中间草稿。这使得设计审阅者能够学习如何识别各种设计错误，而设计贡献者则能够学习如何有效地修复这些错误。\n\n## 实验结果与优势\n\n实验结果表明，DesignLab 在幻灯片设计生成方面优于现有的方法，甚至包括一款商业工具。DesignLab 的卓越表现归因于其采纳了设计的迭代性质，这使得它能够生成高度精美和专业的幻灯片。\n\n**重要提示：** 文章内容中未包含有效的实际图片链接，因此本摘要中不包含任何图片。",
      "shortSummary": "DesignLab 提出了一种通过迭代检测和纠正来设计高质量幻灯片的新方法。针对非专业人士设计难题及现有工具无法自我完善的痛点，DesignLab 将设计过程分解为“设计审阅者”和“设计贡献者”两个角色，通过微调大型语言模型实现迭代优化。审阅者识别问题，贡献者负责纠正，从而持续提升幻灯片质量。实验证明，DesignLab 优于现有设计工具，能生成更专业、精美的幻灯片。",
      "translated_title": "DesignLab：通过迭代检测和纠正设计幻灯片",
      "images": [],
      "contentSource": "完整文章",
      "content": "Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides."
    },
    {
      "title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理 (原标题: ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning)",
      "link": "https://arxiv.org/abs/2507.16815",
      "pubDate": "Tue, 22 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-22T13:59:46.000Z",
      "creator": "Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang",
      "summary": "本文提出了一种名为 ThinkAct 的双系统框架，旨在解决视觉-语言-动作（VLA）推理任务中的挑战。\n\n**背景与问题**\n*   VLA 推理任务要求智能体解释多模态指令、执行长周期规划并在动态环境中自适应行动。\n*   现有方法通常采用端到端训练，直接将输入映射到动作，缺乏显式推理过程。\n*   这限制了它们在多步骤规划和适应复杂任务变化方面的能力。\n\n**ThinkAct 框架**\n*   **核心理念：** ThinkAct 通过强化视觉潜在规划，连接高层推理与低层动作执行。\n*   **工作机制：**\n    1.  **多模态大型语言模型（LLM）训练：** 训练一个多模态 LLM 来生成具身推理计划。\n    2.  **奖励引导：** 这些推理计划通过强化与动作对齐的视觉奖励进行引导，奖励基于目标完成度和轨迹一致性。\n    3.  **视觉计划潜在表示：** 生成的推理计划被压缩成一个视觉计划潜在表示。\n    4.  **下游动作模型：** 这个视觉计划潜在表示作为条件，输入到一个下游动作模型中，以在目标环境中实现鲁棒的动作执行。\n\n**实验结果与优势**\n*   在具身推理和机器人操作基准上的广泛实验表明，ThinkAct 展现出：\n    *   **少样本适应能力：** 能够以少量样本进行适应。\n    *   **长周期规划能力：** 能够进行跨越多步骤的规划。\n    *   **自我纠正行为：** 在复杂的具身 AI 任务中表现出自我纠正能力。",
      "shortSummary": "ThinkAct 是一种双系统框架，通过强化视觉潜在规划，解决了视觉-语言-动作（VLA）推理任务中现有端到端方法缺乏显式推理的问题。它训练一个多模态LLM生成具身推理计划，并将其压缩为视觉潜在表示，以指导下游动作模型执行。实验证明，ThinkAct 在复杂具身AI任务中实现了少样本适应、长周期规划和自我纠正能力。",
      "translated_title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks."
    },
    {
      "title": "用于视觉-语言慢思考推理的半离策略强化学习 (原标题: Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning)",
      "link": "https://arxiv.org/abs/2507.16814",
      "pubDate": "Tue, 22 Jul 2025 13:59:34 GMT",
      "isoDate": "2025-07-22T13:59:34.000Z",
      "creator": "Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, Kai Chen",
      "summary": "## 用于视觉-语言慢思考推理的半离策略强化学习 (SOPHIA)\n\n### 引言\n\n提升大型视觉-语言模型（LVLMs）的视觉慢思考推理能力对于解决复杂的多模态任务至关重要。然而，现有方法面临挑战：\n\n*   **在策略强化学习（On-policy RL）**：由于LVLMs主要通过视觉-语言对齐进行训练，其初始能力限制了策略探索空间，难以有效发展慢思考能力。\n*   **离策略强化学习（Off-policy RL）**：直接从外部模型蒸馏轨迹可能导致视觉幻觉，因为不同模型间的视觉感知能力存在不匹配。\n\n### SOPHIA 方法\n\n为解决上述问题，本文提出了一种名为 **SOPHIA** (Semi-Off-Policy RL for vision-language slow-tHInking reAsoning) 的简单且可扩展的半离策略强化学习方法。\n\nSOPHIA 的核心机制包括：\n\n1.  **构建半离策略行为模型**：SOPHIA 结合了可训练LVLM的“在策略视觉理解”能力与语言模型的“离策略慢思考推理”能力。\n2.  **奖励分配与传播**：\n    *   对推理过程分配基于结果的奖励。\n    *   将视觉奖励反向传播。\n3.  **模型学习**：LVLM 通过离策略RL算法，利用获得的推理轨迹和传播的奖励来学习慢思考推理能力。\n\n### 实验与结果\n\n研究团队使用 InternVL2.5 和 InternVL3.0（包括 8B 和 38B 参数规模）进行了广泛实验，结果表明 SOPHIA 具有显著的有效性：\n\n*   **性能提升**：SOPHIA 使 InternVL3.0-38B 的平均性能提升了 8.50%。\n*   **最先进表现**：在多个多模态推理基准测试中，SOPHIA 在开源LVLM中达到了最先进（state-of-the-art, SOTA）的性能。\n*   **超越闭源模型**：在挑战性的 MathVision 和 OlympiadBench 基准测试中，SOPHIA 甚至超越了一些闭源模型（例如 GPT-4.1），分别取得了 49.08% 和 49.95% 的 pass@1 准确率。\n\n### 分析\n\n分析结果显示，SOPHIA 的表现优于传统的监督微调（supervised fine-tuning）和直接的在策略RL方法。这表明 SOPHIA 为后续的在策略训练提供了更好的策略初始化，进一步提升了模型的推理能力。",
      "shortSummary": "SOPHIA是一种半离策略强化学习方法，旨在增强大型视觉-语言模型（LVLMs）的慢思考推理能力。它通过结合LVLM的视觉理解与语言模型推理，并利用奖励传播机制进行训练。实验表明，SOPHIA显著提升了InternVL3.0-38B的性能，在多模态推理基准测试中达到开源LVLM的最先进水平，并在MathVision和OlympiadBench等挑战性任务上超越了部分闭源模型。",
      "translated_title": "用于视觉-语言慢思考推理的半离策略强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training."
    },
    {
      "title": "HOComp：交互感知的人体-物体合成 (原标题: HOComp: Interaction-Aware Human-Object Composition)",
      "link": "https://arxiv.org/abs/2507.16813",
      "pubDate": "Tue, 22 Jul 2025 13:59:21 GMT",
      "isoDate": "2025-07-22T13:59:21.000Z",
      "creator": "Dong Liang, Jinyuan Jia, Yuhao Liu, Rynson W. H. Lau",
      "summary": "### HOComp：交互感知的人体-物体合成\n\n**问题背景**\n\n现有的图像引导合成方法在将前景对象插入背景图像的指定区域时，通常难以在涉及人体-物体交互的任务中实现无缝且自然的合成。这些方法在处理人与物体之间的复杂互动时，往往无法确保合成结果的和谐与一致性。\n\n**HOComp 方法概述**\n\n本文提出了一种名为 HOComp 的新颖方法，旨在解决人体-物体合成中的挑战。HOComp 专注于将前景对象合成到以人为中心的背景图像上，同时确保前景对象与背景人物之间存在和谐的交互，并保持两者外观的一致性。\n\n**HOComp 的两大核心设计**\n\n1.  **MLLMs 驱动的基于区域的姿态引导 (MRPG)**：\n    *   **交互区域与类型识别**：利用多模态大型语言模型 (MLLMs) 来识别图像中的交互区域以及具体的交互类型（例如，握持、举起）。\n    *   **粗粒度到细粒度姿态约束**：为生成的交互姿态提供从粗粒度到细粒度的约束。这意味着它首先确定大致的交互姿态，然后进行精细调整。\n    *   **人体姿态关键点整合**：结合人体姿态关键点来跟踪动作变化，并施加更精细的姿态约束，确保交互的准确性。\n\n2.  **细节一致的外观保持 (DCAP)**：\n    *   **统一机制**：DCAP 机制整合了多种技术，以确保合成结果的视觉质量。\n    *   **形状感知注意力调制**：通过形状感知的注意力调制机制，确保前景对象的形状和纹理与背景环境保持一致。\n    *   **多视图外观损失**：引入多视图外观损失，进一步提升合成对象外观的真实感和一致性。\n    *   **背景一致性损失**：通过背景一致性损失，确保背景中的人物能够被忠实地再现，且与合成对象无缝融合。\n\n**新数据集：交互感知的人体-物体合成 (IHOC)**\n\n为了支持和评估这项任务，本文首次提出了一个名为“交互感知的人体-物体合成 (IHOC)”的数据集。该数据集专门用于研究和开发人体-物体交互合成方法。\n\n**实验结果**\n\n在 IHOC 数据集上的实验结果表明，HOComp 能够有效地生成具有和谐人体-物体交互和一致外观的图像。无论是从定性还是定量的角度来看，HOComp 都优于现有的相关方法，证明了其在解决该任务上的卓越性能。",
      "shortSummary": "HOComp 是一种新颖的图像合成方法，旨在解决人体-物体交互合成中现有方法的不足。它通过两大核心设计实现和谐自然的合成：一是“MLLMs 驱动的基于区域的姿态引导 (MRPG)”，利用 MLLMs 识别交互区域和类型，并提供精细的姿态约束；二是“细节一致的外观保持 (DCAP)”，通过形状感知注意力、多视图外观损失和背景一致性损失确保外观真实性。论文还提出了首个“交互感知的人体-物体合成 (IHOC)”数据集。实验证明 HOComp 在生成高质量人体-物体交互方面表现优异。",
      "translated_title": "HOComp：交互感知的人体-物体合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively."
    },
    {
      "title": "MegaScience：推动科学推理后训练数据集的前沿 (原标题: MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning)",
      "link": "https://arxiv.org/abs/2507.16812",
      "pubDate": "Tue, 22 Jul 2025 13:59:03 GMT",
      "isoDate": "2025-07-22T13:59:03.000Z",
      "creator": "Run-Ze Fan, Zengzhi Wang, Pengfei Liu",
      "summary": "### MegaScience：推动科学推理后训练数据集的前沿\n\n**引言与背景**\n科学推理能力对于开发AI科学家和支持人类研究人员在自然科学发现领域取得进展至关重要。然而，开源社区主要关注数学和编码领域，却忽视了科学领域，这在很大程度上是由于缺乏开放、大规模、高质量、可验证的科学推理数据集。\n\n**核心贡献**\n为了弥补这一空白，研究团队提出了两项主要贡献：\n\n1.  **TextbookReasoning 数据集**\n    *   这是一个开放数据集，旨在提供真实、高质量的科学推理数据。\n    *   其参考答案从12,000本大学级别的科学教科书中提取。\n    *   包含65万个推理问题，涵盖7个不同的科学学科。\n\n2.  **MegaScience 数据集**\n    *   这是一个大规模、高质量的开源数据集混合体，总计包含125万个实例。\n    *   该数据集是通过系统的消融研究开发的，旨在评估各种数据选择方法，从而为每个公开可用的科学数据集识别出最优子集。\n\n3.  **综合评估系统**\n    *   研究团队构建了一个全面的评估系统，以确保评估的准确性。\n    *   该系统涵盖了15个基准测试中的多样化主题和问题类型。\n    *   整合了全面的答案提取策略，以确保评估指标的准确性。\n\n**实验结果与性能**\n\n*   **卓越的性能与训练效率**：实验结果表明，与现有开源科学数据集相比，TextbookReasoning 和 MegaScience 数据集在性能和训练效率方面表现出显著优势，并且能够生成更简洁的响应。\n*   **模型性能提升**：研究团队在 MegaScience 数据集上训练了Llama3.1、Qwen2.5和Qwen3系列的基础模型。这些模型在平均性能上显著优于相应的官方指令模型。\n*   **规模效益**：MegaScience 数据集对更大、更强的模型表现出更大的有效性，这表明在科学调优方面存在规模效益。\n\n**社区贡献**\n为了推动科学推理研究的进展，研究团队向社区发布了以下资源：\n*   数据整理管道\n*   评估系统\n*   所有数据集（包括TextbookReasoning和MegaScience）\n*   七个训练好的模型\n\n**图片说明**\n文章内容中未包含有效的实际图片链接，因此摘要中不包含图片。",
      "shortSummary": "该研究旨在解决科学推理领域高质量开放数据集的缺乏。为此，团队推出了TextbookReasoning，一个包含65万个大学级别科学推理问题的开放数据集；并引入了MegaScience，一个包含125万个实例的大规模高质量数据集。通过综合评估系统，实验证明这些数据集显著提升了AI模型在科学推理任务上的性能和训练效率，尤其对大型模型效果更佳。所有数据、系统和模型均已开源，以促进科学推理研究。",
      "translated_title": "MegaScience：推动科学推理后训练数据集的前沿",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research."
    },
    {
      "title": "Agentar-Fin-R1：通过领域专业知识、训练效率和高级推理增强金融智能 (原标题: Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning)",
      "link": "https://arxiv.org/abs/2507.16802",
      "pubDate": "Tue, 22 Jul 2025 13:52:16 GMT",
      "isoDate": "2025-07-22T13:52:16.000Z",
      "creator": "Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Jingze Song, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang",
      "summary": "## Agentar-Fin-R1：通过领域专业知识、训练效率和高级推理增强金融智能\n\n### 引言\n\n大型语言模型（LLMs）在金融应用中展现出巨大潜力，但现有模型在面对需要复杂推理能力、严格可信度标准以及高效适应特定领域需求的场景时，常表现出局限性。\n\n### Agentar-Fin-R1 模型概述\n\n为解决上述挑战，我们推出了 Agentar-Fin-R1 系列金融大语言模型（包括 8B 和 32B 参数版本）。这些模型基于 Qwen3 基础模型进行专门工程设计，旨在显著增强金融应用的推理能力、可靠性和领域专业性。\n\n### 优化方法\n\nAgentar-Fin-R1 的优化方法整合了以下关键要素：\n\n*   **高质量、系统化的金融任务标签系统：** 为模型训练提供精确指导。\n*   **全面的多层可信度保障框架：** 该框架包含：\n    *   **高质量可信知识工程：** 确保模型学习到准确可靠的金融知识。\n    *   **多智能体可信数据合成：** 通过多智能体协作生成可信数据。\n    *   **严格的数据验证治理：** 对数据进行严格的质量控制和验证。\n\n### 训练效率提升\n\n通过以下创新方法，Agentar-Fin-R1 实现了训练效率的显著提升：\n\n*   **标签引导的自动化难度感知优化：** 根据任务难度自动调整优化策略。\n*   **两阶段训练流程：** 优化训练过程，提高效率和效果。\n*   **动态归因系统：** 增强模型的可解释性和调试能力。\n\n### 评估与实验结果\n\nAgentar-Fin-R1 接受了全面的评估，包括：\n\n*   **主流金融基准：** Fineva、FinEval 和 FinanceIQ。\n*   **通用推理数据集：** MATH-500 和 GPQA-diamond。\n*   **创新性 Finova 评估基准：** 为彻底评估模型在实际部署中的能力，我们创新性地提出了 Finova 评估基准。该基准专注于智能体级别的金融推理和合规性验证，目前已公开可用。\n\n实验结果表明，Agentar-Fin-R1 不仅在金融任务上取得了最先进的性能（State-of-the-Art, SOTA），而且展现出卓越的通用推理能力。这验证了其作为高风险金融应用中可信解决方案的有效性。",
      "shortSummary": "Agentar-Fin-R1 是一个基于 Qwen3 的金融大语言模型系列（8B/32B），旨在解决现有LLM在金融领域复杂推理、可信度和领域适应性方面的不足。它通过高质量标签系统、多层可信度框架和高效训练方法进行优化。模型在主流金融基准和通用推理数据集上表现出色，并在新提出的Finova基准上验证了其智能体级金融推理和合规性能力。Agentar-Fin-R1 在金融任务上达到SOTA，并展现出卓越的通用推理能力，是高风险金融应用的可信解决方案。",
      "translated_title": "Agentar-Fin-R1：通过领域专业知识、训练效率和高级推理增强金融智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova."
    }
  ],
  "lastUpdated": "2025-07-26T09:31:40.961Z"
}