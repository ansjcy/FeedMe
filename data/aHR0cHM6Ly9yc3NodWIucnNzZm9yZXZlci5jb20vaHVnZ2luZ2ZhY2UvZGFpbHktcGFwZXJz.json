{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "CAR-Flow：条件感知重参数化对齐源和目标以实现更好的流匹配 (原标题: CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching)",
      "link": "https://arxiv.org/abs/2509.19300",
      "pubDate": "Tue, 23 Sep 2025 13:59:31 GMT",
      "isoDate": "2025-09-23T13:59:31.000Z",
      "creator": "Chen Chen, Pengsheng Guo, Liangchen Song, Jiasen Lu, Rui Qian, Xinze Wang, Tsu-Jui Fu, Wei Liu, Yinfei Yang, Alex Schwing",
      "summary": "### CAR-Flow：条件感知重参数化对齐源和目标以实现更好的流匹配\n\n**背景与问题**\n*   条件生成建模旨在从数据-条件对中学习条件数据分布。\n*   现有的扩散和基于流的方法通过一个学习到的模型，将初始的、忽略条件的标准高斯噪声传输到条件数据分布。\n*   这要求模型同时学习质量传输和条件注入，增加了模型的复杂性和学习负担。\n\n**提出的方法：CAR-Flow**\n*   为了减轻模型负担，本文提出了“CAR-Flow”（Condition-Aware Reparameterization for Flow Matching），即用于流匹配的条件感知重参数化方法。\n*   CAR-Flow是一个轻量级的、学习到的偏移量，用于对源分布、目标分布或两者进行条件化。\n\n**CAR-Flow 的优势**\n*   通过重新定位这些分布，CAR-Flow有效缩短了模型必须学习的概率路径。\n*   这在实践中能带来更快的训练速度和更高的效率。\n\n**实验结果**\n*   **低维合成数据：** 在低维合成数据上，研究人员对CAR的效果进行了可视化和量化分析。\n*   **高维自然图像数据（ImageNet-256）：** 将SiT-XL/2模型与CAR-Flow结合后，其FID（Fréchet Inception Distance）从2.07显著降低到1.68。\n*   **参数开销：** 引入CAR-Flow仅增加了不到0.6%的额外参数，表明其具有极高的效率和实用性。\n\n**结论**\nCAR-Flow通过引入条件感知重参数化，有效简化了条件生成模型学习的路径，显著提高了训练效率和生成质量，且仅增加极小的参数量，为条件生成建模提供了一种高效的改进方案。",
      "shortSummary": "CAR-Flow是一种用于流匹配的条件感知重参数化方法，通过学习一个轻量级偏移量来条件化源分布、目标分布或两者，从而缩短模型学习的概率路径，实现更快的训练。在ImageNet-256数据集上，CAR-Flow将SiT-XL/2的FID从2.07降至1.68，而额外参数仅增加不到0.6%，显著提升了条件生成模型的性能和效率。",
      "translated_title": "CAR-Flow：条件感知重参数化对齐源和目标以实现更好的流匹配",
      "images": [],
      "contentSource": "完整文章",
      "content": "Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters."
    },
    {
      "title": "VolSplat：重新思考基于体素对齐预测的前馈3D高斯泼溅 (原标题: VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction)",
      "link": "https://arxiv.org/abs/2509.19297",
      "pubDate": "Tue, 23 Sep 2025 13:59:02 GMT",
      "isoDate": "2025-09-23T13:59:02.000Z",
      "creator": "Weijie Wang, Yeqing Chen, Zeyu Zhang, Hengyu Liu, Haoxiao Wang, Zhiyuan Feng, Wenkang Qin, Zheng Zhu, Donny Y. Chen, Bohan Zhuang",
      "summary": "# VolSplat：重新思考基于体素对齐预测的前馈3D高斯泼溅\n\n## 摘要\n\n本文介绍了VolSplat，一种新的多视图前馈范式，旨在解决现有前馈3D高斯泼溅（3DGS）方法中基于像素对齐的高斯预测所固有的局限性。\n\n## 现有方法的局限性\n\n当前的前馈3DGS方法主要依赖于像素对齐的高斯预测范式，即每个2D像素都映射到一个3D高斯。这种广泛采用的公式存在以下几个固有局限性：\n\n*   **对输入视图数量的依赖性强**：重建的3D模型严重依赖于输入视图的数量。\n*   **视图偏置的密度分布**：导致高斯密度分布存在视图偏置。\n*   **对齐误差**：引入了对齐误差，尤其是在源视图包含遮挡或低纹理区域时。\n\n## VolSplat方法\n\n为了解决上述挑战，VolSplat提出了一种创新的方法，用体素对齐的高斯取代了像素对齐。其核心思想是直接从预测的3D体素网格中预测高斯。\n\n## VolSplat的优势\n\n通过采用体素对齐的高斯预测，VolSplat克服了传统方法的缺点，并带来了显著的改进：\n\n*   **鲁棒的多视图一致性**：避免了对易出错的2D特征匹配的依赖，从而确保了鲁棒的多视图一致性。\n*   **自适应高斯密度控制**：能够根据3D场景的复杂性自适应地控制高斯密度。\n*   **更忠实的高斯点云**：生成更真实的高斯点云。\n*   **改进的几何一致性**：显著提高了几何一致性。\n*   **增强的新视图渲染质量**：提升了新视图的渲染质量。\n\n## 实验结果与贡献\n\n在RealEstate10K和ScanNet等广泛使用的基准测试中进行的实验表明：\n\n*   **最先进的性能**：VolSplat取得了最先进的性能。\n*   **更合理和视图一致的重建**：生成了更合理且视图一致的高斯重建。\n*   **可扩展的框架**：该方法为前馈3D重建建立了一个更具可扩展性的框架，具有更密集和更鲁棒的表示。\n*   **推动未来研究**：为更广泛社区的进一步研究铺平了道路。\n\n## 资源可用性\n\n项目页面提供了视频结果、代码和训练模型。",
      "shortSummary": "VolSplat提出了一种新的前馈3D高斯泼溅（3DGS）范式，通过体素对齐预测取代了传统的像素对齐方法。它直接从3D体素网格预测高斯，克服了现有方法对输入视图的依赖、视图偏置和对齐误差等局限性。VolSplat确保了鲁棒的多视图一致性，实现了自适应高斯密度控制，并显著提升了几何一致性和新视图渲染质量。在基准测试中，VolSplat取得了最先进的性能，并为3D重建提供了一个更可扩展的框架。",
      "translated_title": "VolSplat：重新思考基于体素对齐预测的前馈3D高斯泼溅",
      "images": [],
      "contentSource": "完整文章",
      "content": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat."
    },
    {
      "title": "Lyra：基于视频扩散模型自蒸馏的生成式三维场景重建 (原标题: Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation)",
      "link": "https://arxiv.org/abs/2509.19296",
      "pubDate": "Tue, 23 Sep 2025 13:58:01 GMT",
      "isoDate": "2025-09-23T13:58:01.000Z",
      "creator": "Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren",
      "summary": "## Lyra：通过视频扩散模型自蒸馏生成三维场景重建\n\n### 引言\n\n*   **虚拟环境生成的重要性：** 虚拟环境的生成对于游戏、机器人、自动驾驶和工业AI等物理AI领域至关重要。\n*   **现有方法的局限性：**\n    *   当前基于学习的三维重建方法依赖于真实世界的多视角捕获数据，但此类数据并非总是容易获得。\n    *   视频扩散模型在想象力方面取得了显著进展，但其二维（2D）特性限制了它们在需要机器人导航和与环境交互的模拟应用中的使用。\n\n### Lyra框架概述\n\n*   **核心目标：** 提出一个自蒸馏框架，旨在将视频扩散模型中隐含的三维（3D）知识蒸馏到一个显式的3D高斯泼溅（3DGS）表示中。\n*   **主要优势：** 消除对多视角训练数据的需求。\n\n### 技术细节\n\n*   **架构增强：** 在典型的RGB解码器基础上，增加一个3DGS解码器。\n*   **监督机制：** 3DGS解码器由RGB解码器的输出进行监督。\n*   **训练数据：** 3DGS解码器可以完全使用视频扩散模型生成的合成数据进行训练，无需真实世界的多视角数据。\n\n### 推理能力\n\n*   **静态3D场景生成：** 在推理时，Lyra模型能够从文本提示或单张图像合成3D场景，并支持实时渲染。\n*   **动态3D场景生成：** 该框架进一步扩展，可以从单目输入视频生成动态3D场景。\n\n### 实验结果\n\n*   实验结果表明，Lyra框架在静态和动态3D场景生成方面均达到了最先进的性能。\n\n### 相关信息\n\n*   **作者：** Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David B. Lindell, Zan Gojcic, Sanja Fidler, Huan Ling, Jun Gao, Xuanchi Ren。\n*   **项目页面：** [this https URL](this https URL)\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)；图形学 (cs.GR)。",
      "shortSummary": "Lyra是一个通过视频扩散模型自蒸馏实现生成式三维场景重建的框架。它将视频扩散模型中隐含的3D知识蒸馏到显式的3D高斯泼溅（3DGS）表示中，从而消除了对多视角训练数据的需求。该框架通过增强RGB解码器并使用其输出监督3DGS解码器，可纯粹使用视频扩散模型生成的合成数据进行训练。Lyra能从文本提示或单张图像实时合成3D场景，并可扩展到从单目视频生成动态3D场景，在静态和动态3D场景生成方面均达到最先进性能。",
      "translated_title": "Lyra：基于视频扩散模型自蒸馏的生成式三维场景重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation."
    },
    {
      "title": "有效推理的特征是什么？重新审视思维链的长度、回顾和结构 (原标题: What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT)",
      "link": "https://arxiv.org/abs/2509.19284",
      "pubDate": "Tue, 23 Sep 2025 13:50:54 GMT",
      "isoDate": "2025-09-23T13:50:54.000Z",
      "creator": "Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, Anthony Hartshorn",
      "summary": "## 有效推理的特征：重新审视思维链的长度、回顾和结构\n\n### 引言\n大型推理模型（LRMs）在处理长思维链（CoT）追踪时会消耗大量的测试时间计算资源。然而，究竟是什么因素构成了有效的CoT，目前仍不明确。虽然先前的研究报告称，通过延长CoT并增加回顾（即通过附加`wait`令牌重新审视早期步骤）可以提高性能，但最近的一些研究却指出，较短的思考过程可能比冗长的追踪表现更好。\n\n### 研究方法与发现\n本研究旨在系统性地评估CoT的有效性，在十个LRMs上对数学和科学推理任务进行了广泛测试。\n\n*   **反驳“越长越好”的观点：** 与普遍认为“越长越好”的叙述相反，本研究发现，无论是简单的CoT延长还是增加回顾，都与*较低*的准确性相关联。\n*   **令牌级指标的局限性：** 研究指出，随着CoT的逐步展开，传统的令牌级指标可能将冗长与实际的过程质量混淆。\n\n### 引入结构化视图与新指标\n为了更准确地评估CoT的质量，本研究引入了一种新的方法：\n\n*   **CoT的图视图：** 提出了一种CoT的图视图，用于提取其内在结构。\n*   **失败步骤比例（FSF）：** 基于图视图，研究识别出一个单一的统计量——*失败步骤比例（FSF）*，即在被废弃的分支中步骤所占的比例。FSF在预测模型正确性方面，始终优于CoT的长度和回顾比例。\n\n### 因果关系探究与干预措施\n为了进一步探究CoT结构与推理性能之间的因果关系，研究设计了两项干预措施：\n\n1.  **候选CoT排名：** 在测试时，根据各项指标对候选CoT进行排名。结果显示，FSF在pass@1指标上产生了最大的增益。\n2.  **CoT编辑：** 通过编辑CoT以移除失败的分支，结果显著提高了准确性。这表明，CoT中存在的失败分支确实会对后续的推理过程产生负面影响。\n\n### 结论\n综合这些研究结果，本研究得出结论：\n\n*   **有效CoT的特征：** 有效的CoT的特点是“失败更少”。\n*   **测试时扩展策略：** 研究支持在测试时采用“结构感知”的扩展方法，而非不加区分地生成冗长的CoT，以优化大型推理模型的性能。",
      "shortSummary": "本研究系统评估了大型推理模型（LRMs）中思维链（CoT）的有效性。结果表明，CoT的简单延长和增加回顾反而降低了准确性。研究引入了CoT的图视图，并提出了“失败步骤比例（FSF）”指标，该指标能更准确预测推理正确性。实验证实，移除CoT中的失败分支可显著提高准确性。因此，有效的CoT应“失败更少”，并建议采用结构感知而非盲目延长CoT的方法，以提升推理性能。",
      "translated_title": "有效推理的特征是什么？重新审视思维链的长度、回顾和结构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.   As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT."
    },
    {
      "title": "预训练数据上的强化学习 (原标题: Reinforcement Learning on Pre-Training Data)",
      "link": "https://arxiv.org/abs/2509.19249",
      "pubDate": "Tue, 23 Sep 2025 13:10:40 GMT",
      "isoDate": "2025-09-23T13:10:40.000Z",
      "creator": "Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang",
      "summary": "# 预训练数据上的强化学习 (RLPT)\n\n## 引言与背景\n当前，计算资源的指数级增长与高质量文本数据有限增长之间的差距，正在限制大型语言模型（LLMs）传统扩展方法的有效性。为了应对这一挑战，本文引入了一种名为“预训练数据上的强化学习”（Reinforcement Learning on Pre-Training data, RLPT）的新型训练时扩展范式，旨在优化LLMs。\n\n## RLPT的核心理念与创新\n与以往主要通过监督学习进行训练扩展的方法不同，RLPT使策略能够自主探索有意义的轨迹，从预训练数据中学习并通过强化学习（RL）提升其能力。\n\nRLPT的关键创新在于其奖励信号的获取方式：\n*   **摆脱人工标注依赖**：现有的强化学习策略，如基于人类反馈的强化学习（RLHF）和可验证奖励的强化学习（RLVR），都依赖于人工标注来构建奖励。RLPT则消除了这种依赖，直接从预训练数据中获取奖励信号。\n*   **下一片段推理目标**：具体而言，RLPT采用了一种“下一片段推理目标”（next-segment reasoning objective）。它通过奖励策略准确预测在给定前文语境下后续文本片段的能力来提供奖励。\n\n## RLPT的优势与效果\n这种奖励机制的制定带来了多重优势：\n*   **强化学习的规模化**：它允许强化学习在预训练数据上进行规模化，从而鼓励探索更广泛语境下更丰富的轨迹。\n*   **通用推理能力的提升**：通过这种方式，RLPT能够培养出更具泛化性的推理技能。\n*   **实验验证**：\n    *   在通用领域和数学推理基准上，对多种模型进行了广泛的实验，验证了RLPT的有效性。\n    *   例如，当应用于Qwen3-4B-Base模型时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25等基准上分别取得了3.0、5.1、8.1、6.0、6.6和5.3的绝对提升。\n    *   研究结果还表明了良好的扩展行为，预示着随着计算资源的增加，RLPT有望持续获得收益。\n*   **基础性贡献**：RLPT为扩展LLMs的推理边界和增强RLVR性能提供了坚实的基础。\n\n## 结论\nRLPT提供了一种创新的方法，通过直接从预训练数据中获取奖励信号，利用强化学习来优化和扩展LLMs，有效解决了高质量文本数据稀缺的问题，并显著提升了模型的推理能力和泛化性。",
      "shortSummary": "为应对LLM训练中高质量文本数据稀缺的挑战，本文提出“预训练数据上的强化学习”（RLPT）。RLPT通过让策略自主探索预训练数据，并利用“下一片段推理目标”直接从数据中获取奖励信号，摆脱了对人工标注的依赖。实验证明，RLPT显著提升了LLM在通用和数学推理基准上的性能（如Qwen3-4B-Base在多项任务上取得显著提升），展现出良好的扩展性，并增强了模型的通用推理能力。",
      "translated_title": "预训练数据上的强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of 3.0, 5.1, 8.1, 6.0, 6.6, and 5.3 on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance."
    },
    {
      "title": "零样本多光谱学习：为遥感应用重构通用多模态Gemini 2.5模型 (原标题: Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications)",
      "link": "https://arxiv.org/abs/2509.19087",
      "pubDate": "Tue, 23 Sep 2025 10:40:52 GMT",
      "isoDate": "2025-09-23T10:40:52.000Z",
      "creator": "Ganesh Mallya, Yotam Gigi, Dahun Kim, Maxim Neumann, Genady Beryozkin, Tomer Shekel, Anelia Angelova",
      "summary": "### 零样本多光谱学习：为遥感应用重构通用多模态Gemini 2.5模型\n\n#### 引言\n多光谱图像在土地利用分类、环境监测和城市规划等多种遥感应用中扮演着至关重要的角色。这些图像因其额外的光谱波段与地面物理材料（如冰、水和植被）之间存在强相关性而被广泛采用，从而实现更准确的识别。此外，Sentinel-2和Landsat等任务提供的公开可用性进一步增加了它们的价值。\n\n#### 现有挑战\n目前，此类数据的自动化分析主要通过专门针对多光谱输入训练的机器学习模型进行管理，这些模型的训练和支持成本高昂。尽管多光谱输入为遥感提供了巨大效用，但它们无法与强大的通用大型多模态模型（LMMs）一起使用。这些通用模型虽然能够解决许多视觉问题，却无法理解专业的多光谱信号。\n\n#### 提出的解决方案：零样本方法\n为解决这一问题，本文提出了一种无需训练的方法，以“零样本”模式将新的多光谱数据作为输入引入到仅基于RGB输入训练的通用多模态模型中。\n\n该方法的核心思想包括：\n*   **利用视觉空间理解：** 充分利用多模态模型对视觉空间的现有理解能力。\n*   **输入适应：** 将多光谱输入数据调整到模型所理解的视觉空间。\n*   **指令注入：** 将领域特定的信息作为指令注入到模型中，引导其理解和处理多光谱数据。\n\n#### 实验与成果\n研究人员以Gemini 2.5模型为例，验证了这一方法的有效性。实验结果表明，该方法在流行的遥感基准测试（用于土地覆盖和土地利用分类）上取得了显著的零样本性能提升。这不仅证明了该方法的强大潜力，也展示了Gemini 2.5模型对新输入的轻松适应性。\n\n#### 意义与展望\n这些结果突显了对于处理非标准专业输入的地理空间专业人员而言，能够轻松利用Gemini 2.5等强大多模态模型的巨大潜力。通过利用这些模型丰富的推理和上下文理解能力，并将其与专业传感器数据相结合，可以显著加速他们的工作。",
      "shortSummary": "本文提出一种零样本、无需训练的方法，使通用多模态模型（如Gemini 2.5）能够处理多光谱遥感数据。传统方法成本高昂，且通用模型无法理解多光谱信号。新方法通过利用模型对视觉空间的理解，将多光谱数据适应为模型输入，并注入领域特定指令。实验显示，Gemini 2.5在遥感基准测试中表现出强大的零样本性能，使地理空间专业人员能更高效地利用通用模型分析专业数据。",
      "translated_title": "零样本多光谱学习：为遥感应用重构通用多模态Gemini 2.5模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals.   To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models' understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data."
    },
    {
      "title": "VIR-Bench：通过旅行视频行程重建评估多模态大语言模型（MLLMs）的地理空间和时间理解能力 (原标题: VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction)",
      "link": "https://arxiv.org/abs/2509.19002",
      "pubDate": "Tue, 23 Sep 2025 09:46:31 GMT",
      "isoDate": "2025-09-23T09:46:31.000Z",
      "creator": "Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara",
      "summary": "# VIR-Bench：通过旅行视频行程重建评估多模态大语言模型（MLLMs）的地理空间和时间理解能力\n\n## 引言与背景\n多模态大语言模型（MLLMs）在视频理解方面取得了显著进展，为实际应用开辟了新途径。然而，当前大多数视频基准主要关注室内场景或短距离户外活动，未能充分探索与长距离旅行相关的挑战。掌握扩展的地理空间和时间轨迹对于下一代MLLMs至关重要，它是具身AI规划和导航等现实世界任务的基础。\n\n## 推出 VIR-Bench\n为了弥补现有基准的不足，研究人员提出了 **VIR-Bench**，这是一个新颖的基准，旨在评估和推动 MLLMs 的地理空间和时间智能。\n\n### 核心构成与任务\n*   **内容：** VIR-Bench 包含 200 个旅行视频。\n*   **任务：** 它将“行程重建”作为一项具有挑战性的任务，要求模型理解视频中跨越扩展空间和时间的旅行轨迹。\n\n## 实验结果\n*   **模型表现：** 实验结果显示，包括专有模型在内的最先进 MLLMs 在 VIR-Bench 上难以取得高分。\n*   **挑战凸显：** 这突显了处理跨越扩展空间和时间尺度的视频的难度，以及当前 MLLMs 在此方面的不足。\n\n## 案例研究与实际应用\n*   **原型开发：** 研究人员进行了一项深入的案例研究，开发了一个原型旅行规划代理。\n*   **洞察利用：** 该代理利用了从 VIR-Bench 中获得的洞察，以改进其规划能力。\n*   **性能提升：** 代理的行程推荐显著改进，验证了 VIR-Bench 的评估协议不仅能有效衡量模型性能，还能转化为面向用户应用的具体性能提升。\n\n## 研究领域\n该研究涉及以下领域：\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   机器学习 (cs.LG)",
      "shortSummary": "VIR-Bench是一个新颖的基准，旨在通过200个旅行视频的行程重建任务，评估多模态大语言模型（MLLMs）的地理空间和时间理解能力。现有基准未能覆盖长距离旅行的复杂性。实验表明，当前最先进的MLLMs在此任务上表现不佳，凸显了处理大尺度时空视频的挑战。基于VIR-Bench的洞察，开发的旅行规划代理显著提升了行程推荐质量，证明了该基准在推动实际应用方面的价值。",
      "translated_title": "VIR-Bench：通过旅行视频行程重建评估多模态大语言模型（MLLMs）的地理空间和时间理解能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications."
    },
    {
      "title": "MAPO：混合优势策略优化 (原标题: MAPO: Mixed Advantage Policy Optimization)",
      "link": "https://arxiv.org/abs/2509.18849",
      "pubDate": "Tue, 23 Sep 2025 05:37:16 GMT",
      "isoDate": "2025-09-23T05:37:16.000Z",
      "creator": "Wenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
      "summary": "# MAPO：混合优势策略优化\n\n## 背景与问题\n\n在基础模型的强化学习领域，如群相对策略优化（GRPO），已显著提升了基础模型在推理任务上的性能。其中，优势函数（advantage function）是GRPO中用于评估轨迹重要性的核心机制。然而，现有研究发现，GRPO面临**优势反转（advantage reversion）**和**优势镜像（advantage mirror）**问题，这些问题阻碍了在不同查询样本之间进行合理的优势分配。\n\n## 提出的方法：MAPO\n\n为了解决上述问题，本文提出了一种简单但有效的GRPO策略——**混合优势策略优化（Mixed Advantage Policy Optimization, MAPO）**。\n\n### 核心思想\n\nMAPO的核心洞察是：轨迹以不同的确定性（certainty）出现。\n\n### 关键创新点\n\n1.  **优势百分比偏差：** 针对具有高确定性轨迹的样本，MAPO引入了**优势百分比偏差（advantage percent deviation）**。\n2.  **动态重加权优势函数：** MAPO能够根据样本轨迹确定性的不同，动态地重新加权优势函数。\n3.  **自适应配置：** 通过上述机制，MAPO能够自适应地配置优势函数，以更好地适应样本的特定特性。\n\n## 验证与结果\n\n通过与相关最先进（state-of-the-art）方法的比较，以及对不同优势变体的消融研究，本文验证了MAPO方法的有效性。\n\n## 研究领域\n\n人工智能 (cs.AI)\n\n## 作者\n\nWenke Huang, Quan Zhang, Yiyang Fang, Jian Liang, Xuankun Rong, Huanjin Yao, Guancheng Wan, Ke Liang, Wenwen He, Mingjun Li, Leszek Rutkowski, Mang Ye, Bo Du, Dacheng Tao",
      "shortSummary": "本文提出了MAPO（混合优势策略优化），旨在解决基础模型强化学习中GRPO策略的优势函数分配不合理问题，即优势反转和优势镜像。MAPO通过揭示轨迹确定性差异，引入优势百分比偏差，并动态重新加权优势函数，从而自适应地处理不同样本特性。实验证明，MAPO有效提升了性能，解决了现有问题。",
      "translated_title": "MAPO：混合优势策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach."
    },
    {
      "title": "Hyper-Bagel：一个用于多模态理解与生成的统一加速框架 (原标题: Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation)",
      "link": "https://arxiv.org/abs/2509.18824",
      "pubDate": "Tue, 23 Sep 2025 05:12:46 GMT",
      "isoDate": "2025-09-23T05:12:46.000Z",
      "creator": "Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, Xuefeng Xiao",
      "summary": "## Hyper-Bagel：多模态理解与生成的统一加速框架\n\n### 引言\n\n统一多模态模型在联合理解和生成多样化内容方面展现出卓越能力，因此近期受到了广泛关注。然而，随着上下文集成越来越多的交错多模态令牌，扩散去噪和自回归解码的迭代过程带来了显著的计算开销，这限制了其效率和实时应用。\n\n### Hyper-Bagel 框架\n\n为了解决上述计算瓶颈，研究人员提出了 **Hyper-Bagel**，这是一个统一的加速框架，旨在同时加速多模态理解和生成任务。\n\n*   **核心策略：** Hyper-Bagel 采用“分而治之”（divide-and-conquer）的策略。\n    *   **下一令牌预测：** 对于下一令牌的预测，框架利用了**推测解码（speculative decoding）**技术，以提高预测效率。\n    *   **扩散去噪：** 对于扩散去噪过程，框架引入了**多阶段蒸馏（multi-stage distillation）**方法，以减少迭代次数并保持输出质量。\n\n### 性能提升\n\nHyper-Bagel 框架带来了显著的性能提升：\n\n*   **多模态理解：** 在多模态理解任务中，该框架实现了**超过 2 倍的加速**。\n*   **生成任务：**\n    *   **无损 6-NFE 模型：** 经过优化的无损 6-NFE（Number of Function Evaluations）模型在生成任务中表现出色：\n        *   **文本到图像生成：** 实现了 **16.67 倍的加速**。\n        *   **图像编辑：** 实现了 **22 倍的加速**。\n        *   值得注意的是，这些加速是在**保持原始模型高质量输出**的前提下实现的。\n    *   **高效 1-NFE 模型：** 研究人员进一步开发了一个高效的 1-NFE 模型，该模型能够实现**近乎实时的交互式编辑和生成**。通过结合先进的**对抗性蒸馏（adversarial distillation）**和**人类反馈学习（human feedback learning）**，该模型实现了极致的成本效益和响应速度，使得复杂的多模态交互变得无缝和即时。\n\n### 结论\n\nHyper-Bagel 框架通过其创新的分而治之策略、推测解码和多阶段蒸馏技术，成功地解决了统一多模态模型在计算效率方面的挑战，为多模态理解和生成任务提供了显著的加速和更高的响应性，同时保持了高质量的输出。",
      "shortSummary": "Hyper-Bagel是一个统一的加速框架，旨在解决多模态模型中扩散去噪和自回归解码带来的高计算开销。它采用推测解码和多阶段蒸馏策略，在多模态理解任务中实现超过2倍加速。对于生成任务，其无损6-NFE模型在文本到图像生成中实现16.67倍加速，图像编辑中实现22倍加速，同时保持高质量。高效的1-NFE模型通过对抗性蒸馏和人类反馈学习，实现近实时交互和极致成本效益。",
      "translated_title": "Hyper-Bagel：一个用于多模态理解与生成的统一加速框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous."
    },
    {
      "title": "在视觉运动策略中需要本体感受状态吗？ (原标题: Do You Need Proprioceptive States in Visuomotor Policies?)",
      "link": "https://arxiv.org/abs/2509.18644",
      "pubDate": "Tue, 23 Sep 2025 00:56:59 GMT",
      "isoDate": "2025-09-23T00:56:59.000Z",
      "creator": "Juntu Zhao, Wenbo Lu, Di Zhang, Yufeng Liu, Yushen Liang, Tianluo Zhang, Yifeng Cao, Junyuan Xie, Yingdong Hu, Shengjie Wang, Junliang Guo, Dequan Wang, Yang Gao",
      "summary": "### 视觉运动策略中的本体感受状态：一个重新审视\n\n**1. 问题背景**\n\n*   **传统做法：** 基于模仿学习的视觉运动策略在机器人操作中广泛应用，通常同时采用视觉观测和本体感受状态以实现精确控制。\n*   **发现的问题：** 本研究发现，这种常见做法导致策略过度依赖本体感受状态输入，从而引发对训练轨迹的过拟合，并导致空间泛化能力差。\n\n**2. 提出的解决方案：无状态策略 (State-free Policy)**\n\n*   **核心思想：** 移除本体感受状态输入，仅根据视觉观测来预测动作。\n*   **关键特性：**\n    *   **动作空间：** 构建在相对末端执行器动作空间中。\n    *   **视觉输入：** 必须确保提供完整的任务相关视觉观测，例如通过双广角腕部摄像头提供。\n\n**3. 实验结果与优势**\n\n*   **显著提升空间泛化能力：** 经验结果表明，无状态策略比基于状态的策略实现了显著更强的空间泛化能力。\n    *   **真实世界任务：** 在多种机器人实体上进行了测试，包括抓取放置、具有挑战性的叠衣任务以及复杂的全身操作。\n    *   **成功率提升：**\n        *   **高度泛化：** 平均成功率从0%提高到85%。\n        *   **水平泛化：** 平均成功率从6%提高到64%。\n*   **其他优势：**\n    *   在数据效率方面也显示出优势。\n    *   在跨实体适应性方面表现出色。\n    *   这些优势增强了其在实际部署中的实用性。",
      "shortSummary": "传统视觉运动策略因过度依赖本体感受状态导致过拟合和空间泛化能力差。本文提出“无状态策略”，仅通过视觉观测预测动作，并采用相对末端执行器动作空间。实验表明，该策略在多种真实机器人任务中显著提升了空间泛化能力，例如高度泛化成功率从0%增至85%，水平泛化从6%增至64%。此外，它还提高了数据效率和跨机器人适应性，增强了实际部署的实用性。",
      "translated_title": "在视觉运动策略中需要本体感受状态吗？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment."
    },
    {
      "title": "MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索 (原标题: MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction)",
      "link": "https://arxiv.org/abs/2509.18095",
      "pubDate": "Mon, 22 Sep 2025 13:59:42 GMT",
      "isoDate": "2025-09-22T13:59:42.000Z",
      "creator": "Zilin Xiao, Qi Ma, Mengting Gu, Chun-cheng Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan",
      "summary": "# MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索\n\n本文介绍了一种名为 MetaEmbed 的新型多模态检索框架，旨在解决现有通用多模态嵌入模型在扩展性方面的挑战。\n\n## 现有方法的局限性\n*   **单向量表示：** 当前方法通常将查询和候选对象压缩成单个向量，这可能限制了对细粒度信息的表达能力。\n*   **多向量成本高昂：** 另一种方法生成过多的向量，导致多向量检索的计算成本过高，难以实际应用。\n\n## MetaEmbed 框架\nMetaEmbed 重新思考了多模态嵌入的构建和大规模交互方式。\n\n### 核心机制\n*   **训练阶段：** 在输入序列中附加固定数量的“可学习元令牌”（learnable Meta Tokens）。\n*   **测试阶段：** 这些元令牌的最后一层上下文表示被用作紧凑而富有表现力的多向量嵌入。\n\n### 训练方法\n*   **Matryoshka 多向量检索训练：** 通过这种训练方法，MetaEmbed 学习以不同粒度在多个向量中组织信息。\n\n## 主要优势与成果\n*   **测试时扩展性：** MetaEmbed 实现了多模态检索的测试时扩展。用户可以根据检索质量和效率需求，灵活选择用于索引和检索交互的令牌数量，从而在两者之间取得平衡。\n*   **最先进的性能：** 在大规模多模态嵌入基准 (MMEB) 和视觉文档检索基准 (ViDoRe) 上的广泛评估表明，MetaEmbed 取得了最先进的检索性能。\n*   **鲁棒的扩展能力：** 该框架能够稳定地扩展到具有 320 亿参数的模型。",
      "shortSummary": "MetaEmbed 是一种新的多模态检索框架，旨在解决现有单向量表达能力有限和多向量成本高昂的问题。它通过在训练时使用可学习的元令牌，并在测试时将其上下文表示作为紧凑的多向量嵌入。通过 Matryoshka 训练，MetaEmbed 实现了测试时扩展，允许用户平衡检索质量与效率。它在 MMEB 和 ViDoRe 上取得了最先进的性能，并能鲁棒地扩展到大型模型。",
      "translated_title": "MetaEmbed：通过灵活的后期交互在测试时扩展多模态检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal multimodal embedding models have achieved great success in capturing semantic relevance between queries and candidates. However, current methods either condense queries and candidates into a single vector, potentially limiting the expressiveness for fine-grained information, or produce too many vectors that are prohibitively expensive for multi-vector retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal retrieval that rethinks how multimodal embeddings are constructed and interacted with at scale. During training, a fixed number of learnable Meta Tokens are appended to the input sequence. At test-time, their last-layer contextualized representations serve as compact yet expressive multi-vector embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training, MetaEmbed learns to organize information by granularity across multiple vectors. As a result, we enable test-time scaling in multimodal retrieval, where users can balance retrieval quality against efficiency demands by selecting the number of tokens used for indexing and retrieval interactions. Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed achieves state-of-the-art retrieval performance while scaling robustly to models with 32B parameters."
    },
    {
      "title": "OnePiece：将上下文工程和推理引入工业级级联排序系统 (原标题: OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System)",
      "link": "https://arxiv.org/abs/2509.18091",
      "pubDate": "Mon, 22 Sep 2025 13:59:07 GMT",
      "isoDate": "2025-09-22T13:59:07.000Z",
      "creator": "Sunhao Dai, Jiakai Tang, Jiahua Wu, Kun Wang, Yuxuan Zhu, Bingjun Chen, Bangyang Hong, Yu Zhao, Cong Fu, Kangle Wu, Yabo Ni, Anxiang Zeng, Wenjie Wang, Xu Chen, Jun Xu, See-Kiong Ng",
      "summary": "# OnePiece：将上下文工程和推理引入工业级级联排序系统\n\n## 摘要\n\n尽管人们对在工业搜索和推荐系统中复制大型语言模型（LLMs）的成功越来越感兴趣，但目前大多数工业实践仅限于移植Transformer架构，这相对于强大的深度学习推荐模型（DLRMs）只带来了增量改进。本文提出了一种名为OnePiece的统一框架，旨在将LLM风格的上下文工程和推理机制无缝集成到工业级联管道的检索和排序模型中，以期实现实质性突破。\n\n## 背景与动机\n\nLLMs的突破不仅源于其架构，还源于两个互补的机制：\n\n*   **上下文工程（Context Engineering）**：通过上下文线索丰富原始输入查询，以更好地激发模型能力。\n*   **多步推理（Multi-step Reasoning）**：通过中间推理路径迭代地优化模型输出。\n\n然而，在工业排序系统中，这两个机制及其释放巨大改进潜力的可能性仍未得到充分探索。\n\n## OnePiece框架\n\nOnePiece是一个基于纯Transformer骨干的统一框架，它引入了三项关键创新：\n\n1.  **结构化上下文工程（Structured Context Engineering）**：\n    *   将交互历史与用户偏好和场景信号进行增强。\n    *   将这些信息统一为结构化的标记化输入序列，供检索和排序模型使用。\n\n2.  **块级潜在推理（Block-wise Latent Reasoning）**：\n    *   使模型具备多步表示细化的能力。\n    *   通过调整块大小来扩展推理带宽。\n\n3.  **渐进式多任务训练（Progressive Multi-task Training）**：\n    *   利用用户反馈链在训练过程中有效监督推理步骤。\n\n## 部署与成果\n\nOnePiece已成功部署在Shopee的主要个性化搜索场景中，并取得了显著的在线收益，具体包括：\n\n*   **GMV/UU（每用户商品交易总额）**：提升超过 +2%。\n*   **广告收入**：增长 +2.90%。\n\n这些成果表明OnePiece在实际工业应用中能够带来持续的业务指标提升。",
      "shortSummary": "OnePiece是一个统一框架，旨在将LLM风格的上下文工程和多步推理引入工业级联排序系统。它基于Transformer架构，通过结构化上下文工程、块级潜在推理和渐进式多任务训练三大创新，解决了现有系统仅移植Transformer架构带来的增量改进问题。OnePiece已在Shopee的个性化搜索中部署，并实现了超过+2%的GMV/UU和+2.90%的广告收入增长，证明了其在实际应用中的有效性。",
      "translated_title": "OnePiece：将上下文工程和推理引入工业级级联排序系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the growing interest in replicating the scaled success of large language models (LLMs) in industrial search and recommender systems, most existing industrial efforts remain limited to transplanting Transformer architectures, which bring only incremental improvements over strong Deep Learning Recommendation Models (DLRMs). From a first principle perspective, the breakthroughs of LLMs stem not only from their architectures but also from two complementary mechanisms: context engineering, which enriches raw input queries with contextual cues to better elicit model capabilities, and multi-step reasoning, which iteratively refines model outputs through intermediate reasoning paths. However, these two mechanisms and their potential to unlock substantial improvements remain largely underexplored in industrial ranking systems.   In this paper, we propose OnePiece, a unified framework that seamlessly integrates LLM-style context engineering and reasoning into both retrieval and ranking models of industrial cascaded pipelines. OnePiece is built on a pure Transformer backbone and further introduces three key innovations: (1) structured context engineering, which augments interaction history with preference and scenario signals and unifies them into a structured tokenized input sequence for both retrieval and ranking; (2) block-wise latent reasoning, which equips the model with multi-step refinement of representations and scales reasoning bandwidth via block size; (3) progressive multi-task training, which leverages user feedback chains to effectively supervise reasoning steps during training. OnePiece has been deployed in the main personalized search scenario of Shopee and achieves consistent online gains across different key business metrics, including over +2% GMV/UU and a +2.90% increase in advertising revenue."
    },
    {
      "title": "ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部 (原标题: ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces)",
      "link": "https://arxiv.org/abs/2509.18084",
      "pubDate": "Mon, 22 Sep 2025 13:57:07 GMT",
      "isoDate": "2025-09-22T13:57:07.000Z",
      "creator": "Jiawen Tian, Liqun Huang, Zhongren Cui, Jingchao Qiao, Jiafeng Xu, Xiao Ma, Zeyu Ren",
      "summary": "# ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部\n\n本文介绍了一种名为 **ByteWrist** 的新型高度灵活、拟人化的并联机器人腕部，旨在解决现有串联和并联腕部在狭窄空间操作中的关键局限性。\n\n## 核心设计与创新\n\nByteWrist 的核心在于其紧凑的三级并联驱动机构，该机构与弧形末端连杆巧妙集成，实现了卓越的紧凑性，并能提供精确的 RPY（滚动-俯仰-偏航）运动。这使其特别适用于家庭服务、医疗辅助和精密装配等复杂非结构化环境。\n\n其主要创新点包括：\n\n*   **嵌套式三级电机驱动连杆**：这种设计最大限度地减小了体积，同时能够实现独立的多自由度控制。\n*   **弧形末端连杆**：优化了力传输，并显著扩展了腕部的运动范围。\n*   **中央支撑球**：作为一个球形关节，它在不牺牲灵活性的前提下，有效增强了结构的刚度。\n\n## 运动学建模\n\n为了实现精确控制，研究人员为 ByteWrist 提供了全面的运动学建模，包括正向/逆向运动学和数值雅可比矩阵解。\n\n## 实验结果与性能\n\n通过实证观察，ByteWrist 在狭窄空间机动性和双臂协同操作任务中展现出强大的性能，并优于基于 Kinova 的系统。与传统设计相比，ByteWrist 在紧凑性、效率和刚度方面均取得了显著提升。\n\n## 结论\n\nByteWrist 的设计和性能使其成为受限环境中下一代机器人操作的一个有前景的解决方案。",
      "shortSummary": "ByteWrist 是一种新型并联机器人腕部，专为狭窄空间操作设计。它通过紧凑的三级并联驱动机构和弧形末端连杆，实现了灵活、拟人化的精确 RPY 运动。其创新包括嵌套式连杆、优化力传输的弧形连杆及增强刚度的中央支撑球。实验表明，ByteWrist 在狭窄空间机动性和双臂协作任务中表现出色，优于传统系统，在紧凑性、效率和刚度方面均有显著提升，是受限环境下机器人操作的理想方案。",
      "translated_title": "ByteWrist：一种用于狭窄空间中实现灵活拟人化运动的并联机器人腕部",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic parallel wrist for robotic manipulation. ByteWrist addresses the critical limitations of existing serial and parallel wrists in narrow-space operations through a compact three-stage parallel drive mechanism integrated with arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw) motion while maintaining exceptional compactness, making it particularly suitable for complex unstructured environments such as home services, medical assistance, and precision assembly. The key innovations include: (1) a nested three-stage motor-driven linkages that minimize volume while enabling independent multi-DOF control, (2) arc-shaped end linkages that optimize force transmission and expand motion range, and (3) a central supporting ball functioning as a spherical joint that enhances structural stiffness without compromising flexibility. Meanwhile, we present comprehensive kinematic modeling including forward / inverse kinematics and a numerical Jacobian solution for precise control. Empirically, we observe ByteWrist demonstrates strong performance in narrow-space maneuverability and dual-arm cooperative manipulation tasks, outperforming Kinova-based systems. Results indicate significant improvements in compactness, efficiency, and stiffness compared to traditional designs, establishing ByteWrist as a promising solution for next-generation robotic manipulation in constrained environments."
    },
    {
      "title": "Reasoning Core：一个用于LLM符号推理的可扩展强化学习环境 (原标题: Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning)",
      "link": "https://arxiv.org/abs/2509.18083",
      "pubDate": "Mon, 22 Sep 2025 13:56:38 GMT",
      "isoDate": "2025-09-22T13:56:38.000Z",
      "creator": "Valentin Lacombe, Valentin Quesnel, Damien Sileo",
      "summary": "### Reasoning Core：LLM符号推理的新型可扩展强化学习环境\n\n**1. 项目介绍**\n*   **Reasoning Core** 是一个为**可验证奖励强化学习 (RLVR)** 设计的新型可扩展环境。\n*   其核心目标是推动**大型语言模型 (LLM)** 在基础**符号推理**方面的进步。\n\n**2. 与现有基准的区别**\n*   不同于现有专注于游戏或孤立谜题的基准。\n*   Reasoning Core 通过**程序化生成**方式，在多个核心形式领域创建问题。\n\n**3. 涵盖的核心形式领域**\n*   **PDDL 规划**\n*   **一阶逻辑**\n*   **上下文无关文法解析**\n*   **因果推理**\n*   **系统方程求解**\n\n**4. 关键设计原则**\n*   **高通用性问题分布**：确保生成的问题具有广泛的适用性和多样性。\n*   **通过外部工具进行验证**：利用外部工具对LLM的解决方案进行验证，确保正确性。\n*   **持续难度控制**：环境能够根据需要调整任务难度，提供渐进式的学习曲线。\n\n**5. 设计优势**\n*   这些设计原则共同提供了**几乎无限的新颖训练实例**，解决了传统基准数据量有限的问题。\n\n**6. 初步评估结果**\n*   对前沿LLM进行的**零样本评估**证实了Reasoning Core任务的难度，表明其具有挑战性。\n\n**7. 未来展望**\n*   Reasoning Core 被定位为一个**有前景的资源**，有望显著提升未来LLM的推理能力。",
      "shortSummary": "Reasoning Core是一个新型可扩展的强化学习环境，旨在通过可验证奖励（RLVR）提升大型语言模型（LLM）的符号推理能力。它通过程序化生成PDDL规划、一阶逻辑等核心形式领域的问题，而非依赖孤立谜题。该环境基于高通用性问题分布、外部工具验证和持续难度控制原则，提供无限训练实例。初步评估显示其任务难度高，有望成为提升未来LLM推理能力的关键资源。",
      "translated_title": "Reasoning Core：一个用于LLM符号推理的可扩展强化学习环境",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Reasoning Core, a new scalable environment for Reinforcement Learning with Verifiable Rewards (RLVR), designed to advance foundational symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks that focus on games or isolated puzzles, Reasoning Core procedurally generates problems across core formal domains, including PDDL planning, first-order logic, context-free grammar parsing, causal reasoning, and system equation solving. The environment is built on key design principles of high-generality problem distributions, verification via external tools, and continuous difficulty control, which together provide a virtually infinite supply of novel training instances. Initial zero-shot evaluations with frontier LLMs confirm the difficulty of Reasoning Core's tasks, positioning it as a promising resource to improve the reasoning capabilities of future models."
    },
    {
      "title": "TempSamp-R1：基于强化微调的视频大语言模型有效时序采样 (原标题: TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs)",
      "link": "https://arxiv.org/abs/2509.18056",
      "pubDate": "Mon, 22 Sep 2025 13:30:15 GMT",
      "isoDate": "2025-09-22T13:30:15.000Z",
      "creator": "Yunheng Li, Jing Cheng, Shaoyong Jia, Hangyi Kuang, Shaohui Jiao, Qibin Hou, Ming-Ming Cheng",
      "summary": "### TempSamp-R1：用于视频大语言模型的有效时序采样与强化微调\n\n本文介绍了TempSamp-R1，一个旨在提升多模态大语言模型（MLLMs）在视频时序定位任务中适应性的新型强化微调框架。该框架解决了现有强化学习方法（如Group Relative Policy Optimization, GRPO）在处理具有大时序搜索空间任务时，因依赖on-policy采样而导致的效率低下和性能受限问题。\n\n#### 现有问题与挑战\n*   **on-policy采样的局限性：** 现有强化学习方法通常依赖on-policy采样进行策略更新。在视频时序定位这类具有巨大时序搜索空间的任务中，这种策略难以识别时序准确的解决方案，导致效率低下和性能瓶颈。\n\n#### TempSamp-R1 的核心创新与机制\nTempSamp-R1通过以下关键机制克服了上述限制：\n\n1.  **离线监督（Off-policy Supervision）：**\n    *   TempSamp-R1利用真实标注作为off-policy监督，提供时序精确的指导。\n    *   这有效弥补了on-policy解决方案中常见的稀疏性和错位问题，确保模型能够学习到更准确的时序信息。\n\n2.  **非线性软优势计算（Non-linear Soft Advantage Computation）：**\n    *   为了稳定训练过程并减少基于奖励更新的方差，TempSamp-R1引入了一种非线性软优势计算方法。\n    *   该方法通过非对称变换动态调整奖励反馈，从而优化学习效率和稳定性。\n\n3.  **混合思维链（CoT）训练范式：**\n    *   TempSamp-R1采用混合CoT训练范式，优化一个统一模型。\n    *   该模型能够同时支持CoT和非CoT两种推理模式，从而高效处理具有不同推理复杂度的查询，增强了模型的灵活性和适用性。\n\n#### 实验结果与性能提升\n实验结果表明，TempSamp-R1在多个基准数据集上显著超越了基于GRPO的基线方法，取得了新的最先进（SOTA）性能：\n\n*   **Charades-STA：** R1@0.7 达到 52.9%，相对提升 2.7%。\n*   **ActivityNet Captions：** R1@0.5 达到 56.0%，相对提升 5.3%。\n*   **QVHighlights：** mAP 达到 30.0%，相对提升 3.0%。\n\n此外，TempSamp-R1在有限数据条件下展现出强大的少样本泛化能力，证明了其在数据稀缺场景下的鲁棒性。\n\n#### 总结\nTempSamp-R1通过引入离线监督、非线性软优势计算和混合CoT训练范式，为视频大语言模型在时序定位任务中的强化微调提供了一个高效且性能卓越的解决方案，显著提升了模型在多个关键指标上的表现。",
      "shortSummary": "TempSamp-R1是一个为视频大语言模型（VLLMs）设计的强化微调框架，旨在解决现有方法在视频时序定位任务中on-policy采样效率低下的问题。它通过利用真实标注作为off-policy监督提供精确指导，并引入非线性软优势计算稳定训练。结合混合思维链（CoT）训练范式，TempSamp-R1在Charades-STA、ActivityNet Captions和QVHighlights等基准数据集上取得了新的最先进（SOTA）性能，并展现出强大的少样本泛化能力。",
      "translated_title": "TempSamp-R1：基于强化微调的视频大语言模型有效时序采样",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1"
    },
    {
      "title": "VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频 (原标题: VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models)",
      "link": "https://arxiv.org/abs/2509.17985",
      "pubDate": "Mon, 22 Sep 2025 12:28:47 GMT",
      "isoDate": "2025-09-22T12:28:47.000Z",
      "creator": "Geonung Kim, Janghyeok Han, Sunghyun Cho",
      "summary": "# VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频\n\n本文提出了 **VideoFrom3D**，一个用于从粗糙几何结构、摄像机轨迹和参考图像合成高质量3D场景视频的新颖框架。该方法旨在简化3D图形设计工作流程，实现灵活的设计探索和快速交付成果。\n\n## 挑战与问题\n直接使用视频扩散模型从粗糙几何结构合成视频，往往难以在复杂场景中生成高保真结果。主要原因是现有模型难以同时建模视觉质量、运动和时间一致性。\n\n## 解决方案：VideoFrom3D 框架\nVideoFrom3D 框架通过结合图像和视频扩散模型的互补优势来解决上述挑战。它包含两个核心模块：\n\n### 1. 稀疏锚点视图生成 (Sparse Anchor-view Generation, SAG) 模块\n*   **功能**：生成高质量、跨视图一致的锚点视图。\n*   **技术**：利用图像扩散模型，并通过稀疏外观引导采样（Sparse Appearance-guided Sampling）进行辅助。\n\n### 2. 几何引导生成插值 (Geometry-guided Generative Inbetweening, GGI) 模块\n*   **功能**：基于SAG模块生成的锚点视图，忠实地插值生成中间帧。\n*   **技术**：利用视频扩散模型，并通过基于流的摄像机控制和结构引导进行增强。\n\n## 关键创新与优势\n*   **无配对数据集需求**：VideoFrom3D 的两个模块均无需依赖3D场景模型和自然图像的配对数据集。获取此类数据集极其困难，因此这一特性显著降低了方法实现的门槛。\n*   **高性能表现**：全面的实验结果表明，该方法能够在多样化且具有挑战性的场景下，生成高质量、风格一致的场景视频，并且性能优于简单及扩展的基线方法。\n\n## 输入与输出\n*   **输入**：粗糙几何结构、摄像机轨迹、参考图像。\n*   **输出**：高质量的3D场景视频。",
      "shortSummary": "VideoFrom3D是一个新颖框架，能从粗糙几何、摄像机轨迹和参考图像生成高质量3D场景视频。它通过结合图像和视频扩散模型的优势，解决了现有视频扩散模型在复杂场景中难以同时保证视觉质量、运动和时间一致性的问题。该框架包含稀疏锚点视图生成（SAG）和几何引导生成插值（GGI）两个模块，且无需配对的3D场景模型与自然图像数据集。实验证明其在各种场景下均能生成高质量、风格一致的视频。",
      "translated_title": "VideoFrom3D：通过互补图像和视频扩散模型生成3D场景视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines."
    },
    {
      "title": "ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑 (原标题: ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment)",
      "link": "https://arxiv.org/abs/2509.17818",
      "pubDate": "Mon, 22 Sep 2025 10:13:31 GMT",
      "isoDate": "2025-09-22T10:13:31.000Z",
      "creator": "Yiyang Chen, Xuanhua He, Xiujun Ma, Yue Ma",
      "summary": "# ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑\n\n## 摘要\n\n本文介绍了 ContextFlow，一个新颖的免训练框架，专为基于 Diffusion Transformers (DiTs) 的视频对象编辑而设计。该框架旨在解决现有方法在对象级操作（如插入、交换、删除）中面临的保真度和时间一致性挑战，特别是针对不准确的反演和上下文冲突问题。\n\n## 现有挑战\n\n当前的免训练视频对象编辑方法，尤其是在 U-Net 架构中，存在以下主要局限性：\n\n*   **不准确的反演：** 主要由一阶求解器引起，导致编辑基础不稳固。\n*   **上下文冲突：** 粗糙的“硬”特征替换策略导致编辑区域与周围环境不协调。\n*   **DiT 架构的特殊挑战：** 在 Diffusion Transformers (DiTs) 中，由于先前层选择启发式方法的不适用性，上述问题变得更加严峻，有效引导难以实现。\n\n## ContextFlow 框架\n\nContextFlow 提出了一套创新的解决方案来克服这些挑战：\n\n### 1. 稳健的编辑基础：高阶整流流求解器\n\n*   **目的：** 建立一个强大的编辑基础。\n*   **方法：** 采用高阶整流流 (Rectified Flow) 求解器，有效解决了一阶求解器导致的不准确反演问题，为后续的精确编辑奠定基础。\n\n### 2. 解决上下文冲突：自适应上下文丰富 (Adaptive Context Enrichment, ACE)\n\n*   **目的：** 解决由粗糙特征替换引起的上下文冲突，明确“编辑什么”。\n*   **创新点：**\n    *   **替代硬替换：** ContextFlow 摒弃了传统的直接替换特征的方法。\n    *   **丰富自注意力上下文：** 它通过将来自并行重建路径和编辑路径的 Key-Value 对进行拼接，来丰富模型的自注意力上下文。\n    *   **动态信息融合：** 这种机制赋予模型动态融合信息的能力，使其能够更自然、更流畅地将编辑内容融入视频，避免了生硬的边界和不一致性。\n\n### 3. 精准引导定位：数据驱动的生命层分析\n\n*   **目的：** 确定“在哪里应用丰富”，实现有针对性的编辑。\n*   **方法：**\n    *   **系统性分析：** 提出了一种系统性的、数据驱动的分析方法，以识别 DiT 模型中对特定任务（例如，对象插入、交换）至关重要的层。\n    *   **引导响应度量：** 基于新颖的“引导响应度量 (Guidance Responsiveness Metric)”，该方法能够精确地找出最具影响力的 DiT 块，从而实现高度有效和精准的引导。\n\n## 实验结果\n\n广泛的实验表明，ContextFlow 在性能上取得了显著突破：\n\n*   **超越现有免训练方法：** ContextFlow 显著优于所有现有的免训练视频对象编辑方法。\n*   **媲美甚至超越基于训练的方法：** 甚至在多个方面超越了一些最先进的基于训练的方法。\n*   **高质量输出：** 最终结果展现出卓越的时间连贯性和高保真度。\n\n## 结论\n\nContextFlow 通过其创新的高阶求解器、自适应上下文丰富机制和数据驱动的层分析，为免训练视频对象编辑领域树立了新标杆，有效解决了该领域长期存在的挑战。",
      "shortSummary": "ContextFlow是一个免训练视频对象编辑框架，旨在解决现有方法在保真度和时间一致性方面的挑战。它通过引入高阶整流流求解器、自适应上下文丰富（通过拼接Key-Value对动态融合信息）以及基于引导响应度量的数据驱动层分析（识别关键DiT块）来克服不准确反演和上下文冲突问题。实验证明，ContextFlow在生成时间连贯、高保真结果方面显著优于现有免训练方法，并超越了部分基于训练的先进方法。",
      "translated_title": "ContextFlow：通过自适应上下文丰富实现免训练视频对象编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results."
    },
    {
      "title": "通义千问3-Omni技术报告 (原标题: Qwen3-Omni Technical Report)",
      "link": "https://arxiv.org/abs/2509.17765",
      "pubDate": "Mon, 22 Sep 2025 09:26:24 GMT",
      "isoDate": "2025-09-22T09:26:24.000Z",
      "creator": "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
      "summary": "通义千问3-Omni（Qwen3-Omni）是一款创新的多模态模型，首次在文本、图像、音频和视频等多种模态上实现了与单模态模型相当的顶尖性能，且未出现性能下降。该模型在音频任务上表现尤为突出。\n\n### 主要成就与性能\n*   **多模态SOTA**：Qwen3-Omni在保持与同尺寸单模态模型（Qwen系列内部）性能一致的同时，首次实现了跨文本、图像、音频和视频的顶尖性能。\n*   **音频卓越表现**：在36个音频及音视频基准测试中，Qwen3-Omni在32个基准测试中达到了开源SOTA水平，并在22个基准测试中实现了整体SOTA，超越了包括Gemini-2.5-Pro、Seed-ASR和GPT-4o-Transcribe在内的强大闭源模型。\n\n### 核心架构与功能\n*   **Thinker-Talker MoE架构**：该模型采用Thinker-Talker MoE（混合专家）架构，统一了文本、图像、音频和视频的感知与生成，能够生成流畅的文本和自然的实时语音。\n*   **多语言支持**：\n    *   支持119种语言的文本交互。\n    *   支持19种语言的语音理解。\n    *   支持10种语言的语音生成。\n\n### 语音生成优化\n*   **低延迟流式合成**：为减少流式合成中的首包延迟，Talker组件采用多码本方案自回归预测离散语音编解码器。\n*   **轻量级ConvNet**：利用码本的表示能力，模型用轻量级因果ConvNet取代了计算密集型的块状扩散模型，实现了从第一个编解码帧开始流式传输。\n*   **理论首包延迟**：在冷启动设置下，Qwen3-Omni的理论端到端首包延迟为234毫秒。\n\n### 多模态推理与应用扩展\n*   **Thinking模型**：引入了一个Thinking模型，旨在增强多模态推理能力，能够明确地对来自任何模态的输入进行推理。\n*   **音频字幕模型**：鉴于当前研究社区缺乏通用的音频字幕模型，研究人员对Qwen3-Omni-30B-A3B进行了微调，得到了Qwen3-Omni-30B-A3B-Captioner。该模型能为任意音频输入生成详细、低幻觉的字幕。\n\n### 开源发布\n*   Qwen3-Omni-30B-A3B\n*   Qwen3-Omni-30B-A3B-Thinking\n*   Qwen3-Omni-30B-A3B-Captioner\n\n以上模型均已在Apache 2.0许可下公开发布。\n\n### 页面图标\n*   BibSonomy图标：![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)\n*   Reddit图标：![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)",
      "shortSummary": "通义千问3-Omni（Qwen3-Omni）是一款突破性的多模态模型，首次在文本、图像、音频和视频上实现了与单模态模型相当的顶尖性能，且无性能损失。它在音频任务上表现卓越，在多项基准测试中达到SOTA，超越了包括GPT-4o在内的闭源模型。该模型采用Thinker-Talker MoE架构，支持多语言交互，并通过优化实现了234毫秒的低延迟语音生成。此外，还推出了增强多模态推理的Thinking模型和音频字幕模型，并已开源发布。",
      "translated_title": "通义千问3-Omni技术报告",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license."
    },
    {
      "title": "Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型 (原标题: Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications)",
      "link": "https://arxiv.org/abs/2509.17671",
      "pubDate": "Mon, 22 Sep 2025 08:14:11 GMT",
      "isoDate": "2025-09-22T08:14:11.000Z",
      "creator": "Selva Taş, Mahmut El Huseyni, Özay Ezerceli, Reyhan Bayraktar, Fatma Betül Terzioğlu",
      "summary": "# Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型\n\n## 背景与问题\n\n大型语言模型（LLMs）的广泛应用受到其“幻觉”问题的阻碍，即生成看似合理但实际上不准确的信息。尽管检索增强生成（RAG）系统试图通过将响应基于外部知识来解决这一问题，但幻觉仍然是一个持续的挑战，尤其对于土耳其语这种形态复杂、资源稀缺的语言。\n\n## 解决方案：Turk-LettuceDetect\n\n本文介绍了Turk-LettuceDetect，这是首个专门为土耳其语RAG应用设计的幻觉检测模型套件。该框架基于LettuceDetect，将幻觉检测任务定义为令牌级分类。\n\n## 模型架构与训练\n\n1.  **编码器架构**：微调了三种不同的编码器架构：\n    *   土耳其语特有的ModernBERT\n    *   TurkEmbed4STS\n    *   多语言EuroBERT\n2.  **训练数据集**：模型在RAGTruth基准数据集的机器翻译版本上进行训练，该数据集包含17,790个实例，涵盖了问答、数据到文本生成和摘要任务。\n\n## 实验结果与性能\n\n*   **F1分数**：基于ModernBERT的模型在完整测试集上实现了0.7266的F1分数，在结构化任务上表现尤为出色。\n*   **计算效率与上下文支持**：模型保持了计算效率，并支持长达8,192个令牌的上下文，使其适用于实时部署。\n*   **与现有LLMs的比较**：\n    *   最先进的LLMs虽然召回率高，但由于过度生成幻觉内容，导致精确率较低。\n    *   这突显了专门的幻觉检测机制的必要性。\n\n## 贡献与意义\n\n*   **填补空白**：通过发布模型和翻译后的数据集，这项工作填补了多语言自然语言处理领域的关键空白。\n*   **奠定基础**：为开发更可靠、更值得信赖的土耳其语及其他语言的AI应用奠定了基础。",
      "shortSummary": "Turk-LettuceDetect是首个专为土耳其语RAG应用设计的幻觉检测模型套件。它将幻觉检测视为令牌级分类任务，并微调了ModernBERT、TurkEmbed4STS和EuroBERT等编码器。基于ModernBERT的模型在测试集上F1分数达到0.7266，且计算高效，支持长上下文。该工作旨在解决LLM在土耳其语RAG中幻觉问题，提升AI应用的可靠性，并发布了模型和数据集以促进研究。",
      "translated_title": "Turk-LettuceDetect：土耳其语RAG应用的幻觉检测模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages."
    },
    {
      "title": "AuditoryBench++: 语言模型能否在不听觉的情况下理解听觉知识？ (原标题: AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?)",
      "link": "https://arxiv.org/abs/2509.17641",
      "pubDate": "Mon, 22 Sep 2025 07:45:22 GMT",
      "isoDate": "2025-09-22T07:45:22.000Z",
      "creator": "Hyunjong Ok, Suho Yoo, Hyeonjun Kim, Jaeho Lee",
      "summary": "## AuditoryBench++: 语言模型中的听觉知识与推理\n\n### 核心问题\n\n尽管人类无需直接听觉输入即可轻松推理音高、响度或声源关联等听觉属性，但语言模型（LLMs）通常缺乏这种能力。这限制了它们在多模态交互中的有效性。\n\n### 解决方案的初步步骤\n\n为了弥补这一差距，研究人员提出了两个关键贡献：\n\n1.  **AuditoryBench++ 基准测试**：\n    *   **目的**：这是一个全面的基准，旨在评估文本环境中语言模型的听觉知识和推理能力。\n    *   **任务范围**：它涵盖了从基本的听觉比较到基于上下文的推理等一系列任务。\n    *   **分析能力**：该基准能够对模型如何处理和整合听觉概念进行细致的分析。\n\n2.  **AIR-CoT (Auditory Imagination Reasoning - Chain of Thought) 方法**：\n    *   **创新性**：这是一种新颖的听觉想象推理方法。\n    *   **工作原理**：它通过使用特殊标记进行跨度检测和知识注入，在推理过程中生成并整合听觉信息。\n\n### 实验结果\n\n*   对最新的大型语言模型（LLMs）和多模态大型语言模型（Multimodal LLMs）进行了广泛的实验。\n*   实验结果表明，AIR-CoT 方法普遍优于：\n    *   现成的（off-the-shelf）模型。\n    *   以及那些已通过听觉知识增强的模型。\n\n### 项目信息\n\n*   **评论**：预印本（Preprint）。\n*   **主题**：计算与语言 (cs.CL)、人工智能 (cs.AI)、机器学习 (cs.LG)、声音 (cs.SD)。\n*   **引用方式**：arXiv:2509.17641 [cs.CL]。\n*   **提交历史**：v1 版本于 2025 年 9 月 22 日提交。",
      "shortSummary": "语言模型（LLMs）在没有直接听觉输入的情况下，难以像人类一样进行听觉推理。为解决此问题，研究人员提出了 AuditoryBench++，这是一个评估文本环境中听觉知识和推理的基准。同时，他们引入了 AIR-CoT，一种新颖的听觉想象推理方法，通过生成和整合听觉信息来提高模型性能。实验表明，AIR-CoT 普遍优于现有模型和已增强听觉知识的模型。",
      "translated_title": "AuditoryBench++: 语言模型能否在不听觉的情况下理解听觉知识？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io."
    }
  ],
  "lastUpdated": "2025-09-24T09:32:26.469Z"
}