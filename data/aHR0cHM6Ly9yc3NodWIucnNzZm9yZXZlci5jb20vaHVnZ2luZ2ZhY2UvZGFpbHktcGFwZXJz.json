{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "ZeroGUI：以零人工成本自动化在线GUI学习 (原标题: ZeroGUI: Automating Online GUI Learning at Zero Human Cost)",
      "link": "https://arxiv.org/abs/2505.23762",
      "pubDate": "Thu, 29 May 2025 13:59:51 GMT",
      "isoDate": "2025-05-29T13:59:51.000Z",
      "creator": "Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai",
      "summary": "# ZeroGUI：以零人工成本自动化在线GUI学习\n\n## 引言\n随着大型视觉-语言模型（VLM）的快速发展，纯视觉GUI（图形用户界面）Agent在感知和操作GUI以自主完成用户指令方面取得了显著进展。然而，现有方法通常采用离线学习框架，这带来了两个核心局限性：\n\n1.  **高度依赖人工标注：** 需要大量高质量的人工标注来完成元素定位（element grounding）和动作监督。\n2.  **适应性有限：** 对动态和交互式环境的适应能力有限。\n\n## ZeroGUI框架\n为了解决上述局限性，本文提出了 **ZeroGUI**，一个可扩展的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。ZeroGUI通过整合以下三个关键组件来实现这一目标：\n\n### 1. 基于VLM的自动任务生成\nZeroGUI能够从当前环境状态自动生成多样化的训练目标。这意味着系统不再需要预先定义或人工创建任务，而是可以根据实时的GUI界面情况，利用VLM的能力自动生成新的学习任务，从而大大增加了训练数据的多样性和覆盖范围。\n\n### 2. 基于VLM的自动奖励评估\n该框架利用VLM自动评估任务的成功与否，而无需手动设计复杂的评估函数。VLM可以直接理解任务目标和当前环境状态，并据此判断Agent的行动是否达到了预期效果，从而提供实时的奖励信号，指导Agent的学习过程。\n\n### 3. 两阶段在线强化学习\nZeroGUI采用两阶段的在线强化学习机制，使Agent能够持续与GUI环境进行交互并从中学习。这种在线学习范式使得Agent能够实时适应环境变化，并通过试错不断优化其操作策略，克服了离线学习在动态环境中的适应性问题。\n\n## 实验结果\n研究人员在两个先进的GUI Agent（UI-TARS和Aguvis）上对ZeroGUI进行了实验验证。实验结果表明，ZeroGUI在OSWorld和AndroidLab这两种GUI环境中显著提升了这些Agent的性能。这证明了ZeroGUI框架在提高GUI Agent训练效率和效果方面的有效性。\n\n## 结论\nZeroGUI提供了一种创新的、无需人工干预的在线学习方法，有效解决了当前GUI Agent训练中面临的标注成本高昂和环境适应性差的问题。通过自动化任务生成、奖励评估和在线强化学习，ZeroGUI为未来GUI Agent的自主学习和部署开辟了新的道路。",
      "shortSummary": "ZeroGUI是一个创新的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。它解决了现有离线学习方法对人工标注的重度依赖和对动态环境适应性差的问题。ZeroGUI通过基于VLM的自动任务生成、自动奖励评估以及两阶段在线强化学习实现。实验证明，ZeroGUI显著提升了UI-TARS和Aguvis等GUI Agent在OSWorld和AndroidLab环境中的性能，为GUI Agent的自主学习提供了高效解决方案。",
      "translated_title": "ZeroGUI：以零人工成本自动化在线GUI学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
    },
    {
      "title": "差分信息：偏好优化中的信息论视角 (原标题: Differential Information: An Information-Theoretic Perspective on Preference Optimization)",
      "link": "https://arxiv.org/abs/2505.23761",
      "pubDate": "Thu, 29 May 2025 13:59:50 GMT",
      "isoDate": "2025-05-29T13:59:50.000Z",
      "creator": "Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo",
      "summary": "### 差分信息：偏好优化中的信息论视角\n\n本文旨在从信息论角度深入探讨直接偏好优化（DPO）的理论基础，特别是其对数比率奖励参数化的合理性。\n\n**核心问题与方法：**\n\n*   **问题：** 尽管直接偏好优化（DPO）在监督式对齐语言模型与人类偏好方面取得了经验上的成功，但其对数比率奖励参数化背后的理论依据尚不完整。\n*   **方法：** 本文通过引入**差分信息分布（Differential Information Distribution, DID）**来解决这一理论空白。DID是一种关于令牌序列的分布，它捕获了策略更新过程中获得的信息。\n\n**主要发现与贡献：**\n\n1.  **对数比率奖励的最优性：**\n    *   研究表明，当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO中的对数比率奖励形式是学习目标策略的唯一最优形式。\n    *   这一结果自然地导出了被拒绝响应的最优采样分布的闭式表达式。\n\n2.  **与隐含归纳偏置的关联：**\n    *   偏好编码差分信息的条件与对数边际有序策略（log-margin ordered policies）之间存在根本性联系。\n    *   对数边际有序策略是偏好优化中广泛使用但此前未被识别的一种隐含归纳偏置。\n\n3.  **差分信息熵的分析：**\n    *   通过分析DID的熵，本文揭示了不同熵值对策略行为的影响：\n        *   学习低熵差分信息会强化策略分布。\n        *   学习高熵差分信息会产生平滑效应，这解释了对数似然位移（log-likelihood displacement）现象。\n\n**实验验证与实际启示：**\n\n*   作者在合成实验中验证了理论发现，并将其扩展到真实的指令遵循数据集。\n*   研究结果表明：\n    *   学习高熵差分信息对于**通用指令遵循**至关重要。\n    *   学习低熵差分信息则有利于**知识密集型问答**。\n\n**总结：**\n\n本文通过差分信息的视角，为DPO目标、偏好数据结构以及由此产生的策略行为提供了一个统一的理解框架。",
      "shortSummary": "本文从信息论角度探讨直接偏好优化（DPO）的理论基础。通过引入差分信息分布（DID），研究发现当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO的对数比率奖励是学习目标策略的唯一最优形式，并揭示了DPO与对数边际有序策略的隐含关联。此外，分析DID的熵表明，低熵差分信息强化策略，高熵差分信息则产生平滑效应。实验验证了这些发现，并指出高熵差分信息对通用指令遵循至关重要，而低熵信息则有利于知识密集型问答。",
      "translated_title": "差分信息：偏好优化中的信息论视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information."
    },
    {
      "title": "LoRAShop：基于整流流变压器的免训练多概念图像生成与编辑 (原标题: LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers)",
      "link": "https://arxiv.org/abs/2505.23758",
      "pubDate": "Thu, 29 May 2025 13:59:46 GMT",
      "isoDate": "2025-05-29T13:59:46.000Z",
      "creator": "Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag",
      "summary": "## LoRAShop：免训练多概念图像生成与编辑框架\n\n### 1. 引言\n\nLoRAShop是一个开创性的框架，首次实现了使用LoRA模型进行多概念图像编辑。它旨在将个性化扩散模型转化为一个实用的“LoRA版Photoshop”工具，并为组合式视觉叙事和快速创意迭代开辟新途径。\n\n### 2. 核心观察与方法论\n\nLoRAShop的构建基于对Flux风格扩散变压器中特征交互模式的关键观察：\n\n*   **关键观察：** 概念特定的变压器特征在去噪过程的早期阶段会激活空间连贯的区域。\n\n利用这一观察，LoRAShop采取了以下方法：\n\n*   在一次前向传播中，为每个概念推导出解耦的潜在掩码。\n*   仅在限定概念的区域内混合相应的LoRA权重。\n\n### 3. 主要成果与优势\n\nLoRAShop框架带来了多项显著优势：\n\n*   **无缝集成：** 能够将多个主体或风格无缝地融入原始场景中。\n*   **上下文保留：** 在编辑过程中，能够同时保留图像的全局上下文、光照和精细细节。\n*   **身份保持：** 实验结果表明，LoRAShop在身份保持方面表现出优于现有基线方法的性能。\n*   **免训练：** 该框架消除了对重新训练和外部约束的需求，极大地简化了多概念图像编辑的流程。\n\n### 4. 应用与前景\n\n通过消除重新训练和外部约束，LoRAShop将个性化扩散模型转变为一个实用的“LoRA版Photoshop”工具，为用户提供了前所未有的灵活性和效率。这为以下领域开辟了新的可能性：\n\n*   **组合式视觉叙事：** 创作者可以更轻松地将不同概念融合到单一图像中，讲述更丰富的故事。\n*   **快速创意迭代：** 艺术家和设计师能够快速尝试和迭代不同的视觉概念，加速创作过程。",
      "shortSummary": "LoRAShop是一个开创性的框架，首次实现了使用LoRA模型进行免训练的多概念图像编辑。它基于对Flux风格扩散变压器中特征激活模式的关键观察，通过为每个概念生成解耦的潜在掩码，并在特定区域混合LoRA权重，实现多主体或风格的无缝集成，同时保留图像的全局上下文和细节。实验表明其身份保持效果优于基线。LoRAShop将个性化扩散模型转变为实用的图像编辑工具，为视觉创作提供了新可能。",
      "translated_title": "LoRAShop：基于整流流变压器的免训练多概念图像生成与编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration."
    },
    {
      "title": "DeepTheorem：通过自然语言和强化学习提升LLM在定理证明中的推理能力 (原标题: DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2505.23754",
      "pubDate": "Thu, 29 May 2025 13:59:39 GMT",
      "isoDate": "2025-05-29T13:59:39.000Z",
      "creator": "Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",
      "summary": "# DeepTheorem：提升LLM在定理证明中的推理能力\n\nDeepTheorem 是一个旨在通过利用自然语言来增强大型语言模型（LLM）数学推理能力的综合性非形式化定理证明框架。\n\n## 研究背景与挑战\n*   定理证明是评估LLM复杂推理能力的重要测试平台。\n*   传统的自动化定理证明（ATP）方法严重依赖形式化证明系统，这与LLM在预训练期间通过非形式化、自然语言知识获得的优势不符。\n\n## DeepTheorem 框架的核心组成\n\n### 1. 大规模基准数据集\n*   **规模与质量：** 包含12.1万个高质量的IMO（国际数学奥林匹克）级别非形式化定理及其证明。\n*   **多样性：** 涵盖多种数学领域。\n*   **严格标注：** 对正确性、难度和主题类别进行了严格标注。\n*   **可验证变体：** 附带系统构建的可验证定理变体，用于激励稳健推理。\n\n### 2. 新颖的强化学习策略（RL-Zero）\n*   **定制化：** 专门为非形式化定理证明设计。\n*   **激励机制：** 利用可验证定理变体来激励LLM进行稳健的数学推理。\n\n### 3. 全面评估指标\n*   **多维度评估：** 提出了全面的结果评估和过程评估指标。\n*   **评估内容：** 检查证明的正确性和推理步骤的质量。\n\n## 实验结果与发现\n*   **性能显著提升：** 实验分析表明，DeepTheorem 显著改善了LLM在定理证明方面的表现。\n*   **超越现有方法：** 相比现有数据集和监督微调协议，DeepTheorem 取得了更优异的成果。\n*   **最先进水平：** 实现了最先进的准确性和推理质量。\n*   **未来潜力：** 研究结果强调了 DeepTheorem 在根本上推进自动化非形式化定理证明和数学探索方面的潜力。\n\n## 作者与主题\n*   **主要作者：** Ziyin Zhang 等。\n*   **研究主题：** 计算与语言（cs.CL）、人工智能（cs.AI）。",
      "shortSummary": "DeepTheorem 提出了一种通过自然语言和强化学习提升LLM定理证明能力的新框架。它解决了传统形式化证明与LLM自然语言优势不符的问题。该框架包含一个12.1万条IMO级别非形式化定理和证明的大规模数据集，以及专为非形式化证明设计的RL-Zero强化学习策略。实验证明，DeepTheorem显著提高了LLM的定理证明性能，取得了最先进的准确性和推理质量，展现了在自动化非形式化定理证明领域的巨大潜力。",
      "translated_title": "DeepTheorem：通过自然语言和强化学习提升LLM在定理证明中的推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
    },
    {
      "title": "Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力 (原标题: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence)",
      "link": "https://arxiv.org/abs/2505.23747",
      "pubDate": "Thu, 29 May 2025 13:59:04 GMT",
      "isoDate": "2025-05-29T13:59:04.000Z",
      "creator": "Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan",
      "summary": "### Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力\n\n**1. 背景与挑战**\n*   多模态大语言模型（MLLMs）在2D视觉任务上取得了显著进展，但其空间智能能力仍有待提升。\n*   现有3D MLLMs通常依赖额外的3D或2.5D数据来整合空间感知，这限制了它们在仅有2D输入（如图像或视频）场景中的实用性。\n\n**2. Spatial-MLLM 框架概述**\n*   本文提出了Spatial-MLLM，一个新颖的框架，旨在从纯2D观测中实现基于视觉的空间推理。\n*   **核心洞察：** 与依赖CLIP优化语义理解的传统视频MLLMs不同，Spatial-MLLM的关键在于利用前馈视觉几何基础模型中强大的结构先验。\n\n**3. 架构设计**\n*   **双编码器架构：**\n    *   **预训练2D视觉编码器：** 用于提取语义特征。\n    *   **空间编码器：** 从视觉几何模型的骨干网络初始化，用于提取3D结构特征。\n*   **连接器：** 将上述两种特征整合为统一的视觉token，以增强模型的空间理解能力。\n\n**4. 推理策略**\n*   **空间感知帧采样策略：** 在推理时，该策略选择视频序列中具有空间信息量的帧。\n*   **目的：** 确保即使在有限的token长度下，模型也能专注于对空间推理至关重要的帧，从而提高效率和准确性。\n\n**5. 数据集与训练**\n*   **数据集：** 构建了Spatial-MLLM-120k数据集。\n*   **训练方法：** 模型在该数据集上通过监督微调（supervised fine-tuning）和GRPO进行训练。\n\n**6. 实验结果**\n*   在各种真实世界数据集上进行了广泛的实验。\n*   结果表明，Spatial-MLLM在广泛的基于视觉的空间理解和推理任务中实现了最先进（state-of-the-art）的性能。",
      "shortSummary": "Spatial-MLLM是一个新颖框架，旨在提升多模态大语言模型在纯2D视觉输入下的空间智能。它采用双编码器架构，结合语义特征和从视觉几何模型提取的3D结构特征。通过空间感知帧采样策略，模型能更有效地处理视频。Spatial-MLLM在自建的Spatial-MLLM-120k数据集上训练，并在多项基于视觉的空间理解和推理任务中取得了最先进的性能。",
      "translated_title": "Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/."
    },
    {
      "title": "是否信任你的视觉-语言模型的预测 (原标题: To Trust Or Not To Trust Your Vision-Language Model's Prediction)",
      "link": "https://arxiv.org/abs/2505.23745",
      "pubDate": "Thu, 29 May 2025 13:59:01 GMT",
      "isoDate": "2025-05-29T13:59:01.000Z",
      "creator": "Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink",
      "summary": "### 引言\n\n视觉-语言模型（VLMs）在对齐视觉和文本模态方面展现出强大能力，支持广泛的多模态理解和生成应用。然而，尽管它们在零样本和迁移学习场景中表现出色，VLMs仍易受错误分类的影响，经常给出自信但错误的预测。这一局限性在安全关键领域构成了重大风险，因为错误的预测可能导致严重后果。\n\n### TrustVLM框架\n\n为了解决VLM预测信任度评估这一关键挑战，本文引入了TrustVLM，一个无需训练的框架。\n\n### 核心思想与方法\n\n*   **动机：** TrustVLM的提出受到以下观察和洞察的启发：\n    *   VLMs中存在的模态差距。\n    *   某些概念在图像嵌入空间中表现得更为清晰和独特。\n*   **方法：** TrustVLM提出了一种新颖的置信度评分函数，该函数利用图像嵌入空间来改进错误分类的检测。\n\n### 实验评估与成果\n\n*   **评估范围：** 研究团队在17个多样化的数据集上，使用4种架构和2个VLM对TrustVLM进行了严格评估。\n*   **性能表现：** 与现有基线相比，TrustVLM展示了最先进的性能，具体改进如下：\n    *   AURC（曲线下面积）提升高达51.87%。\n    *   AUROC（接收者操作特征曲线下面积）提升9.14%。\n    *   FPR95（95%召回率下的误报率）降低32.42%。\n\n### 意义与展望\n\n*   通过在不要求重新训练模型的情况下提高其可靠性，TrustVLM为VLMs在实际应用中的更安全部署铺平了道路。\n\n### 代码可用性\n\n*   相关代码将在指定URL提供。",
      "shortSummary": "视觉-语言模型（VLMs）虽强大但常给出自信的错误预测，在安全关键领域构成风险。本文提出TrustVLM，一个无需训练的框架，通过利用图像嵌入空间中的模态差距，改进了VLM预测的信任度估计。TrustVLM在多项基准测试中表现出最先进的性能，显著提高了错误分类检测能力。它在不需模型重训练的情况下增强了VLM的可靠性，为VLMs在实际应用中的安全部署奠定了基础。",
      "translated_title": "是否信任你的视觉-语言模型的预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
    },
    {
      "title": "MAGREF: 任意参考视频生成中的掩码引导 (原标题: MAGREF: Masked Guidance for Any-Reference Video Generation)",
      "link": "https://arxiv.org/abs/2505.23742",
      "pubDate": "Thu, 29 May 2025 13:58:15 GMT",
      "isoDate": "2025-05-29T13:58:15.000Z",
      "creator": "Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma",
      "summary": "## MAGREF：任意参考视频生成中的掩码引导\n\n### 1. 研究背景与挑战\n\n随着深度生成模型，特别是基于扩散的方法的兴起，视频生成技术取得了显著进展。然而，基于多个参考主体的视频生成仍然面临两大挑战：\n\n*   **多主体一致性：** 难以在生成的视频中保持多个主体之间的连贯性。\n*   **生成质量：** 在复杂的多主体场景下，确保高质量的视频输出。\n\n### 2. 提出的方法：MAGREF 框架\n\n本文提出了一种名为 **MAGREF** 的统一框架，用于实现任意参考视频生成。MAGREF 引入了**掩码引导（masked guidance）**机制，旨在实现基于多样参考图像和文本提示的连贯多主体视频合成。\n\n### 3. MAGREF 的核心机制\n\nMAGREF 框架包含两个关键创新机制：\n\n*   **区域感知动态掩码机制：**\n    *   该机制允许单一模型灵活处理各种主体（包括人类、物体和背景）的推理。\n    *   其设计无需对模型架构进行任何修改，提高了模型的通用性和适应性。\n\n*   **像素级通道拼接机制：**\n    *   该机制在通道维度上进行操作，以更好地保留参考图像中的外观特征。\n    *   这有助于确保生成视频中的主体外观与参考图像高度一致。\n\n### 4. 性能与成果\n\n*   **最先进的生成质量：** MAGREF 实现了当前最先进的视频生成质量。\n*   **强大的泛化能力：** 模型能够从单主体训练泛化到复杂的多主体场景。\n*   **连贯合成与精确控制：** 实现了多主体视频的连贯合成，并能对视频中的单个主体进行精确控制。\n*   **超越现有基线：** 在性能上超越了现有的开源和商业基线模型。\n\n### 5. 评估与未来展望\n\n*   **引入综合基准：** 为了促进评估，研究人员还引入了一个全面的多主体视频基准。\n*   **实验验证：** 广泛的实验证明了 MAGREF 方法的有效性。\n*   **重要意义：** MAGREF 的提出为可扩展、可控和高保真的多主体视频合成铺平了道路。",
      "shortSummary": "MAGREF是一个用于任意参考视频生成的统一框架，旨在解决多主体视频生成中的一致性和质量挑战。它通过引入掩码引导，并结合区域感知动态掩码和像素级通道拼接机制，实现了最先进的视频生成质量。MAGREF能从单主体训练泛化到复杂多主体场景，提供连贯合成和精确控制，并优于现有基线。该研究还引入了全面的多主体视频基准，为可扩展、可控、高保真的多主体视频合成奠定了基础。",
      "translated_title": "MAGREF: 任意参考视频生成中的掩码引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF"
    },
    {
      "title": "ATLAS：在测试时优化记忆上下文的学习 (原标题: ATLAS: Learning to Optimally Memorize the Context at Test Time)",
      "link": "https://arxiv.org/abs/2505.23735",
      "pubDate": "Thu, 29 May 2025 13:57:16 GMT",
      "isoDate": "2025-05-29T13:57:16.000Z",
      "creator": "Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni",
      "summary": "### ATLAS：在测试时优化记忆上下文的学习\n\n本文介绍了ATLAS，一个旨在解决现有序列建模模型在处理长序列和长上下文理解方面局限性的新型长期记忆模块。\n\n*   **背景与现有模型局限性**\n    *   **Transformer模型**：\n        *   已成为序列建模领域最流行的骨干网络，在上下文检索任务中表现出色，并具备大规模学习能力。\n        *   其二次方的内存和时间复杂度限制了其在处理更长序列时的应用。\n    *   **现代循环神经网络（RNNs）**：\n        *   作为Transformer的替代架构被探索，尤其是在长期循环记忆模块方面取得了一定成功。\n        *   然而，它们在需要长上下文理解和向更长序列外推的任务中表现不佳。\n\n*   **RNNs设计缺陷分析**\n    作者观察到RNNs的这些不足源于其设计中的三个独立方面：\n    1.  **有限的记忆容量**：受限于记忆架构和输入特征映射。\n    2.  **在线更新性质**：记忆优化仅基于最后一个输入进行。\n    3.  **固定大小记忆的管理表达能力不足**。\n\n*   **ATLAS模型介绍**\n    *   **目标**：旨在增强上述三个方面。\n    *   **核心特点**：\n        *   一个高容量的长期记忆模块。\n        *   通过基于当前和过去令牌优化记忆来学习上下文，从而克服了长期记忆模型的在线更新性质。\n    *   **DeepTransformers**：\n        *   基于ATLAS的洞察，本文提出了一系列新的类Transformer架构，称为DeepTransformers。\n        *   这些架构是原始Transformer架构的严格泛化。\n\n*   **实验结果**\n    *   ATLAS在多项任务中超越了Transformer和最近的线性循环模型。\n    *   实验任务包括：语言建模、常识推理、记忆密集型任务和长上下文理解任务。\n    *   **显著成就**：ATLAS进一步提升了Titans模型在长上下文任务中的性能，在BABILong基准测试的1000万上下文长度上实现了超过80%的准确率提升。",
      "shortSummary": "ATLAS是一种新型长期记忆模块，旨在解决Transformer和现有循环神经网络在处理长序列和长上下文理解方面的局限。它通过优化基于当前和过去令牌的记忆来克服传统模型的在线更新和有限记忆容量问题。实验证明，ATLAS在语言建模、常识推理和长上下文理解等任务中，性能超越了Transformer和线性循环模型，尤其在1000万上下文长度的BABILong基准测试中，准确率提升超过80%。",
      "translated_title": "ATLAS：在测试时优化记忆上下文的学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark."
    },
    {
      "title": "AnySplat：无约束视角下的前馈3D高斯泼溅 (原标题: AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views)",
      "link": "https://arxiv.org/abs/2505.23716",
      "pubDate": "Thu, 29 May 2025 13:49:56 GMT",
      "isoDate": "2025-05-29T13:49:56.000Z",
      "creator": "Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai",
      "summary": "## AnySplat: 无约束视角下的前馈3D高斯泼溅\n\nAnySplat 是一种创新的前馈网络，旨在从非校准图像集合中实现新颖视图合成。它旨在解决传统神经渲染流程和现有前馈方法所面临的挑战。\n\n### 核心创新与区别\n\n*   **传统方法的局限性：** 传统的神经渲染流程需要已知的相机姿态和每场景优化，这限制了其在非受控环境中的应用。\n*   **现有前馈方法的挑战：** 尽管一些前馈方法已经出现，但它们在处理密集视图时往往面临巨大的计算负担。\n*   **AnySplat 的解决方案：** AnySplat 克服了这些限制，它能够通过单次前向传播预测所有必要信息，包括场景几何、外观以及相机参数。\n\n### AnySplat 的输出\n\nAnySplat 的一次前向传播会产生以下关键输出：\n\n*   **3D 高斯基元集合：** 这些基元编码了场景的几何结构和视觉外观。\n*   **相机内参和外参：** 为每张输入图像预测相应的相机内参和外参。\n\n### 主要优势与性能\n\n*   **统一设计与可扩展性：** AnySplat 的统一设计使其能够轻松处理随意捕获的多视图数据集，**无需任何姿态标注**，极大地简化了数据采集过程。\n*   **卓越的零样本评估表现：**\n    *   在零样本评估中，AnySplat 的质量在稀疏和密集视图场景下均能与有姿态感知的基线方法相媲美。\n    *   它显著超越了现有的无姿态方法。\n*   **显著降低渲染延迟：** 与基于优化的神经场相比，AnySplat 大幅减少了渲染延迟，从而实现了更高的效率。\n*   **实现实时新颖视图合成：** 这些优势使得 AnySplat 能够将实时新颖视图合成带入无约束的捕获场景中。\n\n### 作者\n\nLihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai。",
      "shortSummary": "AnySplat是一个创新的前馈网络，专为从非校准图像集合中进行新颖视图合成而设计。它克服了传统方法对已知相机姿态的依赖以及现有前馈方法在密集视图下的计算瓶颈。AnySplat通过单次前向传播即可预测3D高斯基元及相机参数，无需姿态标注。在零样本评估中，其性能媲美甚至超越现有方法，并显著降低渲染延迟，为无约束捕获下的实时视图合成铺平道路。",
      "translated_title": "AnySplat：无约束视角下的前馈3D高斯泼溅",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/"
    },
    {
      "title": "VF-Eval：评估多模态大语言模型在生成AIGC视频反馈方面的能力 (原标题: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos)",
      "link": "https://arxiv.org/abs/2505.23693",
      "pubDate": "Thu, 29 May 2025 13:31:13 GMT",
      "isoDate": "2025-05-29T13:31:13.000Z",
      "creator": "Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao",
      "summary": "### VF-Eval：评估多模态大语言模型在AIGC视频反馈方面的能力\n\n**背景与研究动机：**\n*   多模态大语言模型（MLLMs）近期在视频问答领域得到了广泛研究。\n*   然而，现有的大多数评估主要集中在自然视频上，忽略了合成视频，特别是AI生成内容（AIGC）视频。\n*   尽管一些视频生成工作已开始利用MLLMs来评估生成视频的质量，但MLLMs在解释AIGC视频并提供反馈方面的能力仍未得到充分探索。\n\n**提出的解决方案：VF-Eval基准测试**\n*   为了解决上述研究空白，研究人员提出了一个新的基准测试——VF-Eval。\n*   VF-Eval旨在全面评估MLLMs在AIGC视频上的能力，它引入了四个核心任务：\n    1.  **连贯性验证 (coherence validation)**：评估模型识别AIGC视频内容逻辑连贯性的能力。\n    2.  **错误感知 (error awareness)**：测试模型能否感知到AIGC视频中存在的错误或异常。\n    3.  **错误类型检测 (error type detection)**：要求模型识别并分类AIGC视频中出现的具体错误类型。\n    4.  **推理评估 (reasoning evaluation)**：考察模型对AIGC视频内容进行深层推理和理解的能力。\n\n**评估结果与发现：**\n*   研究团队在VF-Eval基准上评估了13个当前前沿的MLLMs。\n*   评估结果显示，即使是表现最好的模型（如GPT-4.1），也难以在所有任务中持续取得良好表现。\n*   这一发现突出表明了VF-Eval基准测试的挑战性，并揭示了当前MLLMs在处理AIGC视频反馈方面的局限性。\n\n**实际应用探索：RePrompt实验**\n*   为了进一步探究VF-Eval在改进视频生成方面的实际应用潜力，研究人员进行了一项名为RePrompt的实验。\n*   该实验旨在证明，通过使MLLMs更紧密地与人类反馈对齐，可以有效地提升视频生成质量。\n*   这为未来利用MLLMs改进AIGC视频生成提供了新的方向。",
      "shortSummary": "VF-Eval是一个新基准，旨在评估多模态大语言模型（MLLMs）在AI生成内容（AIGC）视频反馈方面的能力。现有研究多集中于自然视频，忽略了AIGC视频的评估。VF-Eval包含连贯性验证、错误感知、错误类型检测和推理评估四个任务。评估发现，即使是GPT-4.1等领先模型，也难以在所有任务中表现出色，这表明该领域仍具挑战。研究还通过RePrompt实验证明，将MLLMs与人类反馈对齐有助于改进视频生成。",
      "translated_title": "VF-Eval：评估多模态大语言模型在生成AIGC视频反馈方面的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation."
    },
    {
      "title": "D-AR：通过自回归模型实现扩散 (原标题: D-AR: Diffusion via Autoregressive Models)",
      "link": "https://arxiv.org/abs/2505.23660",
      "pubDate": "Thu, 29 May 2025 13:09:25 GMT",
      "isoDate": "2025-05-29T13:09:25.000Z",
      "creator": "Ziteng Gao, Mike Zheng Shou",
      "summary": "### D-AR：通过自回归模型实现扩散\n\n本文介绍了一种名为“D-AR”（Diffusion via Autoregressive models）的新范式，它将图像扩散过程重新定义为一种标准的自回归过程，采用传统的“下一个词元预测”方式。\n\n#### 核心机制\n\n*   **词元化器设计**：D-AR首先设计了一个词元化器，将图像转换为离散词元序列。这些词元在不同位置可以被解码为像素空间中不同的扩散去噪步骤。\n*   **粗到细的顺序**：得益于扩散特性，这些词元自然地遵循从粗到细的顺序，这直接适用于自回归建模。\n*   **标准自回归建模**：该方法在这些词元上应用标准的“下一个词元预测”，不修改任何底层设计（无论是因果掩码还是训练/推理策略）。\n*   **镜像扩散过程**：这种顺序的自回归词元生成直接反映了图像空间中的扩散过程。一旦自回归模型生成了一组增量词元，就可以以流式方式直接将这些词元解码为相应的扩散去噪步骤。\n\n#### 显著特性\n\n*   **一致性预览**：D-AR自然支持在仅生成部分词元时提供一致的预览。\n*   **零样本布局控制合成**：该方法还能够实现零样本的布局控制合成。\n\n#### 性能表现\n\n*   在标准ImageNet基准测试中，D-AR使用一个775M Llama骨干网络和256个离散词元，实现了2.09的FID（Fréchet Inception Distance）分数。\n\n#### 未来展望\n\n*   作者希望这项工作能够启发未来在视觉合成统一自回归架构方面的研究，特别是与大型语言模型的结合。",
      "shortSummary": "D-AR（Diffusion via Autoregressive models）是一种将图像扩散过程重构为自回归“下一个词元预测”任务的新范式。它通过将图像转换为遵循粗到细顺序的离散词元序列，并应用标准自回归模型进行生成，直接镜像了扩散过程。该方法支持一致性预览和零样本布局控制合成，并在ImageNet上使用775M Llama骨干网络实现了2.09 FID。这项工作旨在推动视觉合成中统一自回归架构（尤其是与大型语言模型结合）的研究。",
      "translated_title": "D-AR：通过自回归模型实现扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at https://github.com/showlab/D-AR"
    },
    {
      "title": "推理模型更容易产生幻觉吗？ (原标题: Are Reasoning Models More Prone to Hallucination?)",
      "link": "https://arxiv.org/abs/2505.23646",
      "pubDate": "Thu, 29 May 2025 12:53:41 GMT",
      "isoDate": "2025-05-29T12:53:41.000Z",
      "creator": "Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua",
      "summary": "### 推理模型幻觉倾向研究\n\n**引言：**\n\n*   近期发展的大型推理模型（LRMs）在解决复杂任务时展现出强大的链式思考（CoT）推理能力。\n*   然而，由于这些LRMs大多通过对形式推理任务进行后训练而开发，它们是否能将推理能力泛化以帮助减少事实寻求任务中的幻觉，目前尚不明确且存在争议。\n*   例如，DeepSeek-R1报告在SimpleQA（一个事实寻求基准）上性能有所提升，而OpenAI-o3却观察到更严重的幻觉。这种差异自然引出了一个研究问题：“推理模型是否更容易产生幻觉？”\n\n**研究视角与发现：**\n\n本文从三个角度探讨了上述问题：\n\n1.  **对LRM幻觉的整体评估：**\n    *   我们的分析揭示，经过完整后训练流程（包括冷启动监督微调SFT和可验证奖励强化学习RL）的LRMs通常能减轻其幻觉。\n    *   相比之下，仅进行蒸馏训练或在没有冷启动微调的情况下进行RL训练，会引入更细微的幻觉。\n\n2.  **行为分析：**\n    *   为了探究不同后训练流程如何改变对LRM幻觉的影响，我们进行了行为分析。\n    *   我们识别了两种直接影响LRM事实性的关键认知行为：\n        *   **缺陷重复（Flaw Repetition）：** 表面层面的推理尝试反复遵循相同的底层缺陷逻辑。\n        *   **思考-答案不匹配（Think-Answer Mismatch）：** 最终答案未能忠实地匹配先前的链式思考过程。\n\n3.  **从模型不确定性角度探究幻觉机制：**\n    *   我们进一步从模型不确定性的角度调查了LRM幻觉背后的机制。\n    *   研究发现，LRM幻觉的增加通常与模型不确定性与事实准确性之间的错位（misalignment）有关。\n\n**结论：**\n\n*   我们的工作为理解大型推理模型中的幻觉提供了初步的认识。",
      "shortSummary": "本研究探讨了大型推理模型（LRMs）是否更容易产生幻觉。研究发现，经过完整后训练流程（SFT+RL）的LRMs通常能减轻幻觉，而单纯蒸馏或无冷启动RL会引入更多幻觉。此外，模型事实性受“缺陷重复”和“思考-答案不匹配”两种行为影响。幻觉增加常与模型不确定性与事实准确性错位相关。本研究为理解LRM幻觉提供了初步认识。",
      "translated_title": "推理模型更容易产生幻觉吗？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs."
    },
    {
      "title": "ZeroSep：零训练分离音频中的任何内容 (原标题: ZeroSep: Separate Anything in Audio with Zero Training)",
      "link": "https://arxiv.org/abs/2505.23625",
      "pubDate": "Thu, 29 May 2025 12:31:45 GMT",
      "isoDate": "2025-05-29T12:31:45.000Z",
      "creator": "Chao Huang, Yuesheng Ma, Junxuan Huang, Susan Liang, Yunlong Tang, Jing Bi, Wenqiang Liu, Nima Mesgarani, Chenliang Xu",
      "summary": "## ZeroSep：零训练音频源分离\n\n*   **背景与挑战**\n    *   音频源分离是机器理解复杂声学环境的基础，并支撑着众多音频应用。\n    *   当前主流的监督式深度学习方法虽然强大，但存在局限性：\n        *   需要大量、特定任务的标注数据。\n        *   难以泛化到真实世界声学场景的巨大变异性和开放集特性。\n\n*   **核心发现与方法**\n    *   受生成式基础模型成功的启发，研究人员探索了预训练的文本引导音频扩散模型是否能克服上述局限。\n    *   他们发现了一个令人惊讶的结论：在正确配置下，仅通过预训练的文本引导音频扩散模型即可实现零样本（zero-shot）源分离。\n    *   该方法被命名为 **ZeroSep**。\n    *   **工作原理**：\n        1.  将混合音频反转（invert）到扩散模型的潜在空间。\n        2.  利用文本条件（text conditioning）引导去噪过程，以恢复单个声源。\n\n*   **ZeroSep 的优势与特点**\n    *   **零训练/零微调**：无需任何特定任务的训练或微调。\n    *   **任务重用**：将生成式扩散模型重新用于判别性分离任务。\n    *   **开放集支持**：通过其丰富的文本先验，固有地支持开放集场景（即处理未见过的声源）。\n    *   **兼容性强**：兼容多种预训练的文本引导音频扩散骨干网络。\n    *   **性能卓越**：在多个分离基准测试中展现出强大的分离性能，甚至超越了监督式方法。",
      "shortSummary": "ZeroSep是一种创新的音频源分离方法，它利用预训练的文本引导音频扩散模型实现零样本分离。该方法无需任何任务特定训练或微调，通过将混合音频反转到潜在空间并利用文本引导去噪来恢复单个声源。ZeroSep支持开放集场景，兼容多种模型，并在分离性能上超越了许多监督式方法，为音频理解提供了高效且灵活的解决方案。",
      "translated_title": "ZeroSep：零训练分离音频中的任何内容",
      "images": [],
      "contentSource": "完整文章",
      "content": "Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods."
    },
    {
      "title": "Table-R1：表格推理的推理时缩放 (原标题: Table-R1: Inference-Time Scaling for Table Reasoning)",
      "link": "https://arxiv.org/abs/2505.23621",
      "pubDate": "Thu, 29 May 2025 12:28:50 GMT",
      "isoDate": "2025-05-29T12:28:50.000Z",
      "creator": "Zheyuan Yang, Lyuhao Chen, Arman Cohan, Yilun Zhao",
      "summary": "本文首次探讨了表格推理任务中的推理时缩放问题。研究人员开发并评估了两种后训练策略以实现推理时缩放：\n\n*   **从前沿模型推理轨迹中进行蒸馏**：\n    *   引入了一个由DeepSeek-R1生成的大规模推理轨迹数据集。\n    *   该数据集用于微调大型语言模型（LLMs），从而得到Table-R1-SFT模型。\n\n*   **基于可验证奖励的强化学习（RLVR）**：\n    *   提出了任务特定的可验证奖励函数。\n    *   应用GRPO算法来获得Table-R1-Zero模型。\n\n**模型评估**\n\nTable-R1系列模型在多种表格推理任务上进行了评估，包括短形式问答、事实核查和自由形式问答。\n\n**主要成果**\n\n*   值得注意的是，Table-R1-Zero模型（仅使用7B参数的LLM）的性能与GPT-4.1和DeepSeek-R1相当或超越它们。\n*   它还展示了对域外数据集的强大泛化能力。\n\n**深入分析**\n\n广泛的消融实验和定性分析揭示了指令微调、模型架构选择、跨任务泛化以及在RL训练过程中核心表格推理技能涌现的益处。",
      "shortSummary": "本文首次研究表格推理任务的推理时缩放，提出了两种后训练策略：基于DeepSeek-R1推理轨迹的蒸馏（Table-R1-SFT）和基于可验证奖励的强化学习（Table-R1-Zero）。值得关注的是，仅使用7B参数的Table-R1-Zero模型在多种表格推理任务上，其性能可媲美甚至超越GPT-4.1和DeepSeek-R1，并展现出强大的泛化能力，证明了这些方法在高效表格推理方面的有效性。",
      "translated_title": "Table-R1：表格推理的推理时缩放",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we present the first study to explore inference-time scaling on table reasoning tasks. We develop and evaluate two post-training strategies to enable inference-time scaling: distillation from frontier model reasoning traces and reinforcement learning with verifiable rewards (RLVR). For distillation, we introduce a large-scale dataset of reasoning traces generated by DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For RLVR, we propose task-specific verifiable reward functions and apply the GRPO algorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series models across diverse table reasoning tasks, including short-form QA, fact verification, and free-form QA. Notably, the Table-R1-Zero model matches or exceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a 7B-parameter LLM. It also demonstrates strong generalization to out-of-domain datasets. Extensive ablation and qualitative analyses reveal the benefits of instruction tuning, model architecture choices, and cross-task generalization, as well as emergence of essential table reasoning skills during RL training."
    },
    {
      "title": "Muddit：通过统一离散扩散模型解放超越文本到图像的生成能力 (原标题: Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model)",
      "link": "https://arxiv.org/abs/2505.23606",
      "pubDate": "Thu, 29 May 2025 12:15:48 GMT",
      "isoDate": "2025-05-29T12:15:48.000Z",
      "creator": "Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, Shuicheng Yan",
      "summary": "### Muddit：统一离散扩散模型\n\n**1. 引言与背景**\n\n统一生成模型旨在通过单一架构和解码范式处理跨模态任务，例如文本生成、图像生成以及视觉-语言推理。然而，现有方法面临以下挑战：\n\n*   **自回归统一模型：** 由于其顺序解码机制，推理速度较慢。\n*   **非自回归统一模型：** 由于预训练骨干网络的局限性，泛化能力较弱。\n\n**2. Muddit模型介绍**\n\n为了解决上述问题，研究人员引入了Muddit，这是一种统一的离散扩散Transformer模型。Muddit的核心目标是实现文本和图像模态的快速并行生成。\n\n**3. 核心创新与优势**\n\n与以往从头开始训练的统一扩散模型不同，Muddit的关键创新在于：\n\n*   它将来自预训练文本到图像骨干网络的强大视觉先验知识整合到模型中。\n*   同时，它结合了一个轻量级的文本解码器。\n\n这种独特的设计使得Muddit能够在统一的架构下，实现灵活且高质量的多模态生成。\n\n**4. 实验结果**\n\n经验结果表明，与显著更大的自回归模型相比，Muddit在生成质量和效率方面均达到了竞争或更优的性能。\n\n**5. 研究意义**\n\n这项工作突出了纯离散扩散模型在配备强大视觉先验知识时，作为统一生成领域可扩展且有效骨干网络的巨大潜力。",
      "shortSummary": "Muddit是一种统一的离散扩散Transformer模型，旨在实现文本和图像模态的快速并行生成。它通过整合预训练文本到图像骨干网络的强大视觉先验知识与轻量级文本解码器，克服了现有统一模型的局限性。实验证明，Muddit在质量和效率上均优于或媲美大型自回归模型，展现了离散扩散模型在统一生成领域的巨大潜力。",
      "translated_title": "Muddit：通过统一离散扩散模型解放超越文本到图像的生成能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified generation models aim to handle diverse tasks across modalities -- such as text generation, image generation, and vision-language reasoning -- within a single architecture and decoding paradigm. Autoregressive unified models suffer from slow inference due to sequential decoding, and non-autoregressive unified models suffer from weak generalization due to limited pretrained backbones. We introduce Muddit, a unified discrete diffusion transformer that enables fast and parallel generation across both text and image modalities. Unlike prior unified diffusion models trained from scratch, Muddit integrates strong visual priors from a pretrained text-to-image backbone with a lightweight text decoder, enabling flexible and high-quality multimodal generation under a unified architecture. Empirical results show that Muddit achieves competitive or superior performance compared to significantly larger autoregressive models in both quality and efficiency. The work highlights the potential of purely discrete diffusion, when equipped with strong visual priors, as a scalable and effective backbone for unified generation."
    },
    {
      "title": "Satori-SWE：面向样本高效软件工程的演化式测试时扩展 (原标题: Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering)",
      "link": "https://arxiv.org/abs/2505.23604",
      "pubDate": "Thu, 29 May 2025 12:15:36 GMT",
      "isoDate": "2025-05-29T12:15:36.000Z",
      "creator": "Guangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox, Gregory Wornell, Wei Lu, Zhang-Wei Hong, Chuang Gan",
      "summary": "# Satori-SWE：面向样本高效软件工程的演化式测试时扩展\n\n## 引言与背景\n大型语言模型（LMs）在标准化编码基准测试中表现良好，但在解决真实世界的软件工程（SWE）任务（例如SWE-Bench中的GitHub问题）时面临挑战，特别是当模型参数小于100B时。尽管较小的模型因其较低的计算成本而在实践中更受欢迎，但提高其性能仍然具有挑战性。\n\n### 现有方法的局限性\n*   **监督微调（SFT）**：主要依赖高质量数据，但大规模策划这些数据成本高昂。\n*   **测试时扩展（Test-Time Scaling）**：通过生成多个输出、使用验证器评分并选择最佳输出来实现。尽管有效，但这种策略通常需要过多的采样和昂贵的评分，限制了其实际应用。\n\n## EvoScale方法：演化式测试时扩展\n为了解决上述挑战，本文提出了**演化式测试时扩展（EvoScale）**，这是一种样本高效的方法，它将生成过程视为一个演化过程。\n\n### 核心机制\n*   **迭代优化**：EvoScale通过选择和变异迭代地优化输出，将输出分布推向更高分区域，从而减少了找到正确解决方案所需的样本数量。\n*   **自演化（Self-Evolution）**：为了减少重复采样和选择带来的开销，研究人员使用强化学习（RL）训练模型进行自演化。这意味着模型学习在迭代过程中自我提升其生成结果的分数，而无需在推理时依赖外部验证器。\n\n## 实验评估与成果\n*   **评估基准**：EvoScale在SWE-Bench-Verified数据集上进行了评估。\n*   **性能表现**：结果显示，EvoScale使得其32B参数的模型**Satori-SWE-32B**能够以少量样本匹配或超越参数量超过100B的模型性能。\n\n## 开源承诺\n该项目的代码、数据和模型将完全开源。\n\n## 相关领域\n*   计算与语言（cs.CL）\n*   人工智能（cs.AI）\n*   软件工程（cs.SE）",
      "shortSummary": "大型语言模型在真实软件工程任务中表现受限，尤其小模型。现有测试时扩展方法因高采样和评分成本而受限。Satori-SWE提出EvoScale，一种演化式测试时扩展方法，将生成视为演化过程。它通过迭代优化和强化学习实现模型自演化，无需外部验证器，显著减少了样本需求。EvoScale使Satori-SWE-32B（32B模型）在SWE-Bench-Verified上以少量样本达到或超越100B+模型的性能。代码、数据和模型将开源。",
      "translated_title": "Satori-SWE：面向样本高效软件工程的演化式测试时扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced."
    },
    {
      "title": "基于最优奖励基线的在策略强化学习 (原标题: On-Policy RL with Optimal Reward Baseline)",
      "link": "https://arxiv.org/abs/2505.23585",
      "pubDate": "Thu, 29 May 2025 11:58:04 GMT",
      "isoDate": "2025-05-29T11:58:04.000Z",
      "creator": "Yaru Hao, Li Dong, Xun Wu, Shaohan Huang, Zewen Chi, Furu Wei",
      "summary": "## 基于最优奖励基线的在策略强化学习 (OPO)\n\n### 背景与挑战\n\n强化学习（RL）算法在使大型语言模型（LLM）与人类偏好对齐并增强其推理能力方面发挥着基础性作用。然而，当前的强化学习算法普遍面临以下挑战：\n\n*   **训练不稳定**：由于宽松的在策略（on-policy）约束。\n*   **计算效率低下**：由于需要辅助模型。\n\n### OPO算法的提出\n\n本文提出了一种新颖且简化的强化学习算法——**基于最优奖励基线的在策略强化学习（On-Policy RL with Optimal reward baseline, OPO）**，旨在解决上述问题。\n\n### OPO的核心特点\n\nOPO算法通过以下关键创新来提升性能和稳定性：\n\n*   **强调精确的在策略训练**：这在经验上能够稳定训练过程并增强探索能力。\n*   **引入最优奖励基线**：该基线在理论上能够最小化梯度方差。\n\n### 实验评估与结果\n\nOPO在数学推理基准上进行了评估，结果显示出其显著优势：\n\n*   **卓越的性能与训练稳定性**：在不依赖额外模型或正则化项的情况下，OPO展现出优越的性能和训练稳定性。\n*   **更低的策略偏移与更高的输出熵**：这鼓励模型生成更多样化、更少重复的响应。\n\n### 结论\n\n这些结果表明，OPO为大型语言模型对齐和推理任务中的稳定、有效强化学习提供了一个有前景的方向。",
      "shortSummary": "OPO（基于最优奖励基线的在策略强化学习）是一种新型RL算法，旨在解决LLM对齐中RL训练不稳定和计算效率低下的问题。OPO通过强调精确在策略训练来稳定过程并增强探索，同时引入最优奖励基线以最小化梯度方差。实验证明，OPO在数学推理任务上表现出卓越的性能和训练稳定性，且无需额外模型，能生成更具多样性的响应。",
      "translated_title": "基于最优奖励基线的在策略强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo."
    },
    {
      "title": "SafeScientist：面向LLM智能体的风险感知科学发现 (原标题: SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents)",
      "link": "https://arxiv.org/abs/2505.23559",
      "pubDate": "Thu, 29 May 2025 11:35:58 GMT",
      "isoDate": "2025-05-29T11:35:58.000Z",
      "creator": "Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You",
      "summary": "### SafeScientist：面向LLM智能体的风险感知科学发现框架\n\n**引言与背景**\n*   大型语言模型（LLM）智能体在加速科学发现自动化方面取得了显著进展，但同时也带来了关键的伦理和安全问题。\n\n**SafeScientist框架**\n*   **目标：** 旨在系统性地解决LLM智能体在科学发现中的伦理和安全挑战，增强AI驱动科学探索的安全性与伦理责任。\n*   **核心功能：**\n    *   主动拒绝伦理不当或高风险任务。\n    *   在整个研究过程中严格强调安全性。\n*   **防御机制：** 为实现全面的安全监督，SafeScientist集成了多种防御机制，包括：\n    *   提示监控（Prompt monitoring）\n    *   智能体协作监控（Agent-collaboration monitoring）\n    *   工具使用监控（Tool-use monitoring）\n    *   伦理审查组件（Ethical reviewer component）\n\n**SciSafetyBench基准**\n*   **目的：** 作为SafeScientist的补充，SciSafetyBench是一个专门设计用于评估科学背景下AI安全的创新基准。\n*   **构成：**\n    *   包含240个高风险科学任务，涵盖6个领域。\n    *   包含30个专门设计的科学工具。\n    *   包含120个与工具相关的风险任务。\n\n**实验结果与验证**\n*   **安全性能提升：** 大量实验表明，相较于传统AI科学家框架，SafeScientist显著提升了35%的安全性能。\n*   **科学产出质量：** 在提升安全性的同时，并未损害科学产出质量。\n*   **鲁棒性验证：** 严格验证了其安全管道（safety pipeline）对多种对抗性攻击方法的鲁棒性，进一步证实了其集成方法的有效性。\n\n**资源可用性**\n*   相关代码和数据将公开可用。",
      "shortSummary": "SafeScientist是一个创新的AI科学家框架，旨在提升LLM智能体在科学发现中的安全性和伦理责任。它通过主动拒绝高风险任务并集成多重防御机制（如提示和工具使用监控）来实现全面安全监督。为评估其性能，研究者提出了SciSafetyBench基准。实验表明，SafeScientist显著提升了35%的安全性能，且不影响科学产出质量，并对对抗性攻击具有鲁棒性。",
      "translated_title": "SafeScientist：面向LLM智能体的风险感知科学发现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce SafeScientist, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose SciSafetyBench, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. red{Warning: this paper contains example data that may be offensive or harmful.}"
    },
    {
      "title": "SWE-bench 实时上线！ (原标题: SWE-bench Goes Live!)",
      "link": "https://arxiv.org/abs/2505.23419",
      "pubDate": "Thu, 29 May 2025 09:09:44 GMT",
      "isoDate": "2025-05-29T09:09:44.000Z",
      "creator": "Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang",
      "summary": "## SWE-bench-Live：面向实时软件开发任务的动态基准测试\n\n### 引言：现有基准的局限性\n\n大型语言模型（LLM）在解决实际软件bug（即问题解决任务）方面的能力评估日益重要。然而，当前该领域的标准基准，如SWE-bench及其变体，存在以下关键局限性：\n\n*   **缺乏更新**：自首次发布以来未曾更新。\n*   **覆盖范围狭窄**：仅涵盖少数软件仓库。\n*   **高度依赖人工**：实例构建和环境设置需要大量人工投入。\n*   **阻碍可扩展性**：上述因素限制了基准的可扩展性。\n*   **引入风险**：存在过拟合和数据污染的风险。\n\n### SWE-bench-Live：解决方案与核心特性\n\n为克服这些挑战，我们提出了**SWE-bench-Live**，一个**可实时更新**的基准测试。\n\n*   **初始发布内容**：\n    *   包含1,319个任务，这些任务均源自2024年以来创建的真实GitHub问题。\n    *   涵盖93个不同的软件仓库，提供了更广泛的多样性。\n    *   每个任务都附带一个专用的Docker镜像，以确保评估的可复现性。\n\n*   **核心创新：自动化策展流程**：\n    *   SWE-bench-Live的核心是一个名为`\\method`的自动化策展管道。\n    *   该流程简化了从实例创建到环境设置的整个过程，消除了传统的人工瓶颈。\n    *   这种自动化能力使得基准能够实现可扩展性和持续更新。\n\n### 评估与发现\n\n我们在SWE-bench-Live上评估了一系列最先进的代理框架和大型语言模型。评估结果揭示了重要发现：\n\n*   **显著的性能差距**：与SWE-bench等静态基准相比，即使在受控评估条件下，LLM在SWE-bench-Live上的表现也存在显著的性能差距。\n*   **深入分析**：为更好地理解这种差异，我们对仓库来源、问题新旧程度和任务难度进行了详细分析。\n\n### 意义与价值\n\nSWE-bench-Live通过提供一个基于实时仓库活动、全新、多样化且可执行的基准，促进了对LLM和代理在动态、真实世界软件开发环境中进行严格且抗污染的评估。",
      "shortSummary": "SWE-bench-Live是一个可实时更新的基准测试，旨在解决现有LLM软件bug修复基准（如SWE-bench）的局限性，包括缺乏更新、仓库覆盖窄和人工依赖。它包含1319个来自2024年真实GitHub问题的任务，并利用自动化流程和Docker确保可扩展性和可复现性。评估显示，LLM在SWE-bench-Live上的表现与静态基准存在显著差距，为LLM在动态真实世界软件开发中的评估提供了更严格、抗污染的方法。",
      "translated_title": "SWE-bench 实时上线！",
      "images": [],
      "contentSource": "完整文章",
      "content": "The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings."
    },
    {
      "title": "KVzip: 独立于查询的KV缓存压缩与上下文重建 (原标题: KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction)",
      "link": "https://arxiv.org/abs/2505.23416",
      "pubDate": "Thu, 29 May 2025 09:05:47 GMT",
      "isoDate": "2025-05-29T09:05:47.000Z",
      "creator": "Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song",
      "summary": "## KVzip：独立于查询的KV缓存压缩与上下文重建\n\n### 背景与问题\n*   基于Transformer的大型语言模型（LLMs）在推理过程中会将上下文缓存为键值（KV）对。\n*   随着上下文长度的增长，KV缓存的大小会急剧膨胀，这导致了显著的内存开销和注意力机制的延迟增加。\n\n### KVzip方法介绍\n*   KVzip是一种新颖的、独立于查询的KV缓存逐出方法，旨在实现压缩后的KV缓存在不同查询场景下的有效重用。\n*   该方法的核心在于利用底层的LLM来量化每个KV对的重要性。具体而言，它通过从缓存的KV对中重建原始上下文来评估其重要性。\n*   随后，KVzip会逐出那些重要性较低的KV对，从而实现缓存的压缩。\n\n### 实验评估与结果\n*   **缓存大小与延迟优化：** 广泛的实证评估表明，KVzip能够将KV缓存的大小减少3-4倍，并将FlashAttention解码延迟降低约2倍。\n*   **性能影响：** 在问答、信息检索、推理和代码理解等多种任务中，KVzip带来的性能损失可以忽略不计。\n*   **模型与上下文长度支持：** 评估涵盖了多种主流模型，包括LLaMA3.1-8B、Qwen2.5-14B和Gemma3-12B。该方法支持的上下文长度最长可达170K tokens。\n*   **与现有方法的比较：** KVzip显著优于现有的、依赖查询的KV逐出方法。在多查询场景下，即使在90%的缓存预算比率下，这些现有方法也会出现明显的性能下降，而KVzip则表现出更强的鲁棒性。\n\n### 图片信息\n*   文章内容中未包含有效的实际图片链接，因此摘要中不包含图片。",
      "shortSummary": "KVzip是一种独立于查询的KV缓存压缩方法，旨在解决大型语言模型（LLMs）推理过程中KV缓存过大导致的内存和延迟问题。它通过LLM重建上下文来量化KV对的重要性并逐出不重要的对。实验表明，KVzip能将KV缓存大小减少3-4倍，解码延迟降低约2倍，同时在多种任务上保持可忽略的性能损失，并支持长达170K tokens的上下文，显著优于现有方法。",
      "translated_title": "KVzip: 独立于查询的KV缓存压缩与上下文重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by 3-4times and FlashAttention decoding latency by approximately 2times, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios."
    }
  ],
  "lastUpdated": "2025-05-30T09:35:37.194Z"
}