{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "潜在去噪造就优秀的视觉分词器 (原标题: Latent Denoising Makes Good Visual Tokenizers)",
      "link": "https://arxiv.org/abs/2507.15856",
      "pubDate": "Mon, 21 Jul 2025 13:59:56 GMT",
      "isoDate": "2025-07-21T13:59:56.000Z",
      "creator": "Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, Yue Wang",
      "summary": "### 视觉分词器与潜在去噪\n\n**1. 背景与问题**\n\n*   尽管视觉分词器在生成模型中扮演着基础性角色，但目前尚不清楚哪些特性能够使其在生成建模中更有效。\n\n**2. 核心观察与洞察**\n\n*   研究人员观察到，现代生成模型共享一个概念上相似的训练目标：从高斯噪声或掩码等损坏输入中重建干净信号。这一过程被研究人员称为“去噪”（denoising）。\n*   受此启发，研究人员提出将分词器嵌入（tokenizer embeddings）直接与下游的去噪目标对齐。这种对齐旨在鼓励潜在嵌入（latent embeddings）即使在严重损坏的情况下也能更容易地被重建。\n\n**3. 提出的方法：潜在去噪分词器 (l-DeTok)**\n\n*   为了实现上述目标，文章引入了一种简单而有效的视觉分词器——潜在去噪分词器（Latent Denoising Tokenizer, l-DeTok）。\n*   **工作原理：** l-DeTok 的训练目标是从被插值噪声（interpolative noise）和随机掩码（random masking）损坏的潜在嵌入中重建干净图像。\n\n**4. 实验结果与性能**\n\n*   研究人员在 ImageNet 256x256 数据集上进行了广泛的实验。\n*   实验结果表明，l-DeTok 在六种代表性的生成模型中，始终优于标准的分词器。\n\n**5. 结论与未来展望**\n\n*   研究发现强调了“去噪”作为分词器开发的一个基本设计原则。\n*   作者希望这一发现能够为未来的分词器设计提供新的视角和启发。",
      "shortSummary": "该研究提出“潜在去噪”是提升视觉分词器效果的关键原则。文章引入了潜在去噪分词器（l-DeTok），它通过从损坏的潜在嵌入中重建图像进行训练。在ImageNet 256x256上的广泛实验表明，l-DeTok在多种生成模型中表现优于标准分词器。这强调了去噪作为分词器设计的基本原则，为未来发展提供了新视角。",
      "translated_title": "潜在去噪造就优秀的视觉分词器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design."
    },
    {
      "title": "SeC：通过渐进式概念构建推进复杂视频对象分割 (原标题: SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction)",
      "link": "https://arxiv.org/abs/2507.15852",
      "pubDate": "Mon, 21 Jul 2025 13:59:02 GMT",
      "isoDate": "2025-07-21T13:59:02.000Z",
      "creator": "Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang",
      "summary": "## SeC：通过渐进式概念构建推进复杂视频对象分割\n\n### 引言\n\n视频对象分割（VOS）是计算机视觉领域的核心任务，要求模型在视频帧中跟踪和分割目标对象。尽管现有技术取得了显著进展，但在处理剧烈视觉变化、遮挡和复杂场景变化方面，当前技术仍远不及人类能力。这种局限性源于它们过度依赖外观匹配，而忽略了人类在跨时间动态中实现鲁棒识别所依赖的、类似人类的物体概念理解。\n\n### SeC框架：概念驱动的分割\n\n为弥补这一差距，本文提出了 **Segment Concept (SeC)**，一个概念驱动的分割框架。SeC 的核心思想是从传统的特征匹配转向高层、以对象为中心的表示的渐进式构建和利用。\n\n*   **构建鲁棒概念先验：** SeC 利用大型视觉-语言模型（LVLMs）整合来自不同帧的视觉线索，从而构建出鲁棒的概念先验。\n*   **推理阶段的鲁棒分割：** 在推理过程中，SeC 基于已处理的帧形成目标的全面语义表示，从而实现后续帧的鲁棒分割。\n*   **自适应平衡：** SeC 能够自适应地平衡基于 LVLM 的语义推理与增强的特征匹配，根据场景复杂性动态调整计算工作量。\n\n### SeCVOS 基准：评估复杂场景下的概念理解\n\n为了严格评估 VOS 方法在需要高层概念推理和鲁棒语义理解的场景中的表现，本文引入了 **语义复杂场景视频对象分割（SeCVOS）** 基准。\n\n*   **构成：** SeCVOS 包含 160 个手动标注的多场景视频，这些视频旨在通过显著的外观变化和动态场景转换来挑战模型。\n\n### 实验结果\n\nSeC 在 SeCVOS 基准上比 SAM 2.1 取得了 11.8 点的显著提升，这确立了 SeC 在概念感知视频对象分割领域的新技术水平（State-of-the-Art, SOTA）。",
      "shortSummary": "SeC（Segment Concept）是一种新的概念驱动视频对象分割（VOS）框架，旨在解决现有方法在处理复杂场景时缺乏人类概念理解的局限。SeC利用大型视觉-语言模型（LVLMs）构建高层、以对象为中心的语义表示，并自适应地平衡语义推理与特征匹配。为评估概念感知VOS，研究者引入了SeCVOS基准。SeC在SeCVOS上比SAM 2.1提升11.8点，达到了新的技术水平。",
      "translated_title": "SeC：通过渐进式概念构建推进复杂视频对象分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation."
    },
    {
      "title": "GUI-G^2: 用于GUI定位的高斯奖励建模 (原标题: GUI-G^2: Gaussian Reward Modeling for GUI Grounding)",
      "link": "https://arxiv.org/abs/2507.15846",
      "pubDate": "Mon, 21 Jul 2025 13:53:42 GMT",
      "isoDate": "2025-07-21T13:53:42.000Z",
      "creator": "Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
      "summary": "# GUI-G^2: 用于GUI定位的高斯奖励建模\n\n## 引言：GUI定位及其现有挑战\n图形用户界面（GUI）定位是一项关键任务，旨在将自然语言指令映射到精确的界面位置，以实现自主交互。当前基于强化学习的方法通常采用二元奖励机制，将元素视为“命中或未命中”的目标。这种方法产生稀疏的信号，未能充分捕捉空间交互的连续性，从而限制了模型的学习效率和性能。\n\n## GUI-G^2：高斯奖励框架的提出\n受人类点击行为自然形成以目标元素为中心的高斯分布的启发，本文引入了GUI高斯定位奖励（GUI-G^2）框架。这是一个原则性的奖励框架，将GUI元素建模为界面平面上的连续高斯分布。\n\n### GUI-G^2 的协同机制\nGUI-G^2 框架融合了两种协同机制，以提供更丰富、更密集的奖励信号：\n1.  **高斯点奖励（Gaussian Point Rewards）**：\n    *   通过以元素质心为中心的指数衰减分布来建模精确的定位。\n    *   这种机制鼓励模型精确地指向目标元素的中心区域。\n2.  **覆盖奖励（Coverage Rewards）**：\n    *   通过测量预测高斯分布与目标区域之间的重叠程度来评估空间对齐。\n    *   这确保了模型不仅关注中心点，还能考虑到目标元素的整体形状和大小。\n\n### 自适应方差机制\n为了有效处理不同尺寸的GUI元素，GUI-G^2 开发了一种自适应方差机制。该机制根据元素的尺寸校准奖励分布的方差，确保奖励信号能够适应各种大小的交互目标，从而提高模型的泛化能力和精度。\n\n## 优势与影响\nGUI-G^2 框架将GUI定位任务从稀疏的二元分类问题转化为密集的连续优化问题。高斯分布能够生成丰富的梯度信号，有效引导模型学习最佳的交互位置。\n\n## 实验验证与结果\n研究人员在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro等基准测试上进行了广泛的实验。结果表明，GUI-G^2 显著优于当前最先进的方法UI-TARS-72B，其中在ScreenSpot-Pro上的性能提升最为显著，达到了24.7%。\n\n## 结论\n分析表明，连续建模为界面变化提供了卓越的鲁棒性，并增强了对未见布局的泛化能力。GUI-G^2 为GUI交互任务中的空间推理建立了一个新的范式。",
      "shortSummary": "GUI-G^2 提出了一种用于GUI定位的新型高斯奖励模型，旨在解决现有强化学习方法中稀疏二元奖励的问题。该模型将GUI元素建模为连续高斯分布，结合高斯点奖励和覆盖奖励，并采用自适应方差机制。这使得GUI定位从稀疏分类转变为密集连续优化，生成丰富的梯度信号。实验证明，GUI-G^2 在ScreenSpot基准测试中显著优于现有技术，尤其在ScreenSpot-Pro上性能提升24.7%，增强了模型对界面变化的鲁棒性和泛化能力。",
      "translated_title": "GUI-G^2: 用于GUI定位的高斯奖励建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks."
    },
    {
      "title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计 (原标题: LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra)",
      "link": "https://arxiv.org/abs/2507.15815",
      "pubDate": "Mon, 21 Jul 2025 13:21:14 GMT",
      "isoDate": "2025-07-21T13:21:14.000Z",
      "creator": "Seth Karten, Wenzhe Li, Zihan Ding, Samuel Kleiner, Yu Bai, Chi Jin",
      "summary": "# LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计\n\n本文介绍了“LLM经济学家”（LLM Economist），这是一个新颖的框架，利用基于智能体的建模方法，在具有层级决策的策略环境中设计和评估经济政策。\n\n## 框架结构与智能体角色\n\n该框架采用双层结构，模拟经济系统中的决策过程：\n\n*   **下层：有限理性工人智能体**\n    *   这些智能体被实例化为基于“人物画像”的提示，其数据来源于经过美国人口普查校准的收入和人口统计数据。\n    *   它们通过最大化在上下文中学习到的基于文本的效用函数来选择劳动供给。\n*   **上层：规划者智能体**\n    *   该智能体利用上下文强化学习（in-context reinforcement learning）来提出分段线性边际税率表。\n    *   这些税率表以当前美国联邦税级为锚点。\n\n## 经济模拟器的关键能力\n\n这种构建方式赋予了经济模拟器进行可靠财政实验所需的三项关键能力：\n\n1.  **异质性效用优化：** 能够优化不同智能体的异质性效用函数。\n2.  **大规模人口生成：** 能够原则性地生成大规模、符合人口统计学现实的智能体群体。\n3.  **自然语言机制设计：** 机制设计（即终极的“助推”问题）完全以自然语言表达。\n\n## 实验结果与政策含义\n\n研究人员对多达一百个交互智能体的群体进行了实验，结果显示：\n\n*   规划者智能体收敛到接近斯塔克尔伯格均衡（Stackelberg equilibria）的状态。\n*   相对于Saez解决方案，这种均衡显著改善了总社会福利。\n*   在去中心化治理下，周期性、基于人物画像的投票程序进一步提升了这些收益。\n\n## 结论与重要性\n\n这些结果表明，基于大型语言模型的智能体能够联合建模、模拟和治理复杂的经济系统。这为社会层面的政策评估提供了一个可操作的测试平台，有助于构建更美好的文明。",
      "shortSummary": "“LLM经济学家”是一个创新框架，利用基于大型语言模型的智能体进行经济政策设计与评估。它通过模拟具有层级决策的多智能体系统实现此目标：下层工人智能体优化劳动供给，上层规划者智能体设计税收政策。该框架能生成大规模、真实的人口模型，并以自然语言进行机制设计。实验表明，其设计的政策能改善总社会福利。这为社会规模的政策评估提供了一个可行的测试平台。",
      "translated_title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations."
    },
    {
      "title": "稳定知识，促进推理：RLVR的双令牌约束 (原标题: Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR)",
      "link": "https://arxiv.org/abs/2507.15778",
      "pubDate": "Mon, 21 Jul 2025 12:34:01 GMT",
      "isoDate": "2025-07-21T12:34:01.000Z",
      "creator": "Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, Guorui Zhou",
      "summary": "## 稳定知识，促进推理：RLVR的双令牌约束\n\n### 摘要\n\n本文提出了一种名为 Archer 的新型强化学习方法，旨在优化大型语言模型（LLMs）的推理能力。该方法通过引入双令牌约束和同步更新机制，解决了现有强化学习与可验证奖励（RLVR）算法的局限性。\n\n### 现有问题\n\n*   **统一训练信号的局限性**：传统的RLVR算法在训练过程中对所有令牌应用统一的训练信号，未能区分低熵的知识相关令牌和高熵的推理相关令牌的不同作用。\n*   **语义依赖性破坏**：尽管一些近期方法尝试通过梯度掩蔽或异步更新来区分令牌类型，但这些方法可能破坏模型输出中的语义依赖性，从而阻碍有效的学习。\n\n### 提出的解决方案：Archer\n\n*   **熵感知RLVR**：Archer 是一种熵感知的RLVR方法，它引入了双令牌约束和同步更新机制。\n*   **双令牌约束机制**：\n    *   **推理令牌**：对推理相关令牌（高熵）应用较弱的KL正则化和较高的裁剪阈值，以鼓励模型进行探索和生成多样化的推理路径。\n    *   **知识令牌**：对知识相关令牌（低熵）施加强约束，以确保模型保持事实知识的准确性和稳定性。\n*   **同步更新**：与异步更新方法不同，Archer 采用同步更新，避免了破坏模型输出的语义连贯性。\n\n### 实验结果\n\n*   **显著提升**：在多个数学推理和代码生成基准测试中，Archer 方法显著优于先前的RLVR方法。\n*   **达到或超越SOTA**：在同等规模的模型中，Archer 的性能达到或超越了现有最先进的水平。\n\n### 代码可用性\n\n*   相关代码已在 GitHub 上开源。",
      "shortSummary": "本文提出 Archer，一种新的RLVR方法，通过双令牌约束和同步更新来提升LLM的推理能力。它针对知识令牌施加强约束以保持事实，对推理令牌施加弱约束以鼓励探索，解决了传统RLVR统一训练信号的不足。实验表明，Archer 在数学推理和代码生成任务上显著优于现有方法，达到或超越了同等规模模型的SOTA性能。",
      "translated_title": "稳定知识，促进推理：RLVR的双令牌约束",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR."
    },
    {
      "title": "数据混合智能体：学习重新加权领域以进行持续预训练 (原标题: Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training)",
      "link": "https://arxiv.org/abs/2507.15640",
      "pubDate": "Mon, 21 Jul 2025 10:01:54 GMT",
      "isoDate": "2025-07-21T10:01:54.000Z",
      "creator": "Kailai Yang, Xiao Liu, Lei Ji, Hao Li, Yeyun Gong, Peng Cheng, Mao Yang",
      "summary": "## 数据混合智能体：学习重新加权领域以进行持续预训练\n\n### 摘要\n\n本文介绍了一种名为“数据混合智能体”（Data Mixing Agent）的新型框架，旨在解决大型语言模型在小规模任务特定数据上进行持续预训练时面临的灾难性遗忘问题。传统的解决方案依赖于手动指定或基于经验的启发式方法来重新加权源领域和目标领域的训练数据混合，以实现性能平衡。然而，这些方法往往缺乏通用性。\n\n### 提出的方法：数据混合智能体\n\n*   **创新性：** 数据混合智能体是首个模型驱动、端到端的框架，能够通过学习来重新加权领域。\n*   **学习机制：** 该智能体通过强化学习（Reinforcement Learning）来学习可泛化的启发式方法。它利用大量的“数据混合轨迹”以及来自评估环境的相应反馈进行训练。\n\n### 实验结果与优势\n\n*   **性能提升：** 在数学推理的持续预训练实验中，数据混合智能体在实现源领域和目标领域基准的平衡性能方面，显著优于现有强基线方法。\n*   **强大的泛化能力：** 该智能体无需重新训练，即可很好地泛化到未见过的源领域、目标模型和领域空间。\n*   **跨领域适应性：** 直接应用于代码生成领域也表明了其在不同目标领域间的良好适应性。\n*   **启发式方法与人类直觉的一致性：** 进一步分析表明，智能体学习到的启发式方法与人类直觉高度一致。\n*   **高效性：** 智能体能够以更少的源领域数据实现卓越的模型性能，展现出其高效性。\n\n### 研究领域\n\n*   机器学习 (cs.LG)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)",
      "shortSummary": "“数据混合智能体”是一种新颖的模型驱动框架，通过强化学习自动学习如何重新加权源领域和目标领域数据，以优化大型语言模型的持续预训练。它解决了传统手动加权方法导致的灾难性遗忘问题。实验证明，该智能体在数学推理和代码生成等任务中，能实现平衡且优于基线的性能，并展现出良好的泛化能力和数据效率，无需重新训练即可适应新场景。",
      "translated_title": "数据混合智能体：学习重新加权领域以进行持续预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data."
    },
    {
      "title": "基于离散化SDF的高斯泼溅用于可重照明资产 (原标题: Gaussian Splatting with Discretized SDF for Relightable Assets)",
      "link": "https://arxiv.org/abs/2507.15629",
      "pubDate": "Mon, 21 Jul 2025 09:52:33 GMT",
      "isoDate": "2025-07-21T09:52:33.000Z",
      "creator": "Zuo-Liang Zhu, Jian Yang, Beibei Wang",
      "summary": "## 基于离散化SDF的高斯泼溅用于可重照明资产\n\n### 1. 背景与挑战\n\n*   **3D高斯泼溅 (3DGS)**：在新型视图合成 (NVS) 任务中展现出卓越的细节表达能力和高效的渲染速度。\n*   **逆向渲染应用挑战**：将3DGS应用于逆向渲染面临挑战，主要原因是高斯基元的离散性质使得几何约束难以应用。\n*   **现有解决方案及其局限性**：\n    *   近期工作引入**符号距离场 (SDF)** 作为额外的连续表示来规范高斯基元定义的几何。\n    *   这些方法虽然改善了分解质量，但代价是增加了内存使用量并使训练过程复杂化。\n\n### 2. 提出的方法：离散化SDF\n\n*   **核心思想**：与现有方法不同，本文引入了一种**离散化SDF**来表示连续SDF。\n*   **实现方式**：通过在每个高斯内部编码一个采样值，以离散方式表示SDF。\n*   **关键连接**：这种方法允许通过**SDF到不透明度转换**将SDF与高斯不透明度关联起来。\n*   **渲染优势**：\n    *   能够通过泼溅（splatting）渲染SDF，从而避免了传统光线追踪的计算成本。\n\n### 3. 几何正则化：投影一致性损失\n\n*   **主要挑战**：如何规范离散样本使其与底层SDF保持一致，因为离散表示难以应用基于梯度的约束（例如Eikonal损失）。\n*   **解决方案**：\n    *   将高斯投影到SDF的零水平集上。\n    *   强制通过泼溅渲染的表面与SDF的零水平集对齐。\n    *   这被称为**基于投影的一致性损失**。\n\n### 4. 方法优势与实验结果\n\n*   **更高重照明质量**：得益于离散化SDF，本文方法实现了更高的重照明质量。\n*   **内存效率**：无需额外内存，与标准3DGS所需的内存量相同。\n*   **优化简化**：避免了复杂的手动设计优化过程。\n*   **实验结果**：实验表明，本文方法优于现有的基于高斯的反向渲染方法。\n*   **代码可用性**：相关代码已开源。",
      "shortSummary": "本文提出一种基于离散化SDF的高斯泼溅方法，用于可重照明资产的逆向渲染。通过在每个高斯中编码SDF采样值，并将其与不透明度关联，实现了SDF的泼溅渲染，避免了光线追踪。为解决离散表示的几何约束问题，引入了基于投影的一致性损失。该方法在不增加额外内存和避免复杂优化的情况下，显著提高了重照明质量，并优于现有高斯基逆向渲染方法。",
      "translated_title": "基于离散化SDF的高斯泼溅用于可重照明资产",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF."
    },
    {
      "title": "Being-H0：基于大规模人类视频的视觉-语言-动作预训练 (原标题: Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos)",
      "link": "https://arxiv.org/abs/2507.15597",
      "pubDate": "Mon, 21 Jul 2025 09:19:09 GMT",
      "isoDate": "2025-07-21T09:19:09.000Z",
      "creator": "Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, Zongqing Lu",
      "summary": "# Being-H0：基于大规模人类视频的视觉-语言-动作预训练\n\n## 1. 引言与背景\n\n*   **模型名称：** Being-H0\n*   **模型类型：** 一种灵巧的视觉-语言-动作（VLA）模型。\n*   **训练数据来源：** 基于大规模人类视频进行训练。\n*   **解决的问题：**\n    *   现有VLA模型在需要高灵巧度的复杂操作任务上表现不力。\n    *   对新场景和任务的泛化能力较差。\n    *   主要原因在于它们依赖合成数据（存在显著的模拟到现实差距）或缺乏规模和多样性的远程操作演示。\n\n## 2. 核心方法与创新\n\n*   **数据瓶颈应对策略：** 提出利用人类手部作为基础操作器，以充分利用网络数据中丰富的灵巧性和可扩展性。\n*   **核心训练范式——物理指令调优（Physical Instruction Tuning）：**\n    *   这是一种新颖的训练范式，结合了以下几个方面：\n        *   从人类视频中进行大规模VLA预训练。\n        *   为3D推理进行物理空间对齐。\n        *   为机器人任务进行后期训练适应。\n*   **动作学习方法——部分级运动标记化（Part-level Motion Tokenization）：**\n    *   引入了一种部分级运动标记化方法，能够实现毫米级重建精度，以精确建模手部轨迹，从而进行动作学习。\n*   **数据策展管道：**\n    *   开发了一个全面的数据策展管道，能够整合异构数据源，包括动作捕捉、VR和纯RGB视频。\n    *   构建了一个包含数百万个基于运动的指令实例的大规模数据集。\n\n## 3. 实验结果与贡献\n\n*   **卓越表现：** 经验性地证明了Being-H0在手部运动生成和指令遵循方面的卓越性能。\n*   **良好可扩展性：** 模型的性能随模型和数据规模的增大而良好扩展。\n*   **真实世界应用：** 重要的是，当应用物理指令调优时，Being-H0在真实世界机器人操作中显示出预期的性能提升。\n\n## 4. 附加信息\n\n*   **文档长度：** 37页\n*   **相关主题：** 计算机视觉与模式识别 (cs.CV)、机器学习 (cs.LG)、机器人学 (cs.RO)\n*   **更多详情：** 可通过提供的链接获取更多信息。",
      "shortSummary": "Being-H0是一种基于大规模人类视频的灵巧视觉-语言-动作（VLA）模型，旨在解决现有VLA在复杂操作和泛化方面的不足。它利用人类手部作为基础操作器，并引入了“物理指令调优”范式和“部分级运动标记化”方法。Being-H0在手部运动生成、指令遵循以及真实世界机器人操作中表现出色，并能随模型和数据规模良好扩展。",
      "translated_title": "Being-H0：基于大规模人类视频的视觉-语言-动作预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0."
    },
    {
      "title": "GR-3 技术报告 (原标题: GR-3 Technical Report)",
      "link": "https://arxiv.org/abs/2507.15493",
      "pubDate": "Mon, 21 Jul 2025 06:54:13 GMT",
      "isoDate": "2025-07-21T06:54:13.000Z",
      "creator": "Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, Yichu Yang",
      "summary": "# GR-3 技术报告：通用机器人策略的最新进展\n\n## 1. GR-3 概述\nGR-3 是一种大规模视觉-语言-动作 (VLA) 模型，代表了在构建通用机器人策略方面的最新进展。它旨在实现机器人对新颖环境和任务的广泛适应性。\n\n## 2. 核心能力\n*   **卓越的泛化能力：** GR-3 能够泛化到新颖的物体、环境以及涉及抽象概念的指令，展现出强大的适应性。\n*   **高效微调：** 仅需最少的人类轨迹数据即可进行高效微调，从而实现对新设置的快速且经济高效的适应。\n*   **处理复杂任务：** 在处理长时程和灵巧任务方面表现出色，包括需要双手操作和移动的任务，展现出强大的鲁棒性和可靠性。\n\n## 3. 训练方法\nGR-3 的这些能力是通过多方面训练方法实现的，包括：\n*   与网络规模的视觉-语言数据进行协同训练。\n*   利用通过 VR 设备收集的人类轨迹数据进行高效微调。\n*   通过机器人轨迹数据进行有效的模仿学习。\n\n## 4. 配套硬件：ByteMini\n报告中还介绍了 ByteMini，这是一款多功能双手移动机器人。ByteMini 具有卓越的灵活性和可靠性，与 GR-3 集成后，能够完成广泛的任务。\n\n## 5. 实验结果与未来展望\n通过广泛的真实世界实验，GR-3 在各种具有挑战性的任务上超越了最先进的基线方法 $\\pi_0$。研究人员希望 GR-3 能够成为构建能够协助人类日常生活的通用机器人的重要一步。",
      "shortSummary": "GR-3 是一种大规模视觉-语言-动作 (VLA) 模型，在通用机器人策略方面取得显著进展。它展现出对新物体、环境和抽象概念的卓越泛化能力，并能通过少量人类数据高效微调。GR-3 擅长处理长时程和灵巧任务，包括双手操作和移动。其能力得益于结合网络数据、VR 人类轨迹和机器人轨迹的训练。GR-3 与多功能机器人 ByteMini 结合，在真实世界任务中超越了现有基线，旨在推动通用机器人协助人类。",
      "translated_title": "GR-3 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life."
    },
    {
      "title": "STITCH：面向口语语言模型的块状推理同步思考与表达 (原标题: STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models)",
      "link": "https://arxiv.org/abs/2507.15375",
      "pubDate": "Mon, 21 Jul 2025 04:30:03 GMT",
      "isoDate": "2025-07-21T04:30:03.000Z",
      "creator": "Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang",
      "summary": "STITCH：面向口语语言模型的块状推理同步思考与表达\n\n**1. 背景与问题**\n\n*   **口语语言模型 (SLMs)**：旨在接收语音输入并生成语音响应。\n*   **当前SLMs的局限性**：缺乏内部的、无声的思考过程。这与人类形成对比，人类通常会进行复杂的内部心理推理，从而能够清晰简洁地表达思想。\n*   **集成思考的必要性**：因此，将无声的思考过程整合到SLMs中是非常理想的。\n*   **传统方法的挑战**：在开始说话前天真地生成完整的思维链（CoT）推理虽然可以使SLMs进行思考，但这会引入额外的语音响应延迟，因为CoT推理的长度可能是任意的。\n\n**2. STITCH方法**\n\n*   **核心理念**：为了解决高延迟问题，研究人员提出了STITCH，这是一种新颖的生成方法，它在生成无声推理块和口语响应块之间交替进行。\n*   **实现机制**：\n    *   口语响应块的音频持续时间远长于生成该口语响应块中令牌所需的时间。\n    *   STITCH利用这段剩余的“空闲时间”来生成无声推理令牌。\n    *   当一个口语音频块播放给用户时，模型会继续生成下一个无声推理块，从而实现“同步思考与表达”。\n\n**3. 实验结果与优势**\n\n*   **延迟表现**：STITCH在延迟方面与那些设计上无法生成无声CoT的基线模型相匹配。\n*   **推理能力提升**：在数学推理数据集上，STITCH的表现比这些基线模型高出15%。\n*   **非推理任务表现**：在非推理数据集上，STITCH的表现与基线模型同样出色。\n\n**4. 项目状态与领域**\n\n*   **状态**：该工作目前正在进行中。\n*   **相关领域**：计算与语言（cs.CL）；音频与语音处理（eess.AS）。",
      "shortSummary": "STITCH是一种新型生成方法，旨在解决口语语言模型（SLMs）缺乏内部思考且传统思维链（CoT）导致高延迟的问题。它通过交替生成无声推理块和口语响应块，利用口语播放的空闲时间同步生成后续推理内容，实现了“同步思考与表达”。STITCH在保持与基线模型相同延迟的同时，在数学推理任务上性能提升15%，在非推理任务上表现相当。",
      "translated_title": "STITCH：面向口语语言模型的块状推理同步思考与表达",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH."
    },
    {
      "title": "WebShaper：通过信息检索形式化进行智能数据合成 (原标题: WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization)",
      "link": "https://arxiv.org/abs/2507.15061",
      "pubDate": "Sun, 20 Jul 2025 13:53:37 GMT",
      "isoDate": "2025-07-20T13:53:37.000Z",
      "creator": "Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "### WebShaper：通过信息检索形式化进行智能数据合成\n\n**背景与问题**\n\n*   大型语言模型（LLM）驱动的智能体通过其基于网络的**信息检索（IS）**能力，革新了复杂开放式任务的解决方式。\n*   然而，高质量训练数据的稀缺性严重限制了IS智能体的进一步发展。\n*   现有方法通常采用“信息驱动”范式，即首先收集网络数据，然后基于检索结果生成问题。这种方法可能导致信息结构与推理结构之间、以及问题与答案之间存在不一致性。\n\n**WebShaper 框架**\n\n*   为解决上述问题，本文提出了一个名为 **WebShaper** 的“形式化驱动”IS数据合成框架，旨在构建高质量数据集。\n*   **核心理念：** WebShaper 通过集合论系统地形式化信息检索任务。\n*   **关键概念：** “知识投影”（Knowledge Projections, KP）是该形式化的核心。通过KP操作的组合，WebShaper能够对推理结构进行精确控制。\n\n**数据合成过程**\n\n*   合成过程始于创建“种子任务”。\n*   随后，框架采用一个多步骤的扩展过程。\n*   在每个步骤中，一个智能体“扩展器”（Expander）会基于WebShaper的形式化，并利用检索和验证工具，使当前的正式问题变得更加复杂。\n\n**实验结果**\n\n*   在WebShaper合成的数据集上训练的模型，在GAIA和WebWalkerQA基准测试中，在开源IS智能体中取得了最先进的性能。",
      "shortSummary": "WebShaper是一个创新的框架，旨在为大型语言模型驱动的信息检索（IS）智能体合成高质量训练数据。针对现有方法数据稀缺和结构不一致的问题，WebShaper提出了一种形式化驱动的方法，通过集合论和“知识投影”精确控制推理结构。该框架从种子任务开始，通过智能体扩展逐步生成复杂问题。实验证明，在WebShaper合成的数据集上训练的模型，在GAIA和WebWalkerQA基准测试中，表现优于其他开源IS智能体，达到了最先进水平。",
      "translated_title": "WebShaper：通过信息检索形式化进行智能数据合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks."
    },
    {
      "title": "迈向视频思维测试：一个用于高级视频推理和理解的全面基准 (原标题: Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding)",
      "link": "https://arxiv.org/abs/2507.15028",
      "pubDate": "Sun, 20 Jul 2025 12:30:33 GMT",
      "isoDate": "2025-07-20T12:30:33.000Z",
      "creator": "Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, Ziwei Liu",
      "summary": "### 背景与问题\n*   人类智能需要正确性和鲁棒性，其中正确性是鲁棒性的基础。\n*   在视频理解领域，正确性确保对视觉内容的准确解释，而鲁棒性则是在挑战性条件下保持一致的性能。\n*   尽管视频大型语言模型（video LLMs）取得了进展，但现有基准未能充分反映这些模型在视频解释的正确性和鲁棒性方面与人类智能之间的差距。\n\n### 引入Video Thinking Test (Video-TT)\n*   **目的：** 评估视频LLMs是否能像人类一样有效地解释真实世界的视频。\n*   **特点：**\n    *   反映了理解复杂视觉叙事方面的真实差距。\n    *   评估了模型对抗自然对抗性问题的鲁棒性。\n\n### Video-TT构成\n*   包含1,000个YouTube Shorts视频。\n*   每个视频配有一个开放式问题和四个对抗性问题，旨在探究视觉和叙事复杂性。\n\n### 评估结果\n*   评估显示，视频LLMs与人类表现之间存在显著差距。",
      "shortSummary": "为弥补现有视频LLMs基准的不足，研究引入了Video Thinking Test (Video-TT)。该基准包含1,000个YouTube Shorts视频，每个视频配有开放式和对抗性问题，旨在全面评估视频LLMs对复杂视觉叙事的理解能力及其鲁棒性。评估结果显示，当前视频LLMs与人类在视频理解和推理方面存在显著差距。",
      "translated_title": "迈向视频思维测试：一个用于高级视频推理和理解的全面基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance."
    },
    {
      "title": "无形的束缚：为什么RLVR可能无法摆脱其起源 (原标题: The Invisible Leash: Why RLVR May Not Escape Its Origin)",
      "link": "https://arxiv.org/abs/2507.14843",
      "pubDate": "Sun, 20 Jul 2025 03:04:08 GMT",
      "isoDate": "2025-07-20T03:04:08.000Z",
      "creator": "Fang Wu, Weihao Xuan, Ximing Lu, Zaid Harchaoui, Yejin Choi",
      "summary": "## RLVR的局限性研究\n\n### 引言：RLVR的潜力与未解之谜\n\n近期大型推理模型（Large Reasoning Models）的进展突出了“可验证奖励强化学习”（Reinforcement Learning with Verifiable Rewards, RLVR）作为一种增强AI能力，尤其是在解决复杂逻辑任务方面，具有广阔前景的方法。然而，本研究旨在探讨RLVR是否真正扩展了模型的推理边界，抑或仅仅是放大了基模型（base model）已知的高奖励输出以提高精度。\n\n### 理论洞察：RLVR的内在约束\n\n本研究首先提出了一个新颖的理论视角，揭示了RLVR的内在约束：\n\n*   **基模型支持的限制**：RLVR受到基模型“支持”（support）的限制，无法采样初始概率为零的解决方案。这意味着RLVR无法发现基模型从未“见过”或从未赋予任何概率的全新解决方案。\n*   **保守的重加权机制**：RLVR本质上是一种保守的重加权机制，它可能限制了完全原创解决方案的发现。它倾向于在现有知识空间内进行优化，而非创造性地探索新空间。\n*   **熵-奖励权衡**：研究还指出了一种“熵-奖励权衡”现象。虽然RLVR能够可靠地提高精度，但它可能会逐步缩小探索范围，从而可能忽略那些正确但代表性不足（underrepresented）的解决方案。这意味着为了追求高奖励，模型可能会牺牲多样性。\n\n### 实证发现：实验验证与观察\n\n广泛的实证实验验证了上述理论观点：\n\n*   **`pass@1`的提升与支持空间的收缩**：尽管RLVR持续改进了`pass@1`（即第一次尝试就成功的概率），但在更大的采样预算下，经验支持（empirical support）的收缩通常超过了其扩展。这意味着RLVR未能恢复基模型之前可访问的正确答案，反而可能导致一些正确答案变得难以触及。\n*   **熵的变化模式**：有趣的是，研究观察到，虽然RLVR有时会增加令牌级别（token-level）的熵，导致每个生成步骤的不确定性增加，但答案级别（answer-level）的熵却下降了。这表明，这些看似更不确定的路径最终会收敛到一组更小的、不同的答案上，进一步证实了探索范围的缩小。\n\n### 结论与展望：突破“无形的束缚”\n\n综合来看，这些发现揭示了RLVR在扩展推理视野方面的潜在局限性。它似乎被一种“无形的束缚”所限制，无法完全摆脱其基模型的起源。为了打破这种束缚，未来的算法创新可能需要引入明确的探索机制，或者采用混合策略，将概率质量注入到那些代表性不足的解决方案区域，从而鼓励更广泛和原创性的探索。",
      "shortSummary": "本研究探讨了RLVR（可验证奖励强化学习）的局限性。理论分析表明，RLVR受限于基模型已知知识，作为保守的重加权机制，可能限制原创解决方案的发现，并存在熵-奖励权衡，导致探索范围缩小。实证结果显示，RLVR虽提升了`pass@1`，但却收缩了解决方案空间，未能恢复基模型先前可访问的答案。研究认为，RLVR在扩展推理视野方面存在“无形的束缚”，未来需引入明确探索机制或混合策略以克服此限制。",
      "translated_title": "无形的束缚：为什么RLVR可能无法摆脱其起源",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions."
    },
    {
      "title": "MiroMind-M1：通过上下文感知多阶段策略优化在数学推理方面的开源进展 (原标题: MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization)",
      "link": "https://arxiv.org/abs/2507.14683",
      "pubDate": "Sat, 19 Jul 2025 12:21:23 GMT",
      "isoDate": "2025-07-19T12:21:23.000Z",
      "creator": "Xingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong Zhang, Yueyi Zhang, Zonglin Yang, Ruilin Li, Lei Lei, Shihao Xu, Han Zhao, Weiling Chen, Feng Ji, Lidong Bing",
      "summary": "# MiroMind-M1：数学推理的开源进展\n\n本文介绍了MiroMind-M1系列，这是一套基于Qwen-2.5骨干的完全开源推理语言模型（RLMs），旨在推动数学推理领域的发展。\n\n## 背景与问题\n*   **LLMs的演进：** 大型语言模型（LLMs）已从文本生成发展到跨领域的高级推理，催生了推理语言模型（RLMs）。\n*   **数学推理的重要性：** 数学推理是代表性的基准任务，因为它需要精确的多步逻辑和抽象推理能力，这些能力可以泛化到其他任务。\n*   **现有挑战：**\n    *   闭源RLMs（如GPT-o3）虽然表现出色，但其专有性质限制了透明度和可复现性。\n    *   许多现有开源项目缺乏足够的开放性，常省略关键资源（如数据集和详细训练配置），阻碍了研究的可复现性。\n\n## MiroMind-M1的贡献\n为提高RLM开发的透明度，作者团队推出了MiroMind-M1系列。\n*   **完全开源：** 提供了模型、数据集以及所有训练和评估配置。\n*   **性能目标：** 旨在匹配或超越现有开源RLMs的性能。\n\n## 训练方法\nMiroMind-M1模型采用两阶段训练：\n\n1.  **监督微调（SFT）：**\n    *   使用精心策划的719K个数学推理问题语料库进行训练。\n    *   这些问题包含经过验证的思维链（CoT）轨迹。\n2.  **基于可验证奖励的强化学习（RLVR）：**\n    *   在62K个具有挑战性且可验证的问题上进行训练。\n    *   **核心算法：** 引入了“上下文感知多阶段策略优化”（Context-Aware Multi-Stage Policy Optimization）算法。\n        *   该算法结合了长度渐进式训练和自适应重复惩罚机制。\n        *   旨在鼓励上下文感知的强化学习训练，从而增强RLVR过程的鲁棒性和效率。\n\n## 性能表现\n*   在AIME24、AIME25和MATH基准测试中，MiroMind-M1模型在基于Qwen-2.5的开源7B和32B模型中，实现了最先进或具有竞争力的性能。\n*   展现出卓越的Token效率。\n\n## 资源发布\n为促进研究的可复现性，作者团队发布了完整的资源栈：\n*   **模型：** MiroMind-M1-SFT-7B、MiroMind-M1-RL-7B、MiroMind-M1-RL-32B。\n*   **数据集：** MiroMind-M1-SFT-719K、MiroMind-M1-RL-62K。\n*   **配置：** 所有训练和评估配置。\n\n作者希望这些资源能够支持进一步的研究，并促进社区的进步。",
      "shortSummary": "MiroMind-M1系列是一套基于Qwen-2.5的完全开源推理语言模型，专注于数学推理。为解决现有开源RLM透明度和可复现性不足的问题，MiroMind-M1采用两阶段训练：SFT和引入“上下文感知多阶段策略优化”的RLVR。该模型在AIME和MATH等基准测试中表现出色，达到或超越现有开源模型性能，并具有卓越的Token效率。为促进研究，作者完整发布了模型、数据集和训练配置。",
      "translated_title": "MiroMind-M1：通过上下文感知多阶段策略优化在数学推理方面的开源进展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement."
    },
    {
      "title": "VisionThink：通过强化学习实现的智能高效视觉语言模型 (原标题: VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.13348",
      "pubDate": "Thu, 17 Jul 2025 13:59:55 GMT",
      "isoDate": "2025-07-17T13:59:55.000Z",
      "creator": "Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia",
      "summary": "## VisionThink：智能高效视觉语言模型\n\n### 背景与挑战\n\n近期视觉语言模型（VLM）的性能提升通常依赖于增加视觉令牌的数量，这些令牌通常比文本令牌长得多。然而，研究发现，在大多数实际场景中，并不需要如此大量的视觉令牌。尽管在少数OCR（光学字符识别）相关任务中，减少视觉令牌会导致性能显著下降，但在大多数其他通用VQA（视觉问答）任务中，即使分辨率降至1/4，模型仍能保持准确性。\n\n### VisionThink 提出的新范式\n\n针对上述问题，本文提出了 **VisionThink**，一种用于视觉令牌压缩的新范式，旨在动态处理不同分辨率的样本：\n\n*   **动态分辨率处理**：VisionThink 从下采样图像开始处理。\n*   **智能决策**：模型会智能判断下采样图像是否足以解决当前问题。\n*   **按需请求高分辨率**：如果下采样图像不足，模型会输出一个特殊令牌，请求更高分辨率的图像。\n\n### 优势与创新\n\n与现有通过固定剪枝比例或阈值压缩令牌的高效VLM方法不同，VisionThink 能够根据具体情况自主决定是否压缩令牌，从而带来以下优势：\n\n*   **精细视觉理解**：在OCR相关任务上展现出强大的精细视觉理解能力。\n*   **资源节约**：在较简单的任务上显著节省视觉令牌。\n\n### 强化学习与实现\n\nVisionThink 采用了强化学习（RL）技术，并提出了 **LLM-as-Judge** 策略，成功将强化学习应用于通用VQA任务。此外，模型精心设计了奖励函数和惩罚机制，以实现稳定且合理的图像大小调整调用比例。\n\n### 实验结果与可用性\n\n广泛的实验证明了 VisionThink 方法的优越性、效率和有效性。项目的代码和模型已开源。",
      "shortSummary": "VisionThink 是一种智能高效的视觉语言模型，旨在解决现有VLM过度使用视觉令牌的问题。它通过强化学习，动态判断并按需调整图像分辨率，从下采样图像开始，并在必要时请求高分辨率。这种方法在OCR任务上表现出色，同时在简单任务上显著节省视觉令牌，实现了性能与效率的平衡。模型利用“LLM-as-Judge”策略和精心设计的奖励机制来优化图像分辨率调用。",
      "translated_title": "VisionThink：通过强化学习实现的智能高效视觉语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink."
    },
    {
      "title": "π^3: 可扩展的置换等变视觉几何学习 (原标题: π^3: Scalable Permutation-Equivariant Visual Geometry Learning)",
      "link": "https://arxiv.org/abs/2507.13347",
      "pubDate": "Thu, 17 Jul 2025 13:59:53 GMT",
      "isoDate": "2025-07-17T13:59:53.000Z",
      "creator": "Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, Tong He",
      "summary": "## π^3: 视觉几何重建的新范式\n\n### 引言\n\n传统的视觉几何重建方法通常依赖于一个固定的参考视图。这种归纳偏置可能导致模型在参考视图不理想时出现不稳定性和失败。\n\n### π^3 模型介绍\n\n我们引入了 **π^3**，一个前馈神经网络，它为视觉几何重建提供了一种新颖的方法，彻底打破了对传统固定参考视图的依赖。\n\n*   **核心设计：** π^3 采用完全置换等变（permutation-equivariant）的架构。\n*   **预测能力：**\n    *   预测仿射不变的相机姿态（affine-invariant camera poses）。\n    *   预测尺度不变的局部点图（scale-invariant local point maps）。\n*   **关键特点：** 无需任何参考帧，使其设计固有的无偏置。\n\n### 模型优势\n\nπ^3 的设计带来了显著的优势：\n\n*   **鲁棒性：** 对输入顺序具有固有的鲁棒性。\n*   **可扩展性：** 具有高度可扩展性。\n*   **简洁性：** 方法简单且无偏置。\n*   **性能：** 在广泛的任务中实现了最先进的性能。\n\n### 应用领域\n\nπ^3 在以下任务中表现出色：\n\n*   相机姿态估计（camera pose estimation）\n*   单目/视频深度估计（monocular/video depth estimation）\n*   密集点图重建（dense point map reconstruction）\n\n### 可用性\n\n模型的代码和预训练模型已公开提供。",
      "shortSummary": "π^3 是一种创新的前馈神经网络，通过采用完全置换等变架构，彻底改变了视觉几何重建。它摆脱了对固定参考视图的依赖，能够预测仿射不变的相机姿态和尺度不变的局部点图。π^3 对输入顺序具有固有鲁棒性，高度可扩展，并在相机姿态估计、深度估计和密集点图重建等任务中实现了最先进的性能。代码和模型已公开。",
      "translated_title": "π^3: 可扩展的置换等变视觉几何学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available."
    },
    {
      "title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图 (原标题: Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models)",
      "link": "https://arxiv.org/abs/2507.13344",
      "pubDate": "Thu, 17 Jul 2025 13:59:17 GMT",
      "isoDate": "2025-07-17T13:59:17.000Z",
      "creator": "Yudong Jin, Sida Peng, Xuan Wang, Tao Xie, Zhen Xu, Yifan Yang, Yujun Shen, Hujun Bao, Xiaowei Zhou",
      "summary": "### Diffuman4D：基于时空扩散模型从稀疏视角视频合成4D一致性人体视图\n\n本文旨在解决从稀疏视角视频输入进行高保真人体视图合成的挑战。现有方法通过利用4D扩散模型生成新视角的视频来解决观测不足的问题，但这些模型生成的视频通常缺乏时空一致性，从而降低了视图合成的质量。\n\n#### 提出的方法：滑动迭代去噪过程\n\n为了增强4D扩散模型的时空一致性，本文提出了一种新颖的滑动迭代去噪过程。其核心机制如下：\n\n1.  **潜在网格定义**：定义一个潜在网格（latent grid），其中每个潜在（latent）编码特定视角和时间戳的图像、相机姿态和人体姿态信息。\n2.  **交替去噪**：使用滑动窗口，沿着空间和时间维度交替地对潜在网格进行去噪。\n3.  **视频解码**：最后，从相应的去噪潜在中解码出目标视角的视频。\n\n#### 方法优势\n\n*   **增强4D一致性**：通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得大的感受野，从而显著增强了输出的4D一致性。\n*   **内存效率**：同时保持了GPU内存消耗在可承受范围内。\n\n#### 实验结果\n\n在DNA-Rendering和ActorsHQ数据集上的实验表明，本文提出的方法能够合成高质量且一致的新视角视频，并显著优于现有方法。\n\n更多交互式演示和视频结果可在项目页面查看。",
      "shortSummary": "Diffuman4D提出了一种新颖的滑动迭代去噪过程，以解决从稀疏视角视频合成人体视图时，现有4D扩散模型缺乏时空一致性的问题。该方法通过在潜在网格中交替进行空间和时间去噪，增强了4D一致性并优化了内存使用。实验证明，Diffuman4D在DNA-Rendering和ActorsHQ数据集上能合成高质量且一致的新视角视频，显著优于现有方法。",
      "translated_title": "Diffuman4D: 基于时空扩散模型从稀疏视角视频合成4D一致性人体视图",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ ."
    },
    {
      "title": "\"PhyWorldBench\"：文本到视频模型中物理真实性的综合评估 (原标题: \"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models)",
      "link": "https://arxiv.org/abs/2507.13428",
      "pubDate": "Thu, 17 Jul 2025 13:54:09 GMT",
      "isoDate": "2025-07-17T13:54:09.000Z",
      "creator": "Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, Xin Eric Wang",
      "summary": "## PhyWorldBench：文本到视频模型中物理真实性的综合评估\n\n### 引言与背景\n\n视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且尚未解决的挑战。\n\n### PhyWorldBench 介绍\n\n*   本文提出了 **PhyWorldBench**，一个旨在评估视频生成模型对物理定律遵守程度的综合基准。\n*   **评估范围：** 该基准涵盖了多个层面的物理现象，包括：\n    *   **基本原理：** 如物体运动和能量守恒。\n    *   **更复杂的场景：** 涉及刚体交互以及人类或动物运动。\n*   **“反物理”类别：** PhyWorldBench 引入了一个新颖的“反物理”（Anti-Physics）类别。在此类别中，提示故意违反现实世界的物理定律，从而能够评估模型在遵循此类指令的同时能否保持逻辑一致性。\n\n### 评估方法\n\n*   除了大规模的人工评估外，研究人员还设计了一种简单而有效的方法，可以利用当前的多模态大型语言模型（MLLM）以零样本（zero-shot）方式评估物理真实性。\n\n### 模型评估与发现\n\n*   研究评估了 **12个最先进的文本到视频生成模型**，其中包括五个开源模型和五个专有模型。\n*   通过对 **1050个精心策划的提示**（涵盖基本、复合和反物理场景）的输出进行系统测试，研究人员识别出这些模型在遵守现实世界物理定律方面面临的关键挑战。\n*   研究还严格检查了模型在不同提示类型下对各种物理现象的表现，并基于此提出了**针对性建议**，以指导如何编写提示来增强生成内容对物理原理的忠实度。\n\n### 总结\n\nPhyWorldBench 为评估和改进文本到视频模型在物理真实性方面的表现提供了一个全面的框架，并指出了未来研究的方向。",
      "shortSummary": "PhyWorldBench是一个综合基准，旨在评估文本到视频模型在物理真实性方面的表现。它涵盖了从基本运动到复杂交互的多种物理现象，并引入了“反物理”类别。该研究评估了12个最先进的模型，通过1050个提示识别了模型在遵守物理定律方面的关键挑战，并提供了优化提示的建议，以提高物理保真度。",
      "translated_title": "\"PhyWorldBench\"：文本到视频模型中物理真实性的综合评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles."
    },
    {
      "title": "大型语言模型上下文工程综述 (原标题: A Survey of Context Engineering for Large Language Models)",
      "link": "https://arxiv.org/abs/2507.13334",
      "pubDate": "Thu, 17 Jul 2025 13:50:36 GMT",
      "isoDate": "2025-07-17T13:50:36.000Z",
      "creator": "Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, Shenghua Liu",
      "summary": "# 大型语言模型上下文工程综述\n\n本综述深入探讨了“上下文工程”（Context Engineering）这一新兴学科，它超越了简单的提示设计，旨在系统性地优化提供给大型语言模型（LLMs）的输入信息负载，从而根本性地决定LLMs在推理时的性能。\n\n## 核心概念与分类\n\n文章提出了一个全面的分类体系，将上下文工程分解为基础组件和集成这些组件的复杂系统实现。\n\n### 基础组件\n\n1.  **上下文检索与生成（Context Retrieval and Generation）**：\n    *   涉及如何从大量信息中有效地获取相关上下文，以及如何根据需要生成新的上下文信息。\n2.  **上下文处理（Context Processing）**：\n    *   关注对检索或生成的上下文进行预处理、过滤、压缩、重排等操作，以使其更适合LLM的输入格式和理解能力。\n3.  **上下文管理（Context Management）**：\n    *   涵盖了对上下文生命周期、版本控制、多轮对话中的上下文维护等方面的管理策略。\n\n### 系统实现\n\n这些基础组件被巧妙地集成到以下复杂系统中，以提升LLMs的能力：\n\n1.  **检索增强生成（Retrieval-Augmented Generation, RAG）**：\n    *   通过将外部知识库的检索能力与LLM的生成能力相结合，显著提高了模型输出的准确性和信息量。\n2.  **记忆系统与工具集成推理（Memory Systems and Tool-Integrated Reasoning）**：\n    *   记忆系统使LLM能够存储和回顾长期或短期信息，而工具集成推理则允许LLM调用外部工具（如计算器、API等）来辅助解决复杂问题，扩展其能力边界。\n3.  **多智能体系统（Multi-Agent Systems）**：\n    *   构建由多个LLM或其他AI智能体组成的协作系统，每个智能体可能专注于特定任务或角色，通过交互和协作共同完成复杂目标。\n\n## 研究方法与发现\n\n本综述通过对超过1300篇研究论文进行系统性分析，不仅为该领域建立了技术路线图，还揭示了一个关键的研究空白：\n\n### 关键研究空白：能力不对称性\n\n尽管当前的LLMs在先进上下文工程的增强下，在理解复杂上下文方面表现出卓越的能力，但它们在生成同样复杂、长篇幅的输出方面却存在明显的局限性。这种理解能力与生成能力之间的显著不对称性是未来研究的当务之急。\n\n## 总结与展望\n\n本综述为推进上下文感知型人工智能的研究人员和工程师提供了一个统一的框架。解决LLM在生成复杂长篇输出方面的局限性，将是未来研究的重点。",
      "shortSummary": "本综述介绍了“上下文工程”，一个旨在系统优化大型语言模型（LLMs）输入信息的正式学科。它详细阐述了上下文检索、处理、管理等基础组件，以及检索增强生成（RAG）、记忆系统和多智能体系统等高级实现。通过分析1300多篇论文，综述揭示了LLM在理解复杂上下文方面表现出色，但在生成同样复杂的长篇输出方面存在显著局限性，并指出这是未来研究的关键方向。该综述为上下文感知型AI提供了统一框架。",
      "translated_title": "大型语言模型上下文工程综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI."
    },
    {
      "title": "模仿游戏：图灵机模仿器是长度泛化推理器 (原标题: The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner)",
      "link": "https://arxiv.org/abs/2507.13332",
      "pubDate": "Thu, 17 Jul 2025 13:50:07 GMT",
      "isoDate": "2025-07-17T13:50:07.000Z",
      "creator": "Zhouqi Hua, Wenwei Zhang, Chengqi Lyu, Yuzhe Gu, Songyang Gao, Kuikun Liu, Kai Chen",
      "summary": "## 图灵机模仿学习（TAIL）：提升大型语言模型长度泛化能力的新范式\n\n### 引言\n\n*   **核心挑战：** 基于Transformer的大型语言模型（LLM）在长度泛化方面面临严峻挑战，即它们难以解决比训练时所见序列更长的问题。\n*   **现有方法局限：** 当前的数据驱动方法主要集中于算术运算和符号操作，但这些方法通常是任务特定的，且整体性能有限，无法提供通用解决方案。\n\n### TAIL方法概述\n\n*   **研究视角：** 本文旨在寻求更通用的解决方案，将重点放在可计算的推理问题上，即那些可以通过算法解决，从而也能被图灵机解决的问题。\n*   **核心提案：** 提出了一种名为“图灵机模仿学习”（Turing MAchine Imitation Learning, TAIL）的新方法，旨在显著提升LLM的长度泛化能力。\n*   **数据合成机制：** TAIL通过计算机程序合成“思维链”（Chain-of-Thoughts, CoT）数据，这些数据精确模仿了图灵机的执行过程。具体机制包括：\n    *   **线性扩展推理步骤：** 将推理步骤线性地扩展为原子状态，这有助于缓解模型学习“捷径”（shortcut learning）的问题。\n    *   **显式内存获取机制：** 引入显式内存获取机制，以降低在基本操作中进行动态和长距离数据访问的难度。\n\n### 实验验证与结果\n\n*   **数据集构建：** 为验证TAIL的可靠性和普适性，研究团队构建了一个具有挑战性的合成数据集，该数据集涵盖了8类算法和18个具体任务。\n*   **性能表现：** 实验结果显示，TAIL在不依赖额外复杂技巧的情况下，仅使用合成数据就显著提升了Qwen2.5-7B模型在各种任务上的长度泛化能力和整体性能。\n*   **超越现有方法：** TAIL的表现超越了先前的多种方法以及DeepSeek-R1模型。\n\n### 关键发现\n\n*   实验揭示，对于TAIL实现长度泛化而言，图灵机中的**关键概念**（而非其思维风格）是不可或缺的。\n*   模型在注意力层中表现出与图灵机特性高度一致的读写行为，这进一步验证了TAIL方法的有效性。\n\n### 未来展望\n\n*   这项工作为未来从合成数据中学习LLM推理能力提供了一个充满前景的方向。\n\n### 相关信息\n\n*   **主题：** 计算与语言 (cs.CL)\n*   **引用：** arXiv:2507.13332 [cs.CL]",
      "shortSummary": "大型语言模型（LLM）在长度泛化方面面临挑战。本文提出“图灵机模仿学习”（TAIL），通过合成模仿图灵机执行过程的思维链（CoT）数据来解决此问题。TAIL通过将推理步骤原子化并引入显式内存机制，显著提升了Qwen2.5-7B等模型的长度泛化能力和性能，超越现有方法。研究表明，图灵机的核心概念对泛化至关重要。这项工作为LLM从合成数据中学习推理提供了新方向。",
      "translated_title": "模仿游戏：图灵机模仿器是长度泛化推理器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data."
    }
  ],
  "lastUpdated": "2025-07-22T09:38:15.821Z"
}