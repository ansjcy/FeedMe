{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "SEAgent：基于经验自主学习的自进化计算机使用智能体 (原标题: SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience)",
      "link": "https://arxiv.org/abs/2508.04700",
      "pubDate": "Wed, 06 Aug 2025 13:58:46 GMT",
      "isoDate": "2025-08-06T13:58:46.000Z",
      "creator": "Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, Jiaqi Wang",
      "summary": "## SEAgent：基于经验自主学习的自进化计算机使用智能体\n\nSEAgent是一种创新的自进化框架，旨在解决大型视觉语言模型（LVLMs）作为计算机使用智能体（CUAs）在处理新型和专业软件时面临的挑战，尤其是在缺乏人工标注数据的情况下。\n\n### 核心问题与挑战\n*   当前将LVLMs用作CUAs的方法主要依赖于人工标注数据，这限制了它们在面对不熟悉或专业软件时的表现。\n*   在缺乏人类标注的场景中，这些模型难以有效应对。\n\n### SEAgent框架概述\nSEAgent赋能计算机使用智能体通过与陌生软件的交互进行自主进化。其核心在于“经验学习”，智能体通过以下方式掌握新软件环境：\n1.  **探索与试错：** 智能体主动探索新软件，并通过迭代的试错过程进行学习。\n2.  **任务渐进：** 逐步解决从简单到复杂的自动生成任务。\n\n### 关键设计与机制\n为了实现自主学习和进化，SEAgent引入了以下关键组件和策略：\n*   **世界状态模型（World State Model）：** 用于对每一步的轨迹进行评估，确保学习过程的有效性。\n*   **课程生成器（Curriculum Generator）：** 负责生成日益多样化和具有挑战性的任务，引导智能体逐步提升能力。\n*   **策略更新机制：** 智能体的策略通过经验学习进行更新，具体包括：\n    *   **失败动作的对抗性模仿：** 从失败中学习，避免重复错误。\n    *   **成功动作的群组相对策略优化（GRPO）：** 优化成功的行为模式。\n*   **专家到通用型训练策略（Specialist-to-Generalist Training Strategy）：**\n    *   该策略整合了来自单个专家智能体的经验洞察。\n    *   旨在培养一个更强大的通用型CUA，使其能够持续自主进化。\n    *   这种统一的智能体最终在各自的专业软件上，其性能超越了由多个独立专家智能体组成的集合。\n\n### 实验验证与成果\n*   SEAgent在OS-World中的五个新型软件环境中进行了有效性验证。\n*   与竞争性的开源CUA（UI-TARS）相比，SEAgent取得了显著的性能提升。\n*   成功率从11.3%提高到34.5%，实现了23.2%的显著增长。\n\n### 总结\nSEAgent提供了一种有效的方法，使计算机使用智能体能够自主学习和适应新的软件环境，克服了传统方法对大量人工标注数据的依赖，为未来智能体在复杂、动态计算机环境中的应用奠定了基础。",
      "shortSummary": "SEAgent是一个自进化的计算机使用智能体框架，旨在解决现有智能体在缺乏人工标注数据时难以掌握新型软件的问题。它通过经验学习、世界状态模型、课程生成器以及专家到通用型训练策略，使智能体能够自主探索、试错并优化行为。实验结果显示，SEAgent在新型软件环境中的成功率显著提升了23.2%，超越了现有基线。",
      "translated_title": "SEAgent：基于经验自主学习的自进化计算机使用智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS."
    },
    {
      "title": "Sculptor：通过主动上下文管理赋予大型语言模型认知能动性 (原标题: Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management)",
      "link": "https://arxiv.org/abs/2508.04664",
      "pubDate": "Wed, 06 Aug 2025 13:32:58 GMT",
      "isoDate": "2025-08-06T13:32:58.000Z",
      "creator": "Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu",
      "summary": "## Sculptor：通过主动上下文管理增强大型语言模型的认知能动性\n\n### 问题背景\n大型语言模型（LLMs）在处理长上下文时面临显著的性能下降。这主要是由于“前瞻性干扰”（proactive interference），即上下文早期部分的不相关信息会干扰模型的推理过程和记忆回忆能力。\n\n### 解决方案：主动上下文管理（ACM）\n传统的LLM研究多集中于通过外部记忆系统来增强模型能力。本文提出了一种互补的方法：通过赋予LLMs“主动上下文管理”（Active Context Management, ACM）工具，使其能够主动地雕塑自身的内部工作记忆。这种方法类似于人类选择性地关注相关信息并过滤掉干扰。\n\n### Sculptor 框架\n我们引入了 Sculptor 框架，它为LLMs配备了三类工具，以实现主动上下文管理：\n1.  **上下文碎片化（Context Fragmentation）**：将长上下文分解为更小的、可管理的片段。\n2.  **总结、隐藏和恢复（Summary, Hide, and Restore）**：允许LLMs对信息进行总结、暂时隐藏不相关内容，并在需要时恢复。\n3.  **智能搜索（Intelligent Search）**：使LLMs能够高效地在上下文中搜索和检索相关信息。\n\n### 实验评估\n*   **基准测试：** Sculptor 在信息稀疏的基准测试中进行了评估，包括：\n    *   **PI-LLM (Proactive Interference)**：专门用于测试前瞻性干扰的缓解效果。\n    *   **NeedleBench Multi-Needle Reasoning**：用于评估在复杂长上下文中的多点推理能力。\n*   **实验结果：** 评估表明，即使在没有特定训练的情况下，Sculptor 也能显著提高LLMs的性能。这得益于LLMs固有的工具调用泛化能力。\n\n### 结论与意义\n通过启用主动上下文管理，Sculptor 不仅有效缓解了前瞻性干扰，还为LLMs在各种长上下文任务中实现更可靠的推理提供了认知基础。研究强调，明确的上下文控制策略，而非仅仅扩大token窗口，是实现LLMs大规模鲁棒性的关键。",
      "shortSummary": "大型语言模型在处理长上下文时因“前瞻性干扰”导致性能下降。Sculptor框架提出“主动上下文管理”（ACM），通过上下文碎片化、总结/隐藏/恢复和智能搜索等工具，赋予LLMs主动管理其注意力和工作记忆的能力。实验证明，Sculptor显著提升了模型在信息稀疏基准上的表现，无需特定训练。这表明，明确的上下文控制策略对于LLMs在大规模应用中的鲁棒性至关重要，而非简单增加token窗口。",
      "translated_title": "Sculptor：通过主动上下文管理赋予大型语言模型认知能动性",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale."
    },
    {
      "title": "立场：当前人工智能会议模式不可持续！诊断中心化人工智能会议的危机 (原标题: Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference)",
      "link": "https://arxiv.org/abs/2508.04586",
      "pubDate": "Wed, 06 Aug 2025 12:08:27 GMT",
      "isoDate": "2025-08-06T12:08:27.000Z",
      "creator": "Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He",
      "summary": "## 当前人工智能会议模式的危机诊断\n\n人工智能（AI）会议在推动研究、分享知识和促进学术社区发展方面发挥着至关重要的作用。然而，随着AI领域的迅速扩张，当前中心化的会议模式正变得越来越不可持续。本文通过数据驱动的方式，深入诊断了这一结构性危机，指出其正威胁着科学传播、公平性和社区福祉等核心目标。\n\n### 危机的主要压力领域\n\n文章识别出四个关键的压力领域，这些压力共同指向一个与其核心使命不符的系统：\n\n1.  **科学方面：**\n    *   过去十年间，每位作者的论文发表率翻了一倍多，达到每年超过4.5篇。这种高发表率给同行评审系统和研究人员带来了巨大压力。\n\n2.  **环境方面：**\n    *   单次大型会议的碳足迹已超过其主办城市一天的排放量。这凸显了中心化会议模式对环境的巨大影响。\n\n3.  **心理方面：**\n    *   在线社区讨论中，有71%的内容反映出负面情绪。\n    *   其中35%的讨论提及了心理健康问题。这表明高压的会议环境和学术竞争对研究人员的心理健康造成了负面影响。\n\n4.  **物流方面：**\n    *   顶级会议（如NeurIPS 2024）的参会人数已开始超出现有场地容量。这导致了参会体验下降、资源紧张以及潜在的排他性问题。\n\n### 提出的解决方案：社区联邦会议（CFC）模型\n\n为了应对这些日益增长的压力，文章提出了“社区联邦会议（Community-Federated Conference, CFC）”模型。该模型旨在提供一个更可持续、更具包容性和更具弹性的AI研究发展路径。\n\n**CFC模型的核心特点：**\n\n*   **分离功能：** 将同行评审、论文演示和社交活动这三个核心功能进行分离。\n*   **全球协调与本地组织：** 这些分离的功能在全球范围内进行协调，但在本地层面进行组织和实施。\n\n通过这种去中心化和联邦化的方法，CFC模型有望缓解当前中心化会议模式所面临的各项挑战，更好地服务于AI研究社区的长期发展。",
      "shortSummary": "当前人工智能会议的中心化模式因快速扩张而不可持续，面临科学（高发表率）、环境（高碳足迹）、心理（负面情绪与心理健康问题）和物流（场地容量不足）四方面压力。为解决此危机，文章提出“社区联邦会议（CFC）”模型，通过将同行评审、演示和社交活动分离，并进行全球协调、本地组织，旨在为AI研究提供一个更可持续、包容和弹性的发展路径。",
      "translated_title": "立场：当前人工智能会议模式不可持续！诊断中心化人工智能会议的危机",
      "images": [],
      "contentSource": "完整文章",
      "content": "Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research."
    },
    {
      "title": "EVOC2RUST：一个用于项目级C到Rust翻译的骨架引导框架 (原标题: EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation)",
      "link": "https://arxiv.org/abs/2508.04295",
      "pubDate": "Wed, 06 Aug 2025 06:31:23 GMT",
      "isoDate": "2025-08-06T06:31:23.000Z",
      "creator": "Chaofan Wang, Tingrui Yu, Jie Wang, Dong Chen, Wenrui Zhang, Yuling Shi, Xiaodong Gu, Beijun Shen",
      "summary": "## EVOC2RUST：一个用于项目级C到Rust翻译的骨架引导框架\n\n### 引言：现有C到Rust翻译方法的局限性\n\nRust的编译时安全保证使其成为安全关键系统的理想选择，因此将遗留C代码库翻译成Rust的需求日益增长。尽管已出现多种翻译方法，但它们面临固有的权衡：\n\n*   **规则基解决方案：** 难以满足代码安全性和惯用性要求。\n*   **LLM（大型语言模型）基解决方案：** 由于模块在整个代码库中存在重度依赖，通常无法生成语义等效的Rust代码。\n*   **共同局限：** 近期研究表明，这两种解决方案都仅限于小规模程序。\n\n### EvoC2Rust框架介绍\n\n本文提出了EvoC2Rust，一个自动化框架，旨在将整个C项目转换为等效的Rust项目。EvoC2Rust采用骨架引导的翻译策略，用于项目级别的翻译。其流程包括三个演进阶段：\n\n1.  **阶段一：生成可编译的Rust骨架**\n    *   首先将C项目分解为功能模块。\n    *   采用特征映射增强的LLM来转换定义和宏。\n    *   生成类型检查的函数存根，形成一个可编译的Rust骨架。\n\n2.  **阶段二：增量函数翻译**\n    *   随后逐步翻译函数，替换相应的存根占位符。\n\n3.  **阶段三：编译错误修复**\n    *   最后，通过整合LLM和静态分析来修复编译错误。\n\n### EvoC2Rust的优势\n\n通过演进式增强，EvoC2Rust结合了规则基和LLM基解决方案的优点，有效克服了现有方法的局限性。\n\n### 评估与性能\n\nEvoC2Rust在开源基准测试和六个工业项目上的评估结果表明，其在项目级C到Rust翻译方面表现卓越：\n\n*   **与LLM基方法相比：**\n    *   语法准确性平均提高17.24%。\n    *   语义准确性平均提高14.32%。\n*   **与规则基工具相比：**\n    *   代码安全率高96.79%。\n*   **在模块级别（工业项目）：**\n    *   编译通过率达到92.25%。\n    *   测试通过率达到89.53%。\n\n这些结果表明，即使对于复杂的代码库和长函数，EvoC2Rust也能表现良好。",
      "shortSummary": "EvoC2Rust是一个自动化框架，旨在将整个C项目转换为Rust项目。它采用骨架引导的翻译策略，通过分解C项目、生成Rust骨架、增量翻译函数和修复编译错误三个演进阶段进行。该框架结合了规则基和LLM基方法的优点，解决了现有方案在规模和准确性上的局限。评估显示，EvoC2Rust在项目级C到Rust翻译方面表现卓越，显著提高了语法和语义准确性，并大幅提升了代码安全率，尤其在工业项目中展现出高编译和测试通过率。",
      "translated_title": "EVOC2RUST：一个用于项目级C到Rust翻译的骨架引导框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions."
    },
    {
      "title": "VeriGUI：可验证的长链GUI数据集 (原标题: VeriGUI: Verifiable Long-Chain GUI Dataset)",
      "link": "https://arxiv.org/abs/2508.04026",
      "pubDate": "Tue, 05 Aug 2025 22:38:18 GMT",
      "isoDate": "2025-08-05T22:38:18.000Z",
      "creator": "Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao",
      "summary": "# VeriGUI：可验证的长链GUI数据集\n\n## 引言与背景\n当前，构建能够执行复杂图形用户界面（GUI）任务的自主代理是人机交互领域的研究热点。尽管已取得令人鼓舞的成果，但现有研究主要集中于短期交互，并依赖于仅基于结果的验证。这限制了它们在需要长期任务分解和执行的真实世界GUI应用中的可扩展性。\n\n## VeriGUI数据集介绍\n为了解决上述挑战，本研究引入了VeriGUI，一个新颖的可验证长链GUI数据集。VeriGUI旨在促进在真实计算机环境中运行的通用GUI代理的开发和评估。该数据集强调两个关键维度：\n\n*   **长链复杂性：** 任务被分解为一系列相互依赖的子任务，这些子任务跨越数百个步骤。数据集经过精心设计，允许任何子任务作为有效的起始点，从而支持更灵活的任务规划和执行。\n*   **子任务级可验证性：** 确保在每个子任务内部可以采用多样化的探索策略，同时每个子任务级别的目标都保持可验证性和一致性。这有助于更细粒度的评估和调试。\n\n## 数据集构成\nVeriGUI数据集包含由人类专家标注的GUI任务轨迹，涵盖桌面和网页环境。\n\n## 实验与发现\n研究人员使用不同基础模型的各种代理在VeriGUI数据集上进行了广泛的实验。结果揭示了在处理长周期任务方面存在显著的性能差距，这突出表明GUI代理需要更强大的规划和决策能力。\n\n## 结论与意义\nVeriGUI数据集的推出，为开发和评估能够处理复杂、长期GUI任务的通用代理提供了重要的资源，有望推动自主代理在人机交互领域的进一步发展。",
      "shortSummary": "VeriGUI是一个新颖的可验证长链GUI数据集，旨在解决现有GUI代理研究在处理长期复杂任务时的局限性。它通过将任务分解为数百个可验证的子任务，并支持桌面和网页环境，来促进通用GUI代理的开发和评估。实验表明，当前代理在处理长周期任务时表现出显著的性能差距，凸显了提升规划和决策能力的需求。VeriGUI为推动人机交互领域的自主代理发展提供了重要资源。",
      "translated_title": "VeriGUI：可验证的长链GUI数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents."
    },
    {
      "title": "Sotopia-RL: 社交智能的奖励设计 (原标题: Sotopia-RL: Reward Design for Social Intelligence)",
      "link": "https://arxiv.org/abs/2508.03905",
      "pubDate": "Tue, 05 Aug 2025 16:43:42 GMT",
      "isoDate": "2025-08-05T16:43:42.000Z",
      "creator": "Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, Jiaxuan You",
      "summary": "## Sotopia-RL: 社交智能的奖励设计\n\n### 背景与挑战\n\n大型语言模型（LLMs）的社交智能已成为一项关键能力，使其能够有效参与现实世界的社交任务，例如适应、说服、协作和谈判。强化学习（RL）是训练社交智能代理的理想方法，因为它允许模型通过社交互动直接学习复杂的策略。\n\n然而，社交互动具有两个关键特征，为RL训练设置了障碍：\n\n*   **部分可观察性（Partial Observability）**：言语的效应是间接且延迟的，这使得信用分配（credit assignment）变得复杂。\n*   **多维度性（Multi-dimensionality）**：诸如建立融洽关系或寻求知识等行为间接地促进目标实现。\n\n这些特性使得基于马尔可夫决策过程（MDP）的RL，如果采用单维度、回合级的奖励，将变得低效且不稳定。\n\n### Sotopia-RL 框架\n\n为了应对这些挑战，研究人员提出了 **Sotopia-RL**，一个新颖的框架，它将粗粒度的回合级反馈细化为**话语级（utterance-level）**、**多维度（multi-dimensional）**的奖励。\n\n*   **话语级信用分配**：通过将结果归因于单个言语，缓解了部分可观察性问题。\n*   **多维度奖励**：捕捉了社交互动的丰富性，并减少了奖励作弊（reward hacking）的可能性。\n\n### 实验结果与贡献\n\n在 Sotopia（一个开放式社交学习环境）中进行的实验表明，Sotopia-RL 取得了最先进的社交目标完成分数，具体表现为：\n\n*   在 Sotopia-hard 上获得 7.17 分。\n*   在 Sotopia-full 上获得 8.31 分。\n\n这些结果显著优于现有方法。消融研究（Ablation studies）证实了话语级信用分配和多维度奖励设计对于RL训练的必要性。",
      "shortSummary": "Sotopia-RL是一个新颖的强化学习框架，旨在提升大型语言模型的社交智能。针对社交互动中部分可观察性和多维度性导致的传统RL训练挑战，Sotopia-RL通过将粗粒度的回合级反馈细化为话语级、多维度的奖励来解决。实验证明，该框架在Sotopia社交学习环境中取得了最先进的社交目标完成分数，显著优于现有方法，并确认了话语级归因和多维度奖励设计的必要性。",
      "translated_title": "Sotopia-RL: 社交智能的奖励设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl."
    },
    {
      "title": "Agent Lightning：使用强化学习训练任意AI智能体 (原标题: Agent Lightning: Train ANY AI Agents with Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2508.03680",
      "pubDate": "Tue, 05 Aug 2025 13:50:13 GMT",
      "isoDate": "2025-08-05T13:50:13.000Z",
      "creator": "Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang",
      "summary": "## Agent Lightning 框架概述\n\nAgent Lightning 是一个灵活且可扩展的框架，旨在通过强化学习（RL）对大型语言模型（LLM）驱动的AI智能体进行训练。该框架的核心创新在于实现了智能体执行与训练的完全解耦，解决了现有方法中RL训练与智能体紧密耦合或依赖序列拼接与掩码的问题。\n\n### 核心创新与优势\n\n*   **完全解耦**：Agent Lightning 实现了智能体执行与RL训练的彻底分离。这意味着它可以与通过各种方式（如 LangChain、OpenAI Agents SDK、AutoGen 或从零开始构建）开发的现有智能体无缝集成，几乎无需修改代码。\n*   **统一数据接口**：通过将智能体执行建模为马尔可夫决策过程（MDP），Agent Lightning 定义了一个统一的数据接口，使得RL能够处理复杂的交互逻辑，包括多智能体场景和动态工作流。\n\n### 技术实现\n\n*   **LightningRL 算法**：框架提出了一种分层强化学习算法——LightningRL。该算法包含一个“信用分配模块”，能够将任意智能体生成的轨迹分解为可用于训练的转换（training transition）。\n\n### 系统设计\n\n*   **训练-智能体分离架构**：Agent Lightning 引入了一种“训练-智能体分离”（Training-Agent Disaggregation）架构，进一步强化了执行与训练的解耦。\n*   **智能体可观测性**：将智能体可观测性框架引入智能体运行时，提供了一个标准化的智能体微调接口，便于监控和优化训练过程。\n\n### 实验验证与应用前景\n\nAgent Lightning 在多项任务上进行了实验验证，包括：\n\n*   文本到SQL（text-to-SQL）\n*   检索增强生成（retrieval-augmented generation, RAG）\n*   数学工具使用（math tool-use）\n\n实验结果表明，该框架能够实现稳定、持续的性能提升，展示了其在实际智能体训练和部署中的巨大潜力。",
      "shortSummary": "Agent Lightning是一个创新的框架，旨在利用强化学习训练任意AI智能体。其核心在于实现智能体执行与训练的完全解耦，从而能与现有智能体无缝集成，几乎无需代码修改。通过将智能体执行建模为马尔可夫决策过程并引入分层RL算法LightningRL，它能处理复杂交互。实验证明，该框架在文本到SQL、RAG和数学工具使用等任务中均能实现稳定提升，展现了其在实际智能体训练和部署中的巨大潜力。",
      "translated_title": "Agent Lightning：使用强化学习训练任意AI智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment."
    },
    {
      "title": "HPSv3：迈向广谱人类偏好评分 (原标题: HPSv3: Towards Wide-Spectrum Human Preference Score)",
      "link": "https://arxiv.org/abs/2508.03789",
      "pubDate": "Tue, 05 Aug 2025 13:17:13 GMT",
      "isoDate": "2025-08-05T13:17:13.000Z",
      "creator": "Yuhang Ma, Xiaoshi Wu, Keqiang Sun, Hongsheng Li",
      "summary": "# HPSv3：迈向广谱人类偏好评分\n\n## 引言与背景\n评估文本到图像生成模型需要与人类感知对齐，然而，现有以人类为中心的评估指标面临多重挑战。这些挑战主要体现在：\n*   **数据覆盖范围有限**：现有数据集不足以覆盖广泛的图像质量和类型。\n*   **特征提取次优**：模型从图像中提取特征的能力有待提高。\n*   **损失函数效率低下**：用于训练评估模型的损失函数不够高效。\n\n为了解决这些问题，研究人员引入了 **人类偏好评分 v3 (Human Preference Score v3, HPSv3)**。\n\n## HPSv3 的核心贡献\nHPSv3 包含以下关键创新和组成部分：\n\n1.  **HPDv3 数据集发布**\n    *   这是首个 **广谱人类偏好数据集 (wide-spectrum human preference dataset)**。\n    *   整合了 **108 万个文本-图像对**。\n    *   包含了 **117 万个带注释的成对比较**。\n    *   数据来源广泛，包括最先进的生成模型以及从低质量到高质量的真实世界图像。\n\n2.  **基于 VLM 的偏好模型**\n    *   引入了一个基于 **视觉语言模型 (VLM)** 的偏好模型。\n    *   该模型使用 **不确定性感知排序损失 (uncertainty-aware ranking loss)** 进行训练，以实现细粒度排序。\n\n3.  **Chain-of-Human-Preference (CoHP) 方法**\n    *   提出了一种名为 **Chain-of-Human-Preference (CoHP)** 的迭代图像精炼方法。\n    *   该方法能够在 **不额外增加数据** 的情况下提升图像质量。\n    *   CoHP 在每个步骤中利用 HPSv3 来选择最佳图像，从而逐步优化生成结果。\n\n## 实验结果与意义\n广泛的实验证明了 HPSv3 的有效性：\n*   **HPSv3** 作为一个 **鲁棒的指标**，适用于广谱图像评估。\n*   **CoHP** 提供了一种 **高效且与人类偏好对齐** 的方法来改进图像生成质量。\n\n## 资源可用性\n相关的代码和数据集已在 HPSv3 主页上公开。",
      "shortSummary": "HPSv3 引入了一种新的广谱人类偏好评分系统，旨在解决文本到图像生成模型评估中现有指标的数据覆盖、特征提取和损失函数效率问题。它发布了首个广谱数据集 HPDv3（包含108万文本-图像对和117万成对比较），并提出了一个基于VLM的偏好模型。此外，HPSv3还引入了CoHP迭代精炼方法，可在不额外数据的情况下提升图像质量。实验证明HPSv3是鲁棒的评估指标，CoHP能有效提升图像生成质量。",
      "translated_title": "HPSv3：迈向广谱人类偏好评分",
      "images": [],
      "contentSource": "完整文章",
      "content": "Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage."
    },
    {
      "title": "LaTCoder：通过布局即思考将网页设计转换为代码 (原标题: LaTCoder: Converting Webpage Design to Code with Layout-as-Thought)",
      "link": "https://arxiv.org/abs/2508.03560",
      "pubDate": "Tue, 05 Aug 2025 11:28:48 GMT",
      "isoDate": "2025-08-05T11:28:48.000Z",
      "creator": "Yi Gui, Zhen Li, Zhongyi Zhang, Guohao Wang, Tianpeng Lv, Gaoyang Jiang, Yi Liu, Dongping Chen, Yao Wan, Hongyu Zhang, Wenbin Jiang, Xuanhua Shi, Hai Jin",
      "summary": "# LaTCoder：通过布局即思考将网页设计转换为代码\n\n## 引言\n\n将网页设计转换为代码（设计到代码）在前端开发中扮演着至关重要的角色，它弥合了视觉设计与功能实现之间的鸿沟。尽管近期多模态大语言模型（MLLMs）在设计到代码任务中展现出巨大潜力，但它们在代码生成过程中往往难以准确保留布局。\n\n## LaTCoder 方法\n\n为了解决这一问题，我们从人类认知的思维链（Chain-of-Thought, CoT）推理中汲取灵感，提出了一种名为 LaTCoder 的新颖方法。LaTCoder 通过“布局即思考”（Layout-as-Thought, LaT）来增强网页设计在代码生成过程中的布局保留能力。\n\n**具体步骤如下：**\n\n1.  **划分图像块：** 首先，我们引入一种简单而高效的算法，将网页设计划分为独立的图像块。\n2.  **生成块代码：** 接下来，我们使用基于 CoT 的方法提示 MLLM 为每个图像块生成相应的代码。\n3.  **组装与选择：** 最后，我们应用两种组装策略——绝对定位和基于 MLLM 的方法——并通过动态选择来确定最优的输出结果。\n\n## 实验评估\n\n我们使用多个骨干 MLLM（包括 DeepSeek-VL2、Gemini 和 GPT-4o）评估了 LaTCoder 的有效性。评估在公共基准测试集以及我们新引入的、更具挑战性的 CC-HARD 基准测试集上进行，后者以其复杂的布局为特点。\n\n**实验结果：**\n\n*   **自动指标：** 实验结果在自动指标上显示出显著的改进。具体而言，与直接提示相比，在使用 DeepSeek-VL2 时，TreeBLEU 分数提高了 66.67%，平均绝对误差（MAE）降低了 38%。\n*   **人工偏好评估：** 此外，人工偏好评估结果表明，标注者在超过 60% 的情况下更倾向于 LaTCoder 生成的网页，这为我们方法的有效性提供了有力证据。\n\n## 结论\n\nLaTCoder 显著提升了多模态大语言模型在网页设计到代码转换任务中布局的准确性，并通过实验结果（包括自动指标和人工偏好）证明了其卓越的性能。",
      "shortSummary": "LaTCoder 是一种新方法，通过“布局即思考”（LaT）提升多模态大语言模型（MLLM）在网页设计到代码转换中布局的准确性。它将设计分解为图像块，为每个块生成代码，然后智能组装。实验表明，LaTCoder 显著提高了代码生成质量和布局保留，例如使用 DeepSeek-VL2 时，TreeBLEU 分数提高 66.67%，MAE 降低 38%，并获得超过 60% 的人工偏好，证明了其有效性。",
      "translated_title": "LaTCoder：通过布局即思考将网页设计转换为代码",
      "images": [],
      "contentSource": "完整文章",
      "content": "Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method."
    },
    {
      "title": "SonicMaster：迈向可控的一体化音乐修复与母带处理 (原标题: SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering)",
      "link": "https://arxiv.org/abs/2508.03448",
      "pubDate": "Tue, 05 Aug 2025 09:49:04 GMT",
      "isoDate": "2025-08-05T09:49:04.000Z",
      "creator": "Jan Melechovsky, Ambuj Mehrish, Dorien Herremans",
      "summary": "### SonicMaster：可控的一体化音乐修复与母带处理\n\n本文介绍了一种名为 **SonicMaster** 的新型生成模型，旨在解决音乐录音中常见的音频质量问题，并实现一体化的音乐修复与母带处理。\n\n#### 背景与问题\n*   **常见问题：** 音乐录音，尤其是在非专业环境下制作的，常面临混响过多、失真、削波、音调不平衡以及立体声像变窄等音频质量问题。\n*   **传统解决方案：** 通常需要使用多种独立的专业工具和手动调整来纠正这些问题，过程复杂且耗时。\n\n#### SonicMaster 解决方案\n*   **统一模型：** SonicMaster 是首个统一的生成模型，能够处理广泛的音频瑕疵。\n*   **控制方式：**\n    *   **文本控制：** 用户可以通过自然语言指令来应用有针对性的增强。\n    *   **自动模式：** 也可以在自动模式下运行，进行通用修复。\n\n#### 模型训练\n*   **SonicMaster 数据集：** 为了训练该模型，研究团队构建了 SonicMaster 数据集。\n*   **数据集特点：** 这是一个大型的配对数据集，包含降级音轨和高质量音轨。\n*   **降级模拟：** 通过模拟常见的降级类型来创建降级音轨，使用了十九种降级函数，这些函数属于五个增强组：\n    *   均衡 (equalization)\n    *   动态 (dynamics)\n    *   混响 (reverb)\n    *   振幅 (amplitude)\n    *   立体声 (stereo)\n\n#### 方法论\n*   **生成训练范式：** SonicMaster 采用流匹配（flow-matching）生成训练范式。\n*   **音频转换：** 该范式学习一种音频转换，能够将降级的输入映射到经过清理和母带处理的版本。\n*   **文本提示引导：** 整个转换过程由文本提示（text prompts）引导。\n\n#### 结果与评估\n*   **客观指标：** 客观音频质量指标显示，SonicMaster 在所有瑕疵类别上都显著改善了音质。\n*   **主观测试：** 主观听力测试证实，听众更倾向于 SonicMaster 增强后的输出，而非原始的降级音频。\n*   **结论：** 这些结果突显了 SonicMaster 统一方法的有效性。",
      "shortSummary": "SonicMaster 是一种创新的统一生成模型，旨在解决音乐录音中的多种音频质量问题，并实现一体化的修复与母带处理。该模型支持文本指令控制或自动模式，通过流匹配范式训练于大型模拟降级数据集。客观和主观评估均表明，SonicMaster 能显著提升音质，并获得听众偏好，展现了其在音乐音频处理领域的有效性。",
      "translated_title": "SonicMaster：迈向可控的一体化音乐修复与母带处理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach."
    },
    {
      "title": "CoTox：基于思维链的分子毒性推理与预测 (原标题: CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction)",
      "link": "https://arxiv.org/abs/2508.03159",
      "pubDate": "Tue, 05 Aug 2025 03:04:44 GMT",
      "isoDate": "2025-08-05T03:04:44.000Z",
      "creator": "Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang",
      "summary": "# CoTox：基于思维链的分子毒性推理与预测\n\n## 摘要\n\n药物毒性是药物研发中的一个主要挑战。尽管近期机器学习模型在体外毒性预测方面有所改进，但它们对标注数据的依赖性以及缺乏可解释性限制了其应用，尤其是在捕捉由复杂生物机制驱动的器官特异性毒性方面。\n\n## 现有问题与解决方案\n\n*   **现有机器学习模型的局限性：**\n    *   过度依赖标注数据。\n    *   缺乏可解释性，难以理解预测背后的原因。\n    *   难以捕捉复杂的、器官特异性的毒性机制。\n*   **大型语言模型（LLMs）的潜力：**\n    *   通过逐步推理和整合文本数据，LLMs 提供了一种有前景的替代方案。\n*   **现有LLM方法的不足：**\n    *   缺乏生物学背景和透明的推理过程。\n\n## CoTox框架介绍\n\n为了解决上述问题，本文提出了 **CoTox**，一个新颖的框架，它将大型语言模型（LLM）与思维链（CoT）推理相结合，用于多毒性预测。\n\n*   **CoTox的核心组成：**\n    *   **化学结构数据：** 提供分子的基本信息。\n    *   **生物通路：** 整合生物学过程信息。\n    *   **基因本体（GO）术语：** 引入基因功能和细胞组分信息。\n*   **工作原理：** CoTox 通过逐步推理，结合上述数据，生成可解释的毒性预测。\n\n## 性能与优势\n\n*   **卓越的预测性能：**\n    *   使用 **GPT-4o** 进行测试，CoTox 的表现优于传统的机器学习和深度学习模型。\n    *   研究还评估了 CoTox 在不同 LLM 上的表现，以确定其最有效的应用场景。\n*   **提升推理能力：**\n    *   研究发现，使用 **IUPAC 名称** 来表示化学结构（LLM 更易理解）相比 SMILES，能显著增强模型的推理能力并提高预测性能。\n\n## 实际应用与意义\n\n*   **模拟药物开发场景：**\n    *   为了展示 CoTox 在药物开发中的实用性，研究模拟了用药物处理相关细胞类型，并将由此产生的生物学背景整合到 CoTox 框架中。\n    *   这种方法使 CoTox 能够生成与生理反应一致的毒性预测，这一点已通过案例研究得到证实。\n*   **重要意义：**\n    *   CoTox 的成果突显了基于 LLM 的框架在提高可解释性和支持早期药物安全性评估方面的巨大潜力。\n\n## 资源可用性\n\n*   本研究中使用的代码和提示已公开提供。",
      "shortSummary": "CoTox是一个新颖的框架，它结合大型语言模型（LLM）和思维链（CoT）推理，用于分子多毒性预测。该框架整合了化学结构、生物通路和基因本体信息，通过逐步推理生成可解释的毒性预测。实验表明，CoTox（使用GPT-4o）在毒性预测方面优于传统模型，且使用IUPAC名称表示化学结构能进一步提升性能。CoTox能生成与生理反应一致的预测，有望提高药物安全性评估的可解释性，支持早期药物开发。",
      "translated_title": "CoTox：基于思维链的分子毒性推理与预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox."
    },
    {
      "title": "DreamVVT：通过分阶段扩散Transformer框架掌握野外真实视频虚拟试穿技术 (原标题: DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework)",
      "link": "https://arxiv.org/abs/2508.02807",
      "pubDate": "Mon, 04 Aug 2025 14:27:55 GMT",
      "isoDate": "2025-08-04T14:27:55.000Z",
      "creator": "Tongchun Zuo, Zaiyu Huang, Shuliang Ning, Ente Lin, Chao Liang, Zerong Zheng, Jianwen Jiang, Yuan Zhang, Mingyuan Gao, Xin Dong",
      "summary": "## DreamVVT：一种分阶段扩散Transformer框架，用于野外真实视频虚拟试穿\n\n### 引言\n视频虚拟试穿（VVT）技术因其在电子商务广告和娱乐领域的广阔应用前景，已引起学术界的广泛关注。\n\n### 现有挑战\n然而，大多数现有的端到端VVT方法面临以下挑战：\n*   严重依赖稀缺的配对服装中心数据集。\n*   未能有效利用先进视觉模型和测试时输入的先验知识。\n*   导致在非受限场景中难以准确保留精细服装细节和维持时间一致性。\n\n### DreamVVT 解决方案\n为应对这些挑战，我们提出了DreamVVT，一个精心设计的两阶段框架，其核心基于扩散Transformer (DiTs)。\n\n**核心优势：**\n*   DreamVVT天生能够利用多样化的非配对以人为中心的数据，从而增强其在真实世界场景中的适应性。\n\n**第一阶段：关键帧试穿图像合成**\n1.  从输入视频中采样具有代表性的帧。\n2.  利用集成了视觉-语言模型（VLM）的多帧试穿模型，合成高保真且语义一致的关键帧试穿图像。\n3.  这些生成的图像将作为后续视频生成的补充外观指导。\n\n**第二阶段：视频生成与时间一致性**\n1.  从输入内容中提取骨骼图以及精细的运动和外观描述。\n2.  将这些提取的信息与第一阶段生成的关键帧试穿图像一同输入到通过LoRA适配器增强的预训练视频生成模型中。\n3.  此阶段确保了未见区域的长期时间连贯性，并能够生成高度逼真的动态运动。\n\n### 实验结果\n广泛的定量和定性实验表明，DreamVVT在真实世界场景中，在保留详细服装内容和时间稳定性方面均超越了现有方法。\n\n### 项目信息\n*   文章页数：18页\n*   图表数量：12个\n*   主题：计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "DreamVVT是一种基于扩散Transformer的两阶段视频虚拟试穿（VVT）框架，旨在解决现有方法在野外场景中服装细节保留和时间一致性差的问题。第一阶段利用VLM生成高保真关键帧试穿图像作为外观指导；第二阶段结合骨骼图和运动描述，通过LoRA增强的预训练模型生成具有长期时间连贯性和逼真动态的视频。该方法能有效利用非配对数据，在真实世界场景中表现出卓越的服装细节保留和时间稳定性。",
      "translated_title": "DreamVVT：通过分阶段扩散Transformer框架掌握野外真实视频虚拟试穿技术",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/"
    },
    {
      "title": "Qwen-Image 技术报告 (原标题: Qwen-Image Technical Report)",
      "link": "https://arxiv.org/abs/2508.02324",
      "pubDate": "Mon, 04 Aug 2025 07:49:20 GMT",
      "isoDate": "2025-08-04T07:49:20.000Z",
      "creator": "Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu",
      "summary": "# Qwen-Image 技术报告概述\n\nQwen-Image 是 Qwen 系列中的一个图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。\n\n## 核心能力与创新点\n\n### 1. 复杂文本渲染\n\n*   **挑战应对：** 针对复杂文本渲染的挑战，Qwen-Image 采用了多项创新策略：\n    *   **数据管线设计：** 构建了一个全面的数据管线，包括大规模数据收集、过滤、标注、合成和平衡。\n    *   **渐进式训练策略：** 采用课程学习（curriculum learning）方法，逐步提升模型的文本渲染能力：\n        *   从非文本到文本渲染开始。\n        *   逐步从简单到复杂的文本输入进行训练。\n        *   最终扩展到段落级别的描述。\n*   **效果提升：** 这种训练方法显著增强了模型原生的文本渲染能力。\n    *   在英文等字母语言上表现出色。\n    *   在中文等更具挑战性的表意文字语言上也取得了显著进展。\n\n### 2. 精确图像编辑\n\n*   **一致性增强：** 为提升图像编辑的一致性，Qwen-Image 引入了改进的多任务训练范式：\n    *   **任务整合：** 不仅包含传统的文本到图像（T2I）和文本-图像到图像（TI2I）任务，还加入了图像到图像（I2I）重建任务。\n    *   **潜在表示对齐：** 有效地对齐了 Qwen2.5-VL 和 MMDiT 之间的潜在表示。\n    *   **双编码机制：** 将原始图像分别输入到 Qwen2.5-VL 和 VAE 编码器：\n        *   Qwen2.5-VL 用于获取语义表示。\n        *   VAE 编码器用于获取重建表示。\n    *   **平衡语义与视觉：** 这种双编码机制使编辑模块能够在保持语义一致性和维护视觉保真度之间取得平衡。\n\n## 性能表现\n\nQwen-Image 在多个基准测试中均取得了最先进（state-of-the-art）的性能，充分展示了其在图像生成和编辑方面的强大能力。",
      "shortSummary": "Qwen-Image 是 Qwen 系列的图像生成模型，在复杂文本渲染和精确图像编辑方面实现重大突破。它通过设计全面的数据管线和渐进式训练策略，显著提升了多语言（包括中文）文本渲染能力。同时，通过改进的多任务训练范式和双编码机制，增强了图像编辑的一致性，平衡了语义与视觉保真度。模型在图像生成和编辑领域均达到最先进水平。",
      "translated_title": "Qwen-Image 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks."
    },
    {
      "title": "VeOmni：使用以模型为中心的分布式配方库扩展任意模态模型训练 (原标题: VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo)",
      "link": "https://arxiv.org/abs/2508.02317",
      "pubDate": "Mon, 04 Aug 2025 07:33:04 GMT",
      "isoDate": "2025-08-04T07:33:04.000Z",
      "creator": "Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu",
      "summary": "# VeOmni：一种高效的全模态大语言模型训练框架\n\n## 挑战与背景\n\n近期大型语言模型（LLMs）的进展推动了全模态理解和生成能力的显著提升。然而，训练全模态LLMs面临着重大挑战，主要原因包括：\n\n*   **异构模型架构：** 处理多样化模态需要复杂的、异构的模型架构。\n*   **系统设计复杂性：** 实现高效的大规模训练需要精密的系统设计。\n*   **现有框架局限：** 当前框架通常将模型定义与并行逻辑紧密耦合，这导致可扩展性受限，并为端到端全模态训练带来了巨大的工程开销。\n\n## VeOmni 框架特点\n\n为应对上述挑战，研究人员提出了 **VeOmni**，一个模块化且高效的训练框架，旨在加速全模态LLMs的开发。VeOmni 的核心创新和特点包括：\n\n*   **以模型为中心的分布式配方：** VeOmni 引入了“以模型为中心的分布式配方”概念，实现了通信与计算的解耦。\n*   **高效的3D并行化：** 通过这种解耦，VeOmni 能够在全模态LLMs上实现高效的3D并行化。\n*   **灵活的配置接口：** 框架提供了一个灵活的配置接口，支持以最少的代码更改无缝集成新的模态。\n\n## 性能与成果\n\nVeOmni 在实际应用中展现了卓越的效率和可扩展性：\n\n*   **模型规模：** 使用 VeOmni 成功训练了一个参数量达300亿的全模态混合专家（MoE）模型。\n*   **训练吞吐量：** 该模型训练吞吐量超过2,800 tokens/秒/GPU。\n*   **上下文长度与并行规模：** 通过3D并行化，该模型在128个GPU上可扩展到160K的上下文长度。\n*   **总结：** 这些结果充分展示了 VeOmni 在训练大型全模态LLMs方面的卓越效率和可扩展性。",
      "shortSummary": "VeOmni是一个高效的全模态大语言模型（LLMs）训练框架。它通过引入以模型为中心的分布式配方，解耦通信与计算，实现了高效的3D并行化，并支持新模态的灵活集成。VeOmni显著提升了训练效率和可扩展性，例如，一个300亿参数的全模态MoE模型在128个GPU上能达到2,800 tokens/秒/GPU的吞吐量，并支持160K上下文长度。",
      "translated_title": "VeOmni：使用以模型为中心的分布式配方库扩展任意模态模型训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs."
    },
    {
      "title": "CellForge：虚拟细胞模型的智能体设计 (原标题: CellForge: Agentic Design of Virtual Cell Models)",
      "link": "https://arxiv.org/abs/2508.02276",
      "pubDate": "Mon, 04 Aug 2025 06:43:31 GMT",
      "isoDate": "2025-08-04T06:43:31.000Z",
      "creator": "Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein",
      "summary": "## CellForge：虚拟细胞模型的智能体设计\n\n### 引言与背景\n\n虚拟细胞建模是人工智能与生物学交叉领域的新兴前沿，旨在定量预测细胞对各种扰动的响应。然而，由于生物系统的复杂性、数据模态的异质性以及跨多个学科对领域特定专业知识的需求，自主构建虚拟细胞的计算模型极具挑战性。\n\n### CellForge 系统概述\n\nCellForge 是一个创新的智能体系统，它利用多智能体框架，将提供的生物数据集和研究目标直接转化为优化的虚拟细胞计算模型。该系统仅以原始单细胞多组学数据和任务描述作为输入，即可输出优化的模型架构和用于训练虚拟细胞模型及进行推断的可执行代码。\n\n### 核心模块\n\nCellForge 框架集成了三个核心模块：\n\n*   **任务分析 (Task Analysis)**：负责对提供的生物数据集进行特征化，并检索相关的文献信息。\n*   **方法设计 (Method Design)**：在此模块中，专业的智能体协同开发优化的建模策略。设计模块中的智能体分为具有不同视角的专家和一个中央协调者，它们必须协作交换解决方案，直到达成合理的共识。\n*   **实验执行 (Experiment Execution)**：负责自动化生成用于模型训练和推断的代码。\n\n### 能力展示与性能\n\nCellForge 在单细胞扰动预测任务中展示了其强大的能力。研究使用了六个多样化的数据集，涵盖了基因敲除、药物处理和细胞因子刺激等多种模态。CellForge 在这些任务中始终优于特定任务的最新方法。\n\n### 核心洞察与结论\n\nCellForge 的成功表明，具有不同视角的 LLM（大型语言模型）智能体之间的迭代交互，能够比直接解决建模挑战提供更好的解决方案。这突出了多智能体协作在复杂科学问题解决中的潜力。\n\n### 代码可用性\n\nCellForge 的代码已公开可用。",
      "shortSummary": "CellForge是一个多智能体系统，旨在自动化虚拟细胞模型的构建。它接收单细胞多组学数据和任务描述，输出优化的模型架构和可执行代码。系统包含任务分析、方法设计（智能体协作）和实验执行模块。CellForge在单细胞扰动预测等任务中表现出色，超越现有方法，证明了多视角LLM智能体迭代交互能提供更优解决方案。",
      "translated_title": "CellForge：虚拟细胞模型的智能体设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge."
    },
    {
      "title": "LeanK：可学习的K缓存通道剪枝以实现高效解码 (原标题: LeanK: Learnable K Cache Channel Pruning for Efficient Decoding)",
      "link": "https://arxiv.org/abs/2508.02215",
      "pubDate": "Mon, 04 Aug 2025 05:08:43 GMT",
      "isoDate": "2025-08-04T05:08:43.000Z",
      "creator": "Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu",
      "summary": "### 背景与问题\n大型语言模型（LLMs）在处理长上下文任务时，由于键值（KV）缓存的不断增长，面临着显著的效率挑战。\n\n### 解决方案：LeanK\n*   LeanK是一种基于学习的方法，旨在通过利用静态通道稀疏性来剪枝不重要的键（K）缓存通道。\n*   该方法采用新颖的两阶段训练过程，学习通道级的静态掩码，以满足特定的稀疏比和硬件对齐要求。\n\n### 主要优势\n*   显著减少GPU内存占用。\n*   加速解码过程。\n*   在实现上述优势的同时，不牺牲模型的准确性。\n\n### 实验结果\n*   在实验中，LeanK实现了高达70%的K缓存内存减少，以及16%-18%的V缓存内存减少。\n*   通过定制的解码内核，注意力计算速度提升了1.3倍。\n\n### 额外洞察\n*   通过分析学习到的重要性分布，LeanK还为长上下文推理期间的模型通道和注意力头提供了深入见解。\n\n### 代码可用性\n*   相关代码已公开。",
      "shortSummary": "LeanK是一种针对大型语言模型（LLMs）的创新方法，旨在解决长上下文任务中KV缓存效率低下的问题。它通过学习剪枝不重要的K缓存通道，利用静态通道稀疏性来减少GPU内存并加速解码，同时不牺牲准确性。实验显示，K缓存内存减少高达70%，V缓存减少16%-18%，注意力计算速度提升1.3倍。",
      "translated_title": "LeanK：可学习的K缓存通道剪枝以实现高效解码",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK."
    },
    {
      "title": "超越权衡：面向推理模型指令遵循的自监督强化学习 (原标题: Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following)",
      "link": "https://arxiv.org/abs/2508.02150",
      "pubDate": "Mon, 04 Aug 2025 03:48:59 GMT",
      "isoDate": "2025-08-04T03:48:59.000Z",
      "creator": "Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",
      "summary": "# 超越权衡：面向推理模型指令遵循的自监督强化学习\n\n## 摘要\n\n本文提出了一种自监督强化学习（RL）框架，旨在解决推理模型在复杂问题解决能力和指令遵循能力之间存在的权衡问题。\n\n## 核心问题\n\n*   推理模型在复杂问题解决方面表现出色。\n*   然而，这些模型在推理能力和指令遵循能力之间存在一个令人担忧的权衡。\n\n## 现有方法的局限性\n\n*   当前提高指令遵循能力的方法依赖于更强的外部模型。\n*   这导致了方法论上的瓶颈和实际限制，包括成本增加和可访问性受限。\n\n## 提出的解决方案：自监督强化学习框架\n\n*   **方法名称**：自监督强化学习（RL）框架。\n*   **核心思想**：利用推理模型自身的内部信号来改进指令遵循能力。\n*   **关键特点**：无需外部监督。\n\n## 实验结果与优势\n\n*   **效果显著**：广泛的实验证明，该框架显著提高了指令遵循能力。\n*   **性能保持**：在提高指令遵循能力的同时，保持了原有的推理性能。\n*   **实用价值**：提供了一种可扩展且成本效益高的方法，以增强推理模型的指令遵循能力。\n\n## 数据与代码可用性\n\n*   相关数据和代码已公开可用。",
      "shortSummary": "推理模型在推理能力和指令遵循之间存在权衡。现有方法依赖外部模型，成本高昂且受限。本文提出一种自监督强化学习框架，利用模型自身内部信号，无需外部监督，显著提升指令遵循能力，同时保持推理性能。该方法具有可扩展性和成本效益，数据和代码已公开可用。",
      "translated_title": "超越权衡：面向推理模型指令遵循的自监督强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if."
    },
    {
      "title": "SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解 (原标题: SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension)",
      "link": "https://arxiv.org/abs/2508.01959",
      "pubDate": "Sun, 03 Aug 2025 19:59:31 GMT",
      "isoDate": "2025-08-03T19:59:31.000Z",
      "creator": "Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, Mo Yu",
      "summary": "# SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解\n\n## 核心问题与挑战\n在对长文档进行检索增强生成（RAG）时，通常会将文本分割成较小的块进行检索。然而，由于原始文档内部的依赖关系，准确解释每个块往往需要上下文信息。尽管现有工作尝试通过编码更长的上下文窗口来生成更长块的嵌入，但检索和下游任务的性能提升仍然有限。这主要有以下两个原因：\n\n*   **模型容量限制**：更长的文本块增加了嵌入模型需要编码的信息量，从而对其容量造成压力。\n*   **实际应用需求**：许多实际应用场景仍需返回局部证据，以适应模型或人类带宽的限制。\n\n## SitEmb 方法提出\n为了解决上述挑战，本文提出了一种新颖的方法：通过将短文本块的表示与更广泛的上下文窗口进行条件关联，从而增强检索性能。这种方法旨在将文本块的意义“情境化”到其上下文中。\n\n研究发现，现有嵌入模型未能有效编码这种情境化上下文。因此，本文引入了一种新的训练范式，并开发了情境化嵌入模型（SitEmb）。\n\n## 评估与性能\n为了评估SitEmb方法的能力，研究团队专门策划了一个“图书情节检索”数据集，旨在评估情境化检索能力。\n\n*   **SitEmb-v1**：基于BGE-M3模型，参数量仅为10亿，但在该基准测试中显著优于最先进的嵌入模型，包括一些参数量高达70-80亿的模型。\n*   **SitEmb-v1.5**：参数量为80亿，在此基础上进一步将性能提升了10%以上，并在不同语言和多个下游应用中展现出强大的效果。\n\n## 模型可用性\n训练好的模型可从提供的链接下载。",
      "shortSummary": "长文档的检索增强生成（RAG）因文本分块导致上下文丢失而面临挑战。为解决此问题，本文提出了SitEmb模型，通过将短文本块的表示与更广泛的上下文关联，从而增强检索性能。SitEmb-v1（10亿参数）在图书情节检索数据集上显著超越了现有最先进模型（70-80亿参数）。其升级版SitEmb-v1.5（80亿参数）性能进一步提升超10%，并在多语言及多种下游应用中表现出色。",
      "translated_title": "SitEmb-v1.5：改进的上下文感知密集检索，用于语义关联和长篇故事理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications."
    },
    {
      "title": "IAUNet: 实例感知U-Net (原标题: IAUNet: Instance-Aware U-Net)",
      "link": "https://arxiv.org/abs/2508.01928",
      "pubDate": "Sun, 03 Aug 2025 17:36:20 GMT",
      "isoDate": "2025-08-03T17:36:20.000Z",
      "creator": "Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman",
      "summary": "# IAUNet: 实例感知U-Net\n\n## 引言\n实例分割在生物医学成像中至关重要，它能准确区分单个对象，如细胞，而这些细胞通常存在重叠且大小不一。近年来，基于查询（query-based）的方法在引导分割方面表现出色。U-Net作为医学图像分割的常用架构，其在基于查询方法中的潜力尚未得到充分探索。\n\n## IAUNet 模型架构\n本文提出了IAUNet，一种新颖的基于查询的U-Net架构。其核心设计特点包括：\n*   **完整的U-Net架构**：作为模型的基础骨架。\n*   **轻量级卷积像素解码器**：增强了U-Net，使其更高效并减少了参数数量。\n*   **Transformer解码器**：用于在多个尺度上细化对象特定的特征。\n\n## 2025 Revvity 全细胞分割数据集\nIAUNet的开发伴随着一个独特的新资源——2025 Revvity 全细胞分割数据集的发布。\n*   **数据集特点**：该数据集包含亮场图像中重叠细胞细胞质的详细注释。\n*   **意义**：它为生物医学实例分割设定了一个新的基准。\n\n## 实验结果与性能\n研究团队在多个公共数据集和他们自己的数据集上进行了实验。\n*   **性能表现**：IAUNet在细胞实例分割任务中表现出色，超越了大多数最先进的全卷积模型、基于Transformer的模型、基于查询的模型以及专门用于细胞分割的模型。\n*   **基准设定**：IAUNet为细胞实例分割任务设定了一个强大的基线。\n\n## 结论\nIAUNet通过结合U-Net的优势与创新的查询机制和解码器设计，显著提升了生物医学图像中细胞实例分割的性能，并为该领域提供了新的基准数据集。",
      "shortSummary": "IAUNet是一种新颖的基于查询的U-Net架构，专为生物医学图像中的细胞实例分割设计。它解决了细胞重叠和大小差异的挑战，通过结合轻量级像素解码器和Transformer解码器来提高效率和性能。该研究还引入了2025 Revvity全细胞分割数据集。实验证明，IAUNet在多个数据集上均优于现有最先进的模型，为细胞实例分割任务树立了强大的新基线。",
      "translated_title": "IAUNet: 实例感知U-Net",
      "images": [],
      "contentSource": "完整文章",
      "content": "Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet"
    },
    {
      "title": "Web-CogReasoner：迈向知识驱动的Web智能体认知推理 (原标题: Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents)",
      "link": "https://arxiv.org/abs/2508.01858",
      "pubDate": "Sun, 03 Aug 2025 13:17:52 GMT",
      "isoDate": "2025-08-03T13:17:52.000Z",
      "creator": "Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai",
      "summary": "### Web-CogReasoner：迈向知识驱动的Web智能体认知推理\n\n本文探讨了多模态大型模型在推动Web智能体发展方面的显著进步，这些模型使智能体能够像人类一样感知和交互数字环境。作者指出，Web智能体要有效进行认知推理，首先必须获取足够的知识。\n\n**核心论点与框架**\n为了实现这一目标，论文将Web智能体的能力分解为两个基本阶段：\n1.  **知识内容学习 (Knowledge Content Learning)**：对应于智能体的“记忆”（Memorizing）和“理解”（Understanding）过程，主要依赖于事实性知识（Factual）和概念性知识（Conceptual），回答了“是什么”（what）的问题。\n2.  **认知过程 (Cognitive Processes)**：对应于“探索”（Exploring）过程，以程序性知识（Procedural）为基础，定义了推理和行动的“如何做”（how）。\n\n为了形式化这一分解，论文提出了**Web-CogKnowledge框架**，将知识分为事实性、概念性和程序性三类。\n\n**知识获取与数据集**\n为了促进知识获取，研究团队构建了**Web-CogDataset**。这是一个结构化的资源，从14个真实世界的网站中精心策划而来，旨在系统地为Web智能体灌输核心知识。该数据集不仅作为智能体概念基础的“名词”（即理解的基石），也为学习如何推理和行动提供了基础。\n\n**智能体模型**\n在此基础上，论文通过一种新颖的知识驱动的思维链（Chain-of-Thought, CoT）推理框架，将上述过程操作化，并开发和训练了所提出的智能体——**Web-CogReasoner**。\n\n**评估与结果**\n为了进行严格的评估，研究团队引入了**Web-CogBench**，这是一个全面的评估套件，旨在评估和比较智能体在所划分的知识领域和认知能力方面的表现。广泛的实验结果表明，Web-CogReasoner 在性能上显著优于现有模型，尤其是在泛化到结构化知识具有决定性作用的未见任务时，表现出卓越的优势。\n\n**开放资源**\n论文指出，其代码和数据已开源。",
      "shortSummary": "本文提出Web-CogReasoner，旨在通过知识驱动实现Web智能体的认知推理。研究将智能体能力分为知识学习（记忆、理解事实/概念知识）和认知过程（探索程序性知识）。为促进知识获取，构建了Web-CogDataset。Web-CogReasoner采用知识驱动的思维链推理框架，并在Web-CogBench评估套件上表现出显著优于现有模型的泛化能力，尤其在处理新任务时。代码和数据已开源。",
      "translated_title": "Web-CogReasoner：迈向知识驱动的Web智能体认知推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the \"what\" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the \"how\" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner"
    }
  ],
  "lastUpdated": "2025-08-07T09:39:47.087Z"
}