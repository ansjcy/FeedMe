{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "EmbRACE-3K：复杂环境中的具身推理与行动 (原标题: EmbRACE-3K: Embodied Reasoning and Action in Complex Environments)",
      "link": "https://arxiv.org/abs/2507.10548",
      "pubDate": "Mon, 14 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-14T13:59:46.000Z",
      "creator": "Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi",
      "summary": "## EmbRACE-3K：复杂环境中的具身推理与行动\n\n### 引言\n\n近期先进的视觉-语言模型（VLMs）在被动、离线的图像和视频理解任务中展现出强大的性能。然而，它们在具身设置中的有效性仍然有限，这类设置需要在线交互和主动的场景理解。在具身场景中，智能体从第一人称视角感知环境，每个动作都会动态地影响后续的观察。即使是GPT-4o、Claude 3.5 Sonnet和Gemini 2.5 Pro等最先进的模型，在开放环境交互中也表现出明显的局限性，尤其是在空间推理和长程规划方面。\n\n### 解决方案：引入EmRACE-3K数据集\n\n为了弥补这一差距，我们引入了EmRACE-3K数据集，它包含3000多个语言引导任务，这些任务设置在利用虚幻引擎（Unreal Engine）和UnrealCV-Zoo框架构建的各种逼真环境中。\n\n*   **任务范围广泛**：数据集中的任务涵盖了广泛的具身挑战，包括导航、物体操作和多阶段目标执行。\n*   **多步轨迹**：每个任务都以多步轨迹的形式展开，将第一人称视觉观察与高级指令、具身动作以及在每一步表达智能体意图的自然语言理由配对。\n\n### 基准评估\n\n我们利用EmRACE-3K建立了一个基准，以评估VLMs在三个关键维度上的具身推理能力：\n\n1.  **探索（Exploration）**\n2.  **动态空间-语义推理（Dynamic Spatial-Semantic Reasoning）**\n3.  **多阶段目标执行（Multi-stage Goal Execution）**\n\n在零样本（zero-shot）设置下，所有模型的成功率均低于20%，这突显了我们基准测试所带来的挑战以及当前VLMs在交互式环境中的局限性。\n\n### 数据集效用演示\n\n为了展示EmRACE-3K的实用性，我们进一步通过监督学习（supervised learning）和强化学习（reinforcement learning）对Qwen2.5-VL-7B进行了微调。这种方法在所有三个挑战类别中都取得了显著的改进，突显了该数据集在促进具身推理能力发展方面的有效性。",
      "shortSummary": "文章介绍了EmRACE-3K数据集，旨在解决现有视觉-语言模型（VLMs）在复杂具身环境中具身推理和行动能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行。基准测试显示，VLMs在零样本设置下成功率低于20%。通过EmRACE-3K对模型进行微调，能显著提升其在探索、动态空间-语义推理和多阶段目标执行方面的表现，证明了数据集在发展具身推理能力方面的有效性。",
      "translated_title": "EmbRACE-3K：复杂环境中的具身推理与行动",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities."
    },
    {
      "title": "REST：通过同时提出多个问题对大型推理模型进行压力测试 (原标题: REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once)",
      "link": "https://arxiv.org/abs/2507.10541",
      "pubDate": "Mon, 14 Jul 2025 13:58:47 GMT",
      "isoDate": "2025-07-14T13:58:47.000Z",
      "creator": "Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu",
      "summary": "## REST：大型推理模型的压力测试\n\n### 引言：现有评估方法的局限性\n\n当前对大型推理模型（LRMs）的评估主要局限于孤立的问题解决范式，即通过顺序测试评估模型对单个问题的推理能力。这种方法存在以下关键局限性：\n\n*   **数据污染和挑战性不足**：容易受到数据污染，且挑战性较低（例如，DeepSeek-R1在MATH500上达到97.0%），这导致需要投入大量人力和成本持续创建新问题。\n*   **缺乏多上下文压力评估**：未能评估模型在多上下文压力下的表现，而这在现实世界部署中至关重要。\n\n### REST 框架的提出\n\n为了弥补这一空白，研究人员提出了 **REST (Reasoning Evaluation through Simultaneous Testing)**，这是一个对LRMs进行压力测试的框架，它能够同时向模型提出多个问题。\n\n### REST 评估的能力\n\n除了基本的推理能力，REST 还特别评估了几种此前未充分测试的能力：\n\n*   **上下文优先级分配**：模型在多个问题中分配注意力的能力。\n*   **跨问题干扰抵抗**：模型在处理一个问题时，抵抗其他问题干扰的能力。\n*   **动态认知负荷管理**：模型在认知负荷变化时保持性能的能力。\n\n### 关键发现\n\n评估结果揭示了几个显著的发现：\n\n*   **性能显著下降**：即使是像DeepSeek-R1这样的最先进（SOTA）模型，在REST压力测试下也表现出显著的性能下降。\n*   **更强的区分能力**：REST比现有基准测试展现出更强的区分能力，能够揭示在单问题评估中表现相似、接近上限的模型之间明显的性能差异。\n\n### 机制洞察\n\n分析中得出了一些关键的机制洞察：\n\n*   **“过度思考陷阱”**：这是一个导致性能下降的关键因素。\n*   **“long2short”训练技术**：采用“long2short”技术训练的模型在REST测试中能更好地保持其单问题性能的准确性，优于标准训练的模型。\n\n### 结论：REST 的优势\n\n这些结果表明，REST 是一种：\n\n*   **成本效益高**：减少对持续人工标注的依赖。\n*   **面向未来**：能够更好地反映现实世界的推理需求。\n*   **更有效**：提供更全面、更具挑战性的评估范式。",
      "shortSummary": "REST（Reasoning Evaluation through Simultaneous Testing）是一个新颖的压力测试框架，旨在通过同时向大型推理模型（LRMs）提出多个问题，解决现有单问题评估的局限性。它评估模型在多上下文压力下的表现，包括上下文优先级分配和抗干扰能力。研究发现，即使是SOTA模型在REST测试下性能也会显著下降，且REST比现有基准具有更强的区分能力。此外，“过度思考陷阱”是性能下降的原因之一，而“long2short”训练技术有助于模型保持准确性。REST提供了一种更具成本效益且能反映真实世界需求的评估范式。",
      "translated_title": "REST：通过同时提出多个问题对大型推理模型进行压力测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the \"overthinking trap\" is a critical factor contributing to the performance degradation; (2) the models trained with \"long2short\" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation."
    },
    {
      "title": "推理还是记忆？强化学习因数据污染导致结果不可靠 (原标题: Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination)",
      "link": "https://arxiv.org/abs/2507.10532",
      "pubDate": "Mon, 14 Jul 2025 13:55:15 GMT",
      "isoDate": "2025-07-14T13:55:15.000Z",
      "creator": "Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang",
      "summary": "## 强化学习中数据污染导致结果不可靠\n\n### 引言\n\n本文探讨了大型语言模型（LLMs）的推理能力，特别是强化学习（RL）在增强这些能力方面的应用，并指出当前研究中存在的潜在问题。\n\n### 问题与观察\n\n*   **声称的突破与局限性**：近期许多RL方法声称能显著提升LLM的推理能力，甚至在奖励信号随机或不正确的情况下也能实现。然而，这些突破主要集中在Qwen2.5模型家族上，并在MATH-500、AMC和AIME等知名基准上进行评估。令人担忧的是，在Llama等其他模型上未能观察到类似的性能提升，这促使了进一步的深入调查。\n*   **性能差异的疑问**：一些研究甚至提出随机或不正确的奖励信号也能增强推理性能，这与直觉相悖，并加剧了对结果可靠性的质疑。\n\n### 分析与发现\n\n*   **数据污染的风险**：研究分析表明，尽管Qwen2.5在数学推理方面表现出色，但其在大规模网络语料库上的预训练使其容易受到流行基准中数据污染的影响。这意味着模型可能不是真正地进行推理，而是在一定程度上“记忆”了训练数据中包含的基准问题或其解决方案。\n*   **结果的不可靠性**：由于数据污染的存在，从这些受污染基准得出的结果可能并不可靠，无法真实反映RL方法对LLM推理能力的提升效果。\n\n### 解决方案与贡献\n\n*   **生成合成数据集**：为解决数据污染问题，研究引入了一个生成器，能够生成任意长度和难度的完全合成算术问题。这种方法确保了生成的问题是全新的，未曾出现在任何预训练语料或现有基准中。\n*   **构建“RandomCalculation”数据集**：基于此生成器，作者构建了一个名为“RandomCalculation”的干净、无泄露（leakage-free）数据集。这个数据集为评估RL方法提供了一个公正、无偏的环境。\n\n### 关键实验结果\n\n*   **奖励信号的有效性**：使用这些无污染数据集进行评估，研究发现只有准确的奖励信号才能持续且一致地提升性能。这与之前声称随机或不正确奖励信号也能有效的说法形成鲜明对比。\n*   **嘈杂信号的无效性**：实验结果明确指出，嘈杂或不正确的奖励信号并不能带来性能提升，这进一步强调了奖励信号质量的重要性。\n\n### 研究建议\n\n*   **评估标准**：作者强烈倡导在无污染的基准上，并跨越不同的模型家族来评估强化学习方法。\n*   **确保结论可信**：这种严格的评估方法对于确保研究结论的可靠性和可信度至关重要，有助于推动LLM推理能力研究的健康发展。\n\n### 其他信息\n\n本文共26页，属于机器学习（cs.LG）、人工智能（cs.AI）和计算语言学（cs.CL）领域。",
      "shortSummary": "一项研究指出，强化学习（RL）提升大型语言模型（LLM）推理能力的结果可能因数据污染而不可靠。Qwen2.5在流行基准上的出色表现被发现可能源于预训练数据污染，而其他模型未见此提升。为解决此问题，作者创建了无污染的“RandomCalculation”数据集。实验表明，只有准确的奖励信号才能持续提升性能，嘈杂或不正确的信号无效。研究建议在无污染基准和多样模型上评估RL方法，以确保结论可信。",
      "translated_title": "推理还是记忆？强化学习因数据污染导致结果不可靠",
      "images": [],
      "contentSource": "完整文章",
      "content": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions."
    },
    {
      "title": "递归混合体：学习动态递归深度以实现自适应令牌级计算 (原标题: Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation)",
      "link": "https://arxiv.org/abs/2507.10524",
      "pubDate": "Mon, 14 Jul 2025 13:49:00 GMT",
      "isoDate": "2025-07-14T13:49:00.000Z",
      "creator": "Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun",
      "summary": "## 递归混合体 (MoR)：一种高效的语言模型框架\n\n### 核心问题\n\n大型语言模型虽然能力强大，但其训练和部署所需的计算与内存成本高昂。现有的效率提升方法通常侧重于参数共享或自适应计算，但未能同时实现两者。\n\n### 解决方案：递归混合体 (MoR)\n\nMoR 是一种统一的框架，它在一个单一的递归 Transformer 中结合了参数共享和自适应计算这两个效率维度。\n\n### MoR 的工作原理\n\n*   **参数效率**：MoR 通过在递归步骤中重用共享的层堆栈来实现参数效率。\n*   **自适应计算**：轻量级路由器能够动态地为单个令牌分配不同的递归深度，从而实现令牌级的自适应“思考”。\n*   **注意力计算优化**：MoR 仅将二次注意力计算集中在给定递归深度下仍然活跃的令牌上。\n*   **内存访问效率**：通过选择性地缓存仅活跃令牌的键值（KV）对，进一步提高了内存访问效率。\n\n### 额外机制：KV 共享变体\n\nMoR 还提出了一种 KV 共享变体，它重用第一次递归中的 KV 对，专门设计用于减少预填充延迟和内存占用。\n\n### 性能表现\n\nMoR 在 1.35 亿到 17 亿参数的模型规模范围内，形成了一个新的帕累托前沿：\n\n*   在相同的训练 FLOPs 和更小的模型尺寸下，MoR 显著降低了验证困惑度并提高了少样本准确性。\n*   与传统的和现有的递归基线相比，MoR 提供了更高的吞吐量。\n\n### 结论\n\n这些成果表明，MoR 是在不承担大型模型成本的情况下，实现大型模型质量的有效途径。",
      "shortSummary": "递归混合体（MoR）是一种新型框架，旨在解决大型语言模型的高成本问题。它在一个递归 Transformer 中结合了参数共享和自适应计算，通过重用层和动态分配递归深度来提高效率。MoR 优化了注意力计算和内存访问，并引入 KV 共享以减少延迟。实验表明，MoR 在更小的模型尺寸和相同计算量下，显著提升了性能（降低困惑度、提高准确性、增加吞吐量），实现了低成本下的高质量模型。",
      "translated_title": "递归混合体：学习动态递归深度以实现自适应令牌级计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost."
    },
    {
      "title": "MoVieS：一秒内运动感知4D动态视图合成 (原标题: MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second)",
      "link": "https://arxiv.org/abs/2507.10065",
      "pubDate": "Mon, 14 Jul 2025 04:49:57 GMT",
      "isoDate": "2025-07-14T04:49:57.000Z",
      "creator": "Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Honglei Yan, Katerina Fragkiadaki, Yadong Mu",
      "summary": "MoVieS 是一种新颖的前馈模型，它能够在一秒钟内从单目视频合成 4D 动态新视图。\n\n**核心技术与创新点：**\n\n*   **场景表示：** MoVieS 使用高斯基元（Gaussian primitives）的像素对齐网格来表示动态 3D 场景。\n*   **运动监督：** 模型明确地监督这些高斯基元的时变运动，这是其“运动感知”的关键。\n*   **统一建模：** MoVieS 首次实现了外观、几何和运动的统一建模，将这三个关键要素整合到一个框架中。\n\n**多功能性与应用：**\n\n*   **多任务支持：** 在一个单一的基于学习的框架内，MoVieS 能够同时进行视图合成、重建和 3D 点跟踪。\n*   **训练效率：** 通过将新视图合成与动态几何重建相结合，MoVieS 可以在多样化数据集上进行大规模训练，同时对特定任务的监督依赖性极小。\n*   **零样本能力：** 该模型自然支持广泛的零样本应用，例如场景流估计和运动物体分割，无需额外的特定任务训练。\n\n**性能表现：**\n\n*   大量实验验证了 MoVieS 在多项任务中的有效性和效率。\n*   它实现了具有竞争力的性能，同时提供了数量级的速度提升，显著优于现有方法。",
      "shortSummary": "MoVieS是一种创新的前馈模型，能在一秒内从单目视频合成4D动态新视图。它通过高斯基元统一建模外观、几何和运动，并明确监督时变运动。该模型在一个框架内支持视图合成、重建和3D点跟踪，并能进行零样本应用如场景流估计和运动物体分割。实验证明MoVieS高效且性能优异，实现了显著的速度提升。",
      "translated_title": "MoVieS：一秒内运动感知4D动态视图合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups."
    },
    {
      "title": "SpeakerVid-5M：一个用于音视频双向交互式数字人生成的大规模高质量数据集 (原标题: SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation)",
      "link": "https://arxiv.org/abs/2507.09862",
      "pubDate": "Sun, 13 Jul 2025 22:22:47 GMT",
      "isoDate": "2025-07-13T22:22:47.000Z",
      "creator": "Youliang Zhang, Zhaoyang Li, Duomin Wang, Jiahe Zhang, Deyu Zhou, Zixin Yin, Xili Dai, Gang Yu, Xiu Li",
      "summary": "## SpeakerVid-5M：音视频双向交互式数字人生成数据集\n\n### 背景与挑战\n\n随着大规模模型的快速发展，数字人领域取得了显著突破，尤其在虚拟形象驱动和渲染方面实现了高保真解决方案。学术界目前的焦点已转向下一个主要挑战：**音视频双向交互式虚拟人**的生成。\n\n### SpeakerVid-5M 数据集介绍\n\n为了促进这一新兴领域的研究，本文提出了 **SpeakerVid-5M** 数据集，这是首个专为音视频双向交互式虚拟人生成设计的大规模、高质量数据集。\n\n*   **规模宏大**：数据集总时长超过 **8,743 小时**，包含超过 **520 万个**人类肖像视频片段。\n*   **内容多样**：涵盖了多种尺度和交互类型，包括单人说话（monadic talking）、听众反应（listening）以及双向对话（dyadic conversations）。\n\n### 数据集结构与特点\n\nSpeakerVid-5M 数据集在两个关键维度上进行了结构化设计：**交互类型**和**数据质量**。\n\n1.  **基于交互场景的分类（交互类型）**：\n    *   **对话分支 (dialogue branch)**\n    *   **单人分支 (single branch)**\n    *   **听众分支 (listening branch)**\n    *   **多轮分支 (multi-turn branch)**\n\n2.  **基于数据质量的分层（数据质量）**：\n    *   一个**大规模预训练子集**，用于基础模型的训练。\n    *   一个经过精心策划的**高质量子集**，专为监督微调（Supervised Fine-Tuning, SFT）设计。\n\n这种双重结构能够支持广泛的2D虚拟人任务。\n\n### 附加贡献与基准\n\n除了数据集本身，研究团队还提供了以下资源：\n\n*   一个基于该数据训练的**自回归（AR）视频聊天基线模型**。\n*   一套专门的**评估指标和测试数据**，共同构成 **VidChatBench**，作为未来研究的基准。\n\n### 可用性\n\nSpeakerVid-5M 数据集及其相应的数据处理代码将**公开发布**，以促进社区研究。\n\n*   **项目页面**：提供更多详细信息（请注意，文章中提供的URL为占位符，此处不展示具体链接）。\n*   **研究领域**：计算机视觉与模式识别 (cs.CV)；音频与语音处理 (eess.AS)。",
      "shortSummary": "SpeakerVid-5M是首个大规模、高质量的音视频双向交互式数字人生成数据集。它包含超过8,743小时、520万个视频片段，涵盖单人、双向等多种交互类型。数据集按交互场景和数据质量分层，包括预训练和SFT子集，支持多样化的2D虚拟人任务。项目还提供了基于该数据的AR视频聊天基线模型和VidChatBench基准测试集。数据集及处理代码将公开发布，旨在推动相关领域研究。",
      "translated_title": "SpeakerVid-5M：一个用于音视频双向交互式数字人生成的大规模高质量数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/"
    },
    {
      "title": "CompassJudger-2：通过可验证奖励迈向通用评判模型 (原标题: CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards)",
      "link": "https://arxiv.org/abs/2507.09104",
      "pubDate": "Fri, 11 Jul 2025 21:34:24 GMT",
      "isoDate": "2025-07-11T21:34:24.000Z",
      "creator": "Taolin Zhang, Maosong Cao, Alexander Lam, Songyang Zhang, Kai Chen",
      "summary": "### CompassJudger-2：通过可验证奖励迈向通用评判模型\n\n**引言与背景**\n\n*   近年来，大型语言模型（LLM）作为评判者（LLM-as-judge）在评估其他LLM方面的重要性日益凸显。\n*   然而，当前的评判模型普遍存在专业化狭隘和鲁棒性不足的问题，这限制了它们进行全面评估的能力。\n\n**CompassJudger-2 核心方法**\n\n*   本文提出了 CompassJudger-2，一个新型的通用评判模型，旨在克服现有模型的局限性。\n*   **数据策略**：采用任务驱动、多领域的数据整理策略。\n*   **监督机制**：通过可验证奖励（verifiable rewards）监督评判任务，引导内在批判性推理。\n*   **训练优化**：通过拒绝采样（rejection sampling）来培养鲁棒的、可泛化的评判能力。\n*   **学习目标**：引入了带有边际策略梯度损失（margin policy gradient loss）的改进学习目标，以增强模型性能。\n\n**实验结果与性能**\n\n*   CompassJudger-2 在多个评判和奖励基准测试中取得了卓越的成果。\n*   其7B模型展现出与DeepSeek-V3和Qwen3-235B-A22B等显著更大的模型相媲美的评判准确性。\n\n**JudgerBenchV2 新基准**\n\n*   为了标准化评判模型的评估，研究团队还提出了 JudgerBenchV2。\n*   这是一个综合性基准，用于评估跨领域评判准确性和排名一致性。\n\n**贡献与影响**\n\n*   这些贡献推动了鲁棒、可扩展的LLM评判技术的发展。\n*   为评判模型的性能和评估树立了新的标准。",
      "shortSummary": "CompassJudger-2是一个新型通用评判模型，旨在解决现有LLM评判模型专业化狭隘和鲁棒性不足的问题。它采用任务驱动的多领域数据整理策略，并通过可验证奖励和拒绝采样来指导批判性推理。该模型在多个基准测试中表现卓越，其7B版本能与大型模型媲美。此外，研究还提出了JudgerBenchV2，一个用于标准化评估的综合基准。这些进展为LLM评判技术树立了新标准。",
      "translated_title": "CompassJudger-2：通过可验证奖励迈向通用评判模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards."
    },
    {
      "title": "Lumos-1：从统一模型视角看自回归视频生成 (原标题: Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective)",
      "link": "https://arxiv.org/abs/2507.08801",
      "pubDate": "Fri, 11 Jul 2025 13:59:42 GMT",
      "isoDate": "2025-07-11T13:59:42.000Z",
      "creator": "Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang",
      "summary": "## Lumos-1：从统一模型视角看自回归视频生成\n\n### 引言\n大型语言模型（LLMs）在统一各种语言任务方面取得了巨大成功，这启发了自回归视频生成领域的初步探索。然而，现有的自回归视频生成器面临一些挑战，包括：\n*   偏离标准的LLM架构。\n*   依赖笨重的外部文本编码器。\n*   由于逐token解码导致高昂的延迟。\n\n### Lumos-1 介绍\n本文提出了 **Lumos-1**，一个自回归视频生成器。Lumos-1 的核心设计理念是在保持LLM架构的同时，进行最小的架构修改，以克服现有方法的局限性。\n\n### 核心创新点\nLumos-1 引入了多项创新来有效建模时空数据并优化训练过程：\n\n1.  **MM-RoPE (Multimodal Rotational Positional Embedding)**\n    *   **问题识别**：为了在LLMs中注入时空相关性，研究人员发现3D RoPE是有效的，但其频率频谱范围存在不平衡问题。\n    *   **解决方案**：Lumos-1 提出了 MM-RoPE。该方案在保留原始文本RoPE的同时，为建模多模态时空数据提供了全面的频率频谱和缩放的3D位置，从而更有效地捕捉视频的时空信息。\n\n2.  **Token 依赖策略**\n    *   Lumos-1 采用了一种精巧的token依赖策略，该策略遵循 **帧内双向性**（intra-frame bidirectionality）和 **帧间时间因果性**（inter-frame temporal causality）。这种策略有助于模型理解视频帧内部的空间关系以及帧之间的时间演变。\n\n3.  **自回归离散扩散强制 (AR-DF)**\n    *   **问题识别**：基于上述token依赖策略，研究人员发现由于空间信息冗余，存在帧级损失不平衡的问题。\n    *   **解决方案**：Lumos-1 通过提出 AR-DF 来解决这一问题。AR-DF 在训练期间引入了 **时间管掩码**（temporal tube masking），并采用了一种兼容的推理时掩码策略，以避免在生成过程中出现质量下降。\n\n### 训练与性能\nLumos-1 的训练效率显著：\n*   通过使用内存高效的训练技术，Lumos-1 仅在48个GPU上进行了预训练。\n*   尽管训练资源相对较少，Lumos-1 仍取得了令人印象深刻的性能：\n    *   在 GenEval 基准测试上，其性能与 EMU3 相当。\n    *   在 VBench-I2V（图像到视频）任务上，其性能与 COSMOS-Video2World 相当。\n    *   在 VBench-T2V（文本到视频）任务上，其性能与 OpenSoraPlan 相当。\n\n### 可用性\nLumos-1 的代码和模型已公开。",
      "shortSummary": "Lumos-1是一个创新的自回归视频生成器，它在保持LLM架构的同时，解决了现有方法的局限性。该模型引入了MM-RoPE以有效建模时空相关性，并提出了AR-DF来解决帧级损失不平衡问题。Lumos-1通过内存高效的训练，仅使用48个GPU就达到了与EMU3、COSMOS-Video2World和OpenSoraPlan等先进模型相当的性能，展示了其高效性和竞争力。",
      "translated_title": "Lumos-1：从统一模型视角看自回归视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos."
    },
    {
      "title": "NeuralOS：通过神经生成模型模拟操作系统 (原标题: NeuralOS: Towards Simulating Operating Systems via Neural Generative Models)",
      "link": "https://arxiv.org/abs/2507.08800",
      "pubDate": "Fri, 11 Jul 2025 13:59:40 GMT",
      "isoDate": "2025-07-11T13:59:40.000Z",
      "creator": "Luke Rivard, Sun Sun, Hongyu Guo, Wenhu Chen, Yuntian Deng",
      "summary": "## NeuralOS 简介\n\nNeuralOS 是一个创新的神经框架，旨在通过直接预测屏幕帧来模拟操作系统的图形用户界面（GUI），以响应用户的鼠标移动、点击和键盘事件等输入。\n\n### 核心组成与工作原理\n\n*   **状态跟踪与图像生成**：NeuralOS 结合了两种关键组件：\n    *   **循环神经网络（RNN）**：负责跟踪计算机的内部状态。\n    *   **基于扩散的神经渲染器**：用于生成屏幕图像。\n*   **训练数据**：该模型在一个大规模的 Ubuntu XFCE 录制数据集上进行训练。这些录制数据包含了随机生成的交互以及由 AI 代理产生的真实交互。\n\n### 实验成果与能力\n\n实验结果表明，NeuralOS 在以下方面表现出色：\n\n*   **真实 GUI 序列渲染**：成功渲染出逼真的 GUI 序列。\n*   **精确的鼠标交互捕获**：能够准确捕捉并响应鼠标交互。\n*   **可靠的状态转换预测**：能够可靠地预测状态转换，例如应用程序的启动。\n\n### 挑战与未来展望\n\n尽管在精确建模细粒度的键盘交互方面仍面临挑战，但 NeuralOS 的出现标志着向未来人机交互系统迈出了重要一步，有望创建出完全自适应、生成式的神经界面。\n\n### 相关领域\n\n该研究涉及多个计算机科学领域，包括：\n\n*   计算机视觉与模式识别 (cs.CV)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   人机交互 (cs.HC)\n*   机器学习 (cs.LG)",
      "shortSummary": "NeuralOS 是一个神经框架，通过预测屏幕帧来模拟操作系统的图形用户界面（GUI），响应用户输入。它结合了循环神经网络（RNN）和基于扩散的神经渲染器，并在大规模 Ubuntu XFCE 数据集上进行训练。该模型能成功渲染逼真 GUI 序列，准确捕获鼠标交互并预测状态转换。尽管键盘交互仍是挑战，NeuralOS 为未来自适应生成式人机交互界面奠定了基础。",
      "translated_title": "NeuralOS：通过神经生成模型模拟操作系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce NeuralOS, a neural framework that simulates graphical user interfaces (GUIs) of operating systems by directly predicting screen frames in response to user inputs such as mouse movements, clicks, and keyboard events. NeuralOS combines a recurrent neural network (RNN), which tracks computer state, with a diffusion-based neural renderer that generates screen images. The model is trained on a large-scale dataset of Ubuntu XFCE recordings, which include both randomly generated interactions and realistic interactions produced by AI agents. Experiments show that NeuralOS successfully renders realistic GUI sequences, accurately captures mouse interactions, and reliably predicts state transitions like application launches. Although modeling fine-grained keyboard interactions precisely remains challenging, NeuralOS offers a step toward creating fully adaptive, generative neural interfaces for future human-computer interaction systems."
    },
    {
      "title": "KV缓存转向：在小型语言模型中诱导推理 (原标题: KV Cache Steering for Inducing Reasoning in Small Language Models)",
      "link": "https://arxiv.org/abs/2507.08799",
      "pubDate": "Fri, 11 Jul 2025 13:59:36 GMT",
      "isoDate": "2025-07-11T13:59:36.000Z",
      "creator": "Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano",
      "summary": "### 核心提案：缓存转向 (Cache Steering)\n\n*   **定义与机制：** 缓存转向是一种轻量级方法，通过对语言模型的键值（KV）缓存进行“一次性”（one-shot）干预，实现对模型行为的隐式引导。\n\n### 应用与目标\n\n*   **主要应用：** 该方法被应用于在小型语言模型中诱导思维链（chain-of-thought）推理。\n*   **目标：** 在不进行模型微调或修改提示词的情况下，促使模型生成更显式、多步的推理过程。\n\n### 工作原理\n\n*   **转向向量构建：** 缓存转向利用GPT-4o生成的推理轨迹来构建“转向向量”（steering vectors）。\n*   **行为调整：** 这些转向向量直接作用于KV缓存，从而将模型的行为模式转向更倾向于显式、多步骤的推理。\n\n### 主要优势与性能\n\n*   **无需微调或提示词修改：** 显著降低了实施复杂性，无需对模型架构或输入方式进行根本性改变。\n*   **性能提升：** 在多样化的推理基准测试中，实验评估表明缓存转向不仅改善了模型推理的定性结构（即推理过程的清晰度和逻辑性），还提高了量化任务性能（即最终答案的准确性）。\n*   **与现有技术的比较：**\n    *   **一次性干预：** 与需要持续干预的现有激活转向（activation steering）技术相比，缓存转向仅需一次性干预，大大简化了操作。\n    *   **鲁棒性与效率：** 这种一次性干预的特性带来了超参数稳定性、推理时间效率和集成便捷性方面的显著优势。\n    *   **实用性：** 使得缓存转向成为一种更鲁棒、更实用的受控生成解决方案。",
      "shortSummary": "“缓存转向”是一种轻量级方法，通过对键值（KV）缓存进行一次性干预，在小型语言模型中诱导思维链推理。该方法利用GPT-4o生成的推理轨迹构建转向向量，无需微调或修改提示词，即可将模型行为转向更显式、多步的推理。实验表明，它能提升推理质量和任务性能，且相比传统激活转向技术，在效率和鲁棒性方面更具优势，是一种实用的受控生成解决方案。",
      "translated_title": "KV缓存转向：在小型语言模型中诱导推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation."
    },
    {
      "title": "从KMMLU-Redux到KMMLU-Pro：一个用于LLM评估的专业韩语基准测试套件 (原标题: From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation)",
      "link": "https://arxiv.org/abs/2507.08924",
      "pubDate": "Fri, 11 Jul 2025 13:56:32 GMT",
      "isoDate": "2025-07-11T13:56:32.000Z",
      "creator": "Seokhee Hong, Sunkyoung Kim, Guijin Son, Soyeon Kim, Yeonjung Hong, Jinsik Lee",
      "summary": "### 介绍KMMLU-Redux和KMMLU-Pro：用于LLM评估的专业韩语基准测试套件\n\n**引言**\n\n大语言模型（LLM）的开发需要强大的基准测试，这些基准不仅要涵盖学术领域，还要包括工业领域，以有效评估其在实际场景中的适用性。\n\n**新基准介绍**\n\n本文介绍了两个韩语专家级基准测试，旨在满足上述需求：\n\n*   **KMMLU-Redux**\n    *   从现有KMMLU重建而来。\n    *   包含来自韩国国家技术资格考试的问题。\n    *   已移除关键错误，以增强其可靠性。\n\n*   **KMMLU-Pro**\n    *   基于韩国国家专业执照考试。\n    *   旨在反映韩国的专业知识。\n\n**实验结果与数据可用性**\n\n实验表明，这些基准测试能够全面代表韩国的工业知识。作者已将数据集公开发布。",
      "shortSummary": "本文介绍了两个新的韩语专家级大语言模型（LLM）评估基准：KMMLU-Redux和KMMLU-Pro。KMMLU-Redux基于韩国国家技术资格考试并纠正了错误，而KMMLU-Pro则源于韩国国家专业执照考试，旨在反映专业知识。这些基准旨在全面评估LLM在韩国工业和专业领域的实际应用能力。数据集已公开发布。",
      "translated_title": "从KMMLU-Redux到KMMLU-Pro：一个用于LLM评估的专业韩语基准测试套件",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available."
    },
    {
      "title": "一个Token即可欺骗作为评判者的LLM (原标题: One Token to Fool LLM-as-a-Judge)",
      "link": "https://arxiv.org/abs/2507.08794",
      "pubDate": "Fri, 11 Jul 2025 13:55:22 GMT",
      "isoDate": "2025-07-11T13:55:22.000Z",
      "creator": "Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu",
      "summary": "## 摘要：一个Token即可欺骗作为评判者的LLM\n\n### 背景与问题\n\n生成式奖励模型（也称为“作为评判者的LLM”，即LLM-as-a-judge）在可验证奖励强化学习（RLVR）中日益普及。它们通常优于僵化的基于规则的度量标准，尤其适用于涉及自由形式输出的复杂推理任务。在这种范式中，LLM通常被提示将候选答案与真实参考进行比较，并分配一个二进制奖励以指示正确性。\n\n### 核心发现：LLM-as-a-judge的脆弱性\n\n尽管这种比较任务看似简单，但研究发现生成式奖励模型对表面操作表现出惊人的脆弱性：\n\n*   **具体表现**：非单词符号（例如“:”或“.”）或推理引导词（如“Thought process:”和“Let's solve this problem step by step.”）\n*   **结果**：这些简单的操作常常导致假阳性奖励。\n*   **影响范围**：这种弱点在不同的LLM、数据集和提示格式中普遍存在。\n*   **潜在威胁**：这对依赖生成式奖励模型的核心算法范式（如拒绝采样、偏好优化和RLVR）构成了严重威胁。\n\n### 解决方案与成果\n\n为了缓解这个问题，研究人员引入了一种简单而有效的数据增强策略，并训练了一个新的生成式奖励模型。\n\n*   **成果**：新模型表现出显著提高的鲁棒性。\n\n### 结论与展望\n\n这些发现凸显了对更可靠的基于LLM的评估方法的迫切需求。研究团队已发布其鲁棒的通用领域奖励模型及其合成训练数据。",
      "shortSummary": "研究发现，用于评估答案质量的“作为评判者的LLM”（LLM-as-a-judge）模型存在严重漏洞。简单的非单词符号或推理引导词（如“Thought process:”）就能导致模型给出错误的正面评价。这种脆弱性普遍存在，对依赖这些模型的强化学习范式构成威胁。为解决此问题，研究引入了一种数据增强策略，并训练出更鲁棒的奖励模型，强调了开发更可靠LLM评估方法的紧迫性。",
      "translated_title": "一个Token即可欺骗作为评判者的LLM",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., \":\" or \".\") or reasoning openers like \"Thought process:\" and \"Let's solve this problem step by step.\" can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at https://huggingface.co/sarosavo/Master-RM and https://huggingface.co/datasets/sarosavo/Master-RM."
    },
    {
      "title": "CLiFT：用于计算高效和自适应神经渲染的压缩光场令牌 (原标题: CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering)",
      "link": "https://arxiv.org/abs/2507.08776",
      "pubDate": "Fri, 11 Jul 2025 13:38:52 GMT",
      "isoDate": "2025-07-11T13:38:52.000Z",
      "creator": "Zhengqing Wang, Yuefan Wu, Jiacheng Chen, Fuyang Zhang, Yasutaka Furukawa",
      "summary": "### CLiFT：用于计算高效和自适应神经渲染的压缩光场令牌\n\n本文提出了一种名为“压缩光场令牌（CLiFTs）”的神经渲染方法，旨在高效且自适应地表示场景。\n\n**核心概念与优势：**\n\n*   **信息保留：** CLiFTs能够保留场景丰富的外观和几何信息。\n*   **计算效率：** 通过使用压缩令牌，实现了计算高效的渲染。\n*   **自适应性：** 允许在单个训练网络下，根据计算预算（即CLiFTs的数量）灵活地改变用于表示场景的令牌数量，或渲染新的视角。\n\n**CLiFTs的构建过程：**\n\n1.  **多视角编码器：** 给定一组输入图像，多视角编码器利用相应的相机姿态对这些图像进行令牌化处理。\n2.  **潜在空间K-means：** 基于生成的令牌，通过潜在空间K-means算法选择一组数量减少的光线作为聚类中心。\n3.  **多视角“冷凝器”：** 一个多视角“冷凝器”将所有原始令牌的信息压缩到这些选定的中心令牌中，从而构建最终的CLiFTs。\n\n**测试时渲染机制：**\n\n*   在测试阶段，给定一个目标视角和预设的计算预算（即允许使用的CLiFTs数量）。\n*   系统会收集指定数量的邻近令牌。\n*   随后，一个计算自适应渲染器利用这些令牌合成出新的视角。\n\n**实验验证：**\n\n*   研究人员在RealEstate10K和DL3DV数据集上进行了广泛的定量和定性实验。\n*   实验结果验证了CLiFT方法的有效性，表明它在实现显著数据缩减的同时，保持了可比的渲染质量。\n*   该方法取得了最高的整体渲染分数。\n*   CLiFT在数据大小、渲染质量和渲染速度之间提供了灵活的权衡，使其适用于不同的应用需求。",
      "shortSummary": "CLiFT是一种用于神经渲染的压缩光场令牌方法，旨在高效且自适应地表示场景。它通过多视角编码器、潜在空间K-means和“冷凝器”构建压缩令牌。CLiFTs能保留丰富的场景信息，实现计算高效的渲染，并允许在单个网络下灵活调整令牌数量以适应计算预算。实验证明，CLiFT在数据缩减、渲染质量和速度之间提供了良好平衡，并在RealEstate10K和DL3DV数据集上取得了优异表现。",
      "translated_title": "CLiFT：用于计算高效和自适应神经渲染的压缩光场令牌",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed."
    },
    {
      "title": "从一到多：用于3D生成的上下文部分潜在表示 (原标题: From One to More: Contextual Part Latents for 3D Generation)",
      "link": "https://arxiv.org/abs/2507.08772",
      "pubDate": "Fri, 11 Jul 2025 13:33:18 GMT",
      "isoDate": "2025-07-11T13:33:18.000Z",
      "creator": "Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, Dan Xu",
      "summary": "# CoPart：一种面向多部件3D生成的上下文感知扩散框架\n\n## 引言\n\n近期3D生成技术取得了显著进展，已从多视图2D渲染方法转向利用真实数据中几何先验的3D原生潜在扩散框架。然而，现有方法仍存在以下三个主要局限性：\n\n## 现有方法的局限性\n\n1.  **单一体素表示的不足：** 无法有效捕捉复杂的多部件几何结构，导致细节退化。\n2.  **整体潜在编码的缺陷：** 忽略了部件的独立性和相互关系，而这些对于组合设计至关重要。\n3.  **全局条件机制的限制：** 缺乏细粒度的可控性。\n\n## CoPart框架提案\n\n受人类3D设计工作流程的启发，我们提出了 **CoPart**——一个部件感知的扩散框架。该框架将3D对象分解为上下文部分潜在表示，以实现连贯的多部件生成。这种范式带来了以下三个显著优势：\n\n*   **降低编码复杂性：** 通过部件分解简化了编码过程。\n*   **显式部件关系建模：** 能够明确地建模部件之间的相互关系。\n*   **支持部件级条件控制：** 提供了更精细的控制能力。\n\n## 互引导策略与数据集\n\n我们进一步开发了一种**互引导策略**，用于微调预训练的扩散模型，以实现联合部件潜在表示去噪，从而确保几何连贯性和基础模型先验的一致性。\n\n为了支持大规模训练，我们构建了 **Partverse**——一个新颖的3D部件数据集。该数据集通过自动化网格分割和人工验证标注，从Objaverse中提取而来。\n\n## 实验结果\n\n广泛的实验证明，CoPart在部件级编辑、铰接对象生成和场景组合方面展现出卓越的能力，并提供了前所未有的可控性。",
      "shortSummary": "CoPart是一种新型的3D生成框架，旨在解决现有方法在处理复杂多部件对象时的局限性。它通过将3D对象分解为上下文部分潜在表示，实现了部件级控制、降低了编码复杂性并能显式建模部件关系。CoPart引入了互引导策略进行模型微调，并构建了Partverse数据集支持训练。实验证明，CoPart在部件编辑、铰接对象生成和场景组合方面表现出色，提供了前所未有的可控性。",
      "translated_title": "从一到多：用于3D生成的上下文部分潜在表示",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability."
    },
    {
      "title": "BlockFFN：面向端侧加速友好的块级激活稀疏混合专家模型 (原标题: BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity)",
      "link": "https://arxiv.org/abs/2507.08771",
      "pubDate": "Fri, 11 Jul 2025 13:28:56 GMT",
      "isoDate": "2025-07-11T13:28:56.000Z",
      "creator": "Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun",
      "summary": "# BlockFFN：面向端侧加速友好的块级激活稀疏混合专家模型\n\n## 概述\nBlockFFN是一种新颖的混合专家（MoE）架构，旨在解决大型语言模型（LLMs）的计算负担，特别是在低资源（如端侧）设备上的加速问题。它通过引入创新的路由机制、块级稀疏性感知训练目标以及高效的加速核，显著提升了MoE模型的性能和部署友好性。\n\n## 挑战与问题\n传统的MoE架构虽然通过激活稀疏性减轻了计算负担，但存在以下问题：\n\n*   **路由机制缺陷：** 香草MoE的路由是非可微且不灵活的，这会损害模型性能。\n*   **块级稀疏性不足：** 尽管每个token只激活少数参数（token级稀疏性，TLS），但多个连续token的联合激活会导致大量参数被激活，即块级稀疏性（CLS）较低。这种稀疏模式不利于低资源条件下的加速，且与主流加速技术（如推测解码）不兼容。\n\n## BlockFFN的解决方案\n为应对上述挑战，BlockFFN引入了以下关键创新：\n\n### 1. 可微且灵活的路由\n*   BlockFFN采用了一个集成了ReLU激活和RMSNorm的路由器，实现了可微且灵活的路由，从而提升了模型性能。\n\n### 2. 促进块级稀疏性（CLS）\n*   为了同时促进token级稀疏性（TLS）和块级稀疏性（CLS），BlockFFN设计了CLS感知的训练目标。这使得模型在端侧设备上更易于加速。\n\n### 3. 高效加速核\n*   BlockFFN首次将激活稀疏性与推测解码相结合，实现了高效的加速核。这对于在实际端侧设备上部署LLMs至关重要。\n\n## 实验结果与性能\n实验结果表明，BlockFFN在性能上优于其他MoE基线模型：\n\n*   **稀疏性表现：** 实现了超过80%的token级稀疏性（TLS）和70%的8-token块级稀疏性（CLS）。\n*   **加速效果：** 其加速核在实际端侧设备上比密集模型实现了高达3.67倍的加速。\n\n## 可用性\n所有代码和检查点均已公开提供。",
      "shortSummary": "BlockFFN是一种新型混合专家（MoE）架构，旨在提升大型语言模型在端侧设备的加速效率。它通过引入可微路由、块级稀疏性感知训练目标，并首次将激活稀疏性与推测解码结合，解决了传统MoE的性能和部署挑战。实验证明，BlockFFN实现了高token/块级稀疏性，并在端侧设备上比密集模型提速高达3.67倍，使其更适合低资源环境。",
      "translated_title": "BlockFFN：面向端侧加速友好的块级激活稀疏混合专家模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67times speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN)."
    },
    {
      "title": "视觉基础模型作为自回归图像生成的有效视觉分词器 (原标题: Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation)",
      "link": "https://arxiv.org/abs/2507.08441",
      "pubDate": "Fri, 11 Jul 2025 05:32:45 GMT",
      "isoDate": "2025-07-11T05:32:45.000Z",
      "creator": "Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, Xiaojuan Qi",
      "summary": "# VFMTok：基于视觉基础模型的图像分词器\n\n本文探索了一种新颖的方法，利用预训练的视觉基础模型（传统上用于视觉理解）来构建图像分词器。这一领域此前研究较少。\n\n## 核心方法\n\n该研究提出了一种名为 VFMTok 的图像分词器，其核心设计包括：\n\n*   **编码器：** 采用冻结的视觉基础模型作为分词器的编码器。\n*   **关键组件：** 为增强分词器的有效性，引入了两个关键组件：\n    1.  **区域自适应量化框架：** 用于减少预训练特征在常规 2D 网格上的冗余。\n    2.  **语义重建目标：** 旨在使分词器的输出与基础模型的表示对齐，以保持语义保真度。\n\n## 主要成果与性能\n\n基于上述设计，VFMTok 在多个方面取得了显著改进：\n\n*   **图像重建与生成质量：** 实现了大幅提升。\n*   **分词效率：** 显著增强。\n*   **自回归（AR）生成：**\n    *   在 ImageNet 基准测试中，gFID 达到 2.07。\n    *   模型收敛速度加快三倍。\n    *   无需分类器自由指导（CFG）即可实现高保真度的类别条件合成。\n\n## 社区贡献\n\n该项目的代码将公开发布，以惠及社区。",
      "shortSummary": "本文提出VFMTok，一种基于冻结视觉基础模型的新型图像分词器，用于自回归图像生成。它通过区域自适应量化和语义重建目标，显著提升了图像重建和生成质量，并提高了分词效率。VFMTok在ImageNet上实现了2.07的gFID，将模型收敛速度加快三倍，并支持无需CFG的高保真类别条件合成。代码将公开发布。",
      "translated_title": "视觉基础模型作为自回归图像生成的有效视觉分词器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Leveraging the powerful representations of pre-trained vision foundation models -- traditionally used for visual comprehension -- we explore a novel direction: building an image tokenizer directly atop such models, a largely underexplored area. Specifically, we employ a frozen vision foundation model as the encoder of our tokenizer. To enhance its effectiveness, we introduce two key components: (1) a region-adaptive quantization framework that reduces redundancy in the pre-trained features on regular 2D grids, and (2) a semantic reconstruction objective that aligns the tokenizer's outputs with the foundation model's representations to preserve semantic fidelity. Based on these designs, our proposed image tokenizer, VFMTok, achieves substantial improvements in image reconstruction and generation quality, while also enhancing token efficiency. It further boosts autoregressive (AR) generation -- achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model convergence by three times, and enabling high-fidelity class-conditional synthesis without the need for classifier-free guidance (CFG). The code will be released publicly to benefit the community."
    },
    {
      "title": "数学LLM的实用两阶段方案：通过SFT最大化准确性，通过强化学习提高效率 (原标题: A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.08267",
      "pubDate": "Thu, 10 Jul 2025 22:26:01 GMT",
      "isoDate": "2025-07-10T22:26:01.000Z",
      "creator": "Hiroshi Yoshihara, Taiki Yamaguchi, Yuichi Inoue",
      "summary": "## 数学LLM的实用两阶段方案\n\n### 引言\n提升大型语言模型（LLM）的数学推理能力是推动人工智能发展的关键挑战。尽管监督微调（SFT）和强化学习（RL）是主流的训练范式，但如何系统地结合它们以同时最大化准确性和效率，仍是一个有待深入探索的领域。\n\n### 提出的方案\n本研究提出了一种实用且有效的两阶段训练方案，该方案战略性地整合了扩展的监督微调（SFT）与来自在线推理的强化学习（GRPO）。\n\n*   **核心理念**：研究认为SFT和RL扮演着互补而非竞争的角色。\n    *   **第一阶段（SFT）**：通过长时间的SFT阶段，首先将模型的准确性推向其极限。\n    *   **第二阶段（GRPO）**：随后，GRPO阶段在保持这一峰值性能的同时，显著提高token效率。\n\n### 实验结果与发现\n\n*   **SFT的重要性**：实验表明，将SFT扩展至多达10个epoch对于实现性能突破至关重要。\n*   **GRPO的作用**：在此框架中，GRPO的主要作用是优化解决方案的长度（即提高token效率）。\n\n### 性能验证\n该方案的有效性通过在挑战性基准测试中取得顶级表现得到了严格验证，包括在严格无泄露的AI数学奥林匹克（AIMO）竞赛中，从超过2200支队伍中脱颖而出，获得高排名。\n\n### 贡献与可复现性\n这项工作为社区提供了一个经过实战检验的蓝图，用于开发既准确又高效的先进数学推理器。为确保完全可复现性并赋能未来的研究，研究团队将开源整个框架，包括所有代码、模型检查点和训练配置。\n\n### 会议信息\n本研究已在ICML 2025第二届AI for MATH研讨会上发表。",
      "shortSummary": "本研究提出一种数学LLM的实用两阶段训练方案，旨在同时最大化准确性和效率。方案首先通过扩展的监督微调（SFT）提升准确性（SFT多达10个epoch至关重要），随后利用在线推理强化学习（GRPO）在保持性能的同时提高token效率，主要优化解决方案长度。该方案在挑战性基准测试中表现卓越，并在AI数学奥林匹克（AIMO）中获得高排名。研究将开源其框架，为开发先进数学推理器提供实战蓝图。",
      "translated_title": "数学LLM的实用两阶段方案：通过SFT最大化准确性，通过强化学习提高效率",
      "images": [],
      "contentSource": "完整文章",
      "content": "Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at https://github.com/analokmaus/kaggle-aimo2-fast-math-r1."
    },
    {
      "title": "可追溯证据增强的视觉基础推理：评估与方法 (原标题: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology)",
      "link": "https://arxiv.org/abs/2507.07999",
      "pubDate": "Thu, 10 Jul 2025 13:59:58 GMT",
      "isoDate": "2025-07-10T13:59:58.000Z",
      "creator": "Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang",
      "summary": "## 可追溯证据增强的视觉基础推理：评估与方法\n\n### 引言\n\n当前，像OpenAI-o3这样的模型通过动态引用视觉区域，在视觉基础推理方面取得了突破，这类似于人类的“图像思考”能力。然而，目前缺乏一个能够全面评估这些能力的基准。\n\n### TreeBench：诊断性基准\n\n为弥补这一空白，研究人员提出了 **TreeBench (Traceable Evidence Evaluation Benchmark)**，这是一个诊断性基准，其构建基于以下三个核心原则：\n\n1.  **聚焦于复杂场景中细微目标的视觉感知**：强调对难以察觉目标的识别能力。\n2.  **通过边界框评估实现可追溯证据**：确保推理过程中的视觉证据是可定位和可验证的。\n3.  **二阶推理**：测试模型超越简单目标定位，理解对象间交互和空间层次的能力。\n\n**构建过程与挑战：**\n\n*   TreeBench优先选择包含密集对象的图像，最初从SA-1B数据集中采样了1000张高质量图像。\n*   八位大型多模态模型（LMM）专家手动标注了每张图像的问题、候选选项和答案。\n*   经过三阶段的严格质量控制，TreeBench最终包含405个极具挑战性的视觉问答对。\n*   **模型表现**：即使是最先进的模型也难以应对TreeBench，没有一个模型的准确率能达到60%，例如OpenAI-o3的得分仅为54.87%。\n\n### TreeVGR：可追溯证据增强的视觉基础推理训练范式\n\n除了基准，研究还引入了 **TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning)**，这是一种新的训练范式：\n\n*   **方法**：它通过强化学习共同监督定位和推理过程，旨在实现精确的定位和可解释的推理路径。\n*   **初始化与效果**：TreeVGR以Qwen2.5-VL-7B为基础进行初始化，并在多个基准测试中展现出显著提升：\n    *   V* Bench：提升16.8个百分点\n    *   MME-RealWorld：提升12.6个百分点\n    *   TreeBench：提升13.4个百分点\n*   **核心发现**：这些结果证明，可追溯性是推动视觉基础推理进步的关键。\n\n### 结论与代码可用性\n\n该研究通过提出TreeBench基准和TreeVGR训练范式，为视觉基础推理的评估和发展提供了新的方向。代码已公开。",
      "shortSummary": "本研究针对视觉基础推理领域缺乏全面评估基准的问题，提出了TreeBench，一个包含405个挑战性视觉问答对的诊断性基准，强调细微感知、可追溯证据和二阶推理。现有先进模型在该基准上表现不佳。此外，研究还引入了TreeVGR训练范式，通过强化学习联合监督定位和推理，显著提升了模型在多个基准上的性能。研究表明，可追溯性是推动视觉基础推理进步的关键。",
      "translated_title": "可追溯证据增强的视觉基础推理：评估与方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR."
    },
    {
      "title": "PyVision：具有动态工具的智能体视觉 (原标题: PyVision: Agentic Vision with Dynamic Tooling)",
      "link": "https://arxiv.org/abs/2507.07998",
      "pubDate": "Thu, 10 Jul 2025 13:59:55 GMT",
      "isoDate": "2025-07-10T13:59:55.000Z",
      "creator": "Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei",
      "summary": "## PyVision：具有动态工具的智能体视觉\n\n### 引言\n\n大型语言模型（LLMs）正日益被部署为智能体，这些系统能够进行规划、推理并动态调用外部工具。然而，在视觉推理领域，现有方法大多受限于预定义的工作流程和静态工具集，这限制了其灵活性和适应性。\n\n### PyVision 框架概述\n\nPyVision 是一个创新性的交互式、多轮次框架，旨在解决传统视觉推理方法的局限性。其核心功能在于：\n\n*   **自主工具生成与执行**：PyVision 使得多模态大型语言模型（MLLMs）能够自主地生成、执行和完善基于 Python 的工具。\n*   **任务定制化**：这些生成的工具是根据当前任务量身定制的，从而实现了高度灵活且可解释的问题解决能力。\n\n### 研究方法与分析\n\n为了全面评估 PyVision 的能力，研究团队进行了以下工作：\n\n*   **工具分类体系**：开发了一套 PyVision 所创建工具的分类体系，有助于理解其内部机制和应用范围。\n*   **基准测试分析**：分析了这些工具在各种多样化基准测试中的使用情况，以验证其有效性和泛化能力。\n\n### 量化性能提升\n\n实验结果表明，PyVision 带来了显著且持续的性能提升：\n\n*   **GPT-4.1 提升**：在 V* 基准测试中，PyVision 使 GPT-4.1 的性能提升了 **+7.8%**。\n*   **Claude-4.0-Sonnet 提升**：在 VLMsAreBlind-mini 基准测试中，PyVision 使 Claude-4.0-Sonnet 的性能大幅提升了 **+31.1%**。\n\n### 深远意义\n\n这些量化结果指向了一个更广泛、更重要的转变：\n\n*   **从“使用”到“发明”**：动态工具不仅允许模型使用现有工具，更重要的是，它赋予了模型“发明”新工具的能力。\n*   **推动智能体视觉推理**：这一能力上的飞跃，标志着智能体视觉推理领域迈向了更高级、更自主的阶段。\n\n### 其他信息\n\n*   本报告共26页，并包含10张图表（注：文章内容未提供具体的图片链接）。\n*   研究主题涵盖：计算与语言（cs.CL）、人工智能（cs.AI）和计算机视觉与模式识别（cs.CV）。\n*   引用信息：arXiv:2507.07998。",
      "shortSummary": "PyVision 是一个创新框架，使多模态大型语言模型（MLLMs）能够自主生成、执行和优化定制化的 Python 工具，以进行灵活且可解释的视觉推理。它解决了传统方法中静态工具集的局限性。实验证明，PyVision 显著提升了 GPT-4.1 和 Claude-4.0-Sonnet 在视觉基准测试上的表现。这项工作表明，动态工具允许模型“发明”而非仅仅使用工具，从而推动了智能体视觉推理的发展。",
      "translated_title": "PyVision：具有动态工具的智能体视觉",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning."
    },
    {
      "title": "跳过一层还是循环它？预训练大型语言模型在测试时期的深度自适应 (原标题: Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs)",
      "link": "https://arxiv.org/abs/2507.07996",
      "pubDate": "Thu, 10 Jul 2025 13:59:53 GMT",
      "isoDate": "2025-07-10T13:59:53.000Z",
      "creator": "Ziyue Li, Yang Li, Tianyi Zhou",
      "summary": "## 预训练大型语言模型在测试时期的深度自适应：跳过层或循环层\n\n### 核心问题\n\n文章探讨了预训练神经网络在不进行微调的情况下，能否根据不同输入自适应其架构。具体而言，它质疑了对于简单任务是否需要所有层，以及对于复杂任务现有层是否足够。\n\n### 提出的解决方案：层链 (CoLa)\n\n研究发现，预训练大型语言模型（LLM）的层可以被视为独立的模块进行操作，从而为每个测试样本构建一个更优甚至更浅的模型。这种操作包括：\n\n*   **跳过/剪枝层：** 对于某些输入，可以跳过不必要的层。\n*   **重复层（循环神经网络式）：** 同一层可以被重复多次，类似于循环神经网络（RNN）的行为。\n\n这些操作允许层以任意顺序堆叠，为每个样本形成一个独特的“层链”（Chain-of-Layers, CoLa）。这种组合空间极大地扩展了现有关于循环/递归预训练模块、层剪枝或提前退出网络的工作范围。\n\n### 探索与优化方法\n\n研究开发了一种**蒙特卡洛树搜索（MCTS）协议**，用于探索并识别数学和常识推理基准中每个样本的最佳CoLa配置。\n\n### CoLa的优势\n\n与固定深度的静态模型相比，CoLa提供了更灵活、动态的架构，以适应不同的输入：\n\n*   **快捷路径（快速思考）：** 允许跳过层，实现更快的推理。\n*   **相同层的重复（慢速思考）：** 允许重复使用层，进行更深入的计算。\n*   **结合两者：** 能够根据任务需求灵活地结合快速和慢速思考模式。\n\n### 关键发现\n\n对MCTS优化的CoLa进行了广泛分析，得出了两个关键发现：\n\n1.  **推理效率提升：** 对于原始LLM预测正确的样本中，超过75%的样本可以找到更短的CoLa。这表明在提高推理效率方面存在巨大潜力。\n2.  **性能增强：** 对于原始LLM预测错误的样本中，超过60%的样本通过CoLa实现了正确的预测。这表明在提升模型性能方面存在巨大潜力。\n\n### 结论与展望\n\n研究结果强调了使用固定架构的预训练LLM在处理不同样本时的局限性，并为解锁测试时深度自适应的泛化能力铺平了道路。",
      "shortSummary": "本文提出“层链”（CoLa）方法，允许预训练LLM在测试时动态调整架构，通过跳过或重复层来适应不同输入。利用蒙特卡洛树搜索（MCTS）优化CoLa，研究发现：对于原始LLM预测正确的样本，CoLa能显著提高推理效率；对于预测错误的样本，CoLa能提升性能。这表明固定架构LLM存在局限性，测试时深度自适应有望解锁其更强的泛化能力。",
      "translated_title": "跳过一层还是循环它？预训练大型语言模型在测试时期的深度自适应",
      "images": [],
      "contentSource": "完整文章",
      "content": "Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For &gt;75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For &gt;60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation."
    }
  ],
  "lastUpdated": "2025-07-15T09:39:18.947Z"
}