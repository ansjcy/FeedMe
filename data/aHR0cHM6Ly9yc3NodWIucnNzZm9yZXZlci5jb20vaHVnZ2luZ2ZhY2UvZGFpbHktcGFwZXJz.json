{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Echo-4o：利用GPT-4o合成图像的力量改进图像生成 (原标题: Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation)",
      "link": "https://arxiv.org/abs/2508.09987",
      "pubDate": "Wed, 13 Aug 2025 13:59:28 GMT",
      "isoDate": "2025-08-13T13:59:28.000Z",
      "creator": "Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li",
      "summary": "### Echo-4o：利用GPT-4o合成图像的力量改进图像生成\n\n#### 背景与挑战\n\n近期，GPT-4o在图像生成方面展现出强大性能，但开源模型仍显滞后。尽管已有研究探索从GPT-4o中提取图像数据以增强开源模型并取得显著进展，但一个核心问题依然存在：既然真实世界图像数据集已是高质量数据的天然来源，为何还要使用GPT-4o生成的合成数据？\n\n#### 合成图像的优势\n\n本研究识别出合成图像的两个关键优势：\n\n1.  **补充稀有场景**：合成图像能够补充真实世界数据集中罕见的场景，例如用户查询中频繁出现的超现实幻想或多参考图像生成。\n2.  **提供干净且可控的监督信号**：真实世界数据常包含复杂的背景噪声以及文本描述与图像内容之间固有的不一致性。相比之下，合成图像提供纯净的背景和长尾监督信号，有助于实现更精确的文本到图像对齐。\n\n#### Echo-4o-Image数据集与Echo-4o模型\n\n基于这些洞察，研究团队引入了 **Echo-4o-Image**，这是一个由GPT-4o生成的18万规模的合成数据集。该数据集旨在利用合成图像数据的力量，弥补真实世界数据覆盖的盲点。\n\n利用此数据集，研究人员对统一多模态生成基线模型 Bagel 进行了微调，从而获得了 **Echo-4o** 模型。\n\n#### 新型评估基准\n\n为了更准确、更具挑战性地评估图像生成能力，研究团队提出了两个新的评估基准：\n\n1.  **GenEval++**：通过增加指令复杂性来缓解分数饱和问题。\n2.  **Imagine-Bench**：专注于评估想象内容的理解和生成能力。\n\n#### 实验结果与贡献\n\n*   **Echo-4o** 在标准基准测试中表现出强大的性能。\n*   将 **Echo-4o-Image** 应用于其他基础模型（例如 OmniGen2、BLIP3-o）时，在多个指标上均能持续获得性能提升，这凸显了该数据集强大的可迁移性。",
      "shortSummary": "为弥补开源图像生成模型与GPT-4o的差距，本研究强调合成图像在补充稀有场景和提供干净监督方面的优势。研究团队构建了18万规模的GPT-4o合成数据集Echo-4o-Image，并基于此微调得到Echo-4o模型。同时，提出了GenEval++和Imagine-Bench两个新评估基准。实验结果显示，Echo-4o在标准测试中表现出色，且Echo-4o-Image数据集能有效提升其他基础模型的性能，展现了其强大的可迁移性。",
      "translated_title": "Echo-4o：利用GPT-4o合成图像的力量改进图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability."
    },
    {
      "title": "Story2Board：一种无需训练的富有表现力的故事板生成方法 (原标题: Story2Board: A Training-Free Approach for Expressive Storyboard Generation)",
      "link": "https://arxiv.org/abs/2508.09983",
      "pubDate": "Wed, 13 Aug 2025 13:56:26 GMT",
      "isoDate": "2025-08-13T13:56:26.000Z",
      "creator": "David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, Dani Lischinski",
      "summary": "# Story2Board：一种无需训练的富有表现力的故事板生成方法\n\n## 概述\nStory2Board 是一种无需训练的框架，旨在从自然语言中生成富有表现力的故事板。该方法解决了现有故事板生成方法在视觉叙事方面存在的局限性，即过度关注主体身份而忽略了空间构图、背景演变和叙事节奏等关键要素。\n\n## 现有方法的局限性\n当前的故事板生成方法主要集中于保持主体身份的一致性，但未能充分考虑视觉叙事中的其他重要方面，例如：\n*   **空间构图（Spatial Composition）**：场景中元素和角色的布局。\n*   **背景演变（Background Evolution）**：背景在不同面板之间如何自然过渡和变化。\n*   **叙事节奏（Narrative Pacing）**：通过视觉呈现来控制故事的进展和情感张力。\n\n这些缺失导致生成的故事板在视觉多样性和叙事连贯性方面表现不足。\n\n## Story2Board 框架\nStory2Board 引入了一个轻量级的连贯性框架，包含两个核心组件，旨在增强故事板的视觉连贯性和表现力，而无需进行架构更改或模型微调：\n\n1.  **潜在面板锚定（Latent Panel Anchoring）**\n    *   **目的**：在不同故事板面板之间保持共享的角色参考。这意味着即使角色姿态或场景发生变化，其身份和核心特征也能保持一致。\n    *   **机制**：通过在潜在空间中锚定关键特征，确保角色在整个叙事中的视觉连续性。\n\n2.  **互惠注意力值混合（Reciprocal Attention Value Mixing）**\n    *   **目的**：柔和地混合具有强互惠注意力的令牌对之间的视觉特征。这有助于在不同元素之间建立更强的视觉关联和过渡。\n    *   **机制**：通过分析和混合注意力机制中的高相关性区域，促进视觉特征的平滑融合，从而增强场景的整体连贯性。\n\n这两个机制协同作用，使得最先进的扩散模型能够生成视觉多样化且高度一致的故事板。\n\n## 生成结构\n为了有效地构建故事板生成过程，Story2Board 利用一个现成的语言模型。该语言模型负责将自由形式的叙事文本转换为具体到每个面板的提示（grounded panel-level prompts）。这种方法确保了生成内容与原始故事叙事紧密对齐，并为扩散模型提供了清晰的指导。\n\n## 评估方法与结果\n为了全面评估 Story2Board 的性能，研究团队提出了新的评估工具和指标：\n\n1.  **丰富故事板基准（Rich Storyboard Benchmark）**\n    *   **目的**：这是一个包含开放域叙事的套件，专门用于评估故事板的布局多样性、背景接地叙事能力以及整体连贯性。\n    *   **特点**：超越了单一的主体一致性评估，更全面地考量视觉叙事质量。\n\n2.  **场景多样性指标（Scene Diversity metric）**\n    *   **目的**：量化故事板中空间和姿态的变化。\n    *   **作用**：确保生成的故事板不仅连贯，而且在视觉上具有足够的动态性和变化性，避免重复和僵化。\n\n通过定性分析、定量结果以及用户研究，Story2Board 的表现优于现有基线方法。研究表明，Story2Board 能够生成更具动态性、更连贯且叙事更引人入胜的故事板。\n\n## 结论\nStory2Board 提供了一种无需训练的创新方法，通过引入潜在面板锚定和互惠注意力值混合等机制，有效解决了现有故事板生成方法在视觉叙事方面的局限性。它不仅提升了故事板的连贯性，还增强了其视觉多样性和叙事吸引力，为未来的故事板自动化生成奠定了坚实基础。",
      "shortSummary": "Story2Board 是一种无需训练的框架，用于生成富有表现力的故事板。它解决了现有方法在空间构图、背景演变和叙事节奏方面的不足。通过引入“潜在面板锚定”保持角色一致性，以及“互惠注意力值混合”增强视觉连贯性，Story2Board 使扩散模型能生成多样且一致的故事板。评估显示，该方法比现有基线能生成更具动态性、连贯性和叙事吸引力的故事板。",
      "translated_title": "Story2Board：一种无需训练的富有表现力的故事板生成方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines."
    },
    {
      "title": "噪声超网络：分摊扩散模型中的测试时计算 (原标题: Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models)",
      "link": "https://arxiv.org/abs/2508.09968",
      "pubDate": "Wed, 13 Aug 2025 13:33:37 GMT",
      "isoDate": "2025-08-13T13:33:37.000Z",
      "creator": "Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata",
      "summary": "### 噪声超网络：分摊扩散模型中的测试时计算\n\n**背景与问题**\n\n*   **测试时扩展范式**：在大型语言模型（LLMs）和生成式视觉模型中取得了显著进展，允许模型在推理时分配额外计算以解决日益复杂的问题。\n*   **主要限制**：尽管有这些改进，但计算时间的大幅增加使得该过程变得缓慢，对于许多应用而言不切实际。\n\n**提出的解决方案：噪声超网络**\n\n*   **目标**：在保留测试时扩展优势的同时，避免其带来的推理开销。\n*   **核心方法**：引入“噪声超网络”（Noise Hypernetwork）来取代扩散模型中奖励引导的测试时噪声优化。\n*   **工作原理**：噪声超网络通过调制初始输入噪声来发挥作用。\n\n**理论框架与实现**\n\n*   **理论基础**：提出一个理论上扎实的框架，用于学习蒸馏生成器的“奖励倾斜分布”（reward-tilted distribution）。\n*   **优化目标**：通过一个可处理的噪声空间目标来实现，该目标在优化所需特性的同时，保持了对基础模型的忠实度。\n\n**成果与优势**\n\n*   **效率提升**：该方法能够以一小部分计算成本，恢复显式测试时优化所获得的绝大部分质量提升。\n*   **实际意义**：在不牺牲性能的情况下，显著提高了扩散模型的推理效率和实用性。",
      "shortSummary": "测试时扩展在LLMs和生成模型中提高了性能，但计算成本高昂。本文提出“噪声超网络”作为解决方案，通过调制初始输入噪声来替代扩散模型中的奖励引导优化。该方法提供了一个理论框架，能在保持模型保真度的同时，以极低的计算成本恢复大部分测试时优化的质量增益，从而显著提高推理效率。",
      "translated_title": "噪声超网络：分摊扩散模型中的测试时计算",
      "images": [],
      "contentSource": "完整文章",
      "content": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise"
    },
    {
      "title": "VisCodex：通过融合视觉和编码模型实现统一的多模态代码生成 (原标题: VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models)",
      "link": "https://arxiv.org/abs/2508.09945",
      "pubDate": "Wed, 13 Aug 2025 13:00:44 GMT",
      "isoDate": "2025-08-13T13:00:44.000Z",
      "creator": "Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei",
      "summary": "## VisCodex：统一的多模态代码生成框架\n\n### 摘要\n\n多模态大型语言模型（MLLMs）在整合视觉和文本理解方面取得了显著进展，但其从多模态输入生成代码的能力仍然有限。本文介绍了 **VisCodex**，一个统一的框架，旨在通过无缝融合视觉和编码语言模型，增强 MLLMs 的多模态代码生成能力。\n\n### 核心贡献与方法\n\n*   **VisCodex 框架**：\n    *   **模型融合**：利用基于任务向量的模型融合技术，将最先进的编码大型语言模型（LLM）集成到强大的视觉-语言骨干网络中。\n    *   **能力保留**：在融合过程中，VisCodex 能够同时保留模型的视觉理解能力和高级编码技能。\n\n*   **多模态编码数据集（MCD）**：\n    *   **规模与多样性**：引入了一个大规模且多样化的数据集，包含 59.8 万个样本。\n    *   **数据类型**：包括高质量的 HTML 代码、图表图像-代码对、图像增强的 StackOverflow 问答以及算法问题。\n    *   **用途**：该数据集旨在支持模型的训练和评估。\n\n*   **InfiBench-V 基准**：\n    *   **新颖性与挑战性**：提出了一个新颖且具有挑战性的基准测试。\n    *   **评估目标**：专门设计用于评估模型在视觉丰富、真实世界编程问题上的表现。\n    *   **理解要求**：这些问题需要模型对文本和视觉上下文有细致入微的理解。\n\n### 实验结果\n\n广泛的实验表明，VisCodex 在开源 MLLMs 中取得了最先进的性能，并接近 GPT-4o 等专有模型的水平。这突出显示了其模型融合策略和新数据集的有效性。\n\n### 相关领域\n\n*   计算与语言（cs.CL）\n*   人工智能（cs.AI）\n*   计算机视觉与模式识别（cs.CV）",
      "shortSummary": "VisCodex 是一个统一框架，通过融合视觉和编码模型，显著提升了多模态大型语言模型（MLLMs）的代码生成能力。它利用基于任务向量的模型融合技术，并引入了包含59.8万样本的多模态编码数据集（MCD）和用于评估视觉丰富编程问题的 InfiBench-V 基准。实验证明，VisCodex 在开源 MLLMs 中表现最佳，并接近 GPT-4o 等专有模型，验证了其方法和数据集的有效性。",
      "translated_title": "VisCodex：通过融合视觉和编码模型实现统一的多模态代码生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets."
    },
    {
      "title": "AWorld：具有稳定操控能力的动态多智能体系统，用于鲁棒的GAIA问题解决 (原标题: AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving)",
      "link": "https://arxiv.org/abs/2508.09889",
      "pubDate": "Wed, 13 Aug 2025 11:46:25 GMT",
      "isoDate": "2025-08-13T11:46:25.000Z",
      "creator": "Zhitian Xie, Qintong Wu, Chengyue Yu, Chenyi Zhuang, Jinjie Gu",
      "summary": "## AWorld：具有稳定操控能力的动态多智能体系统\n\n### 引言\n\n随着大型语言模型（LLMs）的快速发展，智能体已能利用各种外部工具解决复杂的现实世界问题。然而，智能体对多工具的日益依赖带来了新的挑战：来自不同来源的扩展上下文以及嘈杂或不相关的工具输出，这些都可能损害系统的可靠性和准确性。这些挑战凸显了增强基于智能体系统稳定性的必要性。\n\n### 解决方案：动态监督与操控机制\n\n为了解决上述问题，研究人员在AWorld框架内引入了动态监督和操控机制，构建了一个鲁棒且动态的多智能体系统（MAS）架构。其核心方法如下：\n\n*   **执行智能体与守卫智能体协作**：在AWorld的方法中，执行智能体（Execution Agent）会在关键步骤调用守卫智能体（Guard Agent）。\n*   **验证与纠正**：守卫智能体的作用是验证并纠正推理过程，从而有效减少因噪声引起的错误，并显著增强问题解决的鲁棒性。\n\n### 实验结果与成就\n\n*   **性能提升**：在GAIA测试数据集上进行的广泛实验表明，AWorld的动态操控机制显著提高了解决方案的有效性和稳定性。\n*   **超越现有系统**：该系统表现优于单智能体系统（SAS）和标准工具增强系统。\n*   **GAIA排行榜首位**：AWorld的动态MAS系统在著名的GAIA排行榜上，于开源项目中取得了第一名的成绩。\n\n### 结论\n\n这些研究发现强调了协作智能体角色在开发更可靠、更值得信赖的智能系统方面的实际价值。",
      "shortSummary": "AWorld引入了一个动态多智能体系统（MAS），通过执行智能体在关键步骤调用守卫智能体进行验证和纠正，以解决大型语言模型（LLMs）在多工具使用中面临的上下文冗长和噪声输出问题。该系统显著提高了问题解决的鲁棒性、有效性和稳定性，在GAIA数据集上表现优异，并荣登GAIA开源项目排行榜首位，证明了协作智能体在构建可靠智能系统中的价值。",
      "translated_title": "AWorld：具有稳定操控能力的动态多智能体系统，用于鲁棒的GAIA问题解决",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems."
    },
    {
      "title": "LLM生成的文本解释能否提升模型分类性能？一项实证研究 (原标题: Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study)",
      "link": "https://arxiv.org/abs/2508.09776",
      "pubDate": "Wed, 13 Aug 2025 08:59:08 GMT",
      "isoDate": "2025-08-13T08:59:08.000Z",
      "creator": "Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci",
      "summary": "## LLM生成的文本解释能否提升模型分类性能？一项实证研究\n\n### 核心问题与背景\n\n*   在快速发展的可解释自然语言处理（Explainable NLP）领域，文本解释（即类人理性解释）对于理解模型预测和丰富数据集具有关键作用。\n*   然而，传统的解释生成方法依赖于人工标注，这导致成本高昂、劳动密集且难以实现规模化。\n\n### 研究目标与方法\n\n*   **目标：** 本研究旨在提出一个自动化框架，利用多个最先进的大型语言模型（LLMs）来生成高质量的文本解释，以克服人工标注的局限性。\n*   **解释质量评估：** 研究团队使用一套全面的自然语言生成（NLG）指标，对LLM生成的解释质量进行了严格评估。\n*   **下游影响研究：** 进一步，研究调查了这些LLM生成的解释对预训练语言模型（PLMs）和LLMs在自然语言推理任务上的下游性能影响。\n*   **实验数据：** 实验在两个多样化的基准数据集上进行。\n\n### 主要发现\n\n*   实验结果表明，与人工标注的解释相比，自动化生成的解释在提升模型性能方面表现出高度的竞争力。\n\n### 结论与意义\n\n*   本研究的发现强调了利用LLM进行可扩展、自动化文本解释生成的一个有前景的方向。\n*   这为扩展NLP数据集和进一步增强模型性能提供了一条新的途径。\n\n### 其他信息\n\n*   该研究已被第34届国际人工神经网络会议（ICANN 2025）接受。\n*   研究主题包括计算与语言（cs.CL）和人工智能（cs.AI）。",
      "shortSummary": "该研究提出一个自动化框架，利用大型语言模型（LLMs）生成文本解释，以解决传统人工标注成本高、难扩展的问题。通过评估LLM生成解释的质量，并测试其对预训练语言模型（PLMs）和LLMs在自然语言推理任务上的性能影响，研究发现自动化解释在提升模型性能方面与人工标注解释具有高度竞争力。这为可扩展地利用LLM扩展NLP数据集和增强模型性能提供了新途径。",
      "translated_title": "LLM生成的文本解释能否提升模型分类性能？一项实证研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance."
    },
    {
      "title": "看、听、记忆与推理：一种具备长期记忆的多模态智能体 (原标题: Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory)",
      "link": "https://arxiv.org/abs/2508.09736",
      "pubDate": "Wed, 13 Aug 2025 08:03:03 GMT",
      "isoDate": "2025-08-13T08:03:03.000Z",
      "creator": "Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, Wei Li",
      "summary": "# M3-Agent：一种具备长期记忆的多模态智能体\n\n本文介绍了 **M3-Agent**，一个新颖的多模态智能体框架，其核心特点是配备了长期记忆能力。M3-Agent 旨在模拟人类的认知过程，能够处理实时的视觉和听觉输入，并以此构建和更新其长期记忆。\n\n## M3-Agent 的核心特性\n\n*   **多模态输入处理**：M3-Agent 能够像人类一样处理实时的视觉和听觉输入，从而感知和理解复杂的环境。\n*   **长期记忆构建与更新**：\n    *   它不仅能建立情景记忆（episodic memory），记录特定事件和经历，还能发展语义记忆（semantic memory），从而随着时间积累世界知识和通用概念。\n    *   其记忆以实体为中心、多模态的形式组织，有助于对环境形成更深入、更一致的理解，并支持跨模态信息的整合。\n*   **自主推理能力**：\n    *   M3-Agent 能够根据给定的指令，自主执行多轮、迭代推理。\n    *   它能高效地从其长期记忆中检索相关信息，以支持决策并完成复杂的任务。\n\n## M3-Bench：评估多模态智能体的基准\n\n为了全面评估多模态智能体中记忆的有效性和基于记忆的推理能力，研究人员开发了 **M3-Bench**，这是一个新的长视频问答基准。\n\n*   **组成部分**：\n    *   **M3-Bench-robot**：包含 100 个从机器人视角拍摄的全新真实世界视频，模拟机器人与环境的交互。\n    *   **M3-Bench-web**：包含 929 个来自网络、涵盖多种场景的视频，提供多样化的日常情境。\n*   **问答对设计**：M3-Bench 中的问答对经过精心设计，旨在测试智能体应用所需的关键能力，包括：\n    *   **人类理解**：评估智能体对视频中人类行为、意图和情感的理解能力。\n    *   **通用知识提取**：测试智能体从视频内容中提取并应用通用世界知识的能力。\n    *   **跨模态推理**：考察智能体整合视觉和听觉信息进行复杂推理的能力。\n\n## 实验结果与性能\n\n通过强化学习训练的 M3-Agent 在实验中表现出色，超越了现有最强的基线模型（包括使用 Gemini-1.5-pro 和 GPT-4o 的提示智能体）。\n\n*   **准确率提升**：\n    *   在 M3-Bench-robot 上，M3-Agent 的准确率提高了 6.7%。\n    *   在 M3-Bench-web 上，准确率提高了 7.7%。\n    *   在 VideoMME-long（另一个评估基准）上，准确率提高了 5.3%。\n\n## 贡献与展望\n\n这项工作显著推动了多模态智能体向更类人的长期记忆能力发展，并为其在实际应用中的设计提供了深刻见解。研究团队已公开模型、代码和数据，以促进后续研究和开发。",
      "shortSummary": "M3-Agent是一种新型多模态智能体框架，具备类人长期记忆能力。它能处理实时视听输入，构建情景与语义记忆，并进行多轮推理以完成任务。为评估其记忆与推理能力，研究者开发了M3-Bench长视频问答基准。实验结果显示，M3-Agent在M3-Bench及VideoMME-long等基准测试中显著优于现有最强模型，准确率分别提升6.7%、7.7%和5.3%，标志着多模态智能体在长期记忆方面取得重要进展。",
      "translated_title": "看、听、记忆与推理：一种具备长期记忆的多模态智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent"
    },
    {
      "title": "IAG：面向视觉定位的VLM输入感知后门攻击 (原标题: IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding)",
      "link": "https://arxiv.org/abs/2508.09456",
      "pubDate": "Tue, 12 Aug 2025 23:22:19 GMT",
      "isoDate": "2025-08-12T23:22:19.000Z",
      "creator": "Junxian Li, Beining Xu, Di Zhang",
      "summary": "## IAG：面向视觉定位的VLM输入感知后门攻击\n\n### 概述\n\n本文介绍了IAG（Input-aware Backdoor Attack），一种新颖的输入感知后门攻击方法，旨在操纵视觉语言模型（VLMs）在视觉定位任务中的行为。该攻击迫使模型将输入图像中的特定目标对象作为定位结果，而无论用户的查询内容是什么。\n\n### 攻击动机与挑战\n\n*   **动机**：视觉语言模型在视觉定位等任务中取得了显著进展，但其安全问题，特别是后门攻击，仍未得到充分探索。\n*   **挑战**：传统的后门攻击难以应对开放词汇（open-vocabulary）场景，即攻击目标描述的多样性。\n\n### IAG方法的核心组件\n\n1.  **自适应触发器生成器（Adaptive Trigger Generator）**：\n    *   利用文本条件U-Net将攻击目标描述的语义信息嵌入到原始图像中。\n    *   这一机制有效克服了开放词汇攻击的挑战，使得攻击能够针对任意指定的语义目标。\n2.  **隐蔽性保障**：\n    *   引入重建损失（reconstruction loss），以最小化中毒图像与原始干净图像之间的视觉差异。\n    *   这确保了攻击的隐蔽性，使得中毒图像在视觉上难以被察觉。\n3.  **统一的攻击数据生成方法**：\n    *   提出了一种统一的方法来生成攻击所需的数据集，简化了攻击实施的流程。\n\n### 评估与结果\n\n*   **可行性与有效性**：IAG在理论和经验上都得到了验证，证明了其可行性和有效性。\n*   **攻击成功率（ASR）**：\n    *   在InternVL-2.5-8B模型上，IAG在各种测试集上的ASR@0.5（攻击成功率在IoU阈值为0.5时）达到了65%以上。\n*   **模型泛化性**：\n    *   IAG在操纵Ferret-7B和LlaVA-1.5-7B等模型上也显示出良好的潜力，同时对干净样本的准确性下降非常小。\n*   **鲁棒性与可迁移性**：\n    *   通过广泛的特定实验，如消融研究（ablation study）和潜在防御措施的评估，表明了IAG攻击的鲁棒性和可迁移性。",
      "shortSummary": "本文提出IAG，一种针对视觉语言模型（VLMs）视觉定位任务的新型输入感知后门攻击。IAG通过自适应触发器生成器（利用文本条件U-Net）将攻击目标语义信息隐蔽地嵌入图像，迫使模型无论用户查询如何，都定位特定目标。为确保隐蔽性，IAG采用重建损失最小化视觉差异。实验证明，IAG在InternVL-2.5-8B上ASR@0.5超过65%，且对Ferret-7B和LlaVA-1.5-7B等模型有效，同时对干净样本的准确性影响极小，展现出良好的鲁棒性和可迁移性。",
      "translated_title": "IAG：面向视觉定位的VLM输入感知后门攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack."
    },
    {
      "title": "时间是一种特征：利用扩散语言模型中的时间动态 (原标题: Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models)",
      "link": "https://arxiv.org/abs/2508.09138",
      "pubDate": "Tue, 12 Aug 2025 13:59:57 GMT",
      "isoDate": "2025-08-12T13:59:57.000Z",
      "creator": "Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen",
      "summary": "### 核心问题：扩散语言模型中的时间振荡\n\n*   **背景：** 扩散大型语言模型（dLLMs）通过迭代去噪生成文本。然而，当前的解码策略通常会丢弃丰富的中间预测，只保留最终输出。\n*   **发现：** 本研究揭示了一个关键现象——“时间振荡”（temporal oscillation）。这意味着正确的答案经常在去噪过程的中间阶段出现，但在后续的去噪步骤中被覆盖或改写。\n\n### 解决方案：利用时间动态\n\n为了解决时间振荡问题，本研究引入了两种互补的方法，旨在利用时间一致性：\n\n1.  **时间自洽投票（Temporal Self-Consistency Voting, TSCV）：**\n    *   这是一种无需训练、在测试时使用的解码策略。\n    *   它通过聚合去噪步骤中的预测结果，选择最一致的输出。\n\n2.  **时间一致性强化（Temporal Consistency Reinforcement, TCR）：**\n    *   这是一种后训练方法。\n    *   它使用“时间语义熵”（Temporal Semantic Entropy, TSE）作为奖励信号。TSE衡量的是中间预测结果的语义稳定性。\n    *   通过TSE奖励，该方法鼓励模型生成更稳定的结果。\n\n### 实验结果\n\n在多个基准测试中，本方法展示了其有效性：\n\n*   **仅使用负TSE奖励：** 在Countdown数据集上，相对于现有dLLM，实现了显著的24.7%的平均改进。\n*   **结合准确性奖励：**\n    *   在GSM8K数据集上，绝对增益为2.0%。\n    *   在MATH500数据集上，绝对增益为4.3%。\n    *   在SVAMP数据集上，绝对增益为6.6%。\n    *   在Countdown数据集上，绝对增益为25.3%。\n\n### 结论\n\n本研究强调了dLLMs中未被充分利用的时间动态的潜力，并提供了两种简单而有效的工具来利用这些动态。",
      "shortSummary": "本研究发现扩散语言模型（dLLMs）存在“时间振荡”现象，即正确答案在中间去噪步骤中出现后被覆盖。为解决此问题，论文提出了两种方法：1) 时间自洽投票（TSCV），通过聚合中间预测选择最一致输出；2) 时间一致性强化（TCR），利用时间语义熵（TSE）作为奖励信号鼓励稳定生成。实验结果显示，这些方法显著提升了dLLMs在多个基准测试上的性能，例如在Countdown数据集上最高提升25.3%。这揭示了dLLMs中时间动态的巨大潜力。",
      "translated_title": "时间是一种特征：利用扩散语言模型中的时间动态",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them."
    },
    {
      "title": "OpenCUA：计算机使用代理的开放基础 (原标题: OpenCUA: Open Foundations for Computer-Use Agents)",
      "link": "https://arxiv.org/abs/2508.09123",
      "pubDate": "Tue, 12 Aug 2025 13:52:32 GMT",
      "isoDate": "2025-08-12T13:52:32.000Z",
      "creator": "Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, Tao Yu",
      "summary": "## OpenCUA：计算机使用代理的开放基础框架\n\n### 引言：计算机使用代理（CUA）的背景与挑战\n\n视觉-语言模型在作为计算机使用代理（CUA）方面展现出令人印象深刻的能力，能够自动化各种计算机任务。然而，随着其商业潜力的增长，大多数功能强大的CUA系统的关键细节仍然是封闭的。鉴于这些代理将越来越多地介导数字交互并代表我们执行重要的决策，研究社区迫切需要访问开放的CUA框架，以研究其能力、局限性和风险。\n\n### OpenCUA框架构成\n\n为了弥合这一差距，研究团队提出了OpenCUA，一个全面的开源框架，旨在扩展CUA数据和基础模型。该框架由以下核心组件构成：\n\n1.  **标注基础设施**：\n    *   提供一个能够无缝捕获人类计算机使用演示的标注基础设施。\n\n2.  **AgentNet数据集**：\n    *   首次大规模的计算机使用任务数据集。\n    *   涵盖3个操作系统和200多个应用程序及网站。\n\n3.  **可扩展的管道**：\n    *   一个可扩展的管道，能够将演示转化为状态-动作对。\n    *   结合了反思性的长链式思考（Chain-of-Thought）推理，确保数据规模扩大时仍能保持稳健的性能提升。\n\n### 性能表现\n\nOpenCUA的端到端代理模型在CUA基准测试中展现出强大的性能：\n\n*   **整体性能**：在CUA基准测试中表现出色。\n*   **OpenCUA-32B的具体成就**：\n    *   在OSWorld-Verified数据集上取得了34.8%的平均成功率。\n    *   在开源模型中树立了新的最先进（SOTA）水平。\n    *   性能超越了OpenAI CUA（GPT-4o）。\n*   **泛化能力与计算效益**：\n    *   进一步分析证实，该方法在不同领域具有良好的泛化能力。\n    *   显著受益于增加测试时的计算资源。\n\n### 开放资源\n\n为了为进一步的CUA研究构建开放的基础，研究团队已发布了其标注工具、数据集、代码和模型。\n\n### 研究领域与作者\n\n*   **研究领域**：人工智能（cs.AI）、计算机视觉与模式识别（cs.CV）。\n*   **作者**：本文由Xinyuan Wang、Bowen Wang、Dunjie Lu等多位研究人员共同撰写。",
      "shortSummary": "OpenCUA是一个开源框架，旨在为计算机使用代理（CUA）提供开放基础。它通过提供标注基础设施、首个大规模多操作系统数据集AgentNet，以及可扩展的推理管道，解决了现有CUA系统封闭的挑战。OpenCUA-32B在OSWorld-Verified基准测试中取得了34.8%的成功率，超越了GPT-4o，成为开源模型的最新SOTA。该项目已发布其工具、数据集、代码和模型，以促进CUA领域的开放研究。",
      "translated_title": "OpenCUA：计算机使用代理的开放基础",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research."
    },
    {
      "title": "AutoCodeBench：大型语言模型是自动代码基准生成器 (原标题: AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators)",
      "link": "https://arxiv.org/abs/2508.09101",
      "pubDate": "Tue, 12 Aug 2025 13:29:20 GMT",
      "isoDate": "2025-08-12T13:29:20.000Z",
      "creator": "Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang, Haotian Zhu, Jianwei Cai, Yue Mao, Chenchen Zhang, Lingyun Tan, Ziyan Xu, Bohui Zhai, Hengyi Liu, Speed Zhu, Wiggin Zhou, Fengzong Lian",
      "summary": "# AutoCodeBench：大型语言模型是自动代码基准生成器\n\n## 摘要\n\n大型语言模型（LLMs）在代码生成领域展现出卓越能力，但现有评估其代码生成能力的基准面临多项关键限制。\n\n## 现有基准的局限性\n\n1.  **依赖手动标注**：耗时且难以扩展到不同编程语言和问题复杂度。\n2.  **语言覆盖不均**：多数基准主要关注Python；少数多语言基准则难度有限，且语言分布不均。\n\n## AutoCodeGen：自动化数据集生成方法\n\n为解决上述挑战，研究人员提出了 **AutoCodeGen**，这是一种无需手动标注即可生成高难度多语言代码生成数据集的自动化方法。\n\n*   **确保测试用例的正确性和完整性**：\n    *   利用LLMs生成测试输入。\n    *   通过多语言沙盒获取测试输出。\n*   **实现高数据质量**：\n    *   采用逆序问题生成。\n    *   经过多重过滤步骤。\n\n## AutoCodeBench：大规模代码生成基准\n\n基于AutoCodeGen方法，研究人员引入了 **AutoCodeBench**，这是一个大规模的代码生成基准。\n\n*   **规模与分布**：包含3,920个问题，均匀分布在20种编程语言中。\n*   **设计目标**：专门用于评估LLMs在具有挑战性、多样化和实用的多语言任务上的表现。\n\n## LLM评估与结果\n\n研究人员在AutoCodeBench及其简化版本 **AutoCodeBench-Lite** 上评估了超过30个领先的开源和专有LLMs。\n\n*   **评估结果**：即使是最先进的LLMs，在面对这些任务的复杂性、多样性和多语言特性时也表现出困难。\n\n## AutoCodeBench-Complete：针对基础模型的补充\n\n此外，研究人员还推出了 **AutoCodeBench-Complete**，它专门设计用于评估基础模型的少样本（few-shot）代码生成能力。\n\n## 展望\n\n研究人员希望AutoCodeBench系列能成为宝贵的资源，并激励社区关注更具挑战性和实用性的多语言代码生成场景。",
      "shortSummary": "AutoCodeBench旨在解决现有LLM代码生成基准手动标注、语言偏向和难度不足的问题。通过自动化方法AutoCodeGen，该研究生成了AutoCodeBench，一个包含3,920个问题、覆盖20种语言的大规模、高难度多语言代码生成基准。评估显示，即使是先进的LLMs也难以应对其复杂性。AutoCodeBench-Complete进一步评估基础模型的少样本能力。该系列旨在推动更具挑战性的多语言代码生成研究。",
      "translated_title": "AutoCodeBench：大型语言模型是自动代码基准生成器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios."
    },
    {
      "title": "VertexRegen：连续细节层次的网格生成 (原标题: VertexRegen: Mesh Generation with Continuous Level of Detail)",
      "link": "https://arxiv.org/abs/2508.09062",
      "pubDate": "Tue, 12 Aug 2025 12:25:46 GMT",
      "isoDate": "2025-08-12T12:25:46.000Z",
      "creator": "Xiang Zhang, Yawar Siddiqui, Armen Avetisyan, Chris Xie, Jakob Engel, Henry Howard-Jenkins",
      "summary": "## VertexRegen：连续细节层次的网格生成框架\n\n### 核心贡献\n\n*   **引入新型框架**：本文提出了一种名为 VertexRegen 的全新网格生成框架。\n*   **实现连续细节层次（LOD）**：VertexRegen 的独特之处在于它能够以连续的细节层次生成网格，这意味着在生成过程中可以灵活地控制网格的精细程度。\n\n### 与现有方法的对比\n\n*   **现有自回归方法**：传统的自回归网格生成方法通常采用“从部分到完整”的方式进行，导致在生成过程的中间步骤中，所产生的结构是不完整的。\n\n### VertexRegen 的方法论\n\n*   **灵感来源**：VertexRegen 的设计灵感来源于渐进式网格（progressive meshes）的概念。\n*   **过程重构**：它将网格生成过程重新定义为边坍缩（edge collapse）的逆向操作，即顶点分裂（vertex split）。\n*   **学习机制**：这一逆向过程是通过一个生成模型进行学习的。\n\n### 实验结果与独特优势\n\n*   **生成质量**：实验结果表明，VertexRegen 生成的网格质量与当前最先进的方法相当。\n*   **“随时生成”（Anytime Generation）**：VertexRegen 的一个独特优势是它提供了“随时生成”的能力。这意味着用户可以在生成过程的任何步骤停止，并获得一个有效的、具有不同细节层次的网格。\n*   **灵活性**：这种灵活性使得用户可以根据需求，在计算资源或时间有限的情况下，获得满足特定细节要求的网格。\n\n### 相关信息\n\n*   **发表会议**：该研究成果已被 ICCV 2025 接收。\n*   **研究领域**：主要涉及图形学（cs.GR）、计算机视觉与模式识别（cs.CV）和机器学习（cs.LG）。",
      "shortSummary": "VertexRegen 是一种新型网格生成框架，能够实现连续细节层次（LOD）的网格生成。与现有方法不同，它将生成过程重构为顶点分裂（边坍缩的逆向），并通过生成模型学习。实验证明，VertexRegen 生成的网格质量与最先进方法相当，并提供独特的“随时生成”能力，允许在任何步骤获得具有不同LOD的有效网格，展现出高度灵活性。",
      "translated_title": "VertexRegen：连续细节层次的网格生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail."
    },
    {
      "title": "弥合量子博弈论的理论与实践：在NISQ硬件上优化实现性别之战并进行错误缓解 (原标题: Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware)",
      "link": "https://arxiv.org/abs/2508.09050",
      "pubDate": "Tue, 12 Aug 2025 12:10:05 GMT",
      "isoDate": "2025-08-12T12:10:05.000Z",
      "creator": "Germán Díaz Agreda, Carlos Andres Duran Paredes, Mateo Buenaventura Samboni, Jhon Alejandro Andrade, Sebastián Andrés Cajas Ordoñez",
      "summary": "## 量子博弈论在NISQ硬件上的实验实现：以“性别之战”为例\n\n### 引言\n在真实硬件上实现量子博弈论面临诸多挑战，包括噪声、退相干和有限的量子比特连接性。然而，此类实验演示对于验证理论预测至关重要。本研究旨在通过在噪声中等规模量子（NISQ）硬件上实现一个完整的量子博弈论实验，来弥合理论与实践之间的鸿沟。\n\n### 实验设置\n*   **博弈类型**：本研究首次完整实验实现了Eisert-Wilkens-Lewenstein (EWL) 框架下的“性别之战”（Battle of the Sexes）博弈。\n*   **硬件平台**：实验在IBM Quantum的ibm sherbrooke超导处理器上进行。\n*   **评估策略**：评估了四种量子策略：I（恒等）、H（哈达玛）、R(π/4) 和 R(π)。\n*   **参数配置**：在31个纠缠值 γ ∈ [0, π] 的范围内进行了评估，每个配置执行2048次测量，以便直接比较分析预测与硬件执行结果。\n\n### 错误缓解方法\n为了减轻硬件噪声和变异性的影响，研究引入了一种名为**引导电路映射（Guided Circuit Mapping, GCM）**的新方法。GCM方法能够根据实时的拓扑结构和校准数据，动态地选择最佳的量子比特对并优化电路路由。\n\n### 结果与讨论\n*   **理论预测**：分析模型预测，相对于经典均衡，量子策略的收益可以提高高达108%。\n*   **实验表现**：尽管存在硬件引起的偏差，但通过GCM方法，实验结果成功地保持了预期的收益趋势，相对误差控制在3.5%至12%之间。\n*   **关键发现**：这些发现表明，在现实的NISQ条件下，战略协调中的量子优势依然可以持续存在。\n\n### 结论与意义\n本研究为在多智能体、经济和分布式决策系统等领域中实现量子博弈论的实际应用提供了可行的途径。它证明了即使在当前噪声量子硬件的限制下，通过有效的错误缓解技术，量子博弈论的理论优势仍能得到实验验证和保持。",
      "shortSummary": "该研究在IBM Quantum的NISQ硬件上，首次完整实验实现了EWL框架下的“性别之战”量子博弈。为应对噪声，引入了引导电路映射（GCM）方法。实验结果显示，尽管硬件存在偏差，GCM使量子策略的收益趋势与理论预测保持一致，相对误差在3.5%-12%之间。这证明在现实NISQ条件下，战略协调中的量子优势可以持续存在，为量子博弈论在多智能体和经济决策系统中的实际应用开辟了道路。",
      "translated_title": "弥合量子博弈论的理论与实践：在NISQ硬件上优化实现性别之战并进行错误缓解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Implementing quantum game theory on real hardware is challenging due to noise, decoherence, and limited qubit connectivity, yet such demonstrations are essential to validate theoretical predictions. We present one of the first full experimental realizations of the Battle of the Sexes game under the Eisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke superconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi)) were evaluated across 31 entanglement values gamma in [0, pi] using 2048 shots per configuration, enabling a direct comparison between analytical predictions and hardware execution. To mitigate noise and variability, we introduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit pairs and optimizes routing based on real-time topology and calibration data. The analytical model forecasts up to 108% payoff improvement over the classical equilibrium, and despite hardware-induced deviations, experimental results with GCM preserve the expected payoff trends within 3.5%-12% relative error. These findings show that quantum advantages in strategic coordination can persist under realistic NISQ conditions, providing a pathway toward practical applications of quantum game theory in multi-agent, economic, and distributed decision-making systems."
    },
    {
      "title": "长训练，短思考：面向高效推理的课程学习 (原标题: Train Long, Think Short: Curriculum Learning for Efficient Reasoning)",
      "link": "https://arxiv.org/abs/2508.08940",
      "pubDate": "Tue, 12 Aug 2025 09:48:03 GMT",
      "isoDate": "2025-08-12T09:48:03.000Z",
      "creator": "Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem",
      "summary": "## 长训练，短思考：面向高效推理的课程学习\n\n### 摘要\n\n本文提出了一种新的课程学习策略，旨在提高大型语言模型（LLMs）的推理效率，同时保持准确性。现有方法在控制计算成本时，依赖于固定的训练预算，未能利用学习过程中从探索到压缩的自然进展。\n\n### 核心方法：课程学习与GRPO\n\n*   **问题背景**：LLMs的推理能力提升通常伴随计算成本增加。现有通过显式长度控制的方法，其固定预算训练模式未能有效利用学习的渐进性。\n*   **解决方案**：引入一种基于课程学习的策略，用于长度受控的推理。该策略采用**群组相对策略优化（Group Relative Policy Optimization, GRPO）**。\n*   **训练过程**：\n    *   训练初期：模型被赋予慷慨的token预算，鼓励其充分探索并发现有效的解决方案策略。\n    *   训练后期：token预算逐渐收紧，促使模型将已发现的策略提炼并压缩成更简洁的推理轨迹。\n\n### 奖励函数设计\n\n为了平衡训练目标，GRPO被增强了一个综合奖励函数，该函数结合了三个关键信号：\n\n1.  **任务正确性**：通过验证器反馈评估模型的任务完成准确性。\n2.  **长度效率**：奖励模型生成更简洁、更高效的推理过程。\n3.  **格式依从性**：通过结构化标签（structural tags）确保模型输出符合预设的格式要求。\n\n### 实验结果与贡献\n\n*   **基准测试**：在多个数学和推理数据集上进行了实验，包括GSM8K、MATH500、SVAMP、College Math和GSM+。\n*   **性能提升**：基于课程的训练方法在相同的最终预算下，始终优于固定预算的基线方法。\n*   **关键优势**：\n    *   实现了更高的准确性。\n    *   显著提高了token效率。\n*   **消融研究**：进一步分析了奖励权重和衰减调度设计的影响。研究表明，渐进式约束（progressive constraint）作为一种强大的归纳偏置，对于训练高效的推理模型至关重要。\n\n### 结论\n\n本文提出的课程学习策略，通过动态调整token预算和综合奖励机制，有效提升了LLMs的推理效率和准确性，为开发更高效的推理模型提供了新的方向。",
      "shortSummary": "本文提出一种名为“长训练，短思考”的课程学习策略，用于提高大型语言模型（LLMs）的推理效率。该方法利用群组相对策略优化（GRPO），通过从宽松到严格的token预算渐进式训练，鼓励模型先探索解决方案，再将其精炼为简洁的推理轨迹。结合任务正确性、长度效率和格式依从性的奖励函数，实验证明此方法在多个基准测试中，相较于固定预算基线，能显著提升准确性和token效率。",
      "translated_title": "长训练，短思考：面向高效推理的课程学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo."
    },
    {
      "title": "DeCRED：基于编码器-解码器语音识别的解码器中心正则化 (原标题: DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition)",
      "link": "https://arxiv.org/abs/2508.08938",
      "pubDate": "Tue, 12 Aug 2025 09:44:50 GMT",
      "isoDate": "2025-08-12T09:44:50.000Z",
      "creator": "Alexander Polok, Santosh Kesiraju, Karel Beneš, Bolaji Yusuf, Lukáš Burget, Jan Černocký",
      "summary": "### DeCRED：基于编码器-解码器语音识别的解码器中心正则化\n\n本文介绍了一种名为“DeCRED”（Decoder-Centric Regularization in Encoder-Decoder）的简单而有效的正则化方法，旨在改进编码器-解码器自动语音识别（ASR）模型中由解码器诱导的内部语言模型。该方法通过增强模型的鲁棒性和泛化能力，使其在域内和域外设置中均表现更佳。\n\n**核心方法：**\nDeCRED通过向解码器添加辅助分类器来实现。这些分类器使得模型能够通过中间逻辑（logits）预测下一个词元（token），从而对解码器的行为进行正则化。\n\n**主要研究发现与成果：**\n\n*   **内部语言模型困惑度降低：**\n    *   DeCRED相对于11个测试集，将平均内部语言模型BPE（Byte Pair Encoding）困惑度相对降低了36.6%。\n*   **词错误率（WER）改进：**\n    *   **域内（In-domain）设置：** 在7个域内测试集中的5个上，DeCRED实现了比基线模型更好的WER表现，将宏观WER从6.4%降低到6.3%。\n    *   **域外（Out-of-domain）设置：** 在4个域外测试集中的3个上，DeCRED也展现了WER的改进，将宏观WER从18.2%降低到16.2%。\n*   **在TEDLIUM3数据集上的表现：**\n    *   DeCRED在TEDLIUM3数据集上实现了7.0%的WER，这比基线模型低0.6%，比编码器中心的InterCTC正则化方法低0.5%。\n*   **与现有模型的比较：**\n    *   尽管DeCRED在训练数据量和模型参数方面远少于OWSM v3.1和Whisper-medium等模型，但它仍能展现出具有竞争力的WER表现。\n\n**出版信息：**\n该研究已被IEEE ASRU 2025接受。",
      "shortSummary": "DeCRED是一种针对编码器-解码器ASR模型的解码器中心正则化方法。它通过在解码器中添加辅助分类器，利用中间逻辑预测下一个词元，从而增强内部语言模型。实验表明，DeCRED显著降低了内部语言模型困惑度（36.6%），并在域内和域外设置中有效降低了词错误率（宏观WER分别从6.4%降至6.3%和18.2%降至16.2%）。尽管训练数据和参数较少，DeCRED仍能与OWSM v3.1和Whisper-medium等模型保持竞争力。",
      "translated_title": "DeCRED：基于编码器-解码器语音识别的解码器中心正则化",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a simple yet effective regularization for the internal language model induced by the decoder in encoder-decoder ASR models, thereby improving robustness and generalization in both in- and out-of-domain settings. The proposed method, Decoder-Centric Regularization in Encoder-Decoder (DeCRED), adds auxiliary classifiers to the decoder, enabling next token prediction via intermediate logits. Empirically, DeCRED reduces the mean internal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this translates into actual WER improvements over the baseline in 5 of 7 in-domain and 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and 18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing the baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%, respectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium, showing competitive WERs despite training on much less data with fewer parameters."
    },
    {
      "title": "迈向具有类人先验的机器人灵巧抓取 (原标题: Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors)",
      "link": "https://arxiv.org/abs/2508.08896",
      "pubDate": "Tue, 12 Aug 2025 08:36:01 GMT",
      "isoDate": "2025-08-12T08:36:01.000Z",
      "creator": "Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou",
      "summary": "## AffordDex：迈向可供性感知机器人灵巧抓取\n\n### 引言\n\n通用型具身人工智能的发展，其基础在于灵巧机械手能够普遍性地抓取物体。然而，现有方法主要关注低层次的抓取稳定性指标，却忽视了对下游操作至关重要的可供性感知定位和类人姿态。\n\n### AffordDex 框架\n\n为了解决上述局限性，研究人员提出了 AffordDex，这是一个新颖的框架，采用两阶段训练，旨在学习一种通用的抓取策略，该策略内在地理解运动先验和物体可供性。\n\n#### 两阶段训练过程：\n\n1.  **第一阶段：轨迹模仿器预训练**\n    *   在一个大型人类手部运动语料库上进行预训练，以灌输对自然运动的强大先验知识。\n\n2.  **第二阶段：残差模块训练**\n    *   训练一个残差模块，将这些通用的类人运动适应到特定的物体实例上。\n    *   这一精炼过程关键地由以下两个组件引导：\n        *   **负可供性感知分割 (NAA) 模块**：用于识别功能上不合适的接触区域。\n        *   **特权教师-学生蒸馏过程**：确保最终基于视觉的策略具有高度的成功率。\n\n### 主要特点与优势\n\nAffordDex 不仅实现了通用的灵巧抓取，而且在姿态上保持了显著的类人特性，并在接触位置上实现了功能上的适当性。\n\n### 实验结果\n\n广泛的实验表明，AffordDex 在已见物体、未见实例甚至全新类别上，均显著优于最先进的基线方法。\n\n### 作者\n\nHaoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou。",
      "shortSummary": "AffordDex 是一种新型两阶段训练框架，旨在解决现有机器人灵巧抓取方法忽视可供性感知和类人姿态的问题。它通过模仿人类运动学习通用抓取策略，并利用负可供性感知分割和教师-学生蒸馏进行精炼。AffordDex 实现了通用、类人且功能适当的抓取，在各种物体上均显著超越现有技术。",
      "translated_title": "迈向具有类人先验的机器人灵巧抓取",
      "images": [],
      "contentSource": "完整文章",
      "content": "A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories."
    },
    {
      "title": "通过自动化构建环境实现大型语言模型工具使用能力的反馈驱动改进 (原标题: Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments)",
      "link": "https://arxiv.org/abs/2508.08791",
      "pubDate": "Tue, 12 Aug 2025 05:45:19 GMT",
      "isoDate": "2025-08-12T05:45:19.000Z",
      "creator": "Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Xuanjing Huang, Jiecao Chen",
      "summary": "## 大型语言模型（LLMs）工具使用能力的反馈驱动改进：通过自动化构建环境实现\n\n### 背景与挑战\n\n*   **重要性：** 有效的工具使用对大型语言模型（LLMs）与环境进行有意义的交互至关重要。\n*   **当前限制：** 进展受限于缺乏专门为工具使用设计的、高效的强化学习（RL）框架。这主要源于构建稳定训练环境和设计可验证奖励机制的挑战。\n\n### 提出的解决方案\n\n为了解决上述挑战，本文提出了两项核心创新：\n\n1.  **自动化环境构建流程：**\n    *   **目标：** 创建高质量的训练环境，提供详细且可衡量的反馈，且不依赖外部工具。\n    *   **组成部分：**\n        *   **场景分解：** 将复杂任务分解为可管理的场景。\n        *   **文档生成：** 为工具和任务生成相关文档。\n        *   **函数集成：** 将工具功能集成到环境中。\n        *   **复杂度扩展：** 逐步增加环境的复杂性。\n        *   **本地化部署：** 实现环境的本地化部署以提高稳定性。\n\n2.  **可验证的奖励机制：**\n    *   **评估维度：**\n        *   工具使用的精确性。\n        *   任务执行的完整性。\n    *   **集成方式：** 该机制与从构建环境中收集的轨迹数据相结合，能够无缝集成到标准的强化学习算法中，从而促进反馈驱动的模型训练。\n\n### 实验结果与分析\n\n*   **性能提升：** 在不同规模的LLMs上进行的实验表明，该方法显著提升了模型的工具使用性能。\n*   **通用能力保持：** 这种提升并未损害模型的通用能力，无论采用何种推理模式或训练算法。\n*   **内在机制：** 分析表明，性能提升源于模型底层MLP参数的更新，从而改善了上下文理解和推理能力。",
      "shortSummary": "为提升大型语言模型（LLMs）工具使用能力，本文提出自动化构建环境和可验证奖励机制。该方法解决了强化学习在工具使用中面临的环境与奖励挑战。实验证明，此方案显著增强了LLMs的工具使用性能，且不损害其通用能力，主要得益于模型上下文理解和推理能力的改进。",
      "translated_title": "通过自动化构建环境实现大型语言模型工具使用能力的反馈驱动改进",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models."
    },
    {
      "title": "Aryabhata：一个专注于JEE数学考试的语言模型 (原标题: Aryabhata: An exam-focused language model for JEE Math)",
      "link": "https://arxiv.org/abs/2508.08665",
      "pubDate": "Tue, 12 Aug 2025 02:20:07 GMT",
      "isoDate": "2025-08-12T02:20:07.000Z",
      "creator": "Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma",
      "summary": "### 模型介绍\n*   **名称：** Aryabhata 1.0\n*   **类型：** 一个紧凑的70亿参数数学推理模型。\n*   **优化目标：** 专为印度学术考试——联合入学考试（JEE）优化。\n*   **背景：** 尽管大型语言模型（LLMs）发展迅速，但现有模型通常不适用于教育用途。\n\n### 构建方法\n*   **基础模型合并：** 通过合并强大的开源推理模型构建。\n*   **监督微调（SFT）：** 采用课程学习（curriculum learning）对经过验证的思维链（CoT）轨迹进行SFT。这些CoT轨迹通过“最佳n拒绝采样”（best-of-n rejection sampling）精心策划。\n*   **强化学习（RLVR）：** 应用可验证奖励的强化学习，使用A2C目标和组相对优势估计（group-relative advantage estimation）。\n*   **探索策略：** 引入了新颖的探索策略，如“自适应组大小调整”（Adaptive Group Resizing）和“温度缩放”（Temperature Scaling）。\n\n### 性能评估\n*   **评估基准：**\n    *   **同分布（in-distribution）：** JEE Main 2025。\n    *   **异分布（out-of-distribution）：** MATH、GSM8K。\n*   **评估结果：** Aryabhata在准确性和效率方面均优于现有模型。\n*   **教育价值：** 此外，它还提供了具有教学意义的逐步推理过程。\n\n### 发布与未来展望\n*   **发布形式：** Aryabhata作为基础模型发布，旨在推动以考试为中心、开源的小型语言模型的发展。\n*   **社区反馈：** 这是首次公开发布，以收集社区反馈。\n*   **未来计划：** PW（开发团队）正在积极训练未来的模型，以进一步提高学生的学习成果。",
      "shortSummary": "Aryabhata 1.0是一个紧凑的70亿参数数学推理模型，专为印度JEE考试优化。该模型通过合并开源推理模型、监督微调和可验证奖励的强化学习构建，并引入了新颖的探索策略。在JEE Main 2025、MATH和GSM8K等基准测试中，Aryabhata在准确性和效率上均超越现有模型，并能提供有教学价值的逐步推理。它作为基础模型发布，旨在推动考试导向的开源小型语言模型发展。",
      "translated_title": "Aryabhata：一个专注于JEE数学考试的语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-n rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0 on Hugging Face}); PW is actively training future models to further improve learning outcomes for students."
    },
    {
      "title": "Mol-R1：迈向分子发现中的显式长链思维推理 (原标题: Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery)",
      "link": "https://arxiv.org/abs/2508.08401",
      "pubDate": "Mon, 11 Aug 2025 14:50:05 GMT",
      "isoDate": "2025-08-11T14:50:05.000Z",
      "creator": "Jiatong Li, Weida Wang, Qinggang Zhang, Junxian Li, Di Zhang, Changmeng Zheng, Shufei Zhang, Xiaoyong Wei, Qing Li",
      "summary": "## Mol-R1：分子发现中的显式长链思维推理框架\n\n### 引言\n\n大型语言模型（LLMs），特别是像DeepSeek-R1和QWQ这样的显式长链思维（Long-CoT）推理模型，在常识推理和数学推断等领域展现出强大的能力。然而，这些模型在分子发现等知识密集型领域中，由于其固有的复杂性、对精确领域知识（如分子结构和化学原理）的需求以及高质量专家标注的稀缺性，常被批评为能力有限且效率低下。\n\n### Mol-R1 框架\n\n为了弥补这一差距，本文引入了 **Mol-R1**，这是一个旨在提高R1类显式长链思维LLM在基于文本的分子生成任务中可解释性和推理性能的新颖框架。\n\nMol-R1 的方法主要包括以下几个关键步骤：\n\n1.  **高质量推理数据集的构建：**\n    *   通过 **上下文蒸馏先验规范（Prior Regulation via In-context Distillation, PRID）** 策略来精心策划。PRID 是一种专门的蒸馏策略，旨在有效生成由先验规范指导的配对推理轨迹。\n\n2.  **分子迭代适应（Molecular Iterative Adaptation, MoIA）训练策略：**\n    *   MoIA 是一种复杂的训练策略，它迭代地结合了 **监督微调（Supervised Fine-tuning, SFT）** 和 **强化策略优化（Reinforced Policy Optimization, RPO）**。\n    *   该策略专门用于提升R1类推理模型在分子发现任务中的推理性能。\n\n### 性能评估\n\n研究人员在基于文本的分子推理生成任务中对 Mol-R1 的性能进行了评估，结果表明其表现优于现有基线模型。",
      "shortSummary": "Mol-R1是一个旨在提升大型语言模型在分子发现领域推理能力的框架。针对现有Long-CoT模型在该领域知识密集和效率低下的问题，Mol-R1通过两种核心策略实现改进：一是利用PRID（上下文蒸馏先验规范）构建高质量推理数据集；二是采用MoIA（分子迭代适应）训练策略，结合监督微调和强化策略优化。实验结果表明，Mol-R1在基于文本的分子推理生成任务中表现优异，超越了现有基线模型。",
      "translated_title": "Mol-R1：迈向分子发现中的显式长链思维推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce Mol-R1, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our approach begins with a high-quality reasoning dataset curated through Prior Regulation via In-context Distillation (PRID), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce MoIA, Molecular Iterative Adaptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like reasoning models for molecule discovery. Finally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines."
    },
    {
      "title": "Cut2Next：通过上下文调优生成下一镜头 (原标题: Cut2Next: Generating Next Shot via In-Context Tuning)",
      "link": "https://arxiv.org/abs/2508.08244",
      "pubDate": "Mon, 11 Aug 2025 13:56:59 GMT",
      "isoDate": "2025-08-11T13:56:59.000Z",
      "creator": "Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, Ziwei Liu",
      "summary": "### Cut2Next：通过上下文调优生成下一镜头\n\n本文介绍了一个名为 **Cut2Next** 的创新框架，旨在解决当前多镜头生成方法在电影叙事连贯性和专业编辑模式方面的不足，从而实现 **下一镜头生成 (Next Shot Generation, NSG)**。\n\n#### 现有挑战与NSG目标\n*   **现有问题：** 当前的多镜头生成方法通常优先考虑基本的视觉一致性，但忽视了驱动叙事流程的关键编辑模式（例如，正反打、插入镜头）和严格的电影连续性。这导致生成的输出可能在视觉上连贯，但缺乏叙事复杂性和真正的电影完整性。\n*   **NSG目标：** 旨在合成一个高质量的后续镜头，该镜头必须严格符合专业的编辑模式，并保持严谨的电影连续性。\n\n#### Cut2Next 框架概述\nCut2Next 框架利用 **Diffusion Transformer (DiT)** 作为其核心生成模型，并通过一种新颖的 **分层多提示 (Hierarchical Multi-Prompting)** 策略进行上下文调优。\n\n*   **分层多提示策略：**\n    *   **关系提示 (Relational Prompts)：** 用于定义整体上下文和镜头间的编辑风格。\n    *   **个体提示 (Individual Prompts)：** 用于指定每个镜头的具体内容和电影摄影属性。\n    *   这些提示共同指导 Cut2Next 生成符合电影规范的下一镜头。\n\n*   **架构创新：** 为了进一步整合这些多样化的信号，Cut2Next 引入了两项架构创新，且不引入新的参数：\n    *   **上下文感知条件注入 (Context-Aware Condition Injection, CACI)**\n    *   **分层注意力掩码 (Hierarchical Attention Mask, HAM)**\n\n#### 数据集与评估\n为了支持 Cut2Next 的开发和评估，研究团队构建了两个数据集并引入了一个新的评估基准：\n\n*   **数据集：**\n    *   **RawCuts：** 大规模数据集。\n    *   **CuratedCuts：** 精炼数据集。\n    *   这两个数据集都包含分层提示。\n*   **评估基准：** 引入了 **CutBench** 用于评估。\n*   **实验结果：**\n    *   实验表明 Cut2Next 在视觉一致性和文本忠实度方面表现出色。\n    *   用户研究显示，用户对 Cut2Next 有强烈的偏好，尤其是在其对预期编辑模式的遵循和整体电影连续性方面，这验证了其生成高质量、叙事表达丰富且电影连贯的后续镜头的能力。",
      "shortSummary": "Cut2Next是一个创新框架，旨在通过上下文调优生成符合专业编辑模式和电影连续性的高质量后续镜头。它利用Diffusion Transformer，结合分层多提示策略（关系提示和个体提示）以及上下文感知条件注入（CACI）和分层注意力掩码（HAM）等架构创新。实验和用户研究表明，Cut2Next在视觉一致性、文本忠实度和电影叙事表达方面表现出色，有效解决了现有方法在叙事连贯性上的不足。",
      "translated_title": "Cut2Next：通过上下文调优生成下一镜头",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots."
    }
  ],
  "lastUpdated": "2025-08-14T09:36:20.690Z"
}