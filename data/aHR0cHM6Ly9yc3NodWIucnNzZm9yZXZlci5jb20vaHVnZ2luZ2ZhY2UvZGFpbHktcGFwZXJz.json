{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "MMSI-Bench：多图像空间智能基准 (原标题: MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence)",
      "link": "https://arxiv.org/abs/2505.23764",
      "pubDate": "Thu, 29 May 2025 13:59:52 GMT",
      "isoDate": "2025-05-29T13:59:52.000Z",
      "creator": "Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, Jiangmiao Pang",
      "summary": "# MMSI-Bench：多图像空间智能基准\n\n## 引言与背景\n空间智能对于在复杂物理世界中运行的多模态大型语言模型（MLLMs）至关重要。然而，现有基准仅探测单图像关系，无法评估真实世界部署所需的多图像空间推理能力。为了弥补这一空白，研究人员引入了MMSI-Bench，一个专为多图像空间智能设计的视觉问答（VQA）基准。\n\n## 基准的构建\nMMSI-Bench的构建过程严谨且耗时：\n*   **研究团队：** 由六位3D视觉研究人员共同完成。\n*   **投入时间：** 耗时超过300小时。\n*   **问题数量与来源：** 精心制作了1,000个具有挑战性、无歧义的多项选择题，这些问题来源于超过120,000张图像。\n*   **问题设计：** 每个问题都配有精心设计的干扰项和一步一步的推理过程注释。\n\n## 实验与评估结果\n研究团队对34个开源和专有MLLMs进行了广泛的实验和彻底评估，结果揭示了显著的性能差距：\n*   **模型表现：**\n    *   最强的开源模型准确率约为30%。\n    *   OpenAI的o3推理模型达到40%的准确率。\n    *   人类表现：高达97%的准确率。\n*   **挑战性：** 这些结果凸显了MMSI-Bench的巨大挑战性，以及未来研究的巨大提升空间。\n\n## 自动化错误分析\nMMSI-Bench利用其注释的推理过程，提供了一个自动化的错误分析流程，能够诊断四种主要的失败模式，为推进多图像空间智能提供了宝贵的见解：\n1.  **基础定位错误（Grounding errors）：** 模型未能正确识别或定位图像中的对象。\n2.  **重叠匹配和场景重建错误（Overlap-matching and scene-reconstruction errors）：** 模型在处理图像重叠或重建完整场景时出现问题。\n3.  **情境转换推理错误（Situation-transformation reasoning errors）：** 模型在理解情境变化或动态场景推理时遇到困难。\n4.  **空间逻辑错误（Spatial-logic errors）：** 模型在应用基本的空间逻辑规则时出错。\n\n## 总结\nMMSI-Bench是一个全面、完全由人工策划的多图像空间智能基准，带有推理注释，旨在推动MLLMs在该领域的发展。",
      "shortSummary": "MMSI-Bench是一个新颖的视觉问答（VQA）基准，专注于评估多模态大型语言模型（MLLMs）的多图像空间智能。它通过1,000个精心设计的问题，弥补了现有基准仅关注单图像关系的不足。对34个MLLMs的评估显示，模型表现（约30-40%）与人类（97%）存在巨大差距，表明该领域仍有广阔的研究空间。MMSI-Bench还提供了自动化错误分析，识别出四种主要失败模式，为未来研究提供了方向。",
      "translated_title": "MMSI-Bench：多图像空间智能基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench ."
    },
    {
      "title": "ZeroGUI：以零人工成本自动化在线GUI学习 (原标题: ZeroGUI: Automating Online GUI Learning at Zero Human Cost)",
      "link": "https://arxiv.org/abs/2505.23762",
      "pubDate": "Thu, 29 May 2025 13:59:51 GMT",
      "isoDate": "2025-05-29T13:59:51.000Z",
      "creator": "Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, Wenhai Wang, Yu Qiao, Xizhou Zhu, Jifeng Dai",
      "summary": "# ZeroGUI：以零人工成本自动化在线GUI学习\n\n## 引言\n随着大型视觉-语言模型（VLM）的快速发展，纯视觉GUI（图形用户界面）Agent在感知和操作GUI以自主完成用户指令方面取得了显著进展。然而，现有方法通常采用离线学习框架，这带来了两个核心局限性：\n\n1.  **高度依赖人工标注：** 需要大量高质量的人工标注来完成元素定位（element grounding）和动作监督。\n2.  **适应性有限：** 对动态和交互式环境的适应能力有限。\n\n## ZeroGUI框架\n为了解决上述局限性，本文提出了 **ZeroGUI**，一个可扩展的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。ZeroGUI通过整合以下三个关键组件来实现这一目标：\n\n### 1. 基于VLM的自动任务生成\nZeroGUI能够从当前环境状态自动生成多样化的训练目标。这意味着系统不再需要预先定义或人工创建任务，而是可以根据实时的GUI界面情况，利用VLM的能力自动生成新的学习任务，从而大大增加了训练数据的多样性和覆盖范围。\n\n### 2. 基于VLM的自动奖励评估\n该框架利用VLM自动评估任务的成功与否，而无需手动设计复杂的评估函数。VLM可以直接理解任务目标和当前环境状态，并据此判断Agent的行动是否达到了预期效果，从而提供实时的奖励信号，指导Agent的学习过程。\n\n### 3. 两阶段在线强化学习\nZeroGUI采用两阶段的在线强化学习机制，使Agent能够持续与GUI环境进行交互并从中学习。这种在线学习范式使得Agent能够实时适应环境变化，并通过试错不断优化其操作策略，克服了离线学习在动态环境中的适应性问题。\n\n## 实验结果\n研究人员在两个先进的GUI Agent（UI-TARS和Aguvis）上对ZeroGUI进行了实验验证。实验结果表明，ZeroGUI在OSWorld和AndroidLab这两种GUI环境中显著提升了这些Agent的性能。这证明了ZeroGUI框架在提高GUI Agent训练效率和效果方面的有效性。\n\n## 结论\nZeroGUI提供了一种创新的、无需人工干预的在线学习方法，有效解决了当前GUI Agent训练中面临的标注成本高昂和环境适应性差的问题。通过自动化任务生成、奖励评估和在线强化学习，ZeroGUI为未来GUI Agent的自主学习和部署开辟了新的道路。",
      "shortSummary": "ZeroGUI是一个创新的在线学习框架，旨在以零人工成本自动化GUI Agent的训练。它解决了现有离线学习方法对人工标注的重度依赖和对动态环境适应性差的问题。ZeroGUI通过基于VLM的自动任务生成、自动奖励评估以及两阶段在线强化学习实现。实验证明，ZeroGUI显著提升了UI-TARS和Aguvis等GUI Agent在OSWorld和AndroidLab环境中的性能，为GUI Agent的自主学习提供了高效解决方案。",
      "translated_title": "ZeroGUI：以零人工成本自动化在线GUI学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
    },
    {
      "title": "差分信息：偏好优化中的信息论视角 (原标题: Differential Information: An Information-Theoretic Perspective on Preference Optimization)",
      "link": "https://arxiv.org/abs/2505.23761",
      "pubDate": "Thu, 29 May 2025 13:59:50 GMT",
      "isoDate": "2025-05-29T13:59:50.000Z",
      "creator": "Yunjae Won, Hyunji Lee, Hyeonbin Hwang, Minjoon Seo",
      "summary": "### 差分信息：偏好优化中的信息论视角\n\n本文旨在从信息论角度深入探讨直接偏好优化（DPO）的理论基础，特别是其对数比率奖励参数化的合理性。\n\n**核心问题与方法：**\n\n*   **问题：** 尽管直接偏好优化（DPO）在监督式对齐语言模型与人类偏好方面取得了经验上的成功，但其对数比率奖励参数化背后的理论依据尚不完整。\n*   **方法：** 本文通过引入**差分信息分布（Differential Information Distribution, DID）**来解决这一理论空白。DID是一种关于令牌序列的分布，它捕获了策略更新过程中获得的信息。\n\n**主要发现与贡献：**\n\n1.  **对数比率奖励的最优性：**\n    *   研究表明，当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO中的对数比率奖励形式是学习目标策略的唯一最优形式。\n    *   这一结果自然地导出了被拒绝响应的最优采样分布的闭式表达式。\n\n2.  **与隐含归纳偏置的关联：**\n    *   偏好编码差分信息的条件与对数边际有序策略（log-margin ordered policies）之间存在根本性联系。\n    *   对数边际有序策略是偏好优化中广泛使用但此前未被识别的一种隐含归纳偏置。\n\n3.  **差分信息熵的分析：**\n    *   通过分析DID的熵，本文揭示了不同熵值对策略行为的影响：\n        *   学习低熵差分信息会强化策略分布。\n        *   学习高熵差分信息会产生平滑效应，这解释了对数似然位移（log-likelihood displacement）现象。\n\n**实验验证与实际启示：**\n\n*   作者在合成实验中验证了理论发现，并将其扩展到真实的指令遵循数据集。\n*   研究结果表明：\n    *   学习高熵差分信息对于**通用指令遵循**至关重要。\n    *   学习低熵差分信息则有利于**知识密集型问答**。\n\n**总结：**\n\n本文通过差分信息的视角，为DPO目标、偏好数据结构以及由此产生的策略行为提供了一个统一的理解框架。",
      "shortSummary": "本文从信息论角度探讨直接偏好优化（DPO）的理论基础。通过引入差分信息分布（DID），研究发现当偏好标签编码了将参考策略转换为目标策略所需的差分信息时，DPO的对数比率奖励是学习目标策略的唯一最优形式，并揭示了DPO与对数边际有序策略的隐含关联。此外，分析DID的熵表明，低熵差分信息强化策略，高熵差分信息则产生平滑效应。实验验证了这些发现，并指出高熵差分信息对通用指令遵循至关重要，而低熵信息则有利于知识密集型问答。",
      "translated_title": "差分信息：偏好优化中的信息论视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "Direct Preference Optimization (DPO) has become a standard technique for aligning language models with human preferences in a supervised manner. Despite its empirical success, the theoretical justification behind its log-ratio reward parameterization remains incomplete. In this work, we address this gap by utilizing the Differential Information Distribution (DID): a distribution over token sequences that captures the information gained during policy updates. First, we show that when preference labels encode the differential information required to transform a reference policy into a target policy, the log-ratio reward in DPO emerges as the uniquely optimal form for learning the target policy via preference optimization. This result naturally yields a closed-form expression for the optimal sampling distribution over rejected responses. Second, we find that the condition for preferences to encode differential information is fundamentally linked to an implicit assumption regarding log-margin ordered policies-an inductive bias widely used in preference optimization yet previously unrecognized. Finally, by analyzing the entropy of the DID, we characterize how learning low-entropy differential information reinforces the policy distribution, while high-entropy differential information induces a smoothing effect, which explains the log-likelihood displacement phenomenon. We validate our theoretical findings in synthetic experiments and extend them to real-world instruction-following datasets. Our results suggest that learning high-entropy differential information is crucial for general instruction-following, while learning low-entropy differential information benefits knowledge-intensive question answering. Overall, our work presents a unifying perspective on the DPO objective, the structure of preference data, and resulting policy behaviors through the lens of differential information."
    },
    {
      "title": "被谜题难倒：当视觉-语言模型无法领会暗示时 (原标题: Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint)",
      "link": "https://arxiv.org/abs/2505.23759",
      "pubDate": "Thu, 29 May 2025 13:59:47 GMT",
      "isoDate": "2025-05-29T13:59:47.000Z",
      "creator": "Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan",
      "summary": "### 视觉-语言模型在字谜（Rebus Puzzles）面前的挑战\n\n**1. 引言与挑战**\n*   **字谜（Rebus Puzzles）定义：** 字谜是一种独特的视觉谜题，它通过图像、空间排列和符号替换来编码语言信息。\n*   **对现有模型构成挑战：** 这类谜题对当前的视觉-语言模型（VLMs）构成了独特的挑战。\n*   **与传统任务的区别：** 与传统的图像描述或问答任务不同，解决字谜需要：\n    *   多模态抽象能力\n    *   符号推理能力\n    *   对文化、语音和语言双关语的理解\n\n**2. 研究方法**\n*   **研究目的：** 本文旨在调查当代视觉-语言模型解释和解决字谜的能力。\n*   **基准数据集构建：** 研究人员构建了一个手工生成并标注的、多样化的英语字谜基准数据集。\n*   **谜题范围：** 数据集中的谜题难度各异，从简单的象形替换（如用图片代表单词）到依赖空间排列的线索（如“head”在“heels”上方表示“head over heels”）。\n\n**3. 研究发现**\n*   **模型表现分析：** 研究分析了不同视觉-语言模型的表现。\n*   **初步能力：** 研究结果显示，视觉-语言模型在解码简单的视觉线索方面展现出一些令人惊讶的能力。\n*   **主要局限：** 然而，它们在需要以下能力的任务上表现出显著的困难：\n    *   抽象推理\n    *   横向思维（跳出常规的思考方式）\n    *   理解视觉隐喻",
      "shortSummary": "当前视觉-语言模型（VLMs）在解决字谜（rebus puzzles）方面面临显著挑战。这类视觉谜题要求多模态抽象、符号推理及对文化和语言双关语的理解。研究人员构建了一个手工标注的英语字谜基准，发现VLMs虽能解码简单视觉线索，但在抽象推理、横向思维和理解视觉隐喻方面表现出明显不足。",
      "translated_title": "被谜题难倒：当视觉-语言模型无法领会暗示时",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors."
    },
    {
      "title": "LoRAShop：基于整流流变压器的免训练多概念图像生成与编辑 (原标题: LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers)",
      "link": "https://arxiv.org/abs/2505.23758",
      "pubDate": "Thu, 29 May 2025 13:59:46 GMT",
      "isoDate": "2025-05-29T13:59:46.000Z",
      "creator": "Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag",
      "summary": "## LoRAShop：免训练多概念图像生成与编辑框架\n\n### 1. 引言\n\nLoRAShop是一个开创性的框架，首次实现了使用LoRA模型进行多概念图像编辑。它旨在将个性化扩散模型转化为一个实用的“LoRA版Photoshop”工具，并为组合式视觉叙事和快速创意迭代开辟新途径。\n\n### 2. 核心观察与方法论\n\nLoRAShop的构建基于对Flux风格扩散变压器中特征交互模式的关键观察：\n\n*   **关键观察：** 概念特定的变压器特征在去噪过程的早期阶段会激活空间连贯的区域。\n\n利用这一观察，LoRAShop采取了以下方法：\n\n*   在一次前向传播中，为每个概念推导出解耦的潜在掩码。\n*   仅在限定概念的区域内混合相应的LoRA权重。\n\n### 3. 主要成果与优势\n\nLoRAShop框架带来了多项显著优势：\n\n*   **无缝集成：** 能够将多个主体或风格无缝地融入原始场景中。\n*   **上下文保留：** 在编辑过程中，能够同时保留图像的全局上下文、光照和精细细节。\n*   **身份保持：** 实验结果表明，LoRAShop在身份保持方面表现出优于现有基线方法的性能。\n*   **免训练：** 该框架消除了对重新训练和外部约束的需求，极大地简化了多概念图像编辑的流程。\n\n### 4. 应用与前景\n\n通过消除重新训练和外部约束，LoRAShop将个性化扩散模型转变为一个实用的“LoRA版Photoshop”工具，为用户提供了前所未有的灵活性和效率。这为以下领域开辟了新的可能性：\n\n*   **组合式视觉叙事：** 创作者可以更轻松地将不同概念融合到单一图像中，讲述更丰富的故事。\n*   **快速创意迭代：** 艺术家和设计师能够快速尝试和迭代不同的视觉概念，加速创作过程。",
      "shortSummary": "LoRAShop是一个开创性的框架，首次实现了使用LoRA模型进行免训练的多概念图像编辑。它基于对Flux风格扩散变压器中特征激活模式的关键观察，通过为每个概念生成解耦的潜在掩码，并在特定区域混合LoRA权重，实现多主体或风格的无缝集成，同时保留图像的全局上下文和细节。实验表明其身份保持效果优于基线。LoRAShop将个性化扩散模型转变为实用的图像编辑工具，为视觉创作提供了新可能。",
      "translated_title": "LoRAShop：基于整流流变压器的免训练多概念图像生成与编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration."
    },
    {
      "title": "DeepTheorem：通过自然语言和强化学习提升LLM在定理证明中的推理能力 (原标题: DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2505.23754",
      "pubDate": "Thu, 29 May 2025 13:59:39 GMT",
      "isoDate": "2025-05-29T13:59:39.000Z",
      "creator": "Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhengwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",
      "summary": "# DeepTheorem：提升LLM在定理证明中的推理能力\n\nDeepTheorem 是一个旨在通过利用自然语言来增强大型语言模型（LLM）数学推理能力的综合性非形式化定理证明框架。\n\n## 研究背景与挑战\n*   定理证明是评估LLM复杂推理能力的重要测试平台。\n*   传统的自动化定理证明（ATP）方法严重依赖形式化证明系统，这与LLM在预训练期间通过非形式化、自然语言知识获得的优势不符。\n\n## DeepTheorem 框架的核心组成\n\n### 1. 大规模基准数据集\n*   **规模与质量：** 包含12.1万个高质量的IMO（国际数学奥林匹克）级别非形式化定理及其证明。\n*   **多样性：** 涵盖多种数学领域。\n*   **严格标注：** 对正确性、难度和主题类别进行了严格标注。\n*   **可验证变体：** 附带系统构建的可验证定理变体，用于激励稳健推理。\n\n### 2. 新颖的强化学习策略（RL-Zero）\n*   **定制化：** 专门为非形式化定理证明设计。\n*   **激励机制：** 利用可验证定理变体来激励LLM进行稳健的数学推理。\n\n### 3. 全面评估指标\n*   **多维度评估：** 提出了全面的结果评估和过程评估指标。\n*   **评估内容：** 检查证明的正确性和推理步骤的质量。\n\n## 实验结果与发现\n*   **性能显著提升：** 实验分析表明，DeepTheorem 显著改善了LLM在定理证明方面的表现。\n*   **超越现有方法：** 相比现有数据集和监督微调协议，DeepTheorem 取得了更优异的成果。\n*   **最先进水平：** 实现了最先进的准确性和推理质量。\n*   **未来潜力：** 研究结果强调了 DeepTheorem 在根本上推进自动化非形式化定理证明和数学探索方面的潜力。\n\n## 作者与主题\n*   **主要作者：** Ziyin Zhang 等。\n*   **研究主题：** 计算与语言（cs.CL）、人工智能（cs.AI）。",
      "shortSummary": "DeepTheorem 提出了一种通过自然语言和强化学习提升LLM定理证明能力的新框架。它解决了传统形式化证明与LLM自然语言优势不符的问题。该框架包含一个12.1万条IMO级别非形式化定理和证明的大规模数据集，以及专为非形式化证明设计的RL-Zero强化学习策略。实验证明，DeepTheorem显著提高了LLM的定理证明性能，取得了最先进的准确性和推理质量，展现了在自动化非形式化定理证明领域的巨大潜力。",
      "translated_title": "DeepTheorem：通过自然语言和强化学习提升LLM在定理证明中的推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Theorem proving serves as a major testbed for evaluating complex reasoning abilities in large language models (LLMs). However, traditional automated theorem proving (ATP) approaches rely heavily on formal proof systems that poorly align with LLMs' strength derived from informal, natural language knowledge acquired during pre-training. In this work, we propose DeepTheorem, a comprehensive informal theorem-proving framework exploiting natural language to enhance LLM mathematical reasoning. DeepTheorem includes a large-scale benchmark dataset consisting of 121K high-quality IMO-level informal theorems and proofs spanning diverse mathematical domains, rigorously annotated for correctness, difficulty, and topic categories, accompanied by systematically constructed verifiable theorem variants. We devise a novel reinforcement learning strategy (RL-Zero) explicitly tailored to informal theorem proving, leveraging the verified theorem variants to incentivize robust mathematical inference. Additionally, we propose comprehensive outcome and process evaluation metrics examining proof correctness and the quality of reasoning steps. Extensive experimental analyses demonstrate DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality. Our findings highlight DeepTheorem's potential to fundamentally advance automated informal theorem proving and mathematical exploration."
    },
    {
      "title": "重新排序图像块可改进视觉模型 (原标题: REOrdering Patches Improves Vision Models)",
      "link": "https://arxiv.org/abs/2505.23751",
      "pubDate": "Thu, 29 May 2025 13:59:30 GMT",
      "isoDate": "2025-05-29T13:59:30.000Z",
      "creator": "Declan Kutscher, David M. Chan, Yutong Bai, Trevor Darrell, Ritwik Gupta",
      "summary": "## REOrdering Patches Improves Vision Models：详细摘要\n\n### 1. 问题背景与动机\n\n*   **序列模型与图像展平**：Transformer等序列模型要求输入表示为一维序列。在计算机视觉中，这通常涉及使用固定的行优先（光栅扫描）顺序将图像展平为一维序列。\n*   **排列不变性与近似自注意力**：虽然全自注意力（full self-attention）是排列等变的，但现代长序列Transformer为了效率，越来越多地依赖于架构近似。这些近似破坏了排列不变性，并引入了对图像块排序的敏感性。\n*   **图像块顺序的影响**：研究表明，在这些设置中，图像块的顺序显著影响模型性能。简单的替代方案，如列优先或希尔伯特曲线，会导致显著的准确性变化，这表明存在更优的排序方式。\n\n### 2. REOrder框架\n\n*   **目标**：受上述观察启发，本文提出了REOrder，一个用于发现任务最优图像块排序的两阶段框架。\n*   **第一阶段：信息论先验**\n    *   通过评估各种图像块序列的可压缩性，推导出一个信息论先验。这一阶段旨在识别具有更高信息密度或更易于压缩的图像块排列模式。\n*   **第二阶段：学习排列策略**\n    *   使用REINFORCE算法优化一个Plackett-Luce策略，以学习一个关于排列的策略。这种方法使得在组合排列空间中能够高效地学习和探索最佳的图像块排序。\n\n### 3. 实验结果\n\n*   **ImageNet-1K数据集**：REOrder在ImageNet-1K数据集上，相对于传统的行优先排序，将top-1准确率提高了高达3.01%。\n*   **Functional Map of the World数据集**：在Functional Map of the World数据集上，REOrder将top-1准确率提高了13.35%。\n\n这些结果表明，通过优化图像块的输入顺序，可以显著提升视觉模型在不同任务上的性能。",
      "shortSummary": "本文提出REOrder框架，旨在解决视觉Transformer中图像块排序对模型性能的显著影响。传统方法将图像展平为一维序列，但近似自注意力使其对图像块顺序敏感。REOrder通过信息论先验和基于REINFORCE的策略学习，发现任务最优的图像块排序。实验结果表明，REOrder在ImageNet-1K和Functional Map of the World数据集上，相较于传统行优先排序，显著提高了模型准确率。",
      "translated_title": "重新排序图像块可改进视觉模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%."
    },
    {
      "title": "Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力 (原标题: Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence)",
      "link": "https://arxiv.org/abs/2505.23747",
      "pubDate": "Thu, 29 May 2025 13:59:04 GMT",
      "isoDate": "2025-05-29T13:59:04.000Z",
      "creator": "Diankun Wu, Fangfu Liu, Yi-Hsin Hung, Yueqi Duan",
      "summary": "### Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力\n\n**1. 背景与挑战**\n*   多模态大语言模型（MLLMs）在2D视觉任务上取得了显著进展，但其空间智能能力仍有待提升。\n*   现有3D MLLMs通常依赖额外的3D或2.5D数据来整合空间感知，这限制了它们在仅有2D输入（如图像或视频）场景中的实用性。\n\n**2. Spatial-MLLM 框架概述**\n*   本文提出了Spatial-MLLM，一个新颖的框架，旨在从纯2D观测中实现基于视觉的空间推理。\n*   **核心洞察：** 与依赖CLIP优化语义理解的传统视频MLLMs不同，Spatial-MLLM的关键在于利用前馈视觉几何基础模型中强大的结构先验。\n\n**3. 架构设计**\n*   **双编码器架构：**\n    *   **预训练2D视觉编码器：** 用于提取语义特征。\n    *   **空间编码器：** 从视觉几何模型的骨干网络初始化，用于提取3D结构特征。\n*   **连接器：** 将上述两种特征整合为统一的视觉token，以增强模型的空间理解能力。\n\n**4. 推理策略**\n*   **空间感知帧采样策略：** 在推理时，该策略选择视频序列中具有空间信息量的帧。\n*   **目的：** 确保即使在有限的token长度下，模型也能专注于对空间推理至关重要的帧，从而提高效率和准确性。\n\n**5. 数据集与训练**\n*   **数据集：** 构建了Spatial-MLLM-120k数据集。\n*   **训练方法：** 模型在该数据集上通过监督微调（supervised fine-tuning）和GRPO进行训练。\n\n**6. 实验结果**\n*   在各种真实世界数据集上进行了广泛的实验。\n*   结果表明，Spatial-MLLM在广泛的基于视觉的空间理解和推理任务中实现了最先进（state-of-the-art）的性能。",
      "shortSummary": "Spatial-MLLM是一个新颖框架，旨在提升多模态大语言模型在纯2D视觉输入下的空间智能。它采用双编码器架构，结合语义特征和从视觉几何模型提取的3D结构特征。通过空间感知帧采样策略，模型能更有效地处理视频。Spatial-MLLM在自建的Spatial-MLLM-120k数据集上训练，并在多项基于视觉的空间理解和推理任务中取得了最先进的性能。",
      "translated_title": "Spatial-MLLM：提升多模态大语言模型在基于视觉的空间智能方面的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/."
    },
    {
      "title": "是否信任你的视觉-语言模型的预测 (原标题: To Trust Or Not To Trust Your Vision-Language Model's Prediction)",
      "link": "https://arxiv.org/abs/2505.23745",
      "pubDate": "Thu, 29 May 2025 13:59:01 GMT",
      "isoDate": "2025-05-29T13:59:01.000Z",
      "creator": "Hao Dong, Moru Liu, Jian Liang, Eleni Chatzi, Olga Fink",
      "summary": "### 引言\n\n视觉-语言模型（VLMs）在对齐视觉和文本模态方面展现出强大能力，支持广泛的多模态理解和生成应用。然而，尽管它们在零样本和迁移学习场景中表现出色，VLMs仍易受错误分类的影响，经常给出自信但错误的预测。这一局限性在安全关键领域构成了重大风险，因为错误的预测可能导致严重后果。\n\n### TrustVLM框架\n\n为了解决VLM预测信任度评估这一关键挑战，本文引入了TrustVLM，一个无需训练的框架。\n\n### 核心思想与方法\n\n*   **动机：** TrustVLM的提出受到以下观察和洞察的启发：\n    *   VLMs中存在的模态差距。\n    *   某些概念在图像嵌入空间中表现得更为清晰和独特。\n*   **方法：** TrustVLM提出了一种新颖的置信度评分函数，该函数利用图像嵌入空间来改进错误分类的检测。\n\n### 实验评估与成果\n\n*   **评估范围：** 研究团队在17个多样化的数据集上，使用4种架构和2个VLM对TrustVLM进行了严格评估。\n*   **性能表现：** 与现有基线相比，TrustVLM展示了最先进的性能，具体改进如下：\n    *   AURC（曲线下面积）提升高达51.87%。\n    *   AUROC（接收者操作特征曲线下面积）提升9.14%。\n    *   FPR95（95%召回率下的误报率）降低32.42%。\n\n### 意义与展望\n\n*   通过在不要求重新训练模型的情况下提高其可靠性，TrustVLM为VLMs在实际应用中的更安全部署铺平了道路。\n\n### 代码可用性\n\n*   相关代码将在指定URL提供。",
      "shortSummary": "视觉-语言模型（VLMs）虽强大但常给出自信的错误预测，在安全关键领域构成风险。本文提出TrustVLM，一个无需训练的框架，通过利用图像嵌入空间中的模态差距，改进了VLM预测的信任度估计。TrustVLM在多项基准测试中表现出最先进的性能，显著提高了错误分类检测能力。它在不需模型重训练的情况下增强了VLM的可靠性，为VLMs在实际应用中的安全部署奠定了基础。",
      "translated_title": "是否信任你的视觉-语言模型的预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM."
    },
    {
      "title": "MAGREF: 任意参考视频生成中的掩码引导 (原标题: MAGREF: Masked Guidance for Any-Reference Video Generation)",
      "link": "https://arxiv.org/abs/2505.23742",
      "pubDate": "Thu, 29 May 2025 13:58:15 GMT",
      "isoDate": "2025-05-29T13:58:15.000Z",
      "creator": "Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, Chongyang Ma",
      "summary": "## MAGREF：任意参考视频生成中的掩码引导\n\n### 1. 研究背景与挑战\n\n随着深度生成模型，特别是基于扩散的方法的兴起，视频生成技术取得了显著进展。然而，基于多个参考主体的视频生成仍然面临两大挑战：\n\n*   **多主体一致性：** 难以在生成的视频中保持多个主体之间的连贯性。\n*   **生成质量：** 在复杂的多主体场景下，确保高质量的视频输出。\n\n### 2. 提出的方法：MAGREF 框架\n\n本文提出了一种名为 **MAGREF** 的统一框架，用于实现任意参考视频生成。MAGREF 引入了**掩码引导（masked guidance）**机制，旨在实现基于多样参考图像和文本提示的连贯多主体视频合成。\n\n### 3. MAGREF 的核心机制\n\nMAGREF 框架包含两个关键创新机制：\n\n*   **区域感知动态掩码机制：**\n    *   该机制允许单一模型灵活处理各种主体（包括人类、物体和背景）的推理。\n    *   其设计无需对模型架构进行任何修改，提高了模型的通用性和适应性。\n\n*   **像素级通道拼接机制：**\n    *   该机制在通道维度上进行操作，以更好地保留参考图像中的外观特征。\n    *   这有助于确保生成视频中的主体外观与参考图像高度一致。\n\n### 4. 性能与成果\n\n*   **最先进的生成质量：** MAGREF 实现了当前最先进的视频生成质量。\n*   **强大的泛化能力：** 模型能够从单主体训练泛化到复杂的多主体场景。\n*   **连贯合成与精确控制：** 实现了多主体视频的连贯合成，并能对视频中的单个主体进行精确控制。\n*   **超越现有基线：** 在性能上超越了现有的开源和商业基线模型。\n\n### 5. 评估与未来展望\n\n*   **引入综合基准：** 为了促进评估，研究人员还引入了一个全面的多主体视频基准。\n*   **实验验证：** 广泛的实验证明了 MAGREF 方法的有效性。\n*   **重要意义：** MAGREF 的提出为可扩展、可控和高保真的多主体视频合成铺平了道路。",
      "shortSummary": "MAGREF是一个用于任意参考视频生成的统一框架，旨在解决多主体视频生成中的一致性和质量挑战。它通过引入掩码引导，并结合区域感知动态掩码和像素级通道拼接机制，实现了最先进的视频生成质量。MAGREF能从单主体训练泛化到复杂多主体场景，提供连贯合成和精确控制，并优于现有基线。该研究还引入了全面的多主体视频基准，为可扩展、可控、高保真的多主体视频合成奠定了基础。",
      "translated_title": "MAGREF: 任意参考视频生成中的掩码引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF"
    },
    {
      "title": "动物如何跳舞（当你没在看的时候） (原标题: How Animals Dance (When You're Not Looking))",
      "link": "https://arxiv.org/abs/2505.23738",
      "pubDate": "Thu, 29 May 2025 13:58:02 GMT",
      "isoDate": "2025-05-29T13:58:02.000Z",
      "creator": "Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz",
      "summary": "# 动物舞蹈视频生成框架\n\n本文介绍了一个基于关键帧的框架，用于生成与音乐同步且符合编舞的动物舞蹈视频。\n\n## 核心方法\n\n*   **关键帧生成与输入：**\n    *   该方法从少量代表不同动物姿态的关键帧开始。\n    *   这些关键帧可以通过文本到图像（text-to-image）提示或GPT-4o生成。\n*   **舞蹈合成公式化：**\n    *   舞蹈合成被表述为一个图优化问题。\n    *   目标是找到满足指定节拍编舞模式的最佳关键帧结构。\n    *   编舞模式可以从参考舞蹈视频中自动估计。\n*   **镜像姿态图像生成：**\n    *   引入了一种镜像姿态图像生成方法。\n    *   这对于捕捉舞蹈中的对称性至关重要。\n*   **中间帧合成：**\n    *   关键帧之间的中间帧使用视频扩散模型进行合成。\n\n## 成果与能力\n\n*   **高效输入：** 该方法仅需最少六个输入关键帧。\n*   **视频长度：** 能够生成长达30秒的舞蹈视频。\n*   **广泛适用性：** 适用于各种动物和音乐曲目。",
      "shortSummary": "该研究提出一个基于关键帧的AI框架，用于生成与音乐同步的动物舞蹈视频。它通过文本到图像或GPT-4o生成关键姿态，将舞蹈合成视为图优化问题，并利用视频扩散模型合成中间帧。该方法仅需少量关键帧，即可为多种动物和音乐生成长达30秒的舞蹈视频。",
      "translated_title": "动物如何跳舞（当你没在看的时候）",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present a keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from a few keyframes representing distinct animal poses -- generated via text-to-image prompting or GPT-4o -- we formulate dance synthesis as a graph optimization problem: find the optimal keyframe structure that satisfies a specified choreography pattern of beats, which can be automatically estimated from a reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across a wide range of animals and music tracks."
    },
    {
      "title": "ATLAS：在测试时优化记忆上下文的学习 (原标题: ATLAS: Learning to Optimally Memorize the Context at Test Time)",
      "link": "https://arxiv.org/abs/2505.23735",
      "pubDate": "Thu, 29 May 2025 13:57:16 GMT",
      "isoDate": "2025-05-29T13:57:16.000Z",
      "creator": "Ali Behrouz, Zeman Li, Praneeth Kacham, Majid Daliri, Yuan Deng, Peilin Zhong, Meisam Razaviyayn, Vahab Mirrokni",
      "summary": "### ATLAS：在测试时优化记忆上下文的学习\n\n本文介绍了ATLAS，一个旨在解决现有序列建模模型在处理长序列和长上下文理解方面局限性的新型长期记忆模块。\n\n*   **背景与现有模型局限性**\n    *   **Transformer模型**：\n        *   已成为序列建模领域最流行的骨干网络，在上下文检索任务中表现出色，并具备大规模学习能力。\n        *   其二次方的内存和时间复杂度限制了其在处理更长序列时的应用。\n    *   **现代循环神经网络（RNNs）**：\n        *   作为Transformer的替代架构被探索，尤其是在长期循环记忆模块方面取得了一定成功。\n        *   然而，它们在需要长上下文理解和向更长序列外推的任务中表现不佳。\n\n*   **RNNs设计缺陷分析**\n    作者观察到RNNs的这些不足源于其设计中的三个独立方面：\n    1.  **有限的记忆容量**：受限于记忆架构和输入特征映射。\n    2.  **在线更新性质**：记忆优化仅基于最后一个输入进行。\n    3.  **固定大小记忆的管理表达能力不足**。\n\n*   **ATLAS模型介绍**\n    *   **目标**：旨在增强上述三个方面。\n    *   **核心特点**：\n        *   一个高容量的长期记忆模块。\n        *   通过基于当前和过去令牌优化记忆来学习上下文，从而克服了长期记忆模型的在线更新性质。\n    *   **DeepTransformers**：\n        *   基于ATLAS的洞察，本文提出了一系列新的类Transformer架构，称为DeepTransformers。\n        *   这些架构是原始Transformer架构的严格泛化。\n\n*   **实验结果**\n    *   ATLAS在多项任务中超越了Transformer和最近的线性循环模型。\n    *   实验任务包括：语言建模、常识推理、记忆密集型任务和长上下文理解任务。\n    *   **显著成就**：ATLAS进一步提升了Titans模型在长上下文任务中的性能，在BABILong基准测试的1000万上下文长度上实现了超过80%的准确率提升。",
      "shortSummary": "ATLAS是一种新型长期记忆模块，旨在解决Transformer和现有循环神经网络在处理长序列和长上下文理解方面的局限。它通过优化基于当前和过去令牌的记忆来克服传统模型的在线更新和有限记忆容量问题。实验证明，ATLAS在语言建模、常识推理和长上下文理解等任务中，性能超越了Transformer和线性循环模型，尤其在1000万上下文长度的BABILong基准测试中，准确率提升超过80%。",
      "translated_title": "ATLAS：在测试时优化记忆上下文的学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark."
    },
    {
      "title": "ZPressor：面向可扩展前向3DGS的瓶颈感知压缩 (原标题: ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS)",
      "link": "https://arxiv.org/abs/2505.23734",
      "pubDate": "Thu, 29 May 2025 13:57:04 GMT",
      "isoDate": "2025-05-29T13:57:04.000Z",
      "creator": "Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang",
      "summary": "## ZPressor：面向可扩展前向3DGS的瓶颈感知压缩\n\n### 1. 研究背景与问题\n\n*   **前向3D高斯泼溅（3DGS）模型**：作为一种新兴的新颖视图合成解决方案，它支持单次推理，无需进行每场景的3DGS优化。\n*   **可扩展性限制**：这些模型的扩展性受到其编码器容量的根本限制。\n*   **性能下降与内存消耗**：随着输入视图数量的增加，编码器容量不足会导致性能下降或过度的内存消耗。\n\n### 2. 提出的解决方案：ZPressor\n\n*   **核心理念**：本研究从信息瓶颈原理的角度分析了前向3DGS框架，并引入了ZPressor。\n*   **ZPressor的特性**：\n    *   一个轻量级、与架构无关的模块。\n    *   旨在将多视图输入高效压缩成一个紧凑的潜在状态 `Z`。\n    *   在压缩过程中，保留了场景的基本信息，同时丢弃了冗余。\n\n### 3. ZPressor的工作机制\n\n*   **视图划分**：将输入视图划分为“锚点视图”（anchor views）和“支持视图”（support views）两部分。\n*   **信息压缩**：利用交叉注意力机制，将来自支持视图的信息压缩到锚点视图中。\n*   **生成潜在状态Z**：通过上述压缩过程，形成紧凑的压缩潜在状态 `Z`。\n\n### 4. 实验结果与优势\n\n*   **显著提升可扩展性**：ZPressor使现有前向3DGS模型能够在80GB GPU上扩展到超过100个480P分辨率的输入视图。\n*   **性能提升**：在适度输入视图设置下，将ZPressor集成到多个最先进的前向3DGS模型中，能够持续提升性能。\n*   **鲁棒性增强**：在密集视图设置下，ZPressor显著增强了模型的鲁棒性。\n*   **基准测试**：在DL3DV-10K和RealEstate10K两个大规模基准测试中验证了其有效性。\n\n### 5. 资源可用性\n\n*   项目页面、代码和训练模型可在其项目页面获取。",
      "shortSummary": "ZPressor是一种轻量级、瓶颈感知的压缩模块，旨在解决前向3DGS模型在多视图输入下受限于编码器容量导致的可扩展性问题。它通过将多视图输入压缩成紧凑的潜在状态`Z`，有效保留关键信息并去除冗余。ZPressor使模型能扩展到100多个视图，并在DL3DV-10K和RealEstate10K等基准测试中，持续提升了适度视图下的性能，并增强了密集视图设置下的鲁棒性。",
      "translated_title": "ZPressor：面向可扩展前向3DGS的瓶颈感知压缩",
      "images": [],
      "contentSource": "完整文章",
      "content": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state Z that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state Z. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor."
    },
    {
      "title": "AnySplat：无约束视角下的前馈3D高斯泼溅 (原标题: AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views)",
      "link": "https://arxiv.org/abs/2505.23716",
      "pubDate": "Thu, 29 May 2025 13:49:56 GMT",
      "isoDate": "2025-05-29T13:49:56.000Z",
      "creator": "Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai",
      "summary": "## AnySplat: 无约束视角下的前馈3D高斯泼溅\n\nAnySplat 是一种创新的前馈网络，旨在从非校准图像集合中实现新颖视图合成。它旨在解决传统神经渲染流程和现有前馈方法所面临的挑战。\n\n### 核心创新与区别\n\n*   **传统方法的局限性：** 传统的神经渲染流程需要已知的相机姿态和每场景优化，这限制了其在非受控环境中的应用。\n*   **现有前馈方法的挑战：** 尽管一些前馈方法已经出现，但它们在处理密集视图时往往面临巨大的计算负担。\n*   **AnySplat 的解决方案：** AnySplat 克服了这些限制，它能够通过单次前向传播预测所有必要信息，包括场景几何、外观以及相机参数。\n\n### AnySplat 的输出\n\nAnySplat 的一次前向传播会产生以下关键输出：\n\n*   **3D 高斯基元集合：** 这些基元编码了场景的几何结构和视觉外观。\n*   **相机内参和外参：** 为每张输入图像预测相应的相机内参和外参。\n\n### 主要优势与性能\n\n*   **统一设计与可扩展性：** AnySplat 的统一设计使其能够轻松处理随意捕获的多视图数据集，**无需任何姿态标注**，极大地简化了数据采集过程。\n*   **卓越的零样本评估表现：**\n    *   在零样本评估中，AnySplat 的质量在稀疏和密集视图场景下均能与有姿态感知的基线方法相媲美。\n    *   它显著超越了现有的无姿态方法。\n*   **显著降低渲染延迟：** 与基于优化的神经场相比，AnySplat 大幅减少了渲染延迟，从而实现了更高的效率。\n*   **实现实时新颖视图合成：** 这些优势使得 AnySplat 能够将实时新颖视图合成带入无约束的捕获场景中。\n\n### 作者\n\nLihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, Dahua Lin, Bo Dai。",
      "shortSummary": "AnySplat是一个创新的前馈网络，专为从非校准图像集合中进行新颖视图合成而设计。它克服了传统方法对已知相机姿态的依赖以及现有前馈方法在密集视图下的计算瓶颈。AnySplat通过单次前向传播即可预测3D高斯基元及相机参数，无需姿态标注。在零样本评估中，其性能媲美甚至超越现有方法，并显著降低渲染延迟，为无约束捕获下的实时视图合成铺平道路。",
      "translated_title": "AnySplat：无约束视角下的前馈3D高斯泼溅",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/"
    },
    {
      "title": "VF-Eval：评估多模态大语言模型在生成AIGC视频反馈方面的能力 (原标题: VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos)",
      "link": "https://arxiv.org/abs/2505.23693",
      "pubDate": "Thu, 29 May 2025 13:31:13 GMT",
      "isoDate": "2025-05-29T13:31:13.000Z",
      "creator": "Tingyu Song, Tongyan Hu, Guo Gan, Yilun Zhao",
      "summary": "### VF-Eval：评估多模态大语言模型在AIGC视频反馈方面的能力\n\n**背景与研究动机：**\n*   多模态大语言模型（MLLMs）近期在视频问答领域得到了广泛研究。\n*   然而，现有的大多数评估主要集中在自然视频上，忽略了合成视频，特别是AI生成内容（AIGC）视频。\n*   尽管一些视频生成工作已开始利用MLLMs来评估生成视频的质量，但MLLMs在解释AIGC视频并提供反馈方面的能力仍未得到充分探索。\n\n**提出的解决方案：VF-Eval基准测试**\n*   为了解决上述研究空白，研究人员提出了一个新的基准测试——VF-Eval。\n*   VF-Eval旨在全面评估MLLMs在AIGC视频上的能力，它引入了四个核心任务：\n    1.  **连贯性验证 (coherence validation)**：评估模型识别AIGC视频内容逻辑连贯性的能力。\n    2.  **错误感知 (error awareness)**：测试模型能否感知到AIGC视频中存在的错误或异常。\n    3.  **错误类型检测 (error type detection)**：要求模型识别并分类AIGC视频中出现的具体错误类型。\n    4.  **推理评估 (reasoning evaluation)**：考察模型对AIGC视频内容进行深层推理和理解的能力。\n\n**评估结果与发现：**\n*   研究团队在VF-Eval基准上评估了13个当前前沿的MLLMs。\n*   评估结果显示，即使是表现最好的模型（如GPT-4.1），也难以在所有任务中持续取得良好表现。\n*   这一发现突出表明了VF-Eval基准测试的挑战性，并揭示了当前MLLMs在处理AIGC视频反馈方面的局限性。\n\n**实际应用探索：RePrompt实验**\n*   为了进一步探究VF-Eval在改进视频生成方面的实际应用潜力，研究人员进行了一项名为RePrompt的实验。\n*   该实验旨在证明，通过使MLLMs更紧密地与人类反馈对齐，可以有效地提升视频生成质量。\n*   这为未来利用MLLMs改进AIGC视频生成提供了新的方向。",
      "shortSummary": "VF-Eval是一个新基准，旨在评估多模态大语言模型（MLLMs）在AI生成内容（AIGC）视频反馈方面的能力。现有研究多集中于自然视频，忽略了AIGC视频的评估。VF-Eval包含连贯性验证、错误感知、错误类型检测和推理评估四个任务。评估发现，即使是GPT-4.1等领先模型，也难以在所有任务中表现出色，这表明该领域仍具挑战。研究还通过RePrompt实验证明，将MLLMs与人类反馈对齐有助于改进视频生成。",
      "translated_title": "VF-Eval：评估多模态大语言模型在生成AIGC视频反馈方面的能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation."
    },
    {
      "title": "用于视觉推理的接地强化学习 (原标题: Grounded Reinforcement Learning for Visual Reasoning)",
      "link": "https://arxiv.org/abs/2505.23678",
      "pubDate": "Thu, 29 May 2025 13:20:26 GMT",
      "isoDate": "2025-05-29T13:20:26.000Z",
      "creator": "Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, Katerina Fragkiadaki",
      "summary": "# ViGoRL：用于视觉推理的接地强化学习\n\n## 引言与背景\n强化学习（RL）在处理语言模型（如数学和编程任务）的思维链方面取得了显著进展。然而，视觉推理引入了额外的复杂性，因为它要求模型引导视觉注意力、解释感知输入，并将抽象推理与空间证据相结合。\n\n## ViGoRL模型介绍\n为解决视觉推理的挑战，研究人员引入了**ViGoRL (Visually Grounded Reinforcement Learning)**，这是一个通过RL训练的视觉-语言模型。\n*   **核心理念**：ViGoRL的独特之处在于它能**显式地将每个推理步骤锚定到特定的视觉坐标**，从而将抽象推理与具体的视觉证据联系起来。\n*   **设计灵感与工作机制**：该模型的设计灵感来源于人类的视觉决策过程。ViGoRL学习生成空间接地的推理轨迹，从而在每一步将视觉注意力引导至与任务相关的区域。\n*   **创新特性——多轮RL框架**：当需要进行细粒度探索时，ViGoRL引入了一种新颖的**多轮RL框架**。该框架使模型能够随着推理的展开，动态地放大（zoom into）预测的坐标，从而实现更精细的视觉反馈。\n\n## 实验与结果\nViGoRL在多样化的视觉推理基准测试中进行了评估，包括：\n*   **空间推理**：SAT-2 和 BLINK\n*   **视觉搜索**：V*bench\n*   **基于网络的接地**：ScreenSpot 和 VisualWebArena\n\n实验结果表明：\n*   **卓越性能**：ViGoRL始终优于缺乏显式接地机制的监督微调和传统RL基线模型。\n*   **多轮RL的贡献**：结合了放大视觉反馈的多轮RL显著提升了ViGoRL在定位小型GUI元素和视觉搜索任务上的性能，在V*Bench上达到了86.4%的准确率。\n*   **接地行为的增强**：研究发现，接地机制还放大了其他视觉行为，例如区域探索、接地子目标设定和视觉验证。\n*   **人类评估**：人类评估结果显示，ViGoRL生成的视觉参考不仅在空间上准确，而且有助于理解模型的推理步骤。\n\n## 结论\n研究结果表明，视觉接地的强化学习是一种强大的范式，能够赋予模型通用的视觉推理能力。",
      "shortSummary": "ViGoRL是一种新型的视觉-语言模型，通过强化学习训练，能将每个推理步骤显式地锚定到特定视觉坐标。受人类视觉决策启发，它能生成空间接地的推理轨迹，并利用多轮RL框架动态放大视觉区域进行细粒度探索。ViGoRL在多项视觉推理基准测试中表现优异，显著超越传统方法，尤其在定位小型GUI元素和视觉搜索方面。研究表明，接地机制增强了模型的视觉行为，且其视觉参考准确且有助于理解。这证明视觉接地的RL是通用视觉推理的强大范式。",
      "translated_title": "用于视觉推理的接地强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "While reinforcement learning (RL) over chains of thought has significantly advanced language models in tasks such as mathematics and coding, visual reasoning introduces added complexity by requiring models to direct visual attention, interpret perceptual inputs, and ground abstract reasoning in spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model trained with RL to explicitly anchor each reasoning step to specific visual coordinates. Inspired by human visual decision-making, ViGoRL learns to produce spatially grounded reasoning traces, guiding visual attention to task-relevant regions at each step. When fine-grained exploration is required, our novel multi-turn RL framework enables the model to dynamically zoom into predicted coordinates as reasoning unfolds. Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK for spatial reasoning, V*bench for visual search, and ScreenSpot and VisualWebArena for web-based grounding--ViGoRL consistently outperforms both supervised fine-tuning and conventional RL baselines that lack explicit grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual feedback significantly improves ViGoRL's performance on localizing small GUI elements and visual search, achieving 86.4% on V*Bench. Additionally, we find that grounding amplifies other visual behaviors such as region exploration, grounded subgoal setting, and visual verification. Finally, human evaluations show that the model's visual references are not only spatially accurate but also helpful for understanding model reasoning steps. Our results show that visually grounded RL is a strong paradigm for imbuing models with general-purpose visual reasoning."
    },
    {
      "title": "GSO：用于评估软件工程智能体（SWE-Agents）的挑战性软件优化任务 (原标题: GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents)",
      "link": "https://arxiv.org/abs/2505.23671",
      "pubDate": "Thu, 29 May 2025 13:14:55 GMT",
      "isoDate": "2025-05-29T13:14:55.000Z",
      "creator": "Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, Ion Stoica",
      "summary": "GSO是一个旨在评估语言模型（SWE-Agents）在开发高性能软件方面能力的基准。高性能软件的开发是一项复杂且需要专业知识的任务。\n\n**GSO基准的开发与方法：**\n*   **自动化流程：** GSO开发了一个自动化流程，用于生成和执行性能测试。\n*   **任务识别：** 该流程通过分析代码库的提交历史，识别出102个具有挑战性的优化任务。\n*   **任务范围：** 这些任务涵盖了10个不同的代码库，涉及多种领域和编程语言。\n*   **智能体任务：** 智能体（Agent）会获得一个代码库和性能测试作为精确规范，其任务是提高软件的运行时效率。智能体的表现将与专家开发者的优化结果进行衡量。\n\n**评估结果与失败模式：**\n*   **定量评估：** 对领先的SWE-Agents进行的定量评估显示，它们的表现显著不佳，成功率低于5%。即使通过推理时间扩展，改进也十分有限。\n*   **定性分析：** 研究识别出了一些关键的失败模式，包括：\n    *   难以处理低级语言。\n    *   采用“懒惰”的优化策略。\n    *   在准确识别性能瓶颈方面存在挑战。\n\n**发布与未来研究：**\n*   为了促进未来的研究，GSO基准的代码、工件以及智能体轨迹均已发布。",
      "shortSummary": "GSO是一个新基准，旨在评估AI软件工程智能体（SWE-Agents）的软件优化能力。该基准包含102个来自10个代码库的挑战性优化任务。评估结果显示，领先的SWE-Agents成功率不足5%，表现不佳。主要失败模式包括难以处理低级语言、采用“懒惰”优化策略以及瓶颈定位困难。GSO的代码和相关工件已发布，以推动未来研究。",
      "translated_title": "GSO：用于评估软件工程智能体（SWE-Agents）的挑战性软件优化任务",
      "images": [],
      "contentSource": "完整文章",
      "content": "Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research."
    },
    {
      "title": "D-AR：通过自回归模型实现扩散 (原标题: D-AR: Diffusion via Autoregressive Models)",
      "link": "https://arxiv.org/abs/2505.23660",
      "pubDate": "Thu, 29 May 2025 13:09:25 GMT",
      "isoDate": "2025-05-29T13:09:25.000Z",
      "creator": "Ziteng Gao, Mike Zheng Shou",
      "summary": "### D-AR：通过自回归模型实现扩散\n\n本文介绍了一种名为“D-AR”（Diffusion via Autoregressive models）的新范式，它将图像扩散过程重新定义为一种标准的自回归过程，采用传统的“下一个词元预测”方式。\n\n#### 核心机制\n\n*   **词元化器设计**：D-AR首先设计了一个词元化器，将图像转换为离散词元序列。这些词元在不同位置可以被解码为像素空间中不同的扩散去噪步骤。\n*   **粗到细的顺序**：得益于扩散特性，这些词元自然地遵循从粗到细的顺序，这直接适用于自回归建模。\n*   **标准自回归建模**：该方法在这些词元上应用标准的“下一个词元预测”，不修改任何底层设计（无论是因果掩码还是训练/推理策略）。\n*   **镜像扩散过程**：这种顺序的自回归词元生成直接反映了图像空间中的扩散过程。一旦自回归模型生成了一组增量词元，就可以以流式方式直接将这些词元解码为相应的扩散去噪步骤。\n\n#### 显著特性\n\n*   **一致性预览**：D-AR自然支持在仅生成部分词元时提供一致的预览。\n*   **零样本布局控制合成**：该方法还能够实现零样本的布局控制合成。\n\n#### 性能表现\n\n*   在标准ImageNet基准测试中，D-AR使用一个775M Llama骨干网络和256个离散词元，实现了2.09的FID（Fréchet Inception Distance）分数。\n\n#### 未来展望\n\n*   作者希望这项工作能够启发未来在视觉合成统一自回归架构方面的研究，特别是与大型语言模型的结合。",
      "shortSummary": "D-AR（Diffusion via Autoregressive models）是一种将图像扩散过程重构为自回归“下一个词元预测”任务的新范式。它通过将图像转换为遵循粗到细顺序的离散词元序列，并应用标准自回归模型进行生成，直接镜像了扩散过程。该方法支持一致性预览和零样本布局控制合成，并在ImageNet上使用775M Llama骨干网络实现了2.09 FID。这项工作旨在推动视觉合成中统一自回归架构（尤其是与大型语言模型结合）的研究。",
      "translated_title": "D-AR：通过自回归模型实现扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents Diffusion via Autoregressive models (D-AR), a new paradigm recasting the image diffusion process as a vanilla autoregressive procedure in the standard next-token-prediction fashion. We start by designing the tokenizer that converts images into sequences of discrete tokens, where tokens in different positions can be decoded into different diffusion denoising steps in the pixel space. Thanks to the diffusion properties, these tokens naturally follow a coarse-to-fine order, which directly lends itself to autoregressive modeling. Therefore, we apply standard next-token prediction on these tokens, without modifying any underlying designs (either causal masks or training/inference strategies), and such sequential autoregressive token generation directly mirrors the diffusion procedure in image space. That is, once the autoregressive model generates an increment of tokens, we can directly decode these tokens into the corresponding diffusion denoising step in the streaming manner. Our pipeline naturally reveals several intriguing properties, for example, it supports consistent previews when generating only a subset of tokens and enables zero-shot layout-controlled synthesis. On the standard ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone with 256 discrete tokens. We hope our work can inspire future research on unified autoregressive architectures of visual synthesis, especially with large language models. Code and models will be available at https://github.com/showlab/D-AR"
    },
    {
      "title": "推理模型更容易产生幻觉吗？ (原标题: Are Reasoning Models More Prone to Hallucination?)",
      "link": "https://arxiv.org/abs/2505.23646",
      "pubDate": "Thu, 29 May 2025 12:53:41 GMT",
      "isoDate": "2025-05-29T12:53:41.000Z",
      "creator": "Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, Tat-Seng Chua",
      "summary": "### 推理模型幻觉倾向研究\n\n**引言：**\n\n*   近期发展的大型推理模型（LRMs）在解决复杂任务时展现出强大的链式思考（CoT）推理能力。\n*   然而，由于这些LRMs大多通过对形式推理任务进行后训练而开发，它们是否能将推理能力泛化以帮助减少事实寻求任务中的幻觉，目前尚不明确且存在争议。\n*   例如，DeepSeek-R1报告在SimpleQA（一个事实寻求基准）上性能有所提升，而OpenAI-o3却观察到更严重的幻觉。这种差异自然引出了一个研究问题：“推理模型是否更容易产生幻觉？”\n\n**研究视角与发现：**\n\n本文从三个角度探讨了上述问题：\n\n1.  **对LRM幻觉的整体评估：**\n    *   我们的分析揭示，经过完整后训练流程（包括冷启动监督微调SFT和可验证奖励强化学习RL）的LRMs通常能减轻其幻觉。\n    *   相比之下，仅进行蒸馏训练或在没有冷启动微调的情况下进行RL训练，会引入更细微的幻觉。\n\n2.  **行为分析：**\n    *   为了探究不同后训练流程如何改变对LRM幻觉的影响，我们进行了行为分析。\n    *   我们识别了两种直接影响LRM事实性的关键认知行为：\n        *   **缺陷重复（Flaw Repetition）：** 表面层面的推理尝试反复遵循相同的底层缺陷逻辑。\n        *   **思考-答案不匹配（Think-Answer Mismatch）：** 最终答案未能忠实地匹配先前的链式思考过程。\n\n3.  **从模型不确定性角度探究幻觉机制：**\n    *   我们进一步从模型不确定性的角度调查了LRM幻觉背后的机制。\n    *   研究发现，LRM幻觉的增加通常与模型不确定性与事实准确性之间的错位（misalignment）有关。\n\n**结论：**\n\n*   我们的工作为理解大型推理模型中的幻觉提供了初步的认识。",
      "shortSummary": "本研究探讨了大型推理模型（LRMs）是否更容易产生幻觉。研究发现，经过完整后训练流程（SFT+RL）的LRMs通常能减轻幻觉，而单纯蒸馏或无冷启动RL会引入更多幻觉。此外，模型事实性受“缺陷重复”和“思考-答案不匹配”两种行为影响。幻觉增加常与模型不确定性与事实准确性错位相关。本研究为理解LRM幻觉提供了初步认识。",
      "translated_title": "推理模型更容易产生幻觉吗？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently evolved large reasoning models (LRMs) show powerful performance in solving complex tasks with long chain-of-thought (CoT) reasoning capability. As these LRMs are mostly developed by post-training on formal reasoning tasks, whether they generalize the reasoning capability to help reduce hallucination in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1 reports increased performance on SimpleQA, a fact-seeking benchmark, while OpenAI-o3 observes even severer hallucination. This discrepancy naturally raises the following research question: Are reasoning models more prone to hallucination? This paper addresses the question from three perspectives. (1) We first conduct a holistic evaluation for the hallucination in LRMs. Our analysis reveals that LRMs undergo a full post-training pipeline with cold start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate their hallucination. In contrast, both distillation alone and RL training without cold start fine-tuning introduce more nuanced hallucinations. (2) To explore why different post-training pipelines alters the impact on hallucination in LRMs, we conduct behavior analysis. We characterize two critical cognitive behaviors that directly affect the factuality of a LRM: Flaw Repetition, where the surface-level reasoning attempts repeatedly follow the same underlying flawed logic, and Think-Answer Mismatch, where the final answer fails to faithfully match the previous CoT process. (3) Further, we investigate the mechanism behind the hallucination of LRMs from the perspective of model uncertainty. We find that increased hallucination of LRMs is usually associated with the misalignment between model uncertainty and factual accuracy. Our work provides an initial understanding of the hallucination in LRMs."
    },
    {
      "title": "ZeroSep：零训练分离音频中的任何内容 (原标题: ZeroSep: Separate Anything in Audio with Zero Training)",
      "link": "https://arxiv.org/abs/2505.23625",
      "pubDate": "Thu, 29 May 2025 12:31:45 GMT",
      "isoDate": "2025-05-29T12:31:45.000Z",
      "creator": "Chao Huang, Yuesheng Ma, Junxuan Huang, Susan Liang, Yunlong Tang, Jing Bi, Wenqiang Liu, Nima Mesgarani, Chenliang Xu",
      "summary": "## ZeroSep：零训练音频源分离\n\n*   **背景与挑战**\n    *   音频源分离是机器理解复杂声学环境的基础，并支撑着众多音频应用。\n    *   当前主流的监督式深度学习方法虽然强大，但存在局限性：\n        *   需要大量、特定任务的标注数据。\n        *   难以泛化到真实世界声学场景的巨大变异性和开放集特性。\n\n*   **核心发现与方法**\n    *   受生成式基础模型成功的启发，研究人员探索了预训练的文本引导音频扩散模型是否能克服上述局限。\n    *   他们发现了一个令人惊讶的结论：在正确配置下，仅通过预训练的文本引导音频扩散模型即可实现零样本（zero-shot）源分离。\n    *   该方法被命名为 **ZeroSep**。\n    *   **工作原理**：\n        1.  将混合音频反转（invert）到扩散模型的潜在空间。\n        2.  利用文本条件（text conditioning）引导去噪过程，以恢复单个声源。\n\n*   **ZeroSep 的优势与特点**\n    *   **零训练/零微调**：无需任何特定任务的训练或微调。\n    *   **任务重用**：将生成式扩散模型重新用于判别性分离任务。\n    *   **开放集支持**：通过其丰富的文本先验，固有地支持开放集场景（即处理未见过的声源）。\n    *   **兼容性强**：兼容多种预训练的文本引导音频扩散骨干网络。\n    *   **性能卓越**：在多个分离基准测试中展现出强大的分离性能，甚至超越了监督式方法。",
      "shortSummary": "ZeroSep是一种创新的音频源分离方法，它利用预训练的文本引导音频扩散模型实现零样本分离。该方法无需任何任务特定训练或微调，通过将混合音频反转到潜在空间并利用文本引导去噪来恢复单个声源。ZeroSep支持开放集场景，兼容多种模型，并在分离性能上超越了许多监督式方法，为音频理解提供了高效且灵活的解决方案。",
      "translated_title": "ZeroSep：零训练分离音频中的任何内容",
      "images": [],
      "contentSource": "完整文章",
      "content": "Audio source separation is fundamental for machines to understand complex acoustic environments and underpins numerous audio applications. Current supervised deep learning approaches, while powerful, are limited by the need for extensive, task-specific labeled data and struggle to generalize to the immense variability and open-set nature of real-world acoustic scenes. Inspired by the success of generative foundation models, we investigate whether pre-trained text-guided audio diffusion models can overcome these limitations. We make a surprising discovery: zero-shot source separation can be achieved purely through a pre-trained text-guided audio diffusion model under the right configuration. Our method, named ZeroSep, works by inverting the mixed audio into the diffusion model's latent space and then using text conditioning to guide the denoising process to recover individual sources. Without any task-specific training or fine-tuning, ZeroSep repurposes the generative diffusion model for a discriminative separation task and inherently supports open-set scenarios through its rich textual priors. ZeroSep is compatible with a variety of pre-trained text-guided audio diffusion backbones and delivers strong separation performance on multiple separation benchmarks, surpassing even supervised methods."
    }
  ],
  "lastUpdated": "2025-05-31T09:29:44.771Z"
}