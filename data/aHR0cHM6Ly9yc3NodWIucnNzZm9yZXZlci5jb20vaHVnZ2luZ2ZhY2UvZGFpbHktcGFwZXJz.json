{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Concerto：联合2D-3D自监督学习涌现空间表征 (原标题: Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations)",
      "link": "https://arxiv.org/abs/2510.23607",
      "pubDate": "Mon, 27 Oct 2025 13:59:59 GMT",
      "isoDate": "2025-10-27T13:59:59.000Z",
      "creator": "Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao",
      "summary": "### Concerto：联合2D-3D自监督学习涌现空间表征\n\n**1. 灵感来源与核心理念**\n\n*   **人类学习模式：** 人类通过多感官协同作用学习抽象概念，一旦这些概念形成，通常可以仅通过单一模态进行回忆。\n*   **Concerto的模拟：** 受此原理启发，Concerto被提出，它是一个极简主义模型，旨在模拟人类在空间认知方面的概念学习过程。\n\n**2. 模型机制**\n\n*   Concerto结合了两种关键的自监督学习机制：\n    *   **3D模态内自蒸馏 (3D intra-modal self-distillation)：** 专注于在3D数据内部进行知识蒸馏和学习。\n    *   **2D-3D跨模态联合嵌入 (2D-3D cross-modal joint embedding)：** 旨在学习2D和3D模态之间的共享表征。\n\n**3. 主要成果与性能优势**\n\n*   **高质量空间特征：** 尽管模型设计简洁，Concerto能够学习到更连贯、信息更丰富的空间特征，这一点通过零样本可视化得到了清晰的证明。\n*   **超越现有SOTA模型：**\n    *   在用于3D场景感知的线性探测（linear probing）任务中，Concerto的表现显著优于：\n        *   独立的SOTA 2D自监督模型，性能提升了14.2%。\n        *   独立的SOTA 3D自监督模型，性能提升了4.8%。\n        *   甚至超越了简单地拼接2D和3D特征的方法。\n*   **刷新场景理解基准：**\n    *   通过全面的微调（full fine-tuning），Concerto在多个场景理解基准测试中取得了新的SOTA（State-of-the-Art）结果，例如在ScanNet数据集上实现了80.7%的mIoU。\n\n**4. 扩展应用与未来潜力**\n\n*   **视频提升点云空间理解：** 论文进一步展示了Concerto的一个变体，该变体专门为视频提升点云（video-lifted point cloud）的空间理解任务进行了优化。\n*   **开放世界感知：** 研究团队还开发了一个翻译器，能够将Concerto学习到的表征线性投影到CLIP的语言空间中，从而赋予模型开放世界感知的能力。\n\n**5. 核心结论**\n\n*   这些实验结果有力地证明，Concerto能够涌现出具有卓越细粒度几何和语义一致性的空间表征。\n\n**6. 背景信息**\n\n*   该研究成果将于NeurIPS 2025发表。\n*   由Pointcept团队开发。",
      "shortSummary": "Concerto是一种受人类多感官学习启发的联合2D-3D自监督学习模型，用于空间认知。它结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入，学习到更连贯、信息更丰富的空间特征。Concerto在3D场景感知任务中显著超越了现有SOTA 2D和3D自监督模型，并在多个场景理解基准测试中取得了新的SOTA结果，展现出卓越的细粒度几何和语义一致性，并支持开放世界感知。",
      "translated_title": "Concerto：联合2D-3D自监督学习涌现空间表征",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency."
    },
    {
      "title": "Track, Inpaint, Resplat：基于渐进式纹理填充的主体驱动3D和4D生成 (原标题: Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling)",
      "link": "https://arxiv.org/abs/2510.23605",
      "pubDate": "Mon, 27 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-27T13:59:51.000Z",
      "creator": "Shuhong Zheng, Ashkan Mirzaei, Igor Gilitschenski",
      "summary": "# TIRE：主体驱动3D和4D生成的新方法\n\n本文介绍了一种名为TIRE（Track, Inpaint, REsplat）的新颖方法，旨在解决当前3D/4D生成技术在主体身份保持方面的挑战，特别是在个性化或主体驱动生成场景中。\n\n## 背景与挑战\n*   **现有方法局限：** 当前的3D/4D生成方法通常侧重于实现照片级真实感、高效率和美学效果。\n*   **身份保持不足：** 然而，这些方法在从一个或少数图像生成特定主体（如人物或物体）时，往往难以在不同视角下保持该主体的语义身份。\n*   **个性化生成待探索：** 主体驱动的3D/4D生成（即利用少量图像生成特定主体内容）是一个仍未被充分探索的领域。\n\n## TIRE方法概述\nTIRE方法以一个由现有3D生成模型产生的初始3D资产作为输入，并采用三阶段流程来改进主体身份的保持：\n\n1.  **跟踪 (Track)：**\n    *   利用视频跟踪技术，精确识别出3D资产中需要进行修改或更新的区域。\n2.  **修复 (Inpaint)：**\n    *   采用一个主体驱动的2D修复模型，对第一步中识别出的区域进行渐进式纹理填充。这一步骤确保了修复后的纹理与主体的身份保持一致。\n3.  **重投影 (REsplat)：**\n    *   将经过修改和修复的2D多视角观测结果，重新投影回3D空间。在此过程中，TIRE确保了重投影后的3D模型在保持主体身份的同时，也维持了整体的一致性。\n\n## 实验结果与项目信息\n*   **显著改进：** 广泛的实验结果表明，与现有最先进的方法相比，TIRE显著提高了3D/4D生成中主体身份的保持能力。\n*   **项目网站：** 更多详情和相关资源可在项目网站获取：this https URL\n*   **发表信息：** 本文已提交至NeurIPS 2025，共38页。",
      "shortSummary": "TIRE（Track, Inpaint, REsplat）是一种新颖的主体驱动3D/4D生成方法，旨在解决现有技术在不同视角下难以保持主体身份的问题。该方法通过视频跟踪识别需修改区域，利用主体驱动2D修复模型进行渐进式填充，随后将修改后的2D观测结果重投影回3D。实验证明，TIRE显著提升了3D/4D生成中主体身份的保持能力。",
      "translated_title": "Track, Inpaint, Resplat：基于渐进式纹理填充的主体驱动3D和4D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/."
    },
    {
      "title": "PixelRefer：一个用于任意粒度时空对象指代的统一框架 (原标题: PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity)",
      "link": "https://arxiv.org/abs/2510.23603",
      "pubDate": "Mon, 27 Oct 2025 13:59:32 GMT",
      "isoDate": "2025-10-27T13:59:32.000Z",
      "creator": "Yuqian Yuan, Wenqiao Zhang, Xin Li, Shihao Wang, Kehan Li, Wentong Li, Jun Xiao, Lei Zhang, Beng Chin Ooi",
      "summary": "### PixelRefer：统一的细粒度时空对象指代框架\n\n**核心问题：**\n当前的多模态大语言模型（MLLMs）在开放世界的视觉理解中展现出强大的通用能力，但它们主要侧重于整体的、场景级的理解，往往忽视了对细粒度、以对象为中心的推理的需求。\n\n**PixelRefer 解决方案：**\n本文提出了 PixelRefer，一个统一的区域级 MLLM 框架，旨在实现对用户指定区域（包括图像和视频）的先进细粒度理解。\n\n**关键创新与组件：**\n\n*   **尺度自适应对象分词器（SAOT）：**\n    *   受 LLM 注意力主要集中在对象级 token 上的观察启发，PixelRefer 提出了 SAOT。 \n    *   SAOT 能够从自由形式的区域生成紧凑且语义丰富的对象表示。\n\n*   **PixelRefer-Lite（高效变体）：**\n    *   分析表明，全局视觉 token 主要在 LLM 的早期层中发挥作用。\n    *   基于此洞察，PixelRefer-Lite 被设计为一个高效变体，它采用了一个**对象中心注入模块（Object-Centric Infusion module）**。\n    *   该模块将全局上下文预先融合到对象 token 中，从而形成一个轻量级的**仅对象框架（Object-Only Framework）**。\n    *   这显著降低了计算成本，同时保持了高语义保真度。\n\n*   **PixelRefer-2.2M 数据集：**\n    *   为了促进细粒度指令调优，研究人员精心策划了 PixelRefer-2.2M，这是一个高质量的以对象为中心的指令数据集。\n\n**实验结果：**\n\n*   在各种基准测试中进行的广泛实验验证了 PixelRefer 的卓越性能，它在更少的训练样本下实现了领先的表现。\n*   PixelRefer-Lite 在保持竞争性准确性的同时，在效率方面取得了显著提升。",
      "shortSummary": "PixelRefer是一个统一的区域级多模态大语言模型（MLLM）框架，专注于图像和视频中细粒度、以对象为中心的理解。它引入了尺度自适应对象分词器（SAOT）来生成紧凑的对象表示。其高效变体PixelRefer-Lite通过对象中心注入模块显著降低了计算成本。该框架在多个基准测试中实现了领先性能，并提供了PixelRefer-2.2M数据集以支持细粒度指令调优，同时PixelRefer-Lite在效率上也有显著提升。",
      "translated_title": "PixelRefer：一个用于任意粒度时空对象指代的统一框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency."
    },
    {
      "title": "PRISM-Bench：一个基于谜题的视觉任务基准，包含CoT错误检测 (原标题: PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection)",
      "link": "https://arxiv.org/abs/2510.23594",
      "pubDate": "Mon, 27 Oct 2025 13:57:52 GMT",
      "isoDate": "2025-10-27T13:57:52.000Z",
      "creator": "Yusu Qian, Cheng Wan, Chao Jia, Yinfei Yang, Qingyu Zhao, Zhe Gan",
      "summary": "## PRISM-Bench：一个用于评估多模态推理的诊断基准\n\n本文介绍了 **PRISM-Bench**，这是一个专门设计的基于谜题的视觉挑战基准。它旨在不仅评估模型解决问题的能力，更重要的是，评估其推理过程的展开方式。\n\n### 核心创新：CoT错误检测任务\n\n与以往仅测量最终答案准确性的评估不同，PRISM-Bench引入了一个独特的诊断任务：\n\n*   **任务描述**：给定一个视觉谜题和一个包含**恰好一个错误**的逐步思维链（Chain-of-Thought, CoT）。\n*   **模型目标**：模型必须识别出**第一个不正确的步骤**。\n*   **评估优势**：这种设置能够对模型的逻辑一致性、错误检测能力和视觉推理能力进行细粒度评估。\n\n### 谜题特点\n\nPRISM-Bench中的谜题具有以下特点：\n\n*   要求多步的符号、几何和类比推理。\n*   旨在抵抗基于表面模式匹配的捷径，确保模型进行深层推理。\n\n### 评估结果与发现\n\n对最先进的多模态大语言模型（MLLMs）进行的评估揭示了一个持续存在的差距：\n\n*   **流畅生成与忠实推理的差距**：模型虽然能够生成看似合理的思维链（CoTs），但往往无法定位简单的逻辑错误。\n\n### 重要意义\n\nPRISM-Bench通过以下方式提供了更深入的洞察：\n\n*   **分离答案生成与推理验证**：它将答案的生成与推理过程的验证分离开来。\n*   **提供更清晰的视角**：为多模态推理能力提供了更清晰的视角。\n*   **强调诊断性评估的重要性**：强调了在开发值得信赖的MLLMs时，诊断性评估协议的必要性。",
      "shortSummary": "PRISM-Bench是一个新的视觉谜题基准，旨在评估多模态大语言模型（MLLMs）的推理过程，而不仅仅是最终答案。其核心创新是诊断任务：模型需识别包含一个错误的逐步思维链（CoT）中的第一个错误步骤。评估显示，MLLMs在生成看似合理的CoT时，常难以发现简单的逻辑错误。该基准通过分离答案生成与推理验证，为理解多模态推理能力提供了新视角，并强调了诊断性评估对开发可信赖MLLMs的重要性。",
      "translated_title": "PRISM-Bench：一个基于谜题的视觉任务基准，包含CoT错误检测",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs."
    },
    {
      "title": "FARMER：基于像素的流自回归Transformer (原标题: FARMER: Flow AutoRegressive Transformer over Pixels)",
      "link": "https://arxiv.org/abs/2510.23588",
      "pubDate": "Mon, 27 Oct 2025 13:54:08 GMT",
      "isoDate": "2025-10-27T13:54:08.000Z",
      "creator": "Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, Rui Zhu",
      "summary": "# FARMER：基于像素的流自回归Transformer\n\nFARMER 是一种新颖的端到端生成框架，旨在解决直接建模原始数据（特别是视觉像素数据）显式似然的挑战，该挑战因极长的序列和高维空间而变得复杂。\n\n## 核心理念与方法\n*   **统一模型**：FARMER 创新性地将归一化流（Normalizing Flows, NF）和自回归（Autoregressive, AR）模型结合起来。\n*   **目标**：实现可处理的似然估计和直接从原始像素生成高质量图像。\n*   **工作机制**：\n    *   利用可逆自回归流将图像转换为潜在序列。\n    *   通过自回归模型隐式地建模这些潜在序列的分布。\n\n## 关键创新与技术\n为了解决像素级建模的冗余和复杂性，FARMER 引入了多项创新：\n*   **自监督降维方案**：将 NF 潜在通道划分为信息丰富组和冗余组，从而实现更有效和高效的 AR 建模。\n*   **一步蒸馏方案**：显著加速推理速度。\n*   **基于重采样的无分类器指导算法**：提升图像生成质量。\n\n## 实验结果与性能\n*   FARMER 在实验中展现出与现有基于像素的生成模型相比具有竞争力的性能。\n*   它能够提供精确的似然估计。\n*   支持可扩展的训练。\n\n## 其他信息\n*   本文是一份字节跳动种子技术报告。\n*   研究领域：计算机视觉与模式识别 (cs.CV)。\n*   arXiv 链接：arXiv:2510.23588。",
      "shortSummary": "FARMER 是一种新颖的端到端生成框架，它将归一化流（NF）与自回归（AR）模型相结合，旨在直接从原始像素进行可处理的似然估计和高质量图像合成。该模型通过可逆自回归流将图像转换为潜在序列，并利用自监督降维、一步蒸馏和基于重采样的无分类器指导等技术，有效解决了像素级建模的冗余和复杂性。实验表明，FARMER 在提供精确似然和可扩展训练的同时，实现了与现有模型相当的竞争力。",
      "translated_title": "FARMER：基于像素的流自回归Transformer",
      "images": [],
      "contentSource": "完整文章",
      "content": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training."
    },
    {
      "title": "数据代理综述：新兴范式还是过度炒作？ (原标题: A Survey of Data Agents: Emerging Paradigm or Overstated Hype?)",
      "link": "https://arxiv.org/abs/2510.23587",
      "pubDate": "Mon, 27 Oct 2025 13:54:07 GMT",
      "isoDate": "2025-10-27T13:54:07.000Z",
      "creator": "Yizhang Zhu, Liangwei Wang, Chenyu Yang, Xiaotian Lin, Boyan Li, Wei Zhou, Xinyu Liu, Zhangyang Peng, Tianqi Luo, Yu Li, Chengliang Chai, Chong Chen, Shimin Di, Ju Fan, Ji Sun, Nan Tang, Fugee Tsung, Jiannan Wang, Chenglin Wu, Yanwei Xu, Shaolei Zhang, Yong Zhang, Xuanhe Zhou, Guoliang Li, Yuyu Luo",
      "summary": "### 数据代理综述：新兴范式还是过度炒作？\n\n本综述探讨了数据代理这一新兴领域，旨在解决其当前面临的术语模糊和应用不一致问题，并提出一个系统性的分层分类法来明确其能力和发展路径。\n\n#### 1. 背景与问题\n*   **数据代理的兴起**: 随着大型语言模型（LLM）的快速发展，数据代理作为自主系统应运而生，旨在协调数据与人工智能生态系统，以解决复杂的数据相关任务。\n*   **术语模糊性**: 当前，“数据代理”一词存在严重的术语模糊和应用不一致问题。它常常将简单的查询响应器与复杂的自主架构混为一谈。\n*   **负面影响**: 这种模糊性导致用户期望错位、责任分配挑战，并阻碍了行业的健康发展。\n\n#### 2. 提出的解决方案：数据代理分层分类法\n*   **灵感来源**: 受SAE J3016驾驶自动化标准的启发，本综述首次引入了数据代理的系统性分层分类法。\n*   **六个级别**: 该分类法包含六个级别（L0至L5），旨在描绘和追踪数据代理自主性的渐进式转变：\n    *   **L0 (手动操作)**：完全由人工执行。\n    *   **L1 (辅助)**：系统提供辅助功能。\n    *   **L2 (部分自动化)**：系统在特定条件下执行部分任务。\n    *   **L3 (有条件自动化)**：系统在特定操作设计域内自主执行，但需要人类在必要时接管。\n    *   **L4 (高度自动化)**：系统在特定操作设计域内完全自主，无需人类干预。\n    *   **L5 (完全自主)**：展望未来，实现生成式、完全自主的数据代理，能够在所有条件下自主运行。\n*   **目的**: 通过明确不同级别数据代理的能力边界和责任分配，解决术语模糊问题，并为行业发展提供清晰的指导。\n\n#### 3. 现有研究的结构化回顾\n*   **分类视角**: 本综述以提出的分层分类法为视角，对现有研究进行了结构化回顾，并按照自主性递增的顺序进行组织。\n*   **研究范围**: \n    *   **专业数据代理**: 涵盖了专注于数据管理、数据准备和数据分析等特定任务的专业数据代理。\n    *   **综合系统**: 探讨了朝着具有增强自主性的多功能、综合系统发展的新兴努力。\n\n#### 4. 演进飞跃与技术差距\n*   **关键分析**: 文章进一步分析了推动数据代理发展的关键演进飞跃和当前存在的技术差距。\n*   **L2到L3的过渡**: 特别强调了正在进行的L2到L3的过渡，在此阶段，数据代理正从简单的程序化执行演变为更复杂的自主编排能力。\n\n#### 5. 前瞻性路线图\n*   **未来展望**: 综述最后提出了一个前瞻性的路线图，展望了数据代理的未来发展方向。\n*   **愿景**: 设想主动式、生成式数据代理的到来，它们将能够更智能、更自主地处理数据任务。",
      "shortSummary": "本综述旨在解决数据代理（Data Agents）领域因大型语言模型（LLM）兴起而导致的术语模糊问题。文章引入了一个受SAE J3016标准启发的六级分层分类法，从手动操作（L0）到完全自主（L5），以明确数据代理的能力边界和责任分配。通过此分类法，综述回顾了现有研究，分析了从L2到L3的关键技术演进，并展望了主动式、生成式数据代理的未来发展。该工作旨在为数据代理的理解、开发和标准化提供清晰框架。",
      "translated_title": "数据代理综述：新兴范式还是过度炒作？",
      "images": [],
      "contentSource": "完整文章",
      "content": "The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term \"data agent\" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents."
    },
    {
      "title": "前瞻锚定：在音频驱动的人体动画中保持角色身份 (原标题: Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human Animation)",
      "link": "https://arxiv.org/abs/2510.23581",
      "pubDate": "Mon, 27 Oct 2025 13:50:19 GMT",
      "isoDate": "2025-10-27T13:50:19.000Z",
      "creator": "Junyoung Seo, Rodrigo Mira, Alexandros Haliassos, Stella Bounareli, Honglie Chen, Linh Tran, Seungryong Kim, Zoe Landgraf, Jie Shen",
      "summary": "# 前瞻锚定：解决音频驱动人体动画中的身份漂移问题\n\n## 摘要\n\n本文提出了一种名为“前瞻锚定”（Lookahead Anchoring）的新方法，旨在解决音频驱动的人体动画模型在时间自回归生成过程中常见的“身份漂移”问题，即角色形象随时间逐渐失真。\n\n## 现有挑战与局限\n\n*   **身份漂移问题：** 音频驱动的人体动画模型在长时间生成时，角色往往会逐渐失去其原始身份特征。\n*   **传统解决方案及其不足：**\n    *   一种常见的做法是生成关键帧作为中间时间锚点，以防止身份退化。\n    *   然而，这种方法需要额外的关键帧生成阶段，并且可能限制动画的自然运动动态。\n\n## 提出的解决方案：前瞻锚定 (Lookahead Anchoring)\n\n前瞻锚定通过以下创新机制克服了上述挑战：\n\n*   **核心思想：** 该方法利用当前生成窗口“未来”时间步的关键帧，而非窗口内的关键帧，作为锚点。\n*   **关键帧作用的转变：**\n    *   将关键帧从固定的边界转变为方向性指引（directional beacons）。\n    *   模型在响应即时音频线索的同时，持续追求这些未来的锚点，通过持续的引导保持一致的角色身份。\n*   **主要优势：**\n    *   **自关键帧 (Self-keyframing)：** 参考图像可以直接用作前瞻目标，完全消除了对额外关键帧生成的需求。\n    *   **表达力与一致性的平衡控制：** 时间前瞻距离自然地控制了动画表达力（即运动自由度）和身份一致性（即身份依从性）之间的平衡：\n        *   **较大的前瞻距离：** 允许更大的动作自由度，增强表达力。\n        *   **较小的前瞻距离：** 更强地保持身份一致性，减少身份漂移。\n\n## 实验结果与性能\n\n*   **应用范围：** 前瞻锚定被应用于三种最新的人体动画模型。\n*   **性能提升：** 实验结果表明，该方法在唇部同步、身份保持和视觉质量方面均取得了卓越的性能。\n*   **普适性：** 证明了前瞻锚定在不同架构下均能有效改善时间条件性。\n\n## 相关领域\n\n*   计算机视觉与模式识别 (cs.CV)\n*   机器学习 (cs.LG)",
      "shortSummary": "前瞻锚定是一种解决音频驱动人体动画中角色身份漂移问题的新方法。它利用未来时间步的关键帧作为方向性指引，使模型在响应音频的同时持续保持角色身份。该方法支持自关键帧，并允许通过调整前瞻距离来平衡动作表达力和身份一致性。应用于现有模型后，前瞻锚定显著提升了唇部同步、身份保持和视觉质量，改善了时间条件性。",
      "translated_title": "前瞻锚定：在音频驱动的人体动画中保持角色身份",
      "images": [],
      "contentSource": "完整文章",
      "content": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io."
    },
    {
      "title": "RobotArena infty：通过真实到模拟转换实现可扩展的机器人基准测试 (原标题: RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim Translation)",
      "link": "https://arxiv.org/abs/2510.23571",
      "pubDate": "Mon, 27 Oct 2025 13:41:38 GMT",
      "isoDate": "2025-10-27T13:41:38.000Z",
      "creator": "Yash Jangir, Yidi Zhang, Kashu Yamazaki, Chenyu Zhang, Kuan-Hsun Tu, Tsung-Wei Ke, Lei Ke, Yonatan Bisk, Katerina Fragkiadaki",
      "summary": "## 机器人通用智能体的评估挑战与RobotArena infty框架\n\n### 现有评估方法的局限性\n\n开发能够执行多样任务并适应多样环境的机器人通用智能体，需要严格且可扩展的评估。然而，当前的评估方法面临显著挑战：\n\n*   **真实世界测试：** 劳动密集、耗时、大规模部署不安全，且难以复现实验结果。\n*   **现有模拟基准：** 仅在同一合成领域内训练和测试策略，无法有效评估那些通过真实世界演示或在其他模拟环境中训练的模型。\n*   **复杂性加剧：** 随着机器人策略的范围和复杂性不断增加，上述障碍愈发严重。尤其是在机器人领域，定义“成功”往往需要对执行质量进行细致入微的人类判断。\n\n### RobotArena infty框架介绍\n\n本文提出了一种名为“RobotArena infty”的新型基准测试框架，旨在克服上述挑战。该框架通过将视觉-语言智能体（VLA）的评估转移到大规模模拟环境，并辅以在线人类反馈，从而实现可扩展、可复现的机器人基准测试。\n\n### 核心方法论\n\n1.  **技术基础：** 该框架利用了视觉-语言模型（VLM）、2D到3D生成建模以及可微分渲染的最新进展。\n2.  **数据转换：** 自动将来自广泛使用的机器人数据集的视频演示转换为其模拟对应物，即“数字孪生”。\n3.  **策略评估：** 在这些数字孪生环境中，VLA策略通过以下两种方式进行评估：\n    *   **自动化VLM引导评分：** 利用VLM进行自动化的性能评分。\n    *   **可扩展的人类偏好判断：** 从众包工作者那里收集大规模的人类偏好判断，以评估执行质量。\n4.  **人类参与模式转变：** 将人类的参与从繁琐的场景设置、环境重置和安全监督，转变为更轻量级的偏好比较任务，显著提高了评估效率和可扩展性。\n\n### 鲁棒性测试\n\n为了全面衡量策略的鲁棒性，RobotArena infty框架系统地沿着多个维度（例如，纹理变化和物体放置位置）扰动模拟环境。这种受控的变量改变有助于对策略的泛化能力进行压力测试，确保其在不同条件下的稳定表现。\n\n### 成果与意义\n\nRobotArena infty框架为真实世界训练的机器人操作策略提供了一个持续演进、可复现且可扩展的基准。它解决了当前机器人领域中一个关键的缺失能力，为未来机器人通用智能体的开发和评估提供了重要工具。",
      "shortSummary": "本文介绍了RobotArena infty框架，旨在通过“真实到模拟”转换，解决机器人通用智能体评估的挑战。该框架利用VLM和2D-3D生成建模，将真实世界视频演示转换为模拟数字孪生。在模拟环境中，结合VLM自动评分和众包人类偏好判断来评估视觉-语言智能体（VLA）策略。通过系统扰动模拟环境，测试策略的鲁棒性。RobotArena infty提供了一个可扩展、可复现的基准，填补了机器人基准测试的关键空白。",
      "translated_title": "RobotArena infty：通过真实到模拟转换实现可扩展的机器人基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape."
    },
    {
      "title": "ReCode：统一规划与行动以实现通用粒度控制 (原标题: ReCode: Unify Plan and Action for Universal Granularity Control)",
      "link": "https://arxiv.org/abs/2510.23564",
      "pubDate": "Mon, 27 Oct 2025 13:35:15 GMT",
      "isoDate": "2025-10-27T13:35:15.000Z",
      "creator": "Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu",
      "summary": "## ReCode：统一规划与行动以实现通用粒度控制\n\n### 引言\n\n现实世界中的任务往往需要智能体在不同粒度级别上做出决策。人类在这方面表现出色，他们将规划本质上理解为一种高级形式的行动，从而能够灵活地在不同决策粒度之间切换。然而，当前基于大型语言模型（LLM）的智能体普遍缺乏这种关键能力。其主要限制源于现有范式在高层规划和低层行动之间强制执行的僵硬分离，这严重损害了智能体的动态适应性并限制了其泛化能力。\n\n### ReCode 范式：统一规划与行动\n\n为了解决上述限制，研究人员提出了 **ReCode (Recursive Code Generation)**，这是一种新颖的范式，它通过在单一代码表示中统一规划和行动来弥合这一鸿沟。\n\n**ReCode 的核心机制如下：**\n\n*   **抽象规划：** ReCode 将高层规划视为抽象的占位符函数。\n*   **递归分解：** 智能体随后递归地将这些抽象函数分解为更细粒度的子函数。\n*   **原始行动：** 这一分解过程持续进行，直至达到不可再分的原始行动。\n\n**这种递归方法带来了多项显著优势：**\n\n*   **消除界限：** 它彻底消除了规划与行动之间僵硬的边界，使两者无缝衔接。\n*   **动态粒度控制：** 智能体能够动态地控制其决策的粒度，根据任务需求进行调整。\n*   **丰富训练数据：** 递归结构本身能够生成丰富、多粒度的训练数据，这对于模型学习分层决策过程至关重要。\n\n### 实验结果与验证\n\n广泛的实验结果表明，ReCode 在推理性能上显著超越了现有的先进基线模型。此外，它在训练过程中也展现出卓越的数据效率。这些发现有力地验证了 ReCode 的核心洞察：通过递归代码生成来统一规划与行动，是实现通用粒度控制的一种强大且有效的方法。\n\n### 代码可用性\n\nReCode 的相关代码已公开，供研究人员和开发者使用。",
      "shortSummary": "ReCode（递归代码生成）提出了一种新范式，旨在解决现有大型语言模型（LLM）智能体在处理不同决策粒度时的局限性。它通过在单一代码表示中统一规划与行动，将高层规划视为抽象函数，并递归地将其分解为更细粒度的子函数直至原始行动。这种方法消除了规划与行动的僵硬界限，实现了动态粒度控制，并生成多粒度训练数据。实验证明，ReCode 在推理性能上显著优于现有基线，并展现出卓越的数据效率。",
      "translated_title": "ReCode：统一规划与行动以实现通用粒度控制",
      "images": [],
      "contentSource": "完整文章",
      "content": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode."
    },
    {
      "title": "LimRank：少即是多，用于推理密集型信息重排序 (原标题: LimRank: Less is More for Reasoning-Intensive Information Reranking)",
      "link": "https://arxiv.org/abs/2510.23544",
      "pubDate": "Mon, 27 Oct 2025 13:19:37 GMT",
      "isoDate": "2025-10-27T13:19:37.000Z",
      "creator": "Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan",
      "summary": "### LimRank：推理密集型信息重排序的“少即是多”方法\n\n本文介绍了一种名为 LimRank 的新型方法，旨在解决大型语言模型（LLMs）在信息重排序任务中，现有方法因大规模微调而导致的计算成本高昂问题。\n\n**核心思想与方法：**\n*   **问题识别：** 现有的LLM重排序方法通常依赖于大规模微调，这导致了高昂的计算成本。\n*   **解决方案：** LimRank 证明了现代LLMs可以通过仅使用最少、高质量的监督数据进行有效适应。\n*   **关键组件——LIMRANK-SYNTHESIZER：**\n    *   为了实现这一目标，研究人员设计了 LIMRANK-SYNTHESIZER。\n    *   这是一个可重用且开源的管道，用于生成多样化、具有挑战性且真实的重排序示例。\n    *   这些合成数据是 LimRank 模型微调的基础。\n\n**实验与评估：**\n*   **评估基准：** LimRank 在两个具有挑战性的基准上进行了评估：\n    *   BRIGHT：针对推理密集型检索任务。\n    *   FollowIR：针对指令遵循检索任务。\n*   **主要成果：**\n    *   LimRank 取得了具有竞争力的性能。\n    *   其训练数据量不到以往工作中通常使用的 5%。\n\n**进一步研究与泛化能力：**\n*   **消融研究：** 进一步的消融研究证明了 LIMRANK-SYNTHESIZER 的有效性。\n*   **泛化能力：** LimRank 在下游任务中展现出强大的泛化能力，包括：\n    *   科学文献搜索。\n    *   用于知识密集型问题解决的检索增强生成（RAG）。\n\n**结论：**\nLimRank 提供了一种高效且有效的方法，通过利用高质量的合成数据进行少量监督微调，使LLMs适应信息重排序任务，显著降低了计算成本，同时保持了高性能和强大的泛化能力。",
      "shortSummary": "LimRank 提出了一种高效的LLM信息重排序方法，通过使用 LIMRANK-SYNTHESIZER 生成的少量高质量合成数据进行微调，解决了现有方法计算成本高昂的问题。它在推理密集型和指令遵循检索任务上表现出竞争力，训练数据量不到传统方法的5%。LimRank还展示了强大的泛化能力，适用于科学文献搜索和检索增强生成等下游任务。",
      "translated_title": "LimRank：少即是多，用于推理密集型信息重排序",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving."
    },
    {
      "title": "Omni-Reward：迈向具有自由形式偏好的通用全模态奖励建模 (原标题: Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences)",
      "link": "https://arxiv.org/abs/2510.23451",
      "pubDate": "Mon, 27 Oct 2025 11:53:20 GMT",
      "isoDate": "2025-10-27T11:53:20.000Z",
      "creator": "Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
      "summary": "# Omni-Reward：迈向具有自由形式偏好的通用全模态奖励建模\n\n本文提出了 **Omni-Reward**，旨在解决当前人工智能奖励模型（RMs）在对齐人类偏好方面面临的两个核心挑战，并迈向通用全模态奖励建模，支持自由形式的偏好。\n\n## 核心挑战\n\n1.  **模态不平衡 (Modality Imbalance)**：\n    *   大多数奖励模型主要关注文本和图像模态。\n    *   对视频、音频和其他模态的支持有限。\n2.  **偏好刚性 (Preference Rigidity)**：\n    *   现有模型通常基于固定的二元偏好对进行训练。\n    *   这种训练方式难以捕捉个性化偏好的复杂性和多样性。\n\n## Omni-Reward 的组成部分\n\n为了应对上述挑战，Omni-Reward 提出了以下三个关键组成部分：\n\n### 1. 评估：Omni-RewardBench\n\n*   **首个全模态奖励模型基准**：Omni-RewardBench 是第一个支持自由形式偏好的全模态奖励模型基准。\n*   **广泛覆盖**：它涵盖了九项任务，涉及五种不同的模态，包括：\n    *   文本\n    *   图像\n    *   视频\n    *   音频\n    *   3D\n\n### 2. 数据：Omni-RewardData\n\n*   **多模态偏好数据集**：Omni-RewardData 是一个专门构建的多模态偏好数据集。\n*   **数据规模**：\n    *   包含 24.8 万个通用偏好对。\n    *   包含 6.9 万个指令调优对。\n*   **目的**：用于训练通用全模态奖励模型。\n\n### 3. 模型：Omni-RewardModel\n\n*   **模型结构**：Omni-RewardModel 包含判别式（discriminative）和生成式（generative）两种奖励模型。\n*   **性能表现**：\n    *   在 Omni-RewardBench 上取得了强大的性能。\n    *   在其他广泛使用的奖励建模基准上也表现出色。\n\n**总结**：Omni-Reward 通过引入新的评估基准、构建大规模多模态数据集和提出创新的模型架构，为实现能够理解和适应复杂、多样化人类偏好的通用全模态奖励模型迈出了重要一步。",
      "shortSummary": "Omni-Reward 旨在解决AI奖励模型在模态不平衡和偏好刚性方面的挑战，提出了一个支持自由形式偏好的通用全模态奖励建模框架。该框架包含：首个全模态基准 Omni-RewardBench（涵盖五种模态九项任务），一个包含24.8万通用偏好对和6.9万指令调优对的多模态数据集 Omni-RewardData，以及一个性能强大的判别式和生成式奖励模型 Omni-RewardModel。Omni-Reward 在多模态任务中表现出色，推动了AI与人类偏好对齐的通用化发展。",
      "translated_title": "Omni-Reward：迈向具有自由形式偏好的通用全模态奖励建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks."
    },
    {
      "title": "基于智能体奖励反馈的代码美学 (原标题: Code Aesthetics with Agentic Reward Feedback)",
      "link": "https://arxiv.org/abs/2510.23272",
      "pubDate": "Mon, 27 Oct 2025 08:32:33 GMT",
      "isoDate": "2025-10-27T08:32:33.000Z",
      "creator": "Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei",
      "summary": "# 基于智能体奖励反馈的代码美学\n\n## 摘要\n大型语言模型（LLMs）在代码生成和错误修复等传统编程任务中表现出色，但在涉及视觉导向的编码任务中，其生成的代码美学质量往往不尽如人意。本文提出了一种新的流水线，旨在显著提升LLM生成代码的美学质量。\n\n## 核心贡献与方法\n\n1.  **AesCode-358K 数据集构建**\n    *   构建了一个大规模的指令微调数据集AesCode-358K，该数据集专门用于提升LLM在代码美学方面的能力。\n\n2.  **智能体奖励反馈（Agentic Reward Feedback）系统**\n    *   提出了一种多智能体系统，用于全面评估代码的三个关键方面：\n        *   **可执行性 (Executability)**：确保代码的功能正确性。\n        *   **静态美学 (Static Aesthetics)**：评估代码的视觉布局和设计元素。\n        *   **交互式美学 (Interactive Aesthetics)**：评估用户与代码生成界面交互时的美学体验。\n    *   该系统为后续的强化学习提供了丰富的、多维度的奖励信号。\n\n3.  **GRPO-AR 算法**\n    *   开发了GRPO-AR算法，它将智能体奖励反馈系统产生的信号整合到GRPO（Generalized Reinforcement Learning with Policy Optimization）算法中。\n    *   GRPO-AR旨在实现代码功能性和美学质量的联合优化，以达到两者之间的最佳平衡。\n\n4.  **OpenDesign 基准测试**\n    *   创建了一个新的基准测试OpenDesign，专门用于客观评估LLM生成代码的美学质量。\n    *   该基准测试为研究人员提供了一个标准化的评估工具，以衡量不同方法的有效性。\n\n## 实验结果与性能\n\n*   **显著性能提升**：实验结果表明，结合在AesCode-358K上进行的监督微调（SFT）和使用智能体奖励反馈的强化学习（RL），在OpenDesign基准测试上取得了显著的性能提升。\n*   **现有基准的改进**：该方法不仅在OpenDesign上表现出色，也提升了在现有基准测试（如PandasPlotBench）上的表现，证明了其良好的泛化能力。\n*   **超越SOTA模型**：本文提出的AesCoder-4B模型，其参数量为4B，在代码美学任务上超越了GPT-4o和GPT-4.1等先进的闭源模型。\n*   **媲美大型模型**：AesCoder-4B的性能甚至与参数量高达480B-685B的大型开源模型相当，这充分突显了该方法的有效性和效率。\n\n## 结论\n本文提出的流水线，通过结合专门的数据集、多智能体奖励反馈系统、优化的强化学习算法和新的评估基准，成功解决了LLM在代码美学方面的挑战，为未来LLM在视觉导向编码任务中的应用开辟了新的道路。",
      "shortSummary": "本文提出一种新方法，旨在提升大型语言模型（LLMs）生成代码的美学质量。通过构建大规模代码美学数据集AesCode-358K，并引入评估可执行性、静态和交互美学的多智能体奖励反馈系统，结合GRPO-AR算法进行功能与美学联合优化，并开发OpenDesign基准进行评估。实验结果显示，该方法显著提升了代码美学表现，其AesCoder-4B模型超越了GPT-4o和GPT-4.1，性能媲美大型开源模型。",
      "translated_title": "基于智能体奖励反馈的代码美学",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach."
    },
    {
      "title": "敲头注意力 (原标题: Knocking-Heads Attention)",
      "link": "https://arxiv.org/abs/2510.23052",
      "pubDate": "Mon, 27 Oct 2025 02:28:58 GMT",
      "isoDate": "2025-10-27T02:28:58.000Z",
      "creator": "Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li",
      "summary": "### 敲头注意力 (Knocking-Heads Attention)\n\n**1. 背景与现有问题**\n*   **多头注意力（MHA）的基石地位**：MHA已成为现代大型语言模型的核心组件，通过并行注意力头增强了表示能力。\n*   **MHA的局限性**：\n    *   增加注意力头数量会削弱单个头的容量。\n    *   现有注意力机制（无论是标准MHA还是其变体，如分组查询注意力GQA和分组绑定注意力GTA）只是简单地拼接来自孤立头的输出，缺乏强大的头间交互。\n\n**2. 提出的解决方案：敲头注意力（KHA）**\n*   **核心思想**：KHA允许注意力头之间“相互敲击”，在进行缩放点积注意力之前，促进跨头部的特征级交互。\n*   **实现机制**：通过应用一个共享的、对角线初始化的投影矩阵来实现。\n*   **对角线初始化的优势**：\n    *   在训练初期保留了每个头的特定专业化能力。\n    *   允许模型逐步学习集成化的跨头部表示。\n\n**3. KHA的优势与集成性**\n*   **资源消耗低**：KHA仅增加极少的参数和浮点运算（FLOPs）。\n*   **无缝集成**：可以无缝集成到MHA、GQA、GTA以及其他注意力变体中。\n\n**4. 实验验证与结果**\n*   **模型训练**：通过在一个包含6.1亿参数（激活参数1.01亿）的MoE模型上，使用1万亿高质量tokens进行训练来验证KHA。\n*   **性能表现**：\n    *   与基线注意力机制相比，KHA带来了更优越、更稳定的训练动态。\n    *   在下游任务中取得了更好的性能。",
      "shortSummary": "敲头注意力（KHA）旨在解决多头注意力（MHA）中头容量弱化和头间交互不足的问题。KHA通过引入一个共享的、对角线初始化的投影矩阵，在缩放点积注意力之前实现跨头部的特征级交互。这使得注意力头能“相互敲击”，在保留初始专业化的同时逐步学习集成表示。KHA仅增加少量参数和计算量，可无缝集成，并在实验中展现出更稳定、更优越的训练动态和下游任务性能。",
      "translated_title": "敲头注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to \"knock\" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks."
    },
    {
      "title": "VoMP：预测体积机械性能场 (原标题: VoMP: Predicting Volumetric Mechanical Property Fields)",
      "link": "https://arxiv.org/abs/2510.22975",
      "pubDate": "Sun, 26 Oct 2025 23:56:25 GMT",
      "isoDate": "2025-10-26T23:56:25.000Z",
      "creator": "Rishit Dagli, Donglai Xiang, Vismay Modi, Charles Loop, Clement Fuji Tsang, Anka He Chen, Anita Hu, Gavriel State, David I. W. Levin, Maria Shugrina",
      "summary": "### VoMP：预测体积机械性能场\n\n**背景与挑战**\n物理模拟高度依赖于空间变化的机械性能，但这些属性通常需要耗时费力地手动创建。\n\n**VoMP 方法介绍**\nVoMP（Volumetric Mechanical Property Fields）是一种前馈方法，旨在预测三维物体体积内的杨氏模量（$E$）、泊松比（$\nu$）和密度（$\rho$）等机械性能场。该方法适用于任何可渲染和体素化的三维对象表示。\n\n**核心机制**\n*   **特征聚合与转换**：VoMP 聚合每个体素的多视图特征，并将其输入到经过训练的几何变换器（Geometry Transformer）中。\n*   **材料潜在代码预测**：几何变换器预测每个体素的材料潜在代码。\n*   **物理合理性保证**：这些潜在代码位于一个物理上合理的材料流形上，该流形是从真实世界数据集中学习得到的，从而保证了解码后的每个体素材料的有效性。\n\n**数据标注流程**\n为了获取对象级别的训练数据，研究人员提出了一种新的标注流程。该流程结合了以下信息源：\n*   分割后的三维数据集\n*   材料数据库\n*   视觉-语言模型\n同时，还引入了一个新的基准测试。\n\n**实验结果**\n实验表明，VoMP 能够准确估计体积属性，在准确性和速度方面均远超现有技术。",
      "shortSummary": "VoMP是一种前馈方法，用于预测三维物体体积内的杨氏模量、泊松比和密度等机械性能场。它通过聚合多视图体素特征并输入几何变换器来预测物理上合理的材料潜在代码。为训练，VoMP结合了分割数据集、材料数据库和视觉-语言模型。实验证明，VoMP在准确性和速度上显著优于现有技术，解决了物理模拟中手动创建空间变化机械属性的难题。",
      "translated_title": "VoMP：预测体积机械性能场",
      "images": [],
      "contentSource": "完整文章",
      "content": "Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus (E), Poisson's ratio (nu), and density (rho) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed."
    },
    {
      "title": "LightBagel：一种轻量级双融合框架，用于统一多模态理解和生成 (原标题: LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation)",
      "link": "https://arxiv.org/abs/2510.22946",
      "pubDate": "Sun, 26 Oct 2025 22:59:57 GMT",
      "isoDate": "2025-10-26T22:59:57.000Z",
      "creator": "Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, Cihang Xie",
      "summary": "# LightBagel：一种轻量级双融合框架，用于统一多模态理解和生成\n\n## 核心思想\nLightBagel 提出了一种高效的方法，通过战略性地融合现有公开的、专门用于生成或理解的模型，来构建统一的多模态系统。这解决了当前领先的多模态模型需要从头开始训练并消耗大量计算资源的问题。\n\n## 关键设计：双融合机制\n该框架的核心在于其“双融合”机制：\n*   **保留原始模块：** LightBagel 保留了基础模型的原始模块，从而在很大程度上保持了它们原有的优势。\n*   **交错多模态自注意力模块：** 在整个网络中额外交错多模态自注意力模块。\n\n这种设计实现了：\n1.  **丰富的多模态融合：** 有效地促进了模态间信息的深度融合。\n2.  **协同融合：** 催化了来自理解编码器的高级语义表示与来自生成编码器的低级空间信号之间的协同融合。\n\n## 训练效率与性能\n*   **训练效率：** LightBagel 仅通过约 350 亿（~35B）个 token 的训练，就取得了显著的成果。\n*   **性能表现：** 在多个基准测试中展现出强大的性能：\n    *   **组合式文本到图像生成 (GenEval)：** 0.91\n    *   **复杂文本到图像生成 (DPG-Bench)：** 82.16\n    *   **图像编辑 (GEditBench)：** 6.06\n    *   **图像编辑 (ImgEdit-Bench)：** 3.77\n\n## 资源发布\n为了支持未来在统一多模态建模方面的研究，LightBagel 团队已完整发布了所有代码、模型权重和数据集。",
      "shortSummary": "LightBagel 提出一种轻量级双融合框架，旨在高效实现统一多模态理解和生成。它通过战略性融合现有专业模型，并在网络中交错多模态自注意力模块，形成“双融合”机制。这种方法在保留基础模型优势的同时，有效促进了高级语义与低级空间信号的协同融合。LightBagel 仅用约350亿token训练，便在文本到图像生成和图像编辑等多个基准测试中取得强大性能，并已开源所有代码、模型权重和数据集。",
      "translated_title": "LightBagel：一种轻量级双融合框架，用于统一多模态理解和生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling."
    },
    {
      "title": "语言服务器CLI通过过程奖励赋能语言智能体 (原标题: Language Server CLI Empowers Language Agents with Process Rewards)",
      "link": "https://arxiv.org/abs/2510.22907",
      "pubDate": "Sun, 26 Oct 2025 21:25:20 GMT",
      "isoDate": "2025-10-26T21:25:20.000Z",
      "creator": "Yifan Zhang, Lanser Contributors",
      "summary": "## Lanser-CLI：通过过程奖励赋能语言智能体\n\n本文介绍了Lanser-CLI，一个命令行接口（CLI）优先的编排层，旨在解决大型语言模型（LLMs）在代码操作中常见的幻觉API和编辑定位错误问题。Lanser-CLI通过固定和协调语言服务器协议（LSP）服务器，为编码智能体和持续集成（CI）提供经过验证的、IDE级别的代码事实，并暴露确定性、可重放的工作流。\n\n### 核心观点：语言服务器作为过程奖励\n\n作者提出，语言服务器不仅提供结构化信息（如定义、引用、类型、诊断），还提供了一种可操作的“过程奖励”。这种奖励是机器检查的、逐步的信号，能够将智能体的规划循环与程序实际情况对齐。\n\n### Lanser-CLI 的主要贡献：\n\nLanser-CLI在以下几个方面做出了贡献：\n\n*   **鲁棒的寻址方案：**\n    *   超越了脆弱的“文件:行:列”寻址方式。\n    *   引入了Selector DSL（领域特定语言），支持符号式、AST路径和内容锚定选择器。\n    *   包含一个有原则的重定位算法，确保寻址的稳定性。\n\n*   **确定性分析包（Analysis Bundles）：**\n    *   标准化语言服务器的响应。\n    *   捕获环境和能力元数据。\n    *   使用稳定的内容哈希，确保分析结果的确定性和可重现性。\n\n*   **变异操作的安全包络：**\n    *   为重命名、代码操作等变异操作提供安全保障。\n    *   支持预览功能。\n    *   提供工作区隔离（workspace jails）。\n    *   实现Git感知、事务性的应用（transactional apply）。\n\n*   **过程奖励功能：**\n    *   从语言服务器事实中派生（如诊断差异、消歧置信度、安全应用检查）。\n    *   可在线计算，并可离线重放。\n\n### 形式化与特性：\n\n文章对在冻结快照下的确定性进行了形式化，并为过程奖励建立了单调性属性。这使得过程奖励适用于过程监督和反事实分析。\n\n**图片说明:**\n文章内容中不包含有效的实际图片链接，因此详细摘要中不包含任何图片。",
      "shortSummary": "Lanser-CLI是一个CLI优先的编排层，旨在通过集成语言服务器协议（LSP）来解决大型语言模型在代码操作中的幻觉和错误定位问题。它为编码智能体提供经过验证的IDE级代码事实和“过程奖励”，以将智能体的规划与程序实际对齐。Lanser-CLI贡献了鲁棒的寻址方案、确定性分析包、变异操作的安全包络以及可计算和重放的过程奖励功能。",
      "translated_title": "语言服务器CLI通过过程奖励赋能语言智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli"
    },
    {
      "title": "E^2Rank：您的文本嵌入也可以成为一种有效且高效的列表式重排序器 (原标题: E^2Rank: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker)",
      "link": "https://arxiv.org/abs/2510.22733",
      "pubDate": "Sun, 26 Oct 2025 12:04:48 GMT",
      "isoDate": "2025-10-26T12:04:48.000Z",
      "creator": "Qi Liu, Yanzhao Zhang, Mingxin Li, Dingkun Long, Pengjun Xie, Jiaxin Mao",
      "summary": "### E^2Rank：统一文本嵌入模型的检索与重排序\n\n**背景与问题**\n\n*   **文本嵌入模型的局限性：** 文本嵌入模型在实际搜索应用中是核心组件，通过将查询和文档映射到共享嵌入空间，能够实现高效的检索性能。然而，与专门的重排序器（特别是近期基于大型语言模型（LLM）的列表式重排序器）相比，它们的排序精度仍然有限。后者能够捕捉更细粒度的查询-文档和文档-文档交互。\n\n**E^2Rank 框架的提出**\n\n*   **核心理念：** 论文提出了一个名为 $\text{E}^2\text{Rank}$ 的简单而有效的统一框架，其名称意为“高效嵌入式排序”（Efficient Embedding-based Ranking），也指“嵌入到排序”（Embedding-to-Rank）。\n*   **目标：** 该框架旨在将单个文本嵌入模型扩展为既能执行高质量检索又能进行列表式重排序。\n\n**E^2Rank 的工作机制**\n\n*   **持续训练：** 通过在列表式排序目标下对现有文本嵌入模型进行持续训练来实现这一目标。\n*   **统一排序函数：** 采用查询和文档嵌入之间的余弦相似度作为统一的排序函数。\n*   **增强型查询（列表式排序提示）：**\n    *   构建一个“列表式排序提示”（listwise ranking prompt），该提示由原始查询及其候选文档组成。\n    *   这个提示充当一个增强型查询，富含来自Top-K文档的信号，其作用类似于传统检索模型中的伪相关反馈（PRF）。\n*   **优势：** 这种设计保留了基础嵌入模型的效率和表示质量，同时显著提高了其重排序性能。\n\n**实验结果与发现**\n\n*   **重排序性能：** $\textrm{E}^2\text{Rank}$ 在BEIR重排序基准测试中取得了最先进（state-of-the-art）的结果。\n*   **推理密集型任务：** 在推理密集型BRIGHT基准测试中，$\textrm{E}^2\text{Rank}$ 也表现出有竞争力的性能。\n*   **效率：** 实现了非常低的重排序延迟，证明了其高效性。\n*   **嵌入性能提升：** 研究还表明，排序训练过程同时改善了MTEB基准测试上的嵌入性能。\n\n**结论**\n\n*   研究结果表明，单个嵌入模型可以有效地统一检索和重排序功能，同时提供计算效率和有竞争力的排序准确性。",
      "shortSummary": "E^2Rank 提出一个统一框架，将单个文本嵌入模型扩展为同时进行高效检索和有效列表式重排序。通过在列表式排序目标下持续训练，并利用包含候选文档信息的增强型查询，E^2Rank 显著提升了重排序性能，同时保持了高效率。它在BEIR和BRIGHT基准测试上取得了领先或有竞争力的结果，证明了单个嵌入模型统一检索与重排序的可行性，并能改善嵌入性能。",
      "translated_title": "E^2Rank：您的文本嵌入也可以成为一种有效且高效的列表式重排序器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E^2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E^2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy."
    },
    {
      "title": "IGGT：实例级几何Transformer用于语义3D重建 (原标题: IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction)",
      "link": "https://arxiv.org/abs/2510.22706",
      "pubDate": "Sun, 26 Oct 2025 10:57:44 GMT",
      "isoDate": "2025-10-26T10:57:44.000Z",
      "creator": "Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu",
      "summary": "# IGGT: 实例级几何Transformer用于语义3D重建\n\n## 1. 引言与背景\n\n人类自然地将3D世界的几何结构和语义内容视为相互交织的维度，从而能够连贯而准确地理解复杂场景。然而，大多数现有方法优先训练大型几何模型进行低级3D重建，并孤立地处理高级空间理解，忽视了3D场景分析中这两个基本方面之间的关键相互作用。这限制了模型的泛化能力，并导致在下游3D理解任务中表现不佳。近期一些尝试通过简单地将3D模型与特定语言模型对齐来缓解此问题，但这限制了感知能力，并降低了对下游任务的适应性。\n\n## 2. 核心贡献：Instance-Grounded Geometry Transformer (IGGT)\n\n本文提出了一种名为Instance-Grounded Geometry Transformer (IGGT) 的端到端大型统一Transformer模型。IGGT旨在统一空间重建和实例级上下文理解的知识。\n\n### 2.1. 3D一致性对比学习策略\n\nIGGT的核心在于其设计的“3D一致性对比学习”策略。该策略通过仅使用2D视觉输入，引导IGGT编码一个统一的表示。这个表示融合了几何结构和实例级聚类信息。这种统一的表示能够将2D视觉输入一致地提升为具有明确区分对象实例的连贯3D场景。\n\n### 2.2. InsScene-15K 数据集\n\n为了促进这项任务，研究人员还构建了一个名为InsScene-15K的大规模数据集。该数据集包含：\n*   高质量的RGB图像\n*   姿态信息\n*   深度图\n*   通过新颖数据整理流程生成的3D一致性实例级掩码标注\n\n## 3. 总结\n\nIGGT通过其统一的Transformer架构和3D一致性对比学习策略，旨在弥合3D几何重建与语义理解之间的鸿沟，从而实现对复杂3D场景更连贯和准确的理解。InsScene-15K数据集的发布也为相关研究提供了高质量的资源。",
      "shortSummary": "本文提出了Instance-Grounded Geometry Transformer (IGGT)，一个端到端统一Transformer，旨在解决传统3D重建中几何与语义理解分离的问题。IGGT通过3D一致性对比学习策略，仅利用2D视觉输入，编码融合几何结构和实例级聚类的统一表示，从而实现具有明确对象实例的连贯3D场景重建。为支持此任务，研究人员还构建了包含高质量RGB图像、深度图和3D一致性实例级掩码标注的大规模InsScene-15K数据集。",
      "translated_title": "IGGT：实例级几何Transformer用于语义3D重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline."
    },
    {
      "title": "使用LLMs缓解音视频语音识别中的注意力汇聚和大规模激活 (原标题: Mitigating Attention Sinks and Massive Activations in Audio-Visual Speech Recognition with LLMS)",
      "link": "https://arxiv.org/abs/2510.22603",
      "pubDate": "Sun, 26 Oct 2025 05:44:20 GMT",
      "isoDate": "2025-10-26T05:44:20.000Z",
      "creator": "Anand, Umberto Cappellazzo, Stavros Petridis, Maja Pantic",
      "summary": "# 使用LLMs缓解音视频语音识别中的注意力汇聚和大规模激活\n\n本文首次在多模态语音识别领域研究了大型语言模型（LLMs）中的“注意力汇聚”（attention sinks）和“大规模激活”（massive activations）现象，这些现象此前在自然语言处理（NLP）中已被发现。\n\n## 研究背景与问题\n*   LLMs在听觉语音识别（ASR）、视觉语音识别（VSR）和音视频语音识别（AVSR）方面取得了显著进展。\n*   然而，在微调过程中，LLMs的内部动态机制仍不清楚。\n*   NLP领域的近期研究表明，某些token（即注意力汇聚）会不成比例地吸引高注意力，并导致LLMs中这些汇聚token的某些特征出现巨大的激活（即大规模激活）。\n\n## 主要发现\n*   **现象识别**：通过对音视频LLMs的详细分析，研究发现注意力汇聚和大规模激活不仅出现在BOS（Begin-of-Sentence）token上，也出现在ASR、VSR和AVSR中的中间低语义token上。\n*   **起源与特征**：\n    *   大规模激活源于MLP（多层感知机）层。\n    *   它们对应于所有汇聚token中固定的特征索引。\n*   **放大机制**：中间汇聚token与BOS token表现出高度的余弦相似性，这进一步放大了注意力和激活。\n\n## 提出的解决方案\n*   **去相关损失**：基于上述洞察，本文引入了一种简单的“去相关损失”（decorrelation loss）。\n*   **作用机制**：该损失函数旨在降低BOS token与其他token之间的余弦相似性。\n*   **效果**：有效缓解了中间汇聚和大规模激活现象。\n\n## 实验结果\n*   **性能提升**：在较高的音视频特征下采样率下，该方法显著改善了词错误率（WER）。\n*   **稳定性**：在较低的下采样率下，该方法仍保持稳定。\n\n## 资源\n*   相关代码已公开。",
      "shortSummary": "本文首次在多模态语音识别（ASR、VSR、AVSR）中研究了LLMs的注意力汇聚和大规模激活现象。研究发现，这些现象不仅存在于BOS token，也存在于中间低语义token，且大规模激活源于MLP层，与BOS token的高度余弦相似性会放大这些效应。为缓解此问题，作者提出了一种去相关损失，通过降低BOS与其他token的余弦相似性，有效减轻了中间汇聚和大规模激活，并在高下采样率下提高了词错误率（WER），在低速率下保持稳定。",
      "translated_title": "使用LLMs缓解音视频语音识别中的注意力汇聚和大规模激活",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates."
    },
    {
      "title": "基于记忆的语言模型：一种高效、可解释且环保的大型语言建模方法 (原标题: Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling)",
      "link": "https://arxiv.org/abs/2510.22317",
      "pubDate": "Sat, 25 Oct 2025 10:34:18 GMT",
      "isoDate": "2025-10-25T10:34:18.000Z",
      "creator": "Antal van den Bosch, Ainhoa Risco Patón, Teun Buijse, Peter Berck, Maarten van Gompel",
      "summary": "## 基于记忆的语言模型：高效、可解释且环保的大型语言建模方法\n\n### 引言\n\n本文提出了一种名为**基于记忆的语言模型（Memory-based Language Modeling, MBLM）**的新方法，旨在作为当前主流的基于深度神经网络的语言模型的一种高效、环保的替代方案。该方法在性能、环境影响和可解释性方面展现出显著优势。\n\n### MBLM 的核心优势\n\n*   **高效性**：\n    *   MBLM 在下一词预测性能方面实现了对数线性扩展，这意味着其性能随数据量的增长而高效提升。\n    *   它展现出强大的记忆能力，能够有效存储和利用历史信息。\n    *   通过实现k-近邻分类的快速近似算法，MBLM 能够实现较低的令牌延迟，从而提高处理速度。\n\n*   **环保性**：\n    *   MBLM 在训练和推理模式下都具有相对较小的生态足迹。这得益于其设计，它完全依赖于CPU进行操作，避免了对高能耗GPU的过度依赖。\n    *   这种CPU优先的设计有助于降低能源消耗和碳排放，使其成为一种更“生态友好”的语言建模方法。\n\n*   **可解释性**：\n    *   MBLM 的内部工作原理简单且完全透明。与深度神经网络的“黑箱”特性不同，MBLM 的决策过程易于理解和分析，这对于需要高可信度和可解释性的应用场景尤为重要。\n\n### 实施与比较\n\n研究团队开发并实现了一个名为 **OLIFANT** 的基于记忆的语言模型。为了评估 OLIFANT 的性能和优势，研究人员将其与两种广泛使用的深度神经网络语言模型——**GPT-2** 和 **GPT-Neo** 进行了详细比较。比较的重点包括：\n\n*   **下一词预测准确性**：评估模型预测序列中下一个词语的准确程度。\n*   **估计碳排放量**：量化模型在运行过程中产生的环境影响。\n*   **运行速度**：衡量模型在处理任务时的效率和延迟。\n\n除了这些量化比较，文章还对 OLIFANT 模型进行了更深入的分析，以揭示其工作机制和潜在的应用前景。",
      "shortSummary": "本文提出基于记忆的语言模型（MBLM），作为深度神经网络语言模型的高效、环保替代方案。MBLM提供对数线性可扩展的下一词预测性能和强大的记忆能力，同时在训练和推理过程中具有较小的生态足迹，因为它完全依赖CPU且内部工作原理透明。研究团队实现了OLIFANT模型，并将其与GPT-2和GPT-Neo在预测准确性、碳排放和速度方面进行了比较，并提供了深入分析。",
      "translated_title": "基于记忆的语言模型：一种高效、可解释且环保的大型语言建模方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model."
    }
  ],
  "lastUpdated": "2025-10-28T09:39:41.579Z"
}