{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "GenExam：一个多学科文本到图像考试 (原标题: GenExam: A Multidisciplinary Text-to-Image Exam)",
      "link": "https://arxiv.org/abs/2509.14232",
      "pubDate": "Wed, 17 Sep 2025 13:59:14 GMT",
      "isoDate": "2025-09-17T13:59:14.000Z",
      "creator": "Zhaokai Wang, Penghao Yin, Xiangyu Zhao, Changyao Tian, Yu Qiao, Wenhai Wang, Jifeng Dai, Gen Luo",
      "summary": "## GenExam：一个多学科文本到图像考试基准\n\n### 引言\n现有考试基准主要侧重于理解和推理任务，而当前的生成基准则强调世界知识和视觉概念的说明，却忽视了对严格绘图考试的评估。为了填补这一空白，研究人员引入了GenExam。\n\n### GenExam 介绍\n*   **定义**：GenExam 是首个用于多学科文本到图像考试的基准。\n*   **规模与范围**：\n    *   包含1,000个样本。\n    *   涵盖10个不同的学科。\n    *   考试式提示按照一个四级分类法进行组织。\n\n### 关键特性\n*   **真实图像**：每个问题都配备了真实图像（ground-truth images），作为参考标准。\n*   **细粒度评分**：提供细粒度评分点，能够精确评估生成图像的语义正确性和视觉合理性。\n\n### 实验结果与挑战\n*   **模型表现**：实验结果显示，即使是GPT-Image-1和Gemini-2.5-Flash-Image等最先进的模型，其严格得分也低于15%。\n*   **普遍情况**：大多数模型几乎获得0分。\n*   **挑战性**：这表明GenExam基准带来了巨大的挑战，远超当前模型的极限。\n\n### 意义与展望\n*   **评估能力**：通过将图像生成视为一项考试，GenExam 对模型整合知识、推理和生成的能力进行了严格评估。\n*   **AGI之路**：该基准为通向通用人工智能（AGI）的路径提供了深刻见解和新的研究方向。\n\n### 相关信息\n*   **学科领域**：计算机视觉与模式识别 (cs.CV)\n*   **引用**：arXiv:2509.14232 [cs.CV]",
      "shortSummary": "GenExam是一个开创性的多学科文本到图像考试基准，旨在评估AI模型整合知识、推理和生成的能力。它包含1000个样本，涵盖10个学科，并提供真实图像和细粒度评分点。实验显示，即使是顶尖模型也表现不佳，严格得分低于15%，凸显了该基准的巨大挑战性。GenExam为通用人工智能（AGI）的发展提供了新的评估视角和研究方向。",
      "translated_title": "GenExam：一个多学科文本到图像考试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI."
    },
    {
      "title": "MARS2 2025多模态推理挑战赛：数据集、方法、结果、讨论与展望 (原标题: MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook)",
      "link": "https://arxiv.org/abs/2509.14142",
      "pubDate": "Wed, 17 Sep 2025 12:21:34 GMT",
      "isoDate": "2025-09-17T12:21:34.000Z",
      "creator": "Peng Xu, Shengwu Xiong, Jiajun Zhang, Yaxiong Chen, Bowen Zhou, Chen Change Loy, David A. Clifton, Kyoung Mu Lee, Luc Van Gool, Ruiming He, Ruilin Yao, Xinwei Long, Jirui Huang, Kai Tian, Sa Yang, Yihua Shao, Jin Feng, Yue Zhong, Jiakai Zhou, Cheng Tang, Tianyu Zou, Yifang Zhang, Junming Liang, Guoyou Li, Zhaoxiang Wang, Qiang Zhou, Yichen Zhao, Shili Xiong, Hyeongjin Nam, Jaerin Lee, Jaeyoung Chung, JoonKyu Park, Junghun Oh, Kanggeon Lee, Wooseok Lee, Juneyoung Ro, Turghun Osman, Can Hu, Chaoyang Liao, Cheng Chen, Chengcheng Han, Chenhao Qiu, Chong Peng, Cong Xu, Dailin Li, Feiyu Wang, Feng Gao, Guibo Zhu, Guopeng Tang, Haibo Lu, Han Fang, Han Qi, Hanxiao Wu, Haobo Cheng, Hongbo Sun, Hongyao Chen, Huayong Hu, Hui Li, Jiaheng Ma, Jiang Yu, Jianing Wang, Jie Yang, Jing He, Jinglin Zhou, Jingxuan Li, Josef Kittler, Lihao Zheng, Linnan Zhao, Mengxi Jia, Muyang Yan, Nguyen Thanh Thien, Pu Luo, Qi Li, Shien Song, Shijie Dong, Shuai Shao, Shutao Li, Taofeng Xue, Tianyang Xu, Tianyi Gao, Tingting Li, Wei Zhang, Weiyang Su, Xiaodong Dong, Xiao-Jun Wu, Xiaopeng Zhou, Xin Chen, Xin Wei, Xinyi You, Xudong Kang, Xujie Zhou, Xusheng Liu, Yanan Wang, Yanbin Huang, Yang Liu, Yang Yang, Yanglin Deng, Yashu Kang, Ye Yuan, Yi Wen, Yicen Tian, Yilin Tao, Yin Tang, Yipeng Lin, Yiqing Wang, Yiting Xi, Yongkang Yu, Yumei Li, Yuxin Qin, Yuying Chen, Yuzhe Cen, Zhaofan Zou, Zhaohong Liu, Zhehao Shen, Zhenglin Du, Zhengyang Li, Zhenni Huang, Zhenwei Shao, Zhilong Song, Zhiyong Feng, Zhiyu Wang, Zhou Yu, Ziang Li, Zihan Zhai, Zijian Zhang, Ziyang Peng, Ziyun Xiao, Zongshu Li",
      "summary": "# MARS2 2025多模态推理挑战赛回顾\n\n本文对MARS2 2025多模态推理挑战赛进行了回顾，该挑战赛旨在推动多模态机器学习和大型语言模型（LLMs）领域的发展。\n\n## 挑战目标与背景\n\n*   **汇集前沿方法：** 挑战赛旨在通过一个大型基准测试，汇集多模态机器学习和LLMs领域的各种方法，以帮助研究人员了解该动态领域的最新进展。\n*   **聚焦真实世界与专业场景：** 鉴于通用大型语言模型（LLMs）测试平台日益增多，本届MARS2挑战赛特别关注真实世界和专业场景，旨在拓宽多模态大型语言模型（MLLMs）在多模态推理应用中的范围。\n\n## 发布数据集\n\n为支持挑战赛，组织团队发布了两个定制数据集作为测试集：\n\n*   **Lens：** 支持12种日常场景中的通用推理任务。\n*   **AdsQA：** 支持广告视频中的领域特定推理任务。\n\n## 评估与竞赛赛道\n\n*   **基线模型评估：** 挑战赛评估了40多个基线模型，其中包括通用型多模态大型语言模型（MLLMs）和任务特定模型。\n*   **三大竞赛赛道：** 挑战赛设立了三个竞争赛道，以考察不同方面的多模态推理能力：\n    *   **真实世界场景中的视觉定位 (Visual Grounding in Real-world Scenarios, VG-RS)**\n    *   **具有空间感知能力的视觉问答 (Visual Question Answering with Spatial Awareness, VQA-SA)**\n    *   **创意广告视频中的视觉推理 (Visual Reasoning in Creative Advertisement Videos, VR-Ads)**\n\n## 参与情况与结果\n\n*   **广泛参与：** 共有来自知名学术和工业机构的76支团队注册参与。\n*   **有效提交：** 在超过1200份提交中，有40多份有效提交被纳入最终排名列表。\n\n## 资源可用性与展望\n\n*   **公开资源：** 挑战赛的数据集、代码集（包含40多个基线模型和15个以上参与者的方法）以及排名结果已在MARS2研讨会网站和GitHub组织页面（`https://github.com/MARS2-Challenge`）上公开。\n*   **持续更新：** 该GitHub页面将持续提供更新和未来活动的公告。\n*   **研讨会背景：** 本文是ICCV 2025 MARS2研讨会和挑战赛“大模型时代的多模态推理和慢思考：迈向系统2及更远”的评论。",
      "shortSummary": "MARS2 2025多模态推理挑战赛旨在通过大型基准测试，推动多模态机器学习和LLMs在真实世界及专业场景中的应用。挑战赛发布了Lens（日常场景）和AdsQA（广告视频）两个定制数据集，并设立了视觉定位、空间感知视觉问答和创意广告视频视觉推理三大赛道。共有76支团队参与，40多份有效提交被评估。所有数据集、代码和排名结果已公开，以促进该领域的进一步研究和发展。",
      "translated_title": "MARS2 2025多模态推理挑战赛：数据集、方法、结果、讨论与展望",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided."
    },
    {
      "title": "Wan-Animate：基于整体复制的统一角色动画与替换 (原标题: Wan-Animate: Unified Character Animation and Replacement with Holistic Replication)",
      "link": "https://arxiv.org/abs/2509.14055",
      "pubDate": "Wed, 17 Sep 2025 11:00:57 GMT",
      "isoDate": "2025-09-17T11:00:57.000Z",
      "creator": "Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo",
      "summary": "Wan-Animate是一个创新的统一框架，旨在实现角色动画和替换任务。该框架能够处理多种场景，提供高保真度的输出和无缝的集成。\n\n### 核心功能\n\nWan-Animate主要提供两种核心功能：\n\n1.  **角色动画**：\n    *   用户提供一个角色图像和一个参考视频。\n    *   Wan-Animate能够精确复制参考视频中角色的表情和动作。\n    *   最终生成高保真度的角色视频，使静态图像中的角色栩栩如生。\n\n2.  **角色替换**：\n    *   将动画化的角色集成到参考视频中，替换视频中的原始角色。\n    *   该系统能够复制场景的光照和色调，确保动画角色与新环境的无缝融合，达到逼真的效果。\n\n### 技术基础与创新\n\nWan-Animate基于Wan模型构建，并引入了多项关键技术创新以适应角色动画任务：\n\n*   **修改的输入范式**：采用一种修改后的输入范式，以区分参考条件和生成区域，从而将多项任务统一到一个共同的符号表示中。\n*   **身体动作复制**：利用空间对齐的骨骼信号来精确复制身体动作，确保动画的自然流畅。\n*   **表情重现**：从源图像中提取隐式面部特征，用于重现角色的表情，从而生成具有高可控性和表现力的角色视频。\n*   **环境融合增强**：为了在角色替换过程中增强环境集成，Wan-Animate开发了一个辅助的Relighting LoRA模块。该模块能够在应用适当的环境光照和色调的同时，保持角色外观的一致性。\n\n### 性能与可用性\n\n实验结果表明，Wan-Animate实现了最先进的性能。作者团队承诺将开源模型权重和源代码，以促进社区的进一步研究和应用。",
      "shortSummary": "Wan-Animate是一个统一的角色动画与替换框架。它能根据参考视频，将给定角色图像动画化，精确复制表情和动作，生成高保真角色视频；或将动画角色无缝集成到参考视频中，替换原有角色并匹配场景光照和色调。该框架基于Wan模型，利用骨骼信号和隐式面部特征实现高可控性与表现力，并通过Relighting LoRA增强环境融合。Wan-Animate实现了最先进的性能，并将开源其模型和代码。",
      "translated_title": "Wan-Animate：基于整体复制的统一角色动画与替换",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code."
    },
    {
      "title": "SAIL-VL2 技术报告 (原标题: SAIL-VL2 Technical Report)",
      "link": "https://arxiv.org/abs/2509.14033",
      "pubDate": "Wed, 17 Sep 2025 10:34:02 GMT",
      "isoDate": "2025-09-17T10:34:02.000Z",
      "creator": "Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, Wenzhuo Liu, Xiao Liang, Shuicheng Yan, Chao Feng",
      "summary": "SAIL-VL2 技术报告\n\nSAIL-VL2 是一个开放套件的视觉-语言基础模型（LVM），旨在实现全面的多模态理解和推理。作为其前身 SAIL-VL 的继任者，SAIL-VL2 在2B和8B参数规模上，在各种图像和视频基准测试中均取得了最先进的性能，展示了从细粒度感知到复杂推理的强大能力。\n\n**核心创新**\nSAIL-VL2 的有效性主要归功于以下三项核心创新：\n\n1.  **大规模数据整理管道**\n    *   采用评分和过滤策略，显著提升了字幕、OCR、问答和视频数据的数据质量和分布。\n    *   有效提高了训练效率。\n\n2.  **渐进式训练框架**\n    *   首先利用强大的预训练视觉编码器（SAIL-ViT）。\n    *   随后进行多模态预训练。\n    *   最终采用“思维融合”（thinking-fusion）的SFT-RL混合范式，系统性地增强了模型能力。\n\n3.  **架构改进**\n    *   超越了传统的密集型大型语言模型（LLMs）设计。\n    *   引入了高效的稀疏专家混合（MoE）架构。\n\n**性能表现**\n*   SAIL-VL2 在106个数据集上展现出具有竞争力的性能。\n*   在 MMMU 和 MathVista 等具有挑战性的推理基准测试中取得了最先进的结果。\n*   在 OpenCompass 排行榜上，SAIL-VL2-2B 在4B参数规模以下的官方发布开源模型中排名第一。\n\n**社区贡献**\nSAIL-VL2 为开源多模态社区提供了一个高效且可扩展的基础。",
      "shortSummary": "SAIL-VL2是一个开放套件的视觉-语言基础模型，旨在实现全面的多模态理解和推理。作为SAIL-VL的继任者，它在2B和8B参数规模上取得了最先进的性能，并在106个数据集和复杂推理基准测试（如MMMU、MathVista）上表现出色。其核心创新包括大规模数据整理、渐进式训练框架和高效的稀疏专家混合（MoE）架构。SAIL-VL2-2B在OpenCompass排行榜上4B参数规模以下开源模型中排名第一，为开源多模态社区提供了高效且可扩展的基础。",
      "translated_title": "SAIL-VL2 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community."
    },
    {
      "title": "Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型 (原标题: Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale)",
      "link": "https://arxiv.org/abs/2509.14008",
      "pubDate": "Wed, 17 Sep 2025 10:19:28 GMT",
      "isoDate": "2025-09-17T10:19:28.000Z",
      "creator": "Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem",
      "summary": "# Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型\n\n## 摘要\n\n本技术报告介绍了Hala，一个以阿拉伯语为中心的指令和翻译模型家族。这些模型通过独特的“翻译-调优”管道构建，并在阿拉伯语相关基准测试中取得了最先进的成果。Hala项目旨在加速阿拉伯语自然语言处理（NLP）领域的研究。\n\n## 关键方法与技术\n\nHala模型的开发遵循以下核心流程：\n\n*   **教师模型压缩与双语监督生成**\n    *   首先，将一个强大的阿拉伯语-英语（AR$\\leftrightarrow$EN）教师模型压缩到FP8精度。\n    *   这一压缩过程在不损失模型质量的前提下，将吞吐量提高了约2倍。\n    *   压缩后的教师模型用于生成高质量的双语监督数据。\n*   **轻量级语言模型微调与语料库构建**\n    *   一个轻量级语言模型LFM2-1.2B在此双语数据上进行微调。\n    *   随后，该模型被用于将高质量的英语指令集翻译成阿拉伯语。\n    *   这一过程生成了一个百万规模的语料库，专门用于指令遵循任务。\n*   **模型训练与平衡**\n    *   Hala模型在不同参数规模下进行训练，包括350M、700M、1.2B和9B参数。\n    *   采用slerp合并技术，以平衡模型的阿拉伯语专业化能力与基础模型的通用优势。\n\n## 性能表现\n\nHala模型在阿拉伯语相关基准测试中展现出卓越的性能：\n\n*   在“纳米”（$\\leq$2B参数）和“小型”（7-9B参数）两个类别中均取得了最先进（SOTA）的结果。\n*   其性能显著优于作为基础的模型。\n\n## 资源发布\n\n为了促进阿拉伯语自然语言处理（NLP）领域的研究，Hala项目团队发布了：\n\n*   训练好的模型\n*   相关数据集\n*   评估工具\n*   训练配方（recipes）\n\n## 提交信息\n\n*   **主题：** 计算与语言 (cs.CL); 人工智能 (cs.AI); 机器学习 (cs.LG)\n*   **引用方式：** arXiv:2509.14008 [cs.CL]\n*   **提交日期：** 2025年9月17日",
      "shortSummary": "Hala项目发布了一系列大规模以阿拉伯语为中心的指令和翻译模型。通过“翻译-调优”管道，该项目首先将强大的AR$\\leftrightarrow$EN教师模型压缩并生成高质量双语监督数据，然后用轻量级模型将英语指令集翻译成阿拉伯语。Hala模型（350M至9B参数）在阿拉伯语基准测试中取得了最先进（SOTA）结果，超越了基础模型。为加速研究，模型、数据和训练配方均已发布。",
      "translated_title": "Hala 技术报告：构建大规模以阿拉伯语为中心的指令和翻译模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong ARleftrightarrowEN teacher to FP8 (yielding sim2times higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" (leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP."
    },
    {
      "title": "THOR：基于强化学习的工具集成层次优化，用于数学推理 (原标题: THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning)",
      "link": "https://arxiv.org/abs/2509.13761",
      "pubDate": "Wed, 17 Sep 2025 03:16:12 GMT",
      "isoDate": "2025-09-17T03:16:12.000Z",
      "creator": "Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Jun Du, Quan Liu, Jianqing Gao",
      "summary": "## THOR：基于强化学习的工具集成层次优化，用于数学推理\n\n大型语言模型（LLMs）在数学推理方面取得了显著进展，但在高精度任务（如数值计算和形式符号操作）上仍面临挑战。将外部工具集成被视为弥补这一差距的有效方法。然而，现有方法在以下三个关键方面仍存在不足：\n\n*   构建工具集成推理数据\n*   执行细粒度优化\n*   增强推理能力\n\n为了克服这些限制，我们提出了 **THOR (Tool-Integrated Hierarchical Optimization via RL)** 框架，它通过以下核心组件实现目标：\n\n### 1. TIRGen：高质量工具集成推理数据生成\n\n*   **多智能体Actor-Critic流水线：** THOR引入了TIRGen，这是一个基于多智能体Actor-Critic的流水线，专门用于构建高质量的工具集成推理路径数据集。\n*   **策略对齐与泛化：** TIRGen旨在使生成的数据与策略对齐，并在各种不同的模型中实现良好的泛化能力。\n\n### 2. 分层强化学习优化策略\n\n*   **细粒度优化：** THOR提出了一种强化学习策略，用于执行细粒度的分层优化。\n*   **联合优化：** 该策略同时优化两个关键层面：\n    *   **轨迹级别的问题解决：** 关注整个解题路径的正确性。\n    *   **步骤级别的代码生成：** 优化每个中间步骤中工具调用的代码生成质量。\n*   **核心洞察：** 这一策略的动机源于一个关键洞察——中间工具调用的成功是最终答案正确性的强预测因子。\n\n### 3. 自我纠正机制\n\n*   **动态修正：** THOR在推理过程中整合了一个自我纠正机制。\n*   **即时工具反馈：** 该机制利用即时工具反馈，能够动态地修正错误的推理路径，从而提高推理的鲁棒性和准确性。\n\n### 成果与性能\n\nTHOR方法展现出卓越的性能和广泛的适用性：\n\n*   **强大的泛化能力：** 在不同类型的模型（包括推理模型和非推理模型）上都表现出强大的泛化能力。\n*   **最先进的性能：** 在多个数学基准测试中，对于同等规模的模型，THOR实现了最先进的性能。\n*   **代码基准改进：** 在代码基准测试中也带来了持续的改进。\n\n**代码可用性：** THOR的代码将公开发布。",
      "shortSummary": "THOR是一个基于强化学习的框架，旨在通过工具集成提升大型语言模型（LLMs）的数学推理能力。它通过TIRGen生成高质量工具集成数据，采用分层强化学习策略同时优化解题轨迹和代码生成，并利用自我纠正机制动态修正推理错误。THOR在不同模型上表现出强大的泛化能力，并在数学和代码基准测试中取得了最先进的性能和持续改进。",
      "translated_title": "THOR：基于强化学习的工具集成层次优化，用于数学推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR."
    },
    {
      "title": "擦除它！通过机器遗忘技术擦除代码语言模型中的敏感记忆 (原标题: Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning)",
      "link": "https://arxiv.org/abs/2509.13755",
      "pubDate": "Wed, 17 Sep 2025 03:12:35 GMT",
      "isoDate": "2025-09-17T03:12:35.000Z",
      "creator": "Zhaoyang Chu, Yao Wan, Zhikun Zhang, Di Wang, Zhou Yang, Hongyu Zhang, Pan Zhou, Xuanhua Shi, Hai Jin, David Lo",
      "summary": "## 通过机器遗忘技术擦除代码语言模型中的敏感记忆\n\n### 引言\n\n代码语言模型（CLMs）在软件工程任务中表现出卓越性能，但近期研究揭示了一个关键的隐私漏洞：这些模型会无意中记忆敏感的训练数据，并在特定提示下逐字复现机密信息。为解决此问题，虽然已提出数据去重和差分隐私等方法，但它们需要对已部署的CLMs进行完全模型再训练，这会带来巨大的计算成本。\n\n### 研究问题\n\n本文旨在回答一个核心研究问题：能否有效且高效地擦除CLMs记忆的敏感信息？\n\n### 解决方案：机器遗忘\n\n本文开创性地通过机器遗忘（Machine Unlearning）来解决CLMs中的敏感记忆擦除问题。机器遗忘是一种事后修改方法，它无需完全再训练即可从已训练模型中移除特定信息。\n\n### 方法论\n\n1.  **风险量化与数据集构建**\n    *   首先量化CLMs训练数据集中敏感数据的记忆风险。\n    *   整理了一个包含50,000个敏感记忆样本的高风险数据集，作为遗忘目标。\n2.  **遗忘方法研究**\n    *   研究了两种广泛使用的基于梯度上升的遗忘方法：原始（vanilla）方法和基于约束（constraint-based）的方法。\n3.  **CodeEraser的引入**\n    *   本文引入了CodeEraser，这是一种先进的机器遗忘变体。\n    *   CodeEraser能够选择性地遗忘代码中敏感的记忆片段。\n    *   同时，它能保留周围代码的结构完整性和功能正确性。\n\n### 实验与结果\n\n*   **实验对象：** 在三类CLMs上进行了广泛实验，包括CodeParrot、CodeGen-Mono和Qwen2.5-Coder。\n*   **验证：** 实验结果验证了CodeEraser在擦除目标敏感记忆方面的有效性和效率。\n*   **模型效用：** 实验同时表明，CodeEraser在擦除敏感信息的同时，能保持模型的实用性。\n\n### 结论\n\nCodeEraser提供了一种有效且高效的解决方案，用于在不进行完全再训练的情况下，从代码语言模型中擦除敏感的记忆信息，同时保持模型的性能和实用性。",
      "shortSummary": "代码语言模型（CLMs）存在记忆敏感训练数据的隐私漏洞，现有解决方案成本高昂。本文提出CodeEraser，一种基于机器遗忘的事后修改方法，旨在有效且高效地擦除CLMs中的敏感记忆。CodeEraser通过选择性遗忘敏感代码片段，同时保持模型结构和功能。在CodeParrot、CodeGen-Mono和Qwen2.5-Coder上的实验验证了CodeEraser在擦除目标敏感记忆和保持模型效用方面的有效性。",
      "translated_title": "擦除它！通过机器遗忘技术擦除代码语言模型中的敏感记忆",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently?   We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility."
    },
    {
      "title": "SteeringControl: LLM中对齐引导的整体评估 (原标题: SteeringControl: Holistic Evaluation of Alignment Steering in LLMs)",
      "link": "https://arxiv.org/abs/2509.13450",
      "pubDate": "Tue, 16 Sep 2025 14:36:22 GMT",
      "isoDate": "2025-09-16T14:36:22.000Z",
      "creator": "Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang",
      "summary": "本文介绍了一个名为SteeringControl的基准，用于评估表征引导方法在核心对齐目标（包括偏见、有害生成和幻觉）上的表现，以及这些方法对次要行为（如谄媚和常识道德）的影响。研究发现，以往的对齐工作通常侧重于真实性或推理能力，以展示表征引导的副作用，但存在许多尚未被系统性理解的权衡。研究人员收集了一个与安全相关的首要和次要行为数据集，以评估围绕五种流行的引导方法的引导效果和行为纠缠。为此，他们构建了一个模块化的引导框架，该框架基于独特的组件，这些组件是许多现有方法的构建块。在Qwen-2.5-7B和Llama-3.1-8B上的结果表明，强大的引导性能取决于引导方法、模型和目标行为的特定组合，并且不良的组合可能导致严重的概念纠缠。研究代码已发布。",
      "shortSummary": "本文提出了SteeringControl基准，用于评估LLM中表征引导方法在偏见、有害生成、幻觉等对齐目标上的表现，以及对谄媚、常识道德等次要行为的影响。研究发现，引导性能高度依赖于引导方法、模型和目标行为的特定组合，不良组合可能导致概念纠缠。研究人员构建了一个模块化的引导框架，并发布了相关代码。",
      "translated_title": "SteeringControl: LLM中对齐引导的整体评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git."
    },
    {
      "title": "3D感知区域提示视觉语言模型 (原标题: 3D Aware Region Prompted Vision Language Model)",
      "link": "https://arxiv.org/abs/2509.13317",
      "pubDate": "Tue, 16 Sep 2025 13:59:06 GMT",
      "isoDate": "2025-09-16T13:59:06.000Z",
      "creator": "An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu",
      "summary": "### 3D感知区域提示视觉语言模型 (SR-3D)\n\n本文介绍了一种名为空间区域3D (SR-3D) 感知视觉语言模型，旨在通过共享的视觉令牌空间连接单视图2D图像和多视图3D数据。SR-3D模型在场景理解方面，有效统一了2D和3D表示空间。\n\n#### 核心创新与功能\n\n*   **2D与3D数据统一**：SR-3D通过一个共享的视觉令牌空间，将来自单视图2D图像的特征与多视图3D数据进行关联和整合。\n*   **灵活的区域提示**：该模型支持高度灵活的区域提示功能，用户可以通过多种方式指定感兴趣的区域，包括：\n    *   在任意帧上使用边界框进行标注。\n    *   在任意帧上使用分割掩码进行标注。\n    *   直接在3D空间中进行标注。\n    *   这一特性显著减少了对详尽多帧标注的需求。\n\n#### 技术实现\n\n*   **3D位置嵌入增强2D特征**：SR-3D通过将3D位置嵌入信息融入2D视觉特征中，实现了2D与3D的有效融合。\n*   **利用2D先验进行3D推理**：这种方法使得3D模型能够利用强大的2D先验知识，即使感兴趣的对象不在同一视图中共同出现，也能在不同帧之间进行更准确的空间推理。\n\n#### 性能表现与应用\n\n*   **最先进的性能**：在通用2D视觉语言基准测试和专门的3D空间基准测试中，SR-3D均取得了最先进的性能。这证明了其在统一2D和3D表示空间以实现场景理解方面的有效性。\n*   **广泛的适用性**：该模型还适用于“野外”视频，即使这些视频缺乏感官3D输入或地面实况3D标注，SR-3D也能准确推断空间关系和度量测量。",
      "shortSummary": "SR-3D是一种3D感知区域提示视觉语言模型，通过共享视觉令牌空间统一了2D图像和多视图3D数据。它支持灵活的区域提示，允许用户通过边界框、分割掩码或直接在3D中标注区域，无需详尽的多帧标注。通过将3D位置嵌入到2D特征中，SR-3D利用2D先验进行准确的跨帧空间推理。该模型在2D视觉语言和3D空间基准测试中均达到SOTA性能，并能处理无3D输入的“野外”视频，准确推断空间关系。",
      "translated_title": "3D感知区域提示视觉语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements."
    },
    {
      "title": "ReSum：通过上下文摘要解锁长周期搜索智能 (原标题: ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization)",
      "link": "https://arxiv.org/abs/2509.13313",
      "pubDate": "Tue, 16 Sep 2025 13:57:22 GMT",
      "isoDate": "2025-09-16T13:57:22.000Z",
      "creator": "Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou",
      "summary": "### ReSum：通过上下文摘要解锁长周期搜索智能\n\n**核心问题：**\n*   基于大型语言模型（LLM）的网页代理在知识密集型任务中表现出色，但受限于上下文窗口，尤其是在ReAct等范式中。\n*   涉及多个实体、复杂关系和高不确定性的复杂查询需要大量的搜索周期，这会迅速耗尽上下文预算，导致在达到完整解决方案之前受阻。\n\n**解决方案：ReSum范式**\n*   **引入：** ReSum是一种新颖的范式，通过周期性上下文摘要实现无限探索。\n*   **工作原理：** ReSum将不断增长的交互历史转换为紧凑的推理状态，从而在绕过上下文限制的同时保持对先前发现的感知。\n\n**范式适应：ReSum-GRPO**\n*   为了使代理熟悉摘要条件下的推理，研究人员提出了ReSum-GRPO。\n*   **集成：** ReSum-GRPO将GRPO（Generalized Reinforcement Learning with Policy Optimization）与分段轨迹训练和优势广播相结合。\n\n**实验结果与性能提升：**\n*   **基准测试：** 在三个基准测试中，对不同规模的网页代理进行了广泛实验。\n*   **平均提升：** ReSum在ReAct基础上实现了平均4.5%的绝对性能提升。\n*   **ReSum-GRPO的额外增益：** 经过ReSum-GRPO训练后，性能提升高达8.2%。\n*   **显著成就：**\n    *   仅使用1K训练样本，WebResummer-30B（WebSailor-30B的ReSum-GRPO训练版本）在BrowseComp-zh上实现了33.3%的Pass@1，在BrowseComp-en上实现了18.3%的Pass@1。\n    *   这一表现超越了现有的开源网页代理。",
      "shortSummary": "ReSum是一种新范式，通过周期性上下文摘要解决了LLM网页代理在长周期搜索任务中受上下文窗口限制的问题。它将交互历史转化为紧凑的推理状态，实现无限探索。结合ReSum-GRPO训练后，ReSum在ReAct基础上平均提升4.5%，最高达8.2%。WebResummer-30B在仅1K训练样本下，在BrowseComp-zh和BrowseComp-en上分别达到33.3%和18.3%的Pass@1，超越现有开源代理。",
      "translated_title": "ReSum：通过上下文摘要解锁长周期搜索智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on BrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web agents."
    },
    {
      "title": "WebWeaver：利用动态大纲构建网络规模证据以进行开放式深度研究 (原标题: WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research)",
      "link": "https://arxiv.org/abs/2509.13312",
      "pubDate": "Tue, 16 Sep 2025 13:57:21 GMT",
      "isoDate": "2025-09-16T13:57:21.000Z",
      "creator": "Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, Jingren Zhou",
      "summary": "# WebWeaver：利用动态大纲构建网络规模证据以进行开放式深度研究\n\n本文介绍了一个名为WebWeaver的新型双代理框架，旨在解决开放式深度研究（OEDR）这一复杂挑战。OEDR要求AI代理将海量的网络信息综合成富有洞察力的报告。\n\n## 当前方法的局限性：\n现有的AI代理在处理OEDR时面临双重限制：\n*   **静态研究流程：** 规划与证据获取相互脱节，导致研究过程缺乏灵活性和适应性。\n*   **一次性生成范式：** 容易出现长上下文问题，例如“中间信息丢失”（loss in the middle）和幻觉（hallucinations），影响报告的准确性和完整性。\n\n## WebWeaver框架的核心理念与设计：\nWebWeaver框架模仿人类研究过程，采用双代理结构，以迭代和聚焦的方式进行信息处理：\n\n### 1. 规划器（Planner）：\n*   在一个动态循环中运行，持续优化研究大纲。\n*   迭代地交织证据获取与大纲优化，确保规划与信息收集紧密结合。\n*   生成一个全面、基于来源的大纲，该大纲链接到一个包含所有收集到的证据的记忆库。\n\n### 2. 撰写器（Writer）：\n*   执行分层检索和撰写过程，逐节构建报告。\n*   针对报告的每个部分，从记忆库中进行有针对性的检索，只获取当前部分所需的必要证据。\n*   通过这种聚焦式检索，有效缓解了传统一次性生成范式中常见的长上下文问题，提高了报告的质量和可靠性。\n\n## WebWeaver的优势与成果：\n*   **缓解长上下文问题：** 通过有针对性的分层检索，WebWeaver显著减少了“中间信息丢失”和幻觉的发生。\n*   **卓越的性能：** WebWeaver在主要的OEDR基准测试（包括DeepResearch Bench、DeepConsult和DeepResearchGym）中取得了新的最先进成果。\n*   **方法论验证：** 这些结果验证了WebWeaver以人为中心、迭代式的方法论，证明了自适应规划和聚焦式综合对于生成高质量、可靠且结构良好的报告至关重要。\n\n## 研究领域：\n*   计算与语言 (cs.CL)",
      "shortSummary": "WebWeaver是一个新型双代理框架，旨在解决开放式深度研究（OEDR）中AI代理综合海量网络信息生成报告的挑战。它通过模仿人类研究过程，采用动态规划器迭代优化大纲并获取证据，以及分层撰写器有针对性地检索和合成信息。该框架有效缓解了长上下文问题，并在主要OEDR基准测试中取得了最先进的成果，证明了自适应规划和聚焦式综合对高质量报告的重要性。",
      "translated_title": "WebWeaver：利用动态大纲构建网络规模证据以进行开放式深度研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and one-shot generation paradigms that easily suffer from long-context failure issues like \"loss in the middle\" and hallucinations. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, source-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank for each part, it effectively mitigates long-context issues. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing high-quality, reliable, and well-structured reports."
    },
    {
      "title": "通过环境扩展实现通用智能体智能 (原标题: Towards General Agentic Intelligence via Environment Scaling)",
      "link": "https://arxiv.org/abs/2509.13311",
      "pubDate": "Tue, 16 Sep 2025 13:57:20 GMT",
      "isoDate": "2025-09-16T13:57:20.000Z",
      "creator": "Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "### 通过环境扩展实现通用智能体智能\n\n**背景与挑战**\n\n*   在实际、真实世界的应用中部署大型语言模型（LLMs）需要先进的智能体智能。\n*   多样化的真实世界API要求智能体具备精确、鲁棒的函数调用能力。\n*   智能体需要通过在各种环境中交互来发展这些能力。\n*   函数调用能力的广度与智能体训练环境的多样性紧密相关。\n\n**研究目标**\n\n*   本研究旨在通过扩展环境，作为推进通用智能体智能的一个步骤。\n\n**核心挑战**\n\n这项工作引出了两个核心挑战：\n\n1.  如何以系统化的方式扩展环境。\n2.  如何有效地从与这些环境的交互经验中训练智能体能力。\n\n**解决方案**\n\n为了解决这些挑战，研究人员设计了一个可扩展的框架和训练策略：\n\n*   **可扩展框架设计**：\n    *   该框架能够自动构建异构的、完全模拟的环境。\n    *   系统性地拓宽了函数调用场景的空间。\n*   **两阶段智能体微调策略**：\n    *   **第一阶段**：赋予智能体基础的智能体能力。\n    *   **第二阶段**：针对特定领域上下文对智能体进行专业化。\n\n**实验与成果**\n\n*   在智能体基准测试（tau-bench、tau2-Bench 和 ACEBench）上进行了广泛的实验。\n*   研究结果表明，他们训练的模型 **AgentScaler** 显著增强了模型的函数调用能力。",
      "shortSummary": "该研究旨在通过扩展训练环境来提升大型语言模型的通用智能体智能。为解决环境扩展和有效训练的挑战，作者设计了一个可扩展框架，自动构建异构模拟环境，并提出两阶段微调策略。实验证明，其模型AgentScaler显著增强了模型在函数调用方面的能力，为LLM在实际应用中的部署奠定了基础。",
      "translated_title": "通过环境扩展实现通用智能体智能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models."
    },
    {
      "title": "通过持续预训练扩展智能体 (原标题: Scaling Agents via Continual Pre-training)",
      "link": "https://arxiv.org/abs/2509.13310",
      "pubDate": "Tue, 16 Sep 2025 13:57:19 GMT",
      "isoDate": "2025-09-16T13:57:19.000Z",
      "creator": "Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "### 通过持续预训练扩展智能体：AgentFounder模型\n\n#### 背景与问题\n\n*   **大型语言模型（LLMs）的智能体能力**：LLMs已发展成为能够自主使用工具并进行多步推理以解决复杂问题的智能体系统。\n*   **现有方法的局限性**：然而，基于通用基础模型构建的后训练方法在智能体任务中表现不佳，尤其是在开源实现中。\n*   **根本原因**：研究人员发现，这种表现不佳的根源在于缺乏强大的智能体基础模型。这导致模型在后训练阶段需要同时学习多样化的智能体行为并将其与专家演示对齐，从而产生根本性的优化冲突。\n\n#### 解决方案：智能体持续预训练（Agentic CPT）\n\n*   **创新性提案**：本文首次提出将智能体持续预训练（Agentic CPT）整合到深度研究智能体训练流程中，旨在构建强大的智能体基础模型。\n\n#### 模型开发：AgentFounder\n\n*   **基于Agentic CPT**：研究团队基于Agentic CPT方法，开发了一个名为AgentFounder的深度研究智能体模型。\n\n#### 性能评估\n\n*   **最先进的性能**：AgentFounder-30B在10个基准测试中进行了评估，并取得了最先进的性能（state-of-the-art）。\n*   **强大的工具使用能力**：该模型同时保持了强大的工具使用能力，具体表现为：\n    *   在BrowseComp-en上达到 **39.9%**\n    *   在BrowseComp-zh上达到 **43.3%**\n    *   在HLE上Pass@1达到 **31.5%**",
      "shortSummary": "大型语言模型作为智能体在后训练中表现不佳，主要原因是缺乏强大的智能体基础模型和优化冲突。为解决此问题，本文首次提出智能体持续预训练（Agentic CPT）方法，并开发了AgentFounder模型。AgentFounder-30B在10个基准测试中取得了最先进的性能，并在BrowseComp-en（39.9%）、BrowseComp-zh（43.3%）和HLE（31.5% Pass@1）等任务上展现出强大的工具使用能力。",
      "translated_title": "通过持续预训练扩展智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE."
    },
    {
      "title": "WebResearcher：在长周期智能体中释放无限推理能力 (原标题: WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents)",
      "link": "https://arxiv.org/abs/2509.13309",
      "pubDate": "Tue, 16 Sep 2025 13:57:17 GMT",
      "isoDate": "2025-09-16T13:57:17.000Z",
      "creator": "Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "WebResearcher是一个旨在构建能够自主发现和合成知识的AI智能体的创新框架。该框架通过两个核心组件，解决了现有深度研究系统面临的挑战，并显著提升了智能体的推理能力：\n\n### 1. WebResearcher：迭代式深度研究范式\n\n*   **将深度研究重构为马尔可夫决策过程（MDP）**：智能体能够以结构化的方式进行研究。\n*   **周期性整合发现与报告演进**：智能体定期将其研究成果整合到不断更新的报告中，确保知识的积累和组织。\n*   **维护专注的工作空间**：通过保持工作空间的聚焦，有效避免了现有单上下文方法中常见的“上下文窒息”和“噪声污染”问题。\n\n### 2. WebFrontier：可扩展数据合成引擎\n\n*   **生成高质量训练数据**：该引擎通过“工具增强的复杂性升级”机制，系统性地创建高质量的训练数据。\n*   **弥合知识鸿沟**：它能够生成连接被动知识回忆和主动知识构建的研究任务，促进智能体更深层次的学习和理解。\n\n### 主要发现与优势\n\n*   **显著增强工具使用能力**：WebResearcher范式生成的训练数据，即使对于传统的单上下文方法，也能显著提升其工具使用能力。\n*   **支持并行思维与多智能体探索**：该范式天然支持通过并行思维进行扩展，允许多个智能体同时进行探索，从而得出更全面、更深入的结论。\n\n### 性能表现\n\n*   **最先进的性能**：在6个具有挑战性的基准测试中，WebResearcher展现了卓越的性能，达到了当前最先进的水平。\n*   **超越专有系统**：其表现甚至超越了一些前沿的专有系统。\n\n### 相关信息\n\n*   **主题**：计算与语言 (cs.CL)\n*   **引用**：arXiv:2509.13309 [cs.CL]\n\n（文章内容中不包含有效的实际图片链接，因此详细摘要中不包含图片。）",
      "shortSummary": "WebResearcher是一个创新的AI框架，旨在通过其迭代式深度研究范式和可扩展数据合成引擎，在长周期智能体中释放无限推理能力。它将研究建模为马尔可夫决策过程，通过定期整合发现和维护专注工作空间，有效克服了上下文限制和噪声问题。WebFrontier引擎生成高质量训练数据，显著增强了智能体的工具使用能力。该框架支持并行多智能体探索，并在6个挑战性基准测试中取得了超越现有专有系统的最先进性能。",
      "translated_title": "WebResearcher：在长周期智能体中释放无限推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems."
    },
    {
      "title": "WebSailor-V2：通过合成数据和可扩展强化学习弥合与专有代理之间的鸿沟 (原标题: WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.13305",
      "pubDate": "Tue, 16 Sep 2025 13:57:03 GMT",
      "isoDate": "2025-09-16T13:57:03.000Z",
      "creator": "Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "summary": "## WebSailor-V2：通过合成数据和可扩展强化学习弥合与专有代理之间的鸿沟\n\n本文介绍了WebSailor，一个旨在提升大型语言模型（LLM）在复杂信息检索任务中处理极端不确定性能力的完整后训练方法。\n\n### 背景与挑战\n\n*   **LLM的局限性**：当前LLM在处理需要超越人类认知限制的复杂信息检索任务时面临挑战。\n*   **专有代理的优势**：DeepResearch等专有代理系统在BrowseComp等极其复杂的基准测试中展现出超人能力，这是开源模型此前无法达到的。\n*   **核心洞察**：专有代理的成功在于其独特的推理模式——在广阔的信息环境中系统性地降低极端不确定性。这种能力在开源模型中缺失。\n\n### WebSailor方法论\n\nWebSailor是一个完整的后训练方法，旨在将上述关键能力灌输给开源模型。其方法包括：\n\n*   **高不确定性任务生成**：通过结构化采样和信息混淆，生成新颖的、高不确定性的任务。\n*   **RFT冷启动**：采用强化微调（RFT）进行冷启动。\n*   **高效的代理强化学习训练算法**：引入了“重复采样策略优化”（Duplicating Sampling Policy Optimization, DUPO）。\n\n### 成果与影响\n\n*   **性能提升**：通过这一整合的流程，WebSailor在复杂信息检索任务中显著超越了所有开源代理。\n*   **弥合差距**：WebSailor的性能达到了专有代理的水平，成功弥合了开源模型与专有代理之间的能力差距。",
      "shortSummary": "WebSailor-V2提出了一种后训练方法，旨在弥合开源大型语言模型（LLM）与专有代理在处理复杂信息检索任务中的能力差距。专有代理通过系统性地降低极端不确定性展现出超人能力。WebSailor通过生成高不确定性任务、RFT冷启动和高效的DUPO强化学习算法，使开源代理获得了类似能力。实验表明，WebSailor显著超越了所有开源代理，并达到了专有代理的性能水平。",
      "translated_title": "WebSailor-V2：通过合成数据和可扩展强化学习弥合与专有代理之间的鸿沟",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap."
    },
    {
      "title": "单流策略优化 (原标题: Single-stream Policy Optimization)",
      "link": "https://arxiv.org/abs/2509.13232",
      "pubDate": "Tue, 16 Sep 2025 12:39:11 GMT",
      "isoDate": "2025-09-16T12:39:11.000Z",
      "creator": "Zhongwen Xu, Zihan Ding",
      "summary": "## 单流策略优化 (SPO) 详细摘要\n\n本文重新审视了大型语言模型 (LLM) 的策略梯度优化，并提出了一种名为“单流策略优化 (Single-stream Policy Optimization, SPO)”的新方法，旨在解决现有组基方法（如 GRPO）的固有缺陷。\n\n### 现有方法的挑战\n\n当前主流的组基策略梯度方法（例如 GRPO）通过即时基线来减少方差，但存在以下关键问题：\n\n*   **频繁的退化组**：导致学习信号的丢失，浪费计算资源。\n*   **同步障碍**：阻碍了算法的扩展性，尤其是在生成时间差异较大的长序列或工具集成场景中。\n\n### SPO 的核心思想与机制\n\nSPO 从设计上消除了上述问题，其主要创新点包括：\n\n*   **持久的 KL 自适应价值追踪器**：SPO 用一个持久的、KL 自适应的价值追踪器取代了每个组的基线，从而提供了一个更稳定、低方差的学习信号。\n*   **全局优势归一化**：SPO 在整个批次中对优势进行全局归一化，确保每个样本都能获得稳定的学习信号。\n\n### SPO 的优势\n\nSPO 的无组设计带来了多方面的好处：\n\n*   **消除退化组**：避免了因退化组而浪费的计算和学习信号损失。\n*   **更高的吞吐量和可扩展性**：由于没有组同步障碍，SPO 在生成时间差异大的长序列或工具集成设置中能够实现更高的吞吐量和更有效的扩展。\n*   **自适应课程学习**：持久的价值追踪器自然地支持通过优先采样实现自适应课程学习。\n\n### 实验结果与性能提升\n\n研究团队使用 Qwen3-8B 模型进行了实验，结果表明 SPO 相比 GRPO 具有显著优势：\n\n*   **收敛性与准确性**：SPO 的收敛过程更平滑，并取得了更高的准确性。\n*   **计算效率**：SPO 消除了在退化组上浪费的计算。\n*   **数学基准表现**：在五个困难的数学基准测试中，SPO 将 Qwen3-8B 的平均 maj@32 性能比 GRPO 提高了 +3.4 个百分点 (pp)。具体提升包括：\n    *   BRUMO 25：+7.3 pp\n    *   AIME 25：+4.4 pp\n    *   HMMT 25：+3.3 pp\n*   **pass@k 表现**：在所有评估的 k 值上，SPO 在 pass@k 指标上都实现了持续的相对增益。\n\n### 结论与意义\n\n消融研究证实，SPO 的性能提升源于其在基线估计和优势归一化方面的原则性方法，为 LLM 推理提供了一条更稳健、更高效的路径。SPO 的成功挑战了当前强化学习算法中倾向于增加偶然复杂性的趋势，强调了通过基本原理而非架构性变通方案来推动 LLM 推理领域下一波进展的重要性。",
      "shortSummary": "本文提出单流策略优化 (SPO)，一种用于大型语言模型 (LLM) 的新型策略梯度方法。SPO 通过引入持久的 KL 自适应价值追踪器和全局优势归一化，解决了现有组基方法（如 GRPO）中退化组和同步障碍问题，提供更稳定、低方差的学习信号。实验表明，SPO 在 Qwen3-8B 上收敛更平滑，准确性更高，在五个困难数学基准测试中平均 maj@32 比 GRPO 提高了 +3.4 个百分点，展现了更强的鲁棒性和效率。",
      "translated_title": "单流策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@k across the evaluated k values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning."
    },
    {
      "title": "PANORAMA：具身AI时代全向视觉的崛起 (原标题: PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era)",
      "link": "https://arxiv.org/abs/2509.12989",
      "pubDate": "Tue, 16 Sep 2025 07:54:37 GMT",
      "isoDate": "2025-09-16T07:54:37.000Z",
      "creator": "Xu Zheng, Chenfei Liao, Ziqiao Weng, Kaiyu Lei, Zihao Dongfang, Haocong He, Yuanhuiyi Lyu, Lutao Jiang, Lu Qi, Li Chen, Danda Pani Paudel, Kailun Yang, Linfeng Zhang, Luc Van Gool, Xuming Hu",
      "summary": "# PANORAMA：具身AI时代全向视觉的崛起\n\n## 引言\n\n全向视觉，即利用360度视野理解环境的技术，在机器人、工业检测和环境监测等多个领域变得日益关键。与传统的针孔视觉相比，全向视觉提供了全面的环境感知能力，显著提升了场景理解的完整性和决策的可靠性。\n\n## 发展趋势\n\n尽管全向视觉的基础研究在历史上一直落后于传统的针孔视觉，但在具身AI时代，随着工业需求和学术兴趣的日益增长，全向视觉正经历着快速发展，成为一个新兴趋势。\n\n## 主要突破\n\n文章强调了全向视觉在以下几个方面的最新突破：\n\n*   **全向生成（Omnidirectional Generation）**\n*   **全向感知（Omnidirectional Perception）**\n*   **全向理解（Omnidirectional Understanding）**\n*   **相关数据集（Related Datasets）**\n\n## PANORAMA系统架构\n\n作者结合学术界和工业界的见解，提出了一个在具身AI时代理想的全景系统架构——**PANORAMA**。该架构由四个关键子系统组成，旨在构建一个强大、通用的全向AI系统。\n\n## 未来展望与挑战\n\n文章深入探讨了全景视觉与具身AI交叉领域的新兴趋势和跨社区影响，并提出了未来的发展路线图和开放性挑战。这项概述综合了最先进的进展，并为具身AI时代构建鲁棒、通用的全向AI系统指明了未来研究的挑战和机遇。\n\n## 总结\n\n本文作为一篇初步概述，旨在介绍具身AI背景下全向视觉这一新兴领域，并展望其未来的发展方向和潜在影响。",
      "shortSummary": "本文概述了具身AI时代全向视觉的崛起。全向视觉因其提供全面的环境感知能力，在机器人、工业检测等领域日益关键。尽管曾落后于传统针孔视觉，但目前正快速发展，并在全向生成、感知、理解及数据集方面取得突破。文章提出了一个理想的全景系统架构PANORAMA，并探讨了新兴趋势、未来路线图及开放挑战，旨在推动鲁棒、通用的全向AI系统发展。",
      "translated_title": "PANORAMA：具身AI时代全向视觉的崛起",
      "images": [],
      "contentSource": "完整文章",
      "content": "Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era."
    },
    {
      "title": "混元3D工作室：端到端AI游戏就绪3D资产生成流水线 (原标题: Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation)",
      "link": "https://arxiv.org/abs/2509.12815",
      "pubDate": "Tue, 16 Sep 2025 04:33:03 GMT",
      "isoDate": "2025-09-16T04:33:03.000Z",
      "creator": "Biwen Lei, Yang Li, Xinhai Liu, Shuhui Yang, Lixin Xu, Jingwei Huang, Ruining Tang, Haohan Weng, Jian Liu, Jing Xu, Zhen Zhou, Yiling Zhu, Jiankai Xing, Jiachen Xu, Changfeng Ma, Xinhao Yan, Yunhan Yang, Chunshi Wang, Duoteng Xu, Xueqi Ma, Yuguang Chen, Jing Li, Mingxin Yang, Sheng Zhang, Yifei Feng, Xin Huang, Di Luo, Zebin He, Puhua Jiang, Changrong Hu, Zihan Qin, Shiwei Miao, Haolin Liu, Yunfei Zhao, Zeqiang Lai, Qingxiang Lin, Zibo Zhao, Kunhong Li, Xianghui Yang, Huiwen Shi, Xin Yang, Yuxuan Wang, Zebin Yao, Yihang Lian, Sicong Liu, Xintong Han, Wangchen Qin, Caisheng Ouyang, Jianyin Liu, Tianwen Yuan, Shuai Jiang, Hong Duan, Yanqi Niu, Wencong Lin, Yifu Sun, Shirui Huang, Lin Niu, Gu Gong, Guojian Xiao, Bojian Zheng, Xiang Yuan, Qi Chen, Jie Xiao, Dongyang Zheng, Xiaofeng Yang, Kai Liu, Jianchen Zhu, Lifu Wang, Qinglin Lu, Jie Liu, Liang Dong, Fan Jiang, Ruibin Chen, Lei Wang, Chao Zhang, Jiaxin Lin, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Yinhe Wu, Jiayao Du, Jupeng Chen, Xinyue Mao, Dongyuan Guo, Yixuan Tang, Yulin Tsai, Yonghao Tan, Jiaao Yu, Junlin Yu, Keren Zhang, Yifan Li, Peng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Zhuo Chen, Chunchao Guo",
      "summary": "# 混元3D工作室：端到端AI游戏就绪3D资产生成流水线\n\n## 引言\n高质量3D资产的创建是现代游戏开发的核心环节，但长期以来，这一过程以其劳动密集型和高度专业化的工作流程而著称。\n\n## Hunyuan3D Studio 概述\n本文介绍了Hunyuan3D Studio，这是一个由AI驱动的端到端内容创建平台。它的设计宗旨是通过自动化和简化游戏就绪3D资产的生成，彻底革新当前的游戏生产流水线。\n\n## 核心技术与功能\n*   **集成先进的神经网络模块：** Hunyuan3D Studio 的核心在于它将一系列先进的神经网络模块（例如，部件级3D生成、多边形生成、语义UV等）整合到一个连贯且用户友好的系统中。\n*   **统一框架：** 这一统一的框架使得平台能够将单个概念图像或文本描述快速转换为一个完全实现、达到生产质量标准的3D模型。\n*   **高质量输出：** 生成的3D模型不仅包含优化的几何结构，还配备了高保真PBR（基于物理渲染）纹理，确保了视觉效果的卓越性。\n\n## 优势与影响\n*   **视觉吸引力：** Hunyuan3D Studio 生成的资产不仅在视觉上引人注目。\n*   **符合行业标准：** 它们还能严格遵守当代游戏引擎的技术要求，确保了在实际游戏环境中的可用性。\n*   **效率提升：** 该平台显著减少了3D资产创建的迭代时间，加快了开发周期。\n*   **降低门槛：** 它极大地降低了3D内容创作的准入门槛，使更多创作者能够参与其中。\n*   **无缝衔接：** Hunyuan3D Studio 在创意意图和技术资产之间架起了一座无缝的桥梁，实现了从概念到成品的高效转化。\n\n## 结论\nHunyuan3D Studio 代表了AI辅助工作流程在游戏开发和交互式媒体领域的一个重大飞跃，预示着未来内容创作的新方向。",
      "shortSummary": "Hunyuan3D Studio是一个端到端AI内容创建平台，旨在革新游戏3D资产生成。它通过集成先进的神经网络模块，能将概念图像或文本描述快速转化为符合游戏引擎要求的、具有优化几何和PBR纹理的生产级3D模型。该平台显著减少了迭代时间，降低了3D创作门槛，是AI辅助游戏开发的重要进展。",
      "translated_title": "混元3D工作室：端到端AI游戏就绪3D资产生成流水线",
      "images": [],
      "contentSource": "完整文章",
      "content": "The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media."
    },
    {
      "title": "EconProver：迈向更经济的自动化定理证明测试时扩展 (原标题: EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving)",
      "link": "https://arxiv.org/abs/2509.12603",
      "pubDate": "Mon, 15 Sep 2025 23:00:13 GMT",
      "isoDate": "2025-09-15T23:00:13.000Z",
      "creator": "Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu",
      "summary": "### EconProver：迈向更经济的自动化定理证明测试时扩展\n\n本文探讨了大型语言模型（LLMs）在自动化定理证明（ATP）领域中应用时所面临的计算效率问题，并提出了名为 EconProver 的解决方案。\n\n**背景与问题**\n\n*   **性能提升与计算开销并存**：LLMs通过广泛采用的测试时扩展策略，如反射式思维链（CoT）推理和增加采样次数，显著提升了ATP的性能。\n*   **高昂的推理成本**：这些策略也带来了显著的推理计算开销。\n*   **现有成本分析的局限性**：目前的成本分析通常只关注采样次数，而忽略了不同扩展策略引入的采样成本差异。\n*   **当前SOTA方法的低效**：作者指出，当前最先进的（SOTA）开源方法在效率方面表现不佳。\n\n**研究目标**\n\n*   系统比较ATP模型不同测试时扩展策略的效率。\n*   提出方法以显著减少令牌使用和采样次数，同时保持原始性能。\n\n**EconProver 提出的解决方案**\n\n本文提出了两种互补的方法，可以集成到统一的 EconRL 管道中以放大效益：\n\n1.  **动态思维链（CoT）切换机制**：旨在智能地管理和减轻不必要的令牌消耗，从而提高效率。\n2.  **多样化并行扩展强化学习（RL）与可训练前缀**：在受限的采样次数下，通过增强学习方法和可训练的前缀来提高定理证明的通过率。\n\n**实验结果**\n\n*   在 miniF2F 和 ProofNet 数据集上的实验表明，EconProver 在实现与基线方法相当的性能的同时，仅需 12% 的计算成本。\n\n**结论与意义**\n\n*   这项工作为部署轻量级ATP模型提供了可行的见解，而无需牺牲性能，为ATP领域的高效发展开辟了新途径。",
      "shortSummary": "EconProver 旨在解决大型语言模型在自动化定理证明（ATP）中测试时扩展策略（如思维链和增加采样）带来的高计算成本问题。该研究系统比较了不同策略的效率，并提出了两种互补方法：动态思维链切换机制以减少令牌消耗，以及多样化并行扩展强化学习与可训练前缀以提高通过率。实验证明，EconProver 在保持性能的同时，将计算成本降低至基线方法的12%，为部署轻量级ATP模型提供了高效方案。",
      "translated_title": "EconProver：迈向更经济的自动化定理证明测试时扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance."
    },
    {
      "title": "量子格算法的精确陪集采样 (原标题: Exact Coset Sampling for Quantum Lattice Algorithms)",
      "link": "https://arxiv.org/abs/2509.12341",
      "pubDate": "Mon, 15 Sep 2025 14:10:28 GMT",
      "isoDate": "2025-09-15T14:10:28.000Z",
      "creator": "Yifan Zhang",
      "summary": "# 量子格算法中精确陪集采样的改进\n\n本文提出了一种针对近期带有复高斯窗的窗式QFT量子格算法（如`chen2024quantum`所述）中第9步“域扩展”问题的改进方案。\n\n## 核心问题\n\n*   **现有挑战**：该算法第9步中备受争议的“域扩展”方法存在“周期性/支持不匹配”的问题。\n\n## 提出的解决方案\n\n*   **新方法**：作者提供了一个简单、完全正确且假设较少的替代方案。\n*   **关键构造**：该方案采用了一种“对移差分构造”（pair-shift difference construction）。\n\n## 解决方案的机制与成果\n\n*   **抵消偏移**：通过这种构造，可以相干地抵消所有未知的偏移量。\n*   **状态生成**：它能够生成一个在 $\\mathbb{Z}_{P}$ 上精确的均匀CRT陪集状态。\n*   **关系强制**：随后，利用量子傅里叶变换（QFT）来强制执行预期的模线性关系。\n\n## 新酉变换的特性\n\n*   **可逆性**：该酉变换是可逆的。\n*   **门复杂度**：它使用 $\\mathrm{poly}(\\log M_2)$ 个门。\n*   **渐近性**：它保持了原算法的渐近性能。",
      "shortSummary": "本文针对近期窗式QFT量子格算法第9步中“域扩展”的争议问题，提出了一种简单、正确的替代方案。通过对移差分构造，该方法能相干抵消未知偏移，生成$\\mathbb{Z}_{P}$上的精确均匀CRT陪集状态，并利用QFT强制模线性关系。此可逆酉变换使用$\\mathrm{poly}(\\log M_2)$门，保持算法渐近性，解决了周期性/支持不匹配问题。",
      "translated_title": "量子格算法的精确陪集采样",
      "images": [],
      "contentSource": "完整文章",
      "content": "We give a simple, fully correct, and assumption-light replacement for the contested \"domain-extension\" in Step 9 of a recent windowed-QFT lattice algorithm with complex-Gaussian windows~chen2024quantum. The published Step~9 suffers from a periodicity/support mismatch. We present a pair-shift difference construction that coherently cancels all unknown offsets, produces an exact uniform CRT-coset state over Z_{P}, and then uses the QFT to enforce the intended modular linear relation. The unitary is reversible, uses poly(log M_2) gates, and preserves the algorithm's asymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice."
    }
  ],
  "lastUpdated": "2025-09-18T09:34:59.985Z"
}