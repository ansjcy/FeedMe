{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "全身条件下的第一人称视频预测 (原标题: Whole-Body Conditioned Egocentric Video Prediction)",
      "link": "https://arxiv.org/abs/2506.21552",
      "pubDate": "Thu, 26 Jun 2025 13:59:59 GMT",
      "isoDate": "2025-06-26T13:59:59.000Z",
      "creator": "Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik",
      "summary": "## 全身条件下的第一人称视频预测 (PEVA)\n\n这项研究介绍了一种名为PEVA（Predict Ego-centric Video from human Actions）的模型，旨在根据过去的视频和由相对3D身体姿态表示的人体动作来预测第一人称视角视频。\n\n### 核心目标与方法\n*   **目标**：训练模型，使其能够模拟人类的物理动作如何从第一人称视角塑造环境。\n*   **输入条件**：模型以运动学姿态轨迹为条件，这些轨迹通过身体的关节层级结构化，从而捕捉动作的细节。\n*   **模型架构**：采用了一种自回归条件扩散Transformer模型进行训练。\n\n### 数据集\n*   **名称**：Nymeria\n*   **特点**：这是一个大规模的真实世界数据集，包含了第一人称视角视频和相应的身体姿态捕捉数据，为模型的训练提供了丰富的素材。\n\n### 评估协议\n*   **设计**：研究团队设计了一个分层的评估协议。\n*   **目的**：通过设置挑战性逐渐增加的任务，对模型的具身预测和控制能力进行全面而深入的分析。\n\n### 研究意义\n*   这项工作代表了在从人类视角建模复杂真实世界环境和具身智能体行为的视频预测领域，迈出了初步但重要的尝试。",
      "shortSummary": "这项研究提出了PEVA模型，利用自回归条件扩散Transformer，根据过去的视频和相对3D身体姿态预测第一人称视角视频。模型旨在模拟人类物理动作如何从第一人称视角塑造环境。它在大型真实数据集Nymeria上训练，并通过分层协议评估其具身预测和控制能力。这是首次尝试从人类视角对复杂真实世界环境和具身智能体行为进行视频预测。",
      "translated_title": "全身条件下的第一人称视频预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human."
    },
    {
      "title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程 (原标题: Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test)",
      "link": "https://arxiv.org/abs/2506.21551",
      "pubDate": "Thu, 26 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-26T13:59:58.000Z",
      "creator": "Ziyue Li, Chenrui Fan, Tianyi Zhou",
      "summary": "### Grokking现象在大型语言模型预训练中的发现与机制\n\n**1. 研究背景与Grokking现象**\n*   **Grokking定义：** 指的是神经网络训练中，测试性能在训练损失收敛后仍持续提升的现象。这一现象使得泛化能力和推理等新兴能力的机制变得神秘。\n*   **以往研究局限：** 多数以往研究集中于在少量玩具任务或高度特定任务上训练小型模型，且通常需要数千个epoch。\n\n**2. 本研究的创新点与发现**\n*   **首次大规模研究：** 本研究首次在7B大型语言模型（LLM）OLMoE的单次预训练检查点上，对Grokking现象进行了研究。这与以往小型模型的研究形成对比。\n*   **验证Grokking存在：** 研究首次证实，Grokking现象确实存在于大规模基础模型的预训练中，尽管不同数据可能异步进入Grokking阶段。\n*   **评估方法：** 在研究中，计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索等多样化基准任务上评估了模型的泛化能力。\n\n**3. Grokking的机制解释：记忆到泛化的转变**\n*   **内部动态调查：** 通过深入调查LLM的内部动态，本研究揭示了Grokking中“泛化出现”的内在机制。\n*   **通路演变：** 训练样本的通路（即跨层专家选择）在Grokking过程中，从随机、实例特定演变为更结构化且样本间可共享。\n*   **通路复杂性降低：** 尽管训练损失已经收敛，但样本通路的复杂性却降低了。\n*   **核心结论：** 这些发现共同表明了一个从“记忆”到“泛化”的转换过程，为延迟泛化提供了机械性的解释。\n\n**4. 实用工具与理论贡献**\n*   **新型度量指标：** 本研究开发了两个新颖的度量指标，用于量化通路距离和单个通路的复杂性。\n*   **预测泛化能力：** 这些指标被证明能够有效预测模型在各种下游任务上的泛化改进。\n*   **高效实用：** 这些指标计算高效、简单，并且仅依赖于训练数据，无需额外的测试集或微调。\n*   **实际价值：** 它们对于预训练过程具有重要的实际价值，使得在不进行微调和测试的情况下，也能有效监控模型的泛化性能。\n*   **理论支持：** 理论上，研究表明更结构化的通路能够降低模型复杂性，并改善泛化界限。",
      "shortSummary": "本研究首次在7B大型语言模型（OLMoE）的预训练中证实了Grokking现象的存在。研究发现，Grokking过程中LLM内部的训练样本通路从随机演变为结构化且可共享，通路复杂性降低，这表明了从“记忆”到“泛化”的转变。为监控这一过程，研究开发了两个新颖的度量指标，它们仅依赖训练数据，能有效预测泛化改进，从而无需微调和测试即可监控模型泛化性能，具有重要的实际和理论价值。",
      "translated_title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's \"emergence of generalization\" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound."
    },
    {
      "title": "SAM4D：在相机和激光雷达流中分割一切 (原标题: SAM4D: Segment Anything in Camera and LiDAR Streams)",
      "link": "https://arxiv.org/abs/2506.21547",
      "pubDate": "Thu, 26 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-26T13:59:14.000Z",
      "creator": "Jianyun Xu, Song Wang, Ziqian Ni, Chunyong Hu, Sheng Yang, Jianke Zhu, Qiang Li",
      "summary": "### SAM4D：多模态与时序基础模型\n\nSAM4D是一个为相机和激光雷达流设计的可提示分割的多模态和时序基础模型。它旨在解决自动驾驶场景中动态变化环境下的鲁棒分割问题，并克服传统标注瓶颈。\n\n**核心创新与技术：**\n\n*   **统一多模态位置编码 (UMPE)**\n    *   功能：将相机和激光雷达特征对齐到共享的3D空间中。\n    *   目的：实现无缝的跨模态提示和交互。\n*   **运动感知跨模态记忆注意力 (MCMA)**\n    *   方法：利用自我运动补偿。\n    *   目的：增强时间一致性，实现长距离特征检索，确保在动态变化的自动驾驶场景中进行鲁棒分割。\n*   **多模态自动化数据引擎**\n    *   目的：避免标注瓶颈。\n    *   工作原理：\n        *   协同VFM（Video Foundation Model）驱动的视频masklets。\n        *   结合时空4D重建。\n        *   进行跨模态masklet融合。\n    *   成果：生成相机-激光雷达对齐的伪标签，其速度比人工标注快几个数量级，同时保留了VFM在点云表示中导出的语义保真度。\n\n**实验与成果：**\n\n*   在构建的Waymo-4DSeg数据集上进行了广泛实验。\n*   实验结果证明了SAM4D强大的跨模态分割能力。\n*   展示了其在数据标注方面的巨大潜力。\n\n**其他信息：**\n\n*   该研究已被ICCV2025接受。",
      "shortSummary": "SAM4D是一个多模态、时序基础模型，专为相机和激光雷达流中的可提示分割设计。它引入了统一多模态位置编码（UMPE）以对齐跨模态特征，并采用运动感知跨模态记忆注意力（MCMA）增强时间一致性。为解决标注瓶颈，SAM4D开发了一个自动化数据引擎，能高效生成高质量伪标签。在Waymo-4DSeg上的实验验证了其强大的跨模态分割能力和数据标注潜力。",
      "translated_title": "SAM4D：在相机和激光雷达流中分割一切",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D."
    },
    {
      "title": "WorldVLA：迈向自回归动作世界模型 (原标题: WorldVLA: Towards Autoregressive Action World Model)",
      "link": "https://arxiv.org/abs/2506.21539",
      "pubDate": "Thu, 26 Jun 2025 13:55:40 GMT",
      "isoDate": "2025-06-26T13:55:40.000Z",
      "creator": "Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, Hao Chen",
      "summary": "## WorldVLA：统一动作与图像理解与生成的自回归世界模型\n\nWorldVLA是一种创新的自回归动作世界模型，旨在将动作与图像的理解和生成功能整合到一个单一框架中。该模型通过融合视觉-语言-动作（VLA）模型和世界模型，实现了对复杂环境的全面感知和交互能力。\n\n### 模型核心机制\n\n*   **世界模型（World Model）**：利用对动作和图像的理解来预测未来的图像。其主要目的是学习环境的底层物理规律，从而提升动作生成的质量和准确性。\n*   **动作模型（Action Model）**：根据图像观察生成后续动作。这不仅有助于视觉理解，反过来也支持世界模型的视觉生成能力。\n\n### 关键发现与优势\n\n*   **相互增强**：研究表明，WorldVLA的整体性能优于独立的动作模型和世界模型。这突出显示了世界模型与动作模型之间存在的相互促进和增强关系。\n*   **自回归动作生成挑战**：在自回归地生成一系列动作时，动作模型的性能会出现下降。这一现象归因于模型在动作预测方面的泛化能力有限，导致早期动作的错误会传播并累积到后续动作中。\n\n### 解决方案\n\n*   **注意力掩码策略（Attention Mask Strategy）**：为了解决自回归动作生成中的性能下降问题，研究者提出了一种注意力掩码策略。该策略在生成当前动作时，选择性地掩盖先前的动作信息。实验证明，这一策略显著提升了动作块生成任务的性能。",
      "shortSummary": "WorldVLA是一个统一动作与图像理解和生成的自回归世界模型。它将VLA模型和世界模型整合，通过世界模型预测未来图像以学习物理规律，并由动作模型生成动作以辅助视觉理解和生成。研究发现WorldVLA优于独立模型，展现了模型间的相互增强。针对自回归动作生成中性能下降的问题，WorldVLA提出了一种注意力掩码策略，通过选择性掩盖先前动作来显著提升动作块生成任务的性能。",
      "translated_title": "WorldVLA：迈向自回归动作世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task."
    },
    {
      "title": "MADrive：记忆增强的驾驶场景建模 (原标题: MADrive: Memory-Augmented Driving Scene Modeling)",
      "link": "https://arxiv.org/abs/2506.21520",
      "pubDate": "Thu, 26 Jun 2025 13:41:07 GMT",
      "isoDate": "2025-06-26T13:41:07.000Z",
      "creator": "Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, Dmitry Baranchuk",
      "summary": "## MADrive：记忆增强的驾驶场景建模\n\n### 引言\n\n近期，3D高斯泼溅技术在自动驾驶（AD）环境的场景重建方面取得了显著进展，实现了高度逼真的建模。然而，这些重建结果与原始观测数据紧密相关，难以支持对显著改变或全新驾驶场景进行真实感合成。\n\n### MADrive框架概述\n\n本文介绍了MADrive，一个记忆增强的重建框架。该框架旨在扩展现有场景重建方法的能力，其核心思想是通过从一个大型外部记忆库中检索视觉上相似的3D资产，来替换场景中观察到的车辆。\n\n### 关键组成部分\n\n1.  **MAD-Cars数据集**\n    *   MAD-Cars是一个精心策划的数据集，包含约7万个在野外捕获的360°汽车视频。\n2.  **检索模块**\n    *   该模块负责在记忆库中查找与目标车辆最相似的汽车实例。\n    *   接着，它从相应的视频中重建这些汽车的3D资产。\n    *   最后，通过方向对齐和重新打光（relighting）技术，将重建的3D资产无缝整合到目标场景中。\n\n### 实验结果与优势\n\nMADrive所替换的车辆提供了完整的、多视角的车辆表示。这使得该框架能够实现对大幅改变的配置进行真实感合成，其有效性已在实验中得到验证。\n\n### 相关信息\n\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2506.21520 [cs.CV]",
      "shortSummary": "MADrive是一个记忆增强的驾驶场景建模框架，旨在解决现有3D重建方法难以合成新颖或大幅改变场景的问题。它通过从大型MAD-Cars记忆库中检索并整合视觉相似的3D车辆资产，替换原始场景中的车辆。该方法能实现车辆的完整多视图表示和真实感合成，扩展了自动驾驶环境建模的能力。",
      "translated_title": "MADrive：记忆增强的驾驶场景建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/"
    },
    {
      "title": "Mind2Web 2：使用代理作为评判者评估代理式搜索 (原标题: Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge)",
      "link": "https://arxiv.org/abs/2506.21506",
      "pubDate": "Thu, 26 Jun 2025 13:32:50 GMT",
      "isoDate": "2025-06-26T13:32:50.000Z",
      "creator": "Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su",
      "summary": "## Mind2Web 2：代理式搜索评估的新基准与方法\n\n### 背景与挑战\n\n代理式搜索系统，例如深度研究系统，利用大型语言模型（LLMs）自主浏览网页、综合信息并提供带有引用的全面答案，这代表了用户与海量网络信息交互方式的重大转变。尽管代理式搜索有望提高效率并减轻认知负担，但其日益增长的复杂性和开放性已超越了现有评估基准和方法的能力。现有方法大多假设搜索范围短且答案静态，无法有效评估代理式搜索的动态和复杂性。\n\n### Mind2Web 2 基准\n\n为了应对这一挑战，本文引入了 **Mind2Web 2**，这是一个全新的评估基准，其特点如下：\n\n*   **任务数量与质量**：包含130个真实、高质量、长周期的任务。\n*   **任务要求**：这些任务需要实时网页浏览和广泛的信息综合。\n*   **构建投入**：耗费超过1,000小时的人工劳动构建而成。\n\n### 代理作为评判者（Agent-as-a-Judge）框架\n\n为了解决评估时变和复杂答案的难题，研究团队提出了一种新颖的 **代理作为评判者（Agent-as-a-Judge）框架**。该方法通过以下方式实现自动评估：\n\n*   **评判代理构建**：基于树状结构的评分标准设计，构建特定任务的评判代理。\n*   **评估内容**：自动评估答案的正确性以及来源归属。\n\n### 综合评估与发现\n\n研究团队对九个前沿的代理式搜索系统以及人类表现进行了全面评估，并进行了详细的错误分析，以期为未来的发展提供见解。评估结果显示：\n\n*   **最佳表现系统**：OpenAI Deep Research 系统。\n*   **性能对比**：该系统已能达到人类表现的50-70%。\n*   **效率优势**：同时，其耗时仅为人类的一半。\n*   **发展潜力**：这表明代理式搜索系统具有巨大的发展潜力。\n\n### 结论\n\n总而言之，Mind2Web 2 为下一代代理式搜索系统的开发和基准测试提供了坚实而严谨的基础。",
      "shortSummary": "Mind2Web 2引入了一个新的基准和“代理作为评判者”框架，以评估复杂的代理式搜索系统。该基准包含130个需实时浏览和信息综合的长周期任务。其“代理作为评判者”框架能自动评估答案正确性和来源归属。评估显示，OpenAI Deep Research等领先系统已能以一半时间达到人类表现的50-70%，展现了代理式搜索的巨大潜力。Mind2Web 2为未来代理式搜索系统的发展和基准测试奠定了基础。",
      "translated_title": "Mind2Web 2：使用代理作为评判者评估代理式搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems."
    },
    {
      "title": "FairyGen：从单一儿童手绘角色生成故事驱动的卡通视频 (原标题: FairyGen: Storied Cartoon Video from a Single Child-Drawn Character)",
      "link": "https://arxiv.org/abs/2506.21272",
      "pubDate": "Thu, 26 Jun 2025 09:58:16 GMT",
      "isoDate": "2025-06-26T09:58:16.000Z",
      "creator": "Jiayi Zheng, Xiaodong Cun",
      "summary": "## FairyGen：从儿童手绘角色生成故事驱动的卡通视频\n\nFairyGen是一个创新的自动化系统，旨在从单一的儿童手绘角色草图生成故事驱动的卡通视频，同时忠实地保留其独特的艺术风格。该系统超越了以往仅关注角色一致性和基本动作的叙事方法，通过解耦角色建模与风格化背景生成，并融入电影化的镜头设计，以支持更具表现力和连贯性的故事讲述。\n\n### 核心创新与特点\n\n*   **风格忠实性**：精确捕捉并保留儿童手绘角色的独特视觉风格，并将其应用于整个视频场景。\n*   **故事驱动**：生成具有叙事结构和自然动作的动画，支持个性化和引人入胜的故事动画。\n*   **电影化设计**：通过镜头设计模块增强视觉多样性和电影质量。\n\n### 系统工作流程\n\n1.  **故事板生成**：\n    *   接收单一角色草图作为输入。\n    *   利用多模态大语言模型（MLLM）生成结构化的故事板，其中包含镜头级别的描述，详细说明环境设置、角色动作和摄像机视角。\n\n2.  **视觉一致性与风格传播**：\n    *   引入一个“风格传播适配器”，用于捕捉角色的视觉风格。\n    *   将捕获的风格应用于背景生成，确保在合成风格一致的场景时，忠实地保留角色的完整视觉特征。\n\n3.  **电影化镜头设计**：\n    *   “镜头设计模块”根据故事板，通过帧裁剪和多视角合成，进一步提升视频的视觉多样性和电影质量。\n\n4.  **动画与动作生成**：\n    *   **3D代理重建**：重建角色的3D代理，以推导出符合物理规律的动作序列。\n    *   **模型微调**：利用这些动作序列对基于MMDiT的图像到视频扩散模型进行微调。\n    *   **两阶段动作定制适配器**：\n        *   **第一阶段**：从时间上无序的帧中学习外观特征，从而将身份（identity）与动作（motion）解耦。\n        *   **第二阶段**：在冻结身份权重的情况下，使用时间步移位策略（timestep-shift strategy）对时间动态进行建模。\n\n### 成果与潜力\n\n一旦训练完成，FairyGen能够直接渲染出与故事板对齐的、多样且连贯的视频场景。广泛的实验证明，该系统生成的动画在风格上忠实于原始手绘，叙事上结构清晰，动作自然流畅，突显了其在个性化和沉浸式故事动画领域的巨大潜力。\n\n### 可用性\n\n项目代码和页面将公开提供。",
      "shortSummary": "FairyGen是一个创新系统，能将单一儿童手绘角色转化为故事驱动的卡通视频，并忠实保留其艺术风格。它通过MLLM生成故事板，利用风格传播适配器确保视觉一致性，并通过电影化镜头设计增强视觉效果。系统通过重建角色3D代理和两阶段动作定制适配器生成自然动作，最终渲染出与故事板对齐的连贯视频，展现了在个性化故事动画方面的巨大潜力。",
      "translated_title": "FairyGen：从单一儿童手绘角色生成故事驱动的卡通视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose FairyGen, an automatic system for generating story-driven cartoon videos from a single child's drawing, while faithfully preserving its unique artistic style. Unlike previous storytelling methods that primarily focus on character consistency and basic motion, FairyGen explicitly disentangles character modeling from stylized background generation and incorporates cinematic shot design to support expressive and coherent storytelling. Given a single character sketch, we first employ an MLLM to generate a structured storyboard with shot-level descriptions that specify environment settings, character actions, and camera perspectives. To ensure visual consistency, we introduce a style propagation adapter that captures the character's visual style and applies it to the background, faithfully retaining the character's full visual identity while synthesizing style-consistent scenes. A shot design module further enhances visual diversity and cinematic quality through frame cropping and multi-view synthesis based on the storyboard. To animate the story, we reconstruct a 3D proxy of the character to derive physically plausible motion sequences, which are then used to fine-tune an MMDiT-based image-to-video diffusion model. We further propose a two-stage motion customization adapter: the first stage learns appearance features from temporally unordered frames, disentangling identity from motion; the second stage models temporal dynamics using a timestep-shift strategy with frozen identity weights. Once trained, FairyGen directly renders diverse and coherent video scenes aligned with the storyboard. Extensive experiments demonstrate that our system produces animations that are stylistically faithful, narratively structured natural motion, highlighting its potential for personalized and engaging story animation. The code will be available at https://github.com/GVCLab/FairyGen"
    },
    {
      "title": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架 (原标题: DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster)",
      "link": "https://arxiv.org/abs/2506.21263",
      "pubDate": "Thu, 26 Jun 2025 09:45:04 GMT",
      "isoDate": "2025-06-26T09:45:04.000Z",
      "creator": "Ji Qi, WenPeng Zhu, Li Li, Ming Wu, YingJun Wu, Wu He, Xun Gao, Jason Zeng, Michael Heinrich",
      "summary": "### DiLoCoX：一种用于去中心化集群的低通信大规模训练框架\n\n**1. 问题背景与挑战**\n\n*   **高通信需求：** 基础模型（特别是大型语言模型LLM）的分布式训练需要极高的通信带宽。\n*   **中心化依赖：** 这使得训练高度依赖于具有快速可靠互连的中心化集群。\n*   **核心挑战：** 如何在慢速网络上进行大规模模型（参数量超过1000亿）的训练，从而释放去中心化集群的潜力？\n\n**2. 提出的解决方案：DiLoCoX**\n\n*   DiLoCoX是一种创新的低通信大规模去中心化集群训练框架。\n*   其核心目标是解决在慢速网络环境下，对超大规模模型进行高效预训练的难题。\n\n**3. DiLoCoX的关键技术组成**\n\nDiLoCoX通过结合以下多种策略，显著提升了参数规模和模型预训练速度：\n\n*   **流水线并行（Pipeline Parallelism）：** 有效地将模型划分到不同的设备上，实现并行计算。\n*   **双优化器策略（Dual Optimizer Policy）：** 优化器策略的创新应用，以提高训练效率。\n*   **通信与本地训练的一步延迟重叠（One-Step-Delay Overlap of Communication and Local Training）：** 精心设计的通信与计算重叠机制，最大限度地减少空闲时间。\n*   **自适应梯度压缩方案（Adaptive Gradient Compression Scheme）：** 根据网络条件和模型状态动态调整梯度压缩，以减少通信量。\n\n**4. 理论分析与实验验证**\n\n*   **理论分析：** 通过对收敛性的理论分析，证明了“通信与本地训练的一步延迟重叠”以及“自适应梯度压缩方案”的有效性和益处。\n*   **经验验证：**\n    *   DiLoCoX成功通过1Gbps网络预训练了一个1070亿参数的基础模型。\n    *   与传统的AllReduce方法相比，DiLoCoX在分布式训练中实现了惊人的357倍速度提升。\n    *   在取得显著速度提升的同时，模型收敛性退化可忽略不计，保持了模型性能。\n\n**5. 创新与意义**\n\n*   据作者所知，DiLoCoX是首个成功应用于参数量超过1000亿模型的去中心化训练框架。\n*   这为在资源受限或网络条件不佳的环境下，进行超大规模模型训练开辟了新的可能性。",
      "shortSummary": "DiLoCoX是一种创新的低通信大规模训练框架，旨在解决在慢速网络上训练超千亿参数模型的挑战。它结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩。实验证明，DiLoCoX能在1Gbps网络上预训练1070亿参数模型，相较于AllReduce提速357倍，且收敛性退化可忽略。这是首个应用于超千亿参数模型的去中心化训练框架。",
      "translated_title": "DiLoCoX：一种用于去中心化集群的低通信大规模训练框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The distributed training of foundation models, particularly large language models (LLMs), demands a high level of communication. Consequently, it is highly dependent on a centralized cluster with fast and reliable interconnects. Can we conduct training on slow networks and thereby unleash the power of decentralized clusters when dealing with models exceeding 100 billion parameters? In this paper, we propose DiLoCoX, a low-communication large-scale decentralized cluster training framework. It combines Pipeline Parallelism with Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local Training, and an Adaptive Gradient Compression Scheme. This combination significantly improves the scale of parameters and the speed of model pre-training. We justify the benefits of one-step-delay overlap of communication and local training, as well as the adaptive gradient compression scheme, through a theoretical analysis of convergence. Empirically, we demonstrate that DiLoCoX is capable of pre-training a 107B foundation model over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x speedup in distributed training while maintaining negligible degradation in model convergence. To the best of our knowledge, this is the first decentralized training framework successfully applied to models with over 100 billion parameters."
    },
    {
      "title": "学习跳过Transformer的中间层 (原标题: Learning to Skip the Middle Layers of Transformers)",
      "link": "https://arxiv.org/abs/2506.21103",
      "pubDate": "Thu, 26 Jun 2025 05:01:19 GMT",
      "isoDate": "2025-06-26T05:01:19.000Z",
      "creator": "Tim Lawson, Laurence Aitchison",
      "summary": "## 学习跳过Transformer的中间层：一种条件计算方法\n\n本文提出了一种新的Transformer架构，旨在通过动态跳过其中间层来提高计算效率。该方法基于对Transformer内部工作原理的洞察：\n\n### 背景与动机\n\n*   **条件计算**：是提高Transformer效率的常用策略。\n*   **现有方法**：通常针对单个模块（如专家混合层）或独立跳过层。\n*   **研究洞察**：可解释性研究表明，Transformer的**中间层表现出更大的冗余性**，而早期层则负责将信息聚合到token位置。\n\n### 提出的架构\n\n受上述洞察启发，作者提出了一种**从中间向外动态跳过可变数量层**的新型架构：\n\n*   **学习门控机制**：根据输入，该机制决定是否绕过对称的中心块范围。\n*   **门控注意力机制**：防止后续的token关注被跳过的token位置。\n*   **残差范数控制**：通过“三明治”（'sandwich'）或“逐层范数”（'perilayernorm'）方案来控制残差范数。\n*   **门控稀疏性**：通过自适应正则化损失来控制门控的稀疏性。\n\n### 目标与结果\n\n*   **预期目标**：旨在减少“更简单”token的计算需求，并可能促进出现多级表示层次结构。\n*   **实验结果**：在所研究的规模下，与层数较少的密集基线相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面**未能实现改进**。\n\n### 代码发布\n\n*   作者已在 [this https URL](https://this.https.url/) 发布了相关代码。",
      "shortSummary": "本文提出一种新的Transformer架构，通过学习门控机制动态跳过其中间层，以提高计算效率。该方法基于中间层冗余的洞察，并结合门控注意力机制。尽管旨在减少计算量，但在实验规模下，该方法在计算效率与性能权衡方面未能超越传统的密集基线。代码已开源。",
      "translated_title": "学习跳过Transformer的中间层",
      "images": [],
      "contentSource": "完整文章",
      "content": "Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle."
    },
    {
      "title": "PhysRig：用于真实可变形物体建模的可微分物理骨骼蒙皮与绑定框架 (原标题: PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling)",
      "link": "https://arxiv.org/abs/2506.20936",
      "pubDate": "Wed, 25 Jun 2025 21:58:09 GMT",
      "isoDate": "2025-06-25T21:58:09.000Z",
      "creator": "Hao Zhang, Haolan Xu, Chun Feng, Varun Jampani, Narendra Ahuja",
      "summary": "## PhysRig：可微分物理骨骼蒙皮与绑定框架\n\n### 引言与背景\n\n骨骼蒙皮（Skinning）和绑定（Rigging）是动画、可变形物体重建、动作迁移和4D生成等领域的关键组成部分。当前主流方法，如线性混合蒙皮（LBS），因其简单性和可微分性而被广泛采用。然而，LBS存在显著缺陷，包括：\n\n*   **体积损失（Volume Loss）**：在关节弯曲时，模型体积会不自然地缩小。\n*   **不自然变形（Unnatural Deformations）**：无法准确模拟复杂或弹性材料的形变。\n*   **无法模拟弹性材料**：对于软组织、毛发、象鼻和耳朵等柔性或脂肪组织，LBS难以实现真实的物理效果。\n\n### PhysRig框架概述\n\n为了克服LBS的局限性，本文提出了 **PhysRig**：一个可微分的、基于物理的骨骼蒙皮与绑定框架。PhysRig的核心思想是将刚性骨架嵌入到一个体积表示（例如四面体网格）中，并将其模拟为一个由动画骨架驱动的可变形软体结构。\n\n### 核心技术与方法\n\nPhysRig通过以下关键技术实现其功能：\n\n*   **连续介质力学**：该方法利用连续介质力学原理来模拟物体的变形行为。\n*   **欧拉背景网格离散化**：物体被离散化为嵌入在欧拉背景网格中的粒子。这种离散化方法确保了框架对于材料属性和骨骼运动都具有可微分性，这对于优化和学习至关重要。\n*   **材料原型（Material Prototypes）**：引入材料原型概念，显著减少了学习空间，同时保持了高表达能力，使得模型能够更高效地学习和表示不同材料的物理特性。\n\n### 性能评估与应用\n\n为了全面评估PhysRig框架的有效性，研究团队构建了一个综合的合成数据集。该数据集使用了来自Objaverse、The Amazing Animals Zoo和MixaMo的网格模型，涵盖了多样化的物体类别和运动模式。\n\n评估结果显示：\n\n*   **优于传统LBS**：PhysRig方法在生成更真实、更符合物理规律的结果方面，持续优于传统的基于LBS的方法。\n*   **物理真实性**：能够更好地模拟弹性材料和复杂形变，解决了LBS在体积保持和自然变形方面的不足。\n\n此外，研究还通过姿态迁移（Pose Transfer）任务展示了PhysRig框架的适用性，突显了其在可变形物体建模方面的多功能性。\n\n### 结论\n\nPhysRig为动画和可变形物体建模提供了一个创新的解决方案，通过结合物理模拟和可微分性，显著提升了动画的真实感和物理准确性。该研究已被ICCV 2025接受。",
      "shortSummary": "PhysRig是一个创新的可微分物理骨骼蒙皮与绑定框架，旨在克服传统线性混合蒙皮（LBS）在体积损失和弹性材料模拟方面的局限。它通过将骨架嵌入可变形的体积表示中，并利用连续介质力学和欧拉网格实现物理模拟和可微分性。PhysRig引入材料原型以提高效率和表达力。实验证明，该框架在生成更真实、物理更可信的动画效果方面优于LBS，并在姿态迁移任务中展现了其多功能性，已被ICCV 2025接受。",
      "translated_title": "PhysRig：用于真实可变形物体建模的可微分物理骨骼蒙皮与绑定框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Skinning and rigging are fundamental components in animation, articulated object reconstruction, motion transfer, and 4D generation. Existing approaches predominantly rely on Linear Blend Skinning (LBS), due to its simplicity and differentiability. However, LBS introduces artifacts such as volume loss and unnatural deformations, and it fails to model elastic materials like soft tissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty tissues). In this work, we propose PhysRig: a differentiable physics-based skinning and rigging framework that overcomes these limitations by embedding the rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh), which is simulated as a deformable soft-body structure driven by the animated skeleton. Our method leverages continuum mechanics and discretizes the object as particles embedded in an Eulerian background grid to ensure differentiability with respect to both material properties and skeletal motion. Additionally, we introduce material prototypes, significantly reducing the learning space while maintaining high expressiveness. To evaluate our framework, we construct a comprehensive synthetic dataset using meshes from Objaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object categories and motion patterns. Our method consistently outperforms traditional LBS-based approaches, generating more realistic and physically plausible results. Furthermore, we demonstrate the applicability of our framework in the pose transfer task highlighting its versatility for articulated object modeling."
    },
    {
      "title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑 (原标题: FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing)",
      "link": "https://arxiv.org/abs/2506.20911",
      "pubDate": "Wed, 25 Jun 2025 20:33:43 GMT",
      "isoDate": "2025-06-25T20:33:43.000Z",
      "creator": "Advait Gupta, Rishie Raj, Dang Nguyen, Tianyi Zhou",
      "summary": "FaSTA^* 是一种成本效益高的神经符号代理，旨在解决复杂的多轮图像编辑任务，例如“检测图像中的长凳并将其重新着色为粉红色。同时，为了更清晰的视图，移除猫并将墙壁重新着色为黄色。”\n\n*   **核心方法论**\n    *   FaSTA^* 结合了大型语言模型（LLMs）的快速、高层次子任务规划能力，以及每项子任务的慢速、准确、工具使用和局部 A* 搜索，以找到成本效益高的工具路径（即一系列对 AI 工具的调用）。\n*   **子程序挖掘与重用**\n    *   为了节省在类似子任务上进行 A* 搜索的成本，FaSTA^* 通过 LLMs 对先前成功的工具路径进行归纳推理，持续提取和优化频繁使用的子程序。\n    *   这些子程序被作为新工具重用于未来的任务中，实现了自适应的快速-慢速规划。\n    *   在规划过程中，首先探索高层次的子程序；只有当这些子程序失败时，才会激活低层次的 A* 搜索。\n*   **“快速-慢速”规划机制**\n    *   可重用的符号子程序显著节省了在应用于类似图像的相同类型子任务上的探索成本。\n    *   FaSTA^* 遵循一种“快速-慢速”工具路径代理模式：\n        *   首先，LLMs 尝试进行快速的子任务规划，并为每个子任务选择基于规则的子程序，预计能覆盖大多数任务。\n        *   只有对于新颖和具有挑战性的子任务，才会触发慢速的 A* 搜索。\n*   **性能优势**\n    *   与近期图像编辑方法相比，FaSTA^* 在计算效率上显著更高，同时在成功率方面与最先进的基线保持竞争力。",
      "shortSummary": "FaSTA^* 是一种高效的神经符号代理，用于处理复杂的多轮图像编辑任务。它结合了LLM的快速高层规划和A*搜索的慢速精确工具使用。通过LLM挖掘并重用常用子程序，FaSTA^* 显著降低了探索成本，实现了自适应的“快速-慢速”规划。该方法在计算效率上远超现有技术，同时保持了与最先进方法相当的成功率。",
      "translated_title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as \"Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A^* search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A^* on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A^* search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A^* search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA^* is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate."
    },
    {
      "title": "生成式积木世界：在图片中移动物体 (原标题: Generative Blocks World: Moving Things Around in Pictures)",
      "link": "https://arxiv.org/abs/2506.20703",
      "pubDate": "Wed, 25 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-25T13:59:55.000Z",
      "creator": "Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, D. A. Forsyth, Anand Bhattad",
      "summary": "### 生成式积木世界：在图片中移动物体\n\n“生成式积木世界”（Generative Blocks World, GBW）是一种用于与生成图像场景进行交互的新方法。该方法允许用户通过操纵简单的几何抽象来编辑图像中的场景。\n\n**核心概念与方法：**\n\n*   **场景表示：** GBW 将场景表示为凸面3D基本几何体的组合。这种表示方式使得同一场景可以通过不同数量的基本几何体来表示，从而赋予编辑者极大的灵活性，既可以移动整个结构，也可以调整微小的细节。\n*   **图像生成：** 在场景几何体被编辑后，图像通过一种基于流（flow-based）的方法生成。这种生成过程以深度信息和一种创新的纹理提示为条件。\n*   **纹理提示创新：** GBW 的纹理提示是一个关键的创新点，它能够考虑修改后的3D基本几何体。这使得其纹理一致性显著优于现有基于键值缓存（key-value caching）的技术。\n*   **纹理提示的优势：**\n    *   实现精确的物体和相机移动。\n    *   在很大程度上保留了所描绘物体的身份，确保编辑后的图像自然且连贯。\n\n**实验结果：**\n\n通过定量和定性实验，研究表明 GBW 方法在视觉保真度、可编辑性和组合泛化能力方面均优于现有方法，展示了其在图像编辑领域的强大潜力。",
      "shortSummary": "“生成式积木世界”（GBW）是一种通过操纵简单3D几何抽象来编辑生成图像场景的新方法。它将场景表示为凸面3D基本几何体的组合，允许灵活地移动整体或细节。编辑后，图像通过基于流的方法生成，并利用创新的纹理提示来确保纹理一致性、精确的物体/相机移动和物体身份保留。实验证明，GBW 在视觉保真度、可编辑性和组合泛化方面优于现有技术。",
      "translated_title": "生成式积木世界：在图片中移动物体",
      "images": [],
      "contentSource": "完整文章",
      "content": "We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization."
    },
    {
      "title": "MMSearch-R1: 激励LMMs进行搜索 (原标题: MMSearch-R1: Incentivizing LMMs to Search)",
      "link": "https://arxiv.org/abs/2506.20670",
      "pubDate": "Wed, 25 Jun 2025 13:59:42 GMT",
      "isoDate": "2025-06-25T13:59:42.000Z",
      "creator": "Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu",
      "summary": "MMSearch-R1: 激励LMMs进行搜索\n\n**摘要**\n\n*   **背景与问题**\n    *   大型多模态模型（LMMs）在实际部署中需要访问外部知识源，因为现实世界信息复杂且动态。\n    *   现有方法（如检索增强生成RAG和提示工程搜索代理）依赖于僵化的流程，常导致搜索效率低下或过度搜索行为。\n\n*   **MMSearch-R1框架**\n    *   **创新点**：MMSearch-R1是首个端到端的强化学习（RL）框架，使LMMs能够在真实的互联网环境中执行按需、多轮搜索。\n    *   **核心机制**：\n        *   集成了图像和文本搜索工具。\n        *   模型能够根据结果导向的奖励（包含搜索惩罚）来判断何时以及如何调用这些工具。\n\n*   **数据支持**\n    *   为了支持训练，研究人员通过半自动化流程收集了一个多模态搜索VQA（视觉问答）数据集。\n    *   该数据集涵盖了多样化的视觉和文本知识需求，并策划了一个搜索平衡的子集，包含需要搜索和无需搜索的样本。\n    *   **重要性**：这个平衡的子集对于塑造高效和按需的搜索行为至关重要。\n\n*   **实验结果与发现**\n    *   在知识密集型和信息搜索VQA任务上进行了广泛实验。\n    *   **性能提升**：MMSearch-R1不仅优于相同模型大小的RAG基线模型，而且在性能上与更大的RAG基线模型相当。\n    *   **效率提升**：同时，MMSearch-R1将搜索调用次数减少了30%以上。\n    *   **洞察力**：进一步分析了关键的实证发现，为推进多模态搜索研究提供了可操作的见解。",
      "shortSummary": "MMSearch-R1是一个端到端强化学习框架，旨在解决大型多模态模型（LMMs）在搜索外部知识时效率低下和过度搜索的问题。该框架整合了图像和文本搜索工具，通过结果导向的奖励和搜索惩罚机制，使LMMs能够按需、多轮地进行搜索。通过构建多模态搜索VQA数据集进行训练，MMSearch-R1在实验中表现出色，不仅超越了同等规模的RAG基线模型，还与更大模型性能相当，同时将搜索调用次数减少了30%以上。",
      "translated_title": "MMSearch-R1: 激励LMMs进行搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search."
    },
    {
      "title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处 (原标题: When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs)",
      "link": "https://arxiv.org/abs/2506.20544",
      "pubDate": "Wed, 25 Jun 2025 11:37:53 GMT",
      "isoDate": "2025-06-25T11:37:53.000Z",
      "creator": "Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker",
      "summary": "### 扩展多语言大型语言模型推理计算的益处\n\n**研究背景与问题**\n\n*   近期大型语言模型（LLM）的进展已将重点转向扩展推理时计算，旨在不重新训练模型的情况下提升性能。\n*   一种常见方法是并行采样多个输出，并从中选择一个作为最终输出。\n*   然而，现有工作主要集中于英语以及数学和代码等少数领域，未能推广到开放式任务、可形式验证任务和跨语言场景。\n*   本研究旨在解决如何在多语言、多任务环境下，为开放式生成任务稳健地扩展推理时计算的问题。\n\n**主要发现与挑战**\n\n*   研究发现，基于温度变化的采样策略和选择策略都必须进行调整，以适应多样化的领域和不同的语言设置。\n*   对现有选择方法的评估表明，在英语中有效的策略往往无法推广到其他语言。\n\n**提出的新策略与成果**\n\n*   研究提出了专门针对多语言和多任务推理场景的新型采样和选择策略。\n*   这些策略在不同语言和任务中均取得了显著的性能提升。\n*   **具体成果：**\n    *   对于8B模型，结合本研究的采样和选择方法，在m-ArenaHard-v2.0提示上，与Gemini等专有模型相比，胜率平均提升了+6.8。\n    *   在更大规模的模型上，配备本研究方法的Command-A（111B模型）在相同基准测试中，仅使用五个样本，相较于单样本解码，胜率提升了+9.0，以最小的成本实现了显著的性能提升。\n\n**研究意义**\n\n*   研究结果强调了在推理时计算中采用语言和任务感知方法的必要性。\n*   这有助于在代表性不足的语言中普及性能改进，促进LLM的民主化应用。",
      "shortSummary": "这项研究旨在通过扩展推理计算来提升多语言大型语言模型（LLM）的性能，而无需重新训练。针对现有方法在跨语言和多任务场景中的局限性，研究提出了新颖的采样和选择策略。实验证明，这些优化策略显著提升了模型在多语言基准测试上的表现，例如使8B模型胜率平均提升6.8%，111B模型提升9.0%。研究强调了开发语言和任务感知型推理方法的重要性，以促进LLM在更多语言中的广泛应用。",
      "translated_title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."
    },
    {
      "title": "OctoThinker: 中期训练激励强化学习扩展 (原标题: OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling)",
      "link": "https://arxiv.org/abs/2506.20512",
      "pubDate": "Wed, 25 Jun 2025 10:58:13 GMT",
      "isoDate": "2025-06-25T10:58:13.000Z",
      "creator": "Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu",
      "summary": "### OctoThinker: 中期训练激励强化学习扩展\n\n本研究深入探讨了不同基础语言模型家族（如Llama和Qwen）在通过强化学习（RL）进行后训练时表现出的差异行为，尤其是在推理密集型任务上。理解是什么让一个基础语言模型适合强化学习，对于开发下一代可扩展的RL基础模型至关重要。\n\n**研究焦点与发现：**\n\n本研究主要调查了中期训练策略如何塑造RL动态，重点关注Qwen和Llama这两个代表性模型家族。研究揭示了以下关键洞察：\n\n1.  **高质量数学语料库的重要性：** 高质量的数学语料库，例如MegaMath-Web-Pro，能够显著提升基础模型和RL的性能。相比之下，现有的一些替代方案（如FineMath-4plus）未能达到同样的效果。\n2.  **问答式数据和长思维链（CoT）推理：** 进一步添加问答式数据，特别是长思维链（CoT）推理示例，能够增强RL结果。指令数据（instruction data）则能进一步释放这种效果。\n3.  **长CoT的权衡：** 尽管长CoT能够提高推理深度，但它也可能导致模型响应的冗长以及RL训练的不稳定性，这凸显了数据格式化的重要性。\n4.  **中期训练的扩展效应：** 扩展中期训练（scaling mid-training）能够持续带来更强的下游RL性能。\n\n**OctoThinker模型与“稳定-衰减”策略：**\n\n基于这些洞察，研究团队提出了一种名为“稳定-衰减”（Stable-then-Decay）的两阶段中期训练策略：\n\n*   **第一阶段：** 基础模型首先使用恒定学习率训练2000亿个token。\n*   **第二阶段：** 接着在三个以CoT为重点的分支上训练200亿个token，并采用学习率衰减。\n\n通过这种策略，研究团队开发了OctoThinker模型家族。OctoThinker模型展现出强大的RL兼容性，并成功缩小了与更RL友好模型家族（如Qwen）之间的性能差距。\n\n**研究意义与资源发布：**\n\n本研究旨在帮助塑造RL时代基础模型的预训练策略。为了支持进一步的研究，研究团队发布了开源模型，以及一个精心策划的、包含超过700亿个token的数学推理密集型语料库（即MegaMath-Web-Pro-Max）。",
      "shortSummary": "本研究探讨了中期训练策略对语言模型强化学习（RL）性能的影响。发现高质量数学语料库和长思维链（CoT）推理数据能显著提升RL效果，但需注意数据格式。研究提出“稳定-衰减”两阶段中期训练策略，成功开发了OctoThinker模型家族。OctoThinker展现出强大的RL兼容性，缩小了与RL友好模型的性能差距，旨在为RL时代的基础模型预训练提供指导。研究同时发布了开源模型和大型数学推理语料库。",
      "translated_title": "OctoThinker: 中期训练激励强化学习扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max)."
    },
    {
      "title": "ReCode：使用强化学习更新代码API知识 (原标题: ReCode: Updating Code API Knowledge with Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.20495",
      "pubDate": "Wed, 25 Jun 2025 10:41:13 GMT",
      "isoDate": "2025-06-25T10:41:13.000Z",
      "creator": "Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "## ReCode：使用强化学习更新代码API知识\n\n### 引言\n大型语言模型（LLMs）在代码生成方面展现出卓越的能力，但当外部库API频繁更新时，它们难以适应这些变化。这种局限性源于LLMs依赖其训练数据中过时的API知识，即使能够访问最新的文档，也无法在动态环境中可靠地生成代码。\n\n### ReCode框架\n为了解决这一关键问题，我们提出了ReCode（rule-based Reinforcement learning for Code Update），一个新颖的框架，旨在模仿人类程序员适应API变化的方式。\n\n### 核心机制\n*   **数据集构建：** 我们构建了一个包含大约2,000个数据条目的数据集，用于训练LLMs根据更新的信息执行版本迁移。\n*   **强化学习奖励：** 我们引入了一种修改后的字符串相似度度量作为强化学习的奖励，用于代码评估。\n\n### 实验结果与优势\n*   **性能显著提升：** 实验表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未曾见过的CodeUpdateArena任务上。\n*   **对通用能力影响小：** 与监督微调相比，ReCode对LLMs的通用代码生成能力影响较小。\n*   **广泛适用性：** 我们将ReCode应用于各种LLMs和强化学习算法（GRPO和DAPO），所有这些都实现了一致的改进。\n*   **卓越表现：** 值得注意的是，经过训练后，Qwen2.5-Coder-7B 的性能超越了相同架构的32B参数代码指令微调模型和推理模型。\n\n### 其他信息\n*   **项目状态：** 该工作目前正在进行中。\n*   **代码可用性：** 相关代码已提供。\n*   **研究领域：** 本研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、信息检索（cs.IR）、机器学习（cs.LG）和软件工程（cs.SE）等领域。",
      "shortSummary": "ReCode是一个基于强化学习的框架，旨在解决大型语言模型（LLMs）因API频繁更新而导致的过时代码生成问题。它通过构建数据集训练LLMs进行版本迁移，并使用修改后的字符串相似度作为奖励。实验证明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，且对通用能力影响较小。训练后的7B参数模型甚至超越了32B参数模型，显示出其有效性和潜力。",
      "translated_title": "ReCode：使用强化学习更新代码API知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."
    },
    {
      "title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成 (原标题: HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling)",
      "link": "https://arxiv.org/abs/2506.20452",
      "pubDate": "Wed, 25 Jun 2025 09:58:37 GMT",
      "isoDate": "2025-06-25T09:58:37.000Z",
      "creator": "Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber",
      "summary": "## HiWave：基于小波的扩散采样实现免训练高分辨率图像生成\n\n### 引言\n扩散模型已成为图像合成领域的领先方法，展现出卓越的真实感和多样性。然而，在高分辨率下训练扩散模型计算成本极高，且现有零样本生成技术在超越训练分辨率合成图像时常产生伪影，例如物体重复和空间不连贯。\n\n### HiWave 方法概述\n本文引入了 HiWave，这是一种免训练、零样本的方法，旨在利用预训练的扩散模型，显著增强超高分辨率图像合成的视觉保真度和结构连贯性。\n\n### 核心流程\nHiWave 采用一个两阶段的管道：\n\n1.  **基础图像生成与反演：**\n    *   首先，利用预训练模型生成一个基础图像。\n    *   接着，通过一个分块（patch-wise）的 DDIM 反演步骤，从该基础图像中导出初始噪声向量，以确保全局的连贯性。\n\n2.  **小波域细节增强：**\n    *   在采样过程中，HiWave 引入了一个新颖的小波域细节增强模块。\n    *   该模块的关键在于：\n        *   保留基础图像中的低频分量，以确保图像的结构一致性。\n        *   选择性地引导高频分量，以丰富图像的精细细节和纹理。\n\n### 优势与评估\n*   **免训练与零样本：** HiWave 的核心优势在于其无需额外的训练或对现有模型架构进行修改，即可实现高质量的超高分辨率图像生成。\n*   **伪影缓解：** 广泛的评估（使用 Stable Diffusion XL）表明，HiWave 能够有效缓解先前方法中常见的视觉伪影，如物体重复和空间不连贯。\n*   **卓越感知质量：** 该方法在图像合成中实现了卓越的感知质量。\n*   **用户研究验证：** 一项用户研究进一步证实了 HiWave 的性能。在超过 80% 的比较中，用户更倾向于 HiWave 而非最先进的替代方法，这充分凸显了其在高质量、超高分辨率图像合成方面的有效性。",
      "shortSummary": "HiWave 是一种免训练、零样本的方法，用于利用预训练扩散模型生成超高分辨率图像。它通过两阶段管道工作：首先生成基础图像并进行 DDIM 反演，然后通过新颖的小波域细节增强模块，保留低频结构并丰富高频细节。该方法有效缓解了伪影，实现了卓越的感知质量，并在用户研究中表现出色，无需重新训练或修改模型架构。",
      "translated_title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications."
    },
    {
      "title": "一个具有可追溯推理能力的罕见病诊断智能体系统 (原标题: An Agentic System for Rare Disease Diagnosis with Traceable Reasoning)",
      "link": "https://arxiv.org/abs/2506.20430",
      "pubDate": "Wed, 25 Jun 2025 09:42:26 GMT",
      "isoDate": "2025-06-25T09:42:26.000Z",
      "creator": "Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie",
      "summary": "# DeepRare：一个具有可追溯推理能力的罕见病诊断智能体系统\n\n## 引言\n\n罕见病在全球范围内影响超过3亿人，但及时准确的诊断仍然是一个普遍存在的挑战。这主要是由于其临床异质性、个体患病率低以及大多数临床医生对罕见病缺乏了解。\n\n## DeepRare 系统概述\n\n本文介绍了 **DeepRare**，这是首个由大型语言模型（LLM）驱动的罕见病诊断智能体系统，能够处理异构的临床输入。该系统能够：\n\n*   生成按优先级排序的罕见病诊断假设。\n*   为每个假设提供透明的推理链，将中间分析步骤与可验证的医学证据关联起来。\n\n## DeepRare 核心组件\n\nDeepRare 系统由三个关键组件构成：\n\n1.  **中央宿主 (Central Host)**：包含一个长期记忆模块，用于存储和管理信息。\n2.  **专业智能体服务器 (Specialized Agent Servers)**：\n    *   负责执行领域特定的分析任务。\n    *   集成了超过40种专业工具和网络规模的最新医学知识来源，确保获取最当前的临床信息。\n\n这种模块化和可扩展的设计使得系统能够进行复杂的诊断推理，同时保持可追溯性和适应性。\n\n## 性能评估\n\nDeepRare 在八个数据集上进行了评估，结果显示出卓越的诊断性能：\n\n*   **诊断范围**：在2,919种疾病中进行诊断。\n*   **高准确率**：对1,013种疾病实现了100%的准确率。\n*   **HPO-based 评估**：\n    *   DeepRare 显著优于其他15种方法，包括传统的生物信息学诊断工具、LLM和其他智能体系统。\n    *   平均 Recall@1 得分为 57.18%。\n    *   比第二名（Reasoning LLM）高出23.79个百分点。\n*   **多模态输入场景**：\n    *   在109个病例中，DeepRare 的 Recall@1 达到 70.60%，而 Exomiser 为 53.20%。\n*   **推理链人工验证**：临床专家对推理链的人工验证达到了95.40%的一致性。\n\n## 系统可用性\n\nDeepRare 系统已实现为一个用户友好的网络应用程序。",
      "shortSummary": "DeepRare是一个由大型语言模型驱动的罕见病诊断智能体系统，旨在解决罕见病诊断难题。它能处理异构临床输入，生成带可追溯推理链的诊断假设，并整合了40多种专业工具和最新医学知识。在评估中，DeepRare在2,919种疾病中表现出色，对1,013种疾病达到100%准确率，并在HPO和多模态评估中显著优于现有方法。临床专家对推理链的验证一致性高达95.40%。该系统已作为网络应用推出。",
      "translated_title": "一个具有可追溯推理能力的罕见病诊断智能体系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor."
    },
    {
      "title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法 (原标题: Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content)",
      "link": "https://arxiv.org/abs/2506.20331",
      "pubDate": "Wed, 25 Jun 2025 07:30:25 GMT",
      "isoDate": "2025-06-25T07:30:25.000Z",
      "creator": "Rian Touchent, Nathan Godey, Eric de la Clergerie",
      "summary": "## Biomed-Enriched：一个用于预训练和提取稀有及隐藏内容的生物医学数据集\n\nBiomed-Enriched是一个新颖的生物医学文本数据集，它通过两阶段的标注过程从PubMed构建而成。该数据集旨在为生物医学和临床自然语言处理（NLP）提供一个宝贵的资源，特别是用于预训练和提取难以获取的稀有及隐藏内容。\n\n### 数据集构建过程：\n\n1.  **第一阶段：大型语言模型（LLM）标注**\n    *   一个大型语言模型对来自PubMed科学文章的40万个段落进行标注。\n    *   标注内容包括段落的类型（综述、研究、临床病例、其他）、领域（临床、生物医学、其他）以及教育质量评分。\n    *   教育质量评分范围为1到5，用于评估段落对大学水平学习的有用性。\n2.  **第二阶段：小型语言模型（SLM）传播标签**\n    *   利用第一阶段生成的标注数据对一个小型语言模型进行微调。\n    *   该微调后的SLM将这些标签传播到整个PMC-OA语料库中。\n\n### 数据集的成果与价值：\n\n*   **精炼子集提取：** 生成的元数据使得能够提取出精炼的子集，例如：\n    *   200万个临床病例段落。\n    *   其中包含超过45万个来自具有商业使用许可文章的高质量临床病例段落。\n*   **解决数据访问难题：** 临床文本通常由于隐私限制（医院记录无法公开共享）而难以获取。Biomed-Enriched提供了一个替代方案，即一个大规模、公开可用的PubMed临床病例集合。\n*   **对NLP的价值：** 它是生物医学和临床NLP领域的一个重要资源。\n\n### 初步预训练实验结果（使用OLMo2）：\n\n*   **有针对性的改进：** 经过精心策划的子集能够实现有针对性的性能提升。\n    *   **临床上采样（Clinical Upsampling）：** 在MMLU ProfMed基准测试中，性能提升了约5%。\n    *   **教育质量过滤（Educational Quality Filtering）：** 在MedQA和MedMCQA基准测试中，性能提升了约1%。\n*   **训练效率提升：** 结合这些技术可以实现更快的收敛速度，仅用三分之一的训练令牌就能达到相同的性能。\n*   **未来潜力：** 这表明Biomed-Enriched数据集有潜力支持更高效、更有效的生物医学预训练策略。\n\n### 数据可用性：\n\n数据集链接已提供。",
      "shortSummary": "Biomed-Enriched是一个新的生物医学文本数据集，通过LLM对PubMed文章进行两阶段标注构建。它提供了200万个临床病例段落，包括45万个高质量病例，解决了临床文本隐私限制导致的访问难题。该数据集是生物医学和临床NLP的重要资源。初步实验表明，其精选子集能显著提升预训练模型的性能，如临床上采样使MMLU ProfMed提升约5%，并能加速模型收敛，实现更高效的生物医学预训练。",
      "translated_title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."
    },
    {
      "title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画 (原标题: AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.19851",
      "pubDate": "Tue, 24 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-24T13:59:58.000Z",
      "creator": "Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng",
      "summary": "# AnimaX：一种创新的3D动画框架\n\nAnimaX是一个前馈式3D动画框架，旨在弥合视频扩散模型的运动先验与基于骨骼动画的可控结构之间的鸿沟。\n\n## 传统方法的局限性\n\n传统的运动合成方法存在以下限制：\n*   受限于固定的骨骼拓扑结构。\n*   需要在高维变形空间中进行昂贵的优化。\n\n## AnimaX 的核心优势与方法\n\nAnimaX通过以下创新方法克服了传统方法的局限性：\n\n*   **知识迁移：** 有效地将基于视频的运动知识迁移到3D领域。\n*   **广泛支持：** 能够支持具有任意骨骼的各种关节网格模型。\n*   **运动表示：** 将3D运动表示为多视角、多帧的2D姿态图。\n*   **联合扩散：** 实现联合视频-姿态扩散，其条件是模板渲染和文本运动提示。\n*   **对齐机制：** 引入共享位置编码和模态感知嵌入，以确保视频和姿态序列之间的时空对齐，从而将视频先验有效地转移到运动生成任务中。\n*   **3D重建：** 生成的多视角姿态序列被三角化为3D关节位置。\n*   **网格动画：** 通过逆运动学（inverse kinematics）将3D关节位置转换为最终的网格动画。\n\n## 训练与性能\n\n*   AnimaX 在一个新策展的、包含160,000个绑定序列的数据集上进行了训练。\n*   在VBench基准测试中，AnimaX在泛化能力、运动保真度和效率方面取得了最先进（state-of-the-art）的结果。\n\n## 应用前景\n\nAnimaX为类别无关的3D动画提供了一个可扩展的解决方案，具有广泛的应用潜力。\n\n**项目页面：** 提供了项目页面链接以获取更多信息。",
      "shortSummary": "AnimaX是一个创新的3D动画框架，它结合了视频扩散模型的运动先验与骨骼动画的可控性。该方法将3D运动表示为多视角2D姿态图，并通过联合视频-姿态扩散生成动画。AnimaX支持任意骨骼的网格模型，解决了传统方法的局限性。它在16万个绑定序列数据集上训练，并在泛化、运动保真度和效率方面达到了最先进水平，为类别无关的3D动画提供了可扩展的解决方案。",
      "translated_title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}."
    }
  ],
  "lastUpdated": "2025-06-29T09:29:09.327Z"
}