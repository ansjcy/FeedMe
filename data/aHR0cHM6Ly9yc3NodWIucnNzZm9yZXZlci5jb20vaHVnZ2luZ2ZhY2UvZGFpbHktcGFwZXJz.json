{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VoiceAssistant-Eval：评估AI助手在听觉、口语和视觉方面的表现 (原标题: VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing)",
      "link": "https://arxiv.org/abs/2509.22651",
      "pubDate": "Fri, 26 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-26T13:59:59.000Z",
      "creator": "Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li",
      "summary": "# VoiceAssistant-Eval：评估AI助手在听觉、口语和视觉方面的表现\n\n## 摘要介绍\n随着大型语言模型和多模态系统能力的不断增强，语音优先的AI助手引起了广泛关注。然而，现有基准测试在全面评估这些系统的能力方面存在不足。为此，本文引入了 **VoiceAssistant-Eval**，这是一个旨在全面评估AI助手在听觉、口语和视觉方面表现的综合基准。\n\n## VoiceAssistant-Eval 的构成与范围\n*   **数据集规模**：包含10,497个精心策划的示例。\n*   **任务类别**：涵盖13个任务类别。\n*   **评估维度**：\n    *   **听觉（Listening）**：包括自然声音、音乐和口语对话。\n    *   **口语（Speaking）**：涉及多轮对话、角色扮演模仿和各种场景。\n    *   **视觉（Viewing）**：包含高度异构的图像。\n\n## 实用性展示与评估结果\n为了证明其效用，研究人员评估了21个开源模型和GPT-4o-Audio。评估指标包括响应内容和语音的质量，以及它们的一致性。\n\n### 关键发现\n研究结果揭示了三个关键发现：\n1.  **专有模型并非普遍优于开源模型**：这表明在某些领域，开源解决方案可能与商业产品相媲美甚至超越。\n2.  **模型在口语任务上表现出色，但在音频理解方面滞后**：大多数模型在生成语音响应方面表现良好，但在理解和处理音频输入（如自然声音、音乐或复杂对话）方面仍有显著不足。\n3.  **精心设计的小型模型可以与大型模型竞争**：这强调了模型架构和训练策略的重要性，而非仅仅是模型规模。\n\n### 显著案例\n一个值得注意的例子是，中等规模的 **Step-Audio-2-mini (7B)** 在听觉准确性方面达到了 **LLaMA-Omni2-32B-Bilingual** 的两倍以上。\n\n## 现有挑战与未来方向\n尽管取得了进展，但当前模型仍面临挑战：\n*   **多模态输入（音频加视觉）任务**：对现有模型来说仍然困难。\n*   **角色扮演语音模仿任务**：也是一个难以攻克的领域。\n*   **鲁棒性和安全对齐**：在这些方面仍然存在显著差距。\n\nVoiceAssistant-Eval 通过识别这些差距，建立了一个严谨的框架，用于评估和指导下一代AI助手的开发。\n\n## 资源可用性\n相关代码和数据将在未来发布。",
      "shortSummary": "VoiceAssistant-Eval是一个新的综合基准，旨在评估AI助手在听觉、口语和视觉方面的能力。它包含10,497个示例和13个任务类别。研究评估了21个开源模型和GPT-4o-Audio，发现专有模型并非普遍优于开源模型，模型在口语方面表现优异但音频理解能力不足，且小型模型可与大型模型竞争。尽管如此，多模态输入和角色扮演语音模仿仍是挑战，鲁棒性和安全对齐存在差距。该基准为AI助手发展提供了评估框架。",
      "translated_title": "VoiceAssistant-Eval：评估AI助手在听觉、口语和视觉方面的表现",
      "images": [],
      "contentSource": "完整文章",
      "content": "The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ ."
    },
    {
      "title": "看、指、飞：一种无需学习的VLM通用无人机导航框架 (原标题: See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation)",
      "link": "https://arxiv.org/abs/2509.22653",
      "pubDate": "Fri, 26 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-26T13:59:59.000Z",
      "creator": "Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu",
      "summary": "### 看、指、飞（SPF）：一种无需学习的VLM通用无人机导航框架\n\n**核心贡献：**\n\n*   SPF是一个无需训练的空中视觉-语言导航（AVLN）框架，它基于视觉-语言模型（VLM）构建。\n*   该框架能够根据任何类型的自由形式指令，在任何环境中导航到任何目标。\n\n**创新方法：**\n\n*   与现有基于VLM的方法将动作预测视为文本生成任务不同，SPF的关键洞察在于将AVLN的动作预测视为2D空间定位任务。\n\n**工作原理：**\n\n*   **指令分解：** SPF利用VLM将模糊的语言指令分解为输入图像上的迭代2D航点标注。\n*   **动作转换：** 结合预测的行进距离，SPF将预测的2D航点转换为3D位移向量，作为无人机（UAV）的动作指令。\n*   **自适应调整：** SPF还能自适应地调整行进距离，以促进更高效的导航。\n*   **闭环控制：** SPF以闭环控制方式执行导航，使无人机能够跟踪动态环境中的动态目标。\n\n**性能表现：**\n\n*   **模拟基准：** 在DRL模拟基准测试中，SPF取得了新的最先进（SOTA）性能，比之前最好的方法绝对提高了63%。\n*   **真实世界评估：** 在广泛的真实世界评估中，SPF大幅优于强大的基线方法。\n*   **设计有效性：** 通过全面的消融研究，突出了其设计选择的有效性。\n*   **泛化能力：** SPF对不同的VLM表现出卓越的泛化能力。\n\n**项目信息：**\n\n*   该研究已提交至CoRL 2025。",
      "shortSummary": "SPF（看、指、飞）是一个无需训练的VLM框架，用于通用无人机导航。它创新性地将空中视觉-语言导航中的动作预测视为2D空间定位任务，而非文本生成。SPF利用VLM将自由形式指令分解为2D航点，并转换为3D动作指令，实现闭环控制和自适应距离调整。该框架在DRL模拟和真实世界评估中均达到最先进水平，并展现出强大的泛化能力。",
      "translated_title": "看、指、飞：一种无需学习的VLM通用无人机导航框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev"
    },
    {
      "title": "RefAM：零样本指代分割的注意力磁铁 (原标题: RefAM: Attention Magnets for Zero-Shot Referral Segmentation)",
      "link": "https://arxiv.org/abs/2509.22650",
      "pubDate": "Fri, 26 Sep 2025 13:59:57 GMT",
      "isoDate": "2025-09-26T13:59:57.000Z",
      "creator": "Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele",
      "summary": "RefAM：零样本指代分割的注意力磁铁\n\n本文介绍了一种名为 RefAM 的新方法，旨在解决指代分割任务中现有方法需要额外训练或架构修改的问题。RefAM 直接利用扩散 Transformer 中的特征（注意力分数）进行下游任务，无需进行架构修改或额外的训练。\n\n### 主要观点和方法：\n\n*   **问题背景**：\n    *   大多数现有的指代分割方法需要通过微调或组合多个预训练模型才能达到良好性能，这通常会增加训练成本和架构复杂性。\n    *   大规模生成扩散模型编码了丰富的语义信息，使其成为通用的特征提取器。\n\n*   **RefAM 的核心思想**：\n    *   **停用词作为注意力磁铁**：作者发现停用词（stop words）充当“注意力磁铁”，它们会积累多余的注意力，通过过滤这些停用词可以有效减少噪声。\n    *   **全局注意力汇聚点（GAS）**：在更深层中会出现全局注意力汇聚点（Global Attention Sinks, GAS）。研究表明，这些 GAS 可以被安全地抑制或重定向到辅助 token 上，从而产生更清晰、更准确的接地（grounding）图。\n    *   **注意力重新分配策略**：通过附加停用词，可以将背景激活（background activations）划分为更小的簇，从而生成更清晰、更局部化的热图。\n\n*   **RefAM 框架**：\n    *   RefAM 是一个简单的、无需训练的接地框架，它结合了以下关键组件：\n        1.  交叉注意力图（cross-attention maps）\n        2.  GAS 处理机制\n        3.  注意力重新分配策略\n\n*   **评估与性能**：\n    *   为了系统地评估这些特征，作者扩展了基准测试，纳入了跨图像和视频的视觉-语言接地任务。\n    *   在零样本指代图像和视频分割基准测试中，RefAM 方法始终优于现有方法，无需微调或额外组件，便建立了新的最先进性能（state of the art）。",
      "shortSummary": "RefAM 提出了一种无需训练的零样本指代分割框架。它直接利用扩散 Transformer 的注意力分数，通过将停用词作为“注意力磁铁”过滤噪声，处理全局注意力汇聚点（GAS），并采用注意力重新分配策略。RefAM 在零样本图像和视频分割基准测试中表现出色，无需微调或额外组件，便达到了新的最先进水平。",
      "translated_title": "RefAM：零样本指代分割的注意力磁铁",
      "images": [],
      "contentSource": "完整文章",
      "content": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components."
    },
    {
      "title": "CapRL：通过强化学习激发密集图像字幕能力 (原标题: CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.22647",
      "pubDate": "Fri, 26 Sep 2025 13:59:55 GMT",
      "isoDate": "2025-09-26T13:59:55.000Z",
      "creator": "Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin",
      "summary": "### CapRL：通过强化学习提升图像字幕能力\n\n本文介绍了CapRL（Captioning Reinforcement Learning），一个旨在通过强化学习（RL）范式解决当前图像字幕模型局限性的新型训练框架。当前最先进的字幕模型通常采用监督微调（SFT）进行训练，这种方法依赖于昂贵且难以扩展的人工或专有模型标注数据，导致模型倾向于记忆特定的标准答案，从而限制了其通用性和生成多样化、创意描述的能力。\n\n#### 核心问题与CapRL的解决方案\n\n*   **SFT的局限性**：SFT训练的模型容易过度拟合特定标注，缺乏泛化能力和生成多样化、创造性描述的能力。\n*   **CapRL的提出**：为了克服SFT的限制，CapRL将“可验证奖励强化学习”（RLVR）范式应用于开放式图像字幕任务。\n*   **挑战**：设计一个客观的奖励函数来评估本质上主观的“好”字幕是一个主要挑战。\n\n#### CapRL的设计理念与工作机制\n\n*   **重新定义字幕质量**：CapRL通过字幕的“效用”来重新定义其质量。一个高质量的字幕应该能够让一个非视觉语言模型（LLM）仅凭该字幕准确回答关于相应图像的问题。\n*   **解耦的两阶段流程**：\n    1.  **字幕生成**：一个大型视觉-语言模型（LVLM）生成图像字幕。\n    2.  **奖励评估**：一个独立的、不具备视觉能力的LLM仅基于生成的字幕回答多项选择题（MCQ）。该LLM的回答准确性被用作客观奖励。\n\n#### 主要贡献与实验结果\n\n*   **开创性应用**：CapRL是首个将RLVR应用于主观图像字幕任务的研究。\n*   **显著性能提升**：\n    *   在CapRL-3B标注的CapRL-5M字幕数据集上进行预训练，CapRL在12个基准测试中取得了显著的性能提升。\n    *   在Prism字幕质量评估框架中，CapRL的性能与Qwen2.5-VL-72B相当，并且平均超出基线模型8.4%。\n\n#### 代码可用性\n\n*   相关代码已公开提供。",
      "shortSummary": "CapRL提出了一种新颖的强化学习（RL）框架，旨在通过解决监督微调（SFT）的局限性来提升图像字幕能力。它通过字幕的“效用”来定义质量，即字幕能否帮助非视觉语言模型准确回答图像相关问题。CapRL采用两阶段流程：LVLM生成字幕，然后一个独立的LLM基于字幕回答多选题以获取客观奖励。作为首个将可验证奖励强化学习（RLVR）应用于主观图像字幕任务的研究，CapRL在多个基准测试中取得了显著提升，并在Prism框架中表现优异，超越基线8.4%。",
      "translated_title": "CapRL：通过强化学习激发密集图像字幕能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \"good\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL."
    },
    {
      "title": "WebGen-Agent：通过多级反馈和步级强化学习增强交互式网站生成 (原标题: WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.22644",
      "pubDate": "Fri, 26 Sep 2025 13:59:51 GMT",
      "isoDate": "2025-09-26T13:59:51.000Z",
      "creator": "Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li",
      "summary": "# WebGen-Agent：通过多级反馈和步级强化学习增强交互式网站生成\n\n## 1. 引言与问题背景\n大型语言模型（LLMs）驱动的智能体系统在代码生成任务上表现出色。然而，对于网站代码库生成这类高度依赖视觉效果和用户交互反馈的任务，现有代码智能体仅依赖简单的代码执行进行反馈和验证。这种方法无法准确捕捉生成代码的实际质量。\n\n## 2. WebGen-Agent 提案\n本文提出 WebGen-Agent，一种新颖的网站生成智能体。它利用全面且多层次的视觉反馈来迭代生成和完善网站代码库。\n\n## 3. 多级视觉反馈机制\n*   **视觉语言模型（VLM）生成反馈：** WebGen-Agent 利用视觉语言模型（VLM）根据网站的屏幕截图和 GUI 智能体测试，生成详细且富有表现力的文本描述、建议以及量化的质量分数。\n*   **分数集成与优化：** 生成的屏幕截图和 GUI 智能体分数进一步与回溯（backtracking）和“选择最佳”（select-best）机制相结合，从而增强智能体的性能。\n\n## 4. 步级强化学习（Step-GRPO）\n*   **引入 Step-GRPO：** 利用 WebGen-Agent 工作流中固有的准确视觉分数，本文进一步引入了“带有屏幕截图和 GUI 智能体反馈的 Step-GRPO”机制。\n*   **增强 LLM 推理能力：** 该机制旨在提高 LLM 作为 WebGen-Agent 推理引擎的能力。\n*   **密集且可靠的监督信号：** 通过将每一步的屏幕截图和 GUI 智能体分数作为 Step-GRPO 中的奖励，提供了密集且可靠的过程监督信号，有效提升了模型的网站生成能力。\n\n## 5. 实验结果\n在 WebGen-Bench 数据集上，WebGen-Agent 取得了显著的性能提升：\n*   **Claude-3.5-Sonnet 性能提升：**\n    *   准确率从 26.4% 提高到 51.9%。\n    *   外观分数从 3.0 提高到 3.9。\n    *   超越了现有最先进的智能体系统。\n*   **Step-GRPO 训练效果：**\n    *   Qwen2.5-Coder-7B-Instruct 的准确率从 38.9% 提高到 45.4%。\n    *   外观分数从 3.4 提高到 3.7。\n\n## 6. 结论\nWebGen-Agent 通过引入多级视觉反馈和步级强化学习，显著提升了 LLM 在交互式网站生成任务上的性能和代码质量。",
      "shortSummary": "WebGen-Agent 提出一种新颖的网站生成智能体，通过多级视觉反馈和步级强化学习（Step-GRPO）解决现有 LLM 智能体在网站生成中缺乏有效视觉反馈的问题。它利用视觉语言模型（VLM）生成屏幕截图和 GUI 测试的详细反馈及分数，并结合回溯机制。Step-GRPO 将这些步级分数作为奖励，密集监督 LLM 的推理过程。实验表明，WebGen-Agent 显著提升了 Claude-3.5-Sonnet 和 Qwen2.5-Coder-7B-Instruct 在网站生成准确率和外观分数上的表现，超越了现有技术。",
      "translated_title": "WebGen-Agent：通过多级反馈和步级强化学习增强交互式网站生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce Step-GRPO with Screenshot and GUI-agent Feedback to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7."
    },
    {
      "title": "WoW：通过具身交互迈向全知世界模型 (原标题: WoW: Towards a World omniscient World model Through Embodied Interaction)",
      "link": "https://arxiv.org/abs/2509.22642",
      "pubDate": "Fri, 26 Sep 2025 13:59:07 GMT",
      "isoDate": "2025-09-26T13:59:07.000Z",
      "creator": "Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang",
      "summary": "### WoW：通过具身交互迈向全知世界模型\n\n**核心思想与问题背景**\n*   **人类学习方式与现有模型的对比：** 人类通过与世界的积极互动来发展对直观物理的理解。这与Sora等现有视频模型形成鲜明对比，后者依赖被动观察，因此难以掌握物理因果关系。\n*   **核心假设：** 世界模型要获得真实的物理直觉，必须根植于与真实世界广泛且因果丰富的交互。\n\n**WoW模型介绍**\n*   **模型名称与规模：** WoW（World omniscient World model），是一个拥有140亿参数的生成式世界模型。\n*   **训练数据：** 该模型在200万条机器人交互轨迹上进行训练。\n\n**研究发现与挑战**\n*   **物理理解的概率性：** 研究发现，WoW模型对物理的理解表现为可能结果的概率分布，这导致了随机不稳定性和物理幻觉。\n\n**SOPHIA与逆动力学模型：提升物理真实性**\n*   **SOPHIA机制：** 为了解决上述挑战，引入了SOPHIA机制。视觉-语言模型代理（SOPHIA）评估DiT生成的输出，并通过迭代演化语言指令来指导其细化，从而将模型的涌现能力主动约束到物理真实性。\n*   **想象到行动的闭环：** 一个协同训练的逆动力学模型将这些细化后的计划转化为可执行的机器人动作，从而闭合了从想象到行动的循环。\n\n**WoWBench基准测试**\n*   **目的：** 建立了一个名为WoWBench的新基准，专注于视频中的物理一致性和因果推理。\n*   **WoW的性能：** WoW在该基准的人工和自主评估中均达到了最先进的性能。\n*   **展现能力：** 模型在物理因果关系、碰撞动力学和物体永存性方面表现出强大的能力。\n\n**结论与意义**\n*   这项工作系统性地证明了大规模、真实世界的交互是人工智能发展物理直觉的基石。\n*   模型、数据和基准将开源。",
      "shortSummary": "WoW是一个140亿参数的生成式世界模型，通过200万条机器人交互轨迹训练，旨在通过具身交互解决现有模型在物理因果理解上的不足。研究表明，大规模真实世界交互是AI发展物理直觉的关键。尽管模型存在物理幻觉，但通过SOPHIA（视觉-语言代理）和逆动力学模型，WoW能将想象转化为具身行动。WoW在新的WoWBench基准上表现出色，在物理因果、碰撞动力学和物体永存性方面展现强大能力。模型、数据和基准将开源。",
      "translated_title": "WoW：通过具身交互迈向全知世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced."
    },
    {
      "title": "语言模型可以从口头反馈中学习，而无需标量奖励 (原标题: Language Models Can Learn from Verbal Feedback Without Scalar Rewards)",
      "link": "https://arxiv.org/abs/2509.22638",
      "pubDate": "Fri, 26 Sep 2025 13:58:27 GMT",
      "isoDate": "2025-09-26T13:58:27.000Z",
      "creator": "Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang",
      "summary": "## 语言模型从口头反馈中学习的新范式：反馈条件策略（FCP）\n\n### 引言：现有方法的局限性\n\n当前，大型语言模型（LLMs）的训练常常依赖于人类或AI反馈的强化学习（RL）方法。然而，这种传统方法存在一个显著问题：它通常将细致入微、信息丰富的口头反馈压缩成简单的标量奖励。这种压缩过程导致了大量信息的丢失，并可能引入尺度不平衡问题，从而限制了LLMs从反馈中学习的效率和深度。\n\n### 核心贡献：反馈条件策略（FCP）\n\n为了解决上述挑战，本文提出了一种创新的方法——**反馈条件策略（Feedback-Conditional Policy, FCP）**。FCP的核心思想是将口头反馈视为一种**条件信号**，而非传统的标量奖励。\n\n*   **灵感来源：** 这一方法受到了文本到图像生成领域中语言先验的启发。在文本到图像生成中，模型能够根据前所未见的提示生成新颖的输出，这表明语言本身可以作为强大的条件信号。\n*   **FCP的工作原理：**\n    *   FCP直接从**响应-反馈对**中进行学习。\n    *   它通过在离线数据上进行**最大似然训练**，来近似反馈条件后验分布。\n\n### 在线自举阶段\n\n为了进一步完善模型，本文还开发了一个**在线自举（online bootstrapping）阶段**：\n\n*   在此阶段，策略会在积极的条件（即期望获得正面反馈的条件）下生成响应。\n*   随后，模型会接收到新鲜的反馈，并利用这些反馈来进一步精炼和优化自身。\n\n### 范式转变与优势\n\nFCP的提出标志着反馈驱动学习范式的一次重要转变：\n\n*   它将反馈驱动的学习重新定义为**条件生成**，而非传统的奖励优化。\n*   这种方法为LLMs提供了一种**更具表达力**的方式，使其能够更直接、更有效地从口头反馈中学习，从而克服了标量奖励所带来的信息瓶颈。",
      "shortSummary": "针对LLMs从反馈中学习时将口头反馈压缩为标量奖励导致信息丢失的问题，本文提出反馈条件策略（FCP）。FCP将口头反馈作为条件信号，通过最大似然训练直接从响应-反馈对中学习，并引入在线自举阶段进行优化。这种方法将反馈驱动学习重构为条件生成而非奖励优化，使LLMs能更直接、更具表达力地从口头反馈中学习。",
      "translated_title": "语言模型可以从口头反馈中学习，而无需标量奖励",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy."
    },
    {
      "title": "语言模型的变分推理 (原标题: Variational Reasoning for Language Models)",
      "link": "https://arxiv.org/abs/2509.22637",
      "pubDate": "Fri, 26 Sep 2025 13:58:10 GMT",
      "isoDate": "2025-09-26T13:58:10.000Z",
      "creator": "Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang",
      "summary": "## 语言模型的变分推理框架\n\n本文介绍了一种用于语言模型（LMs）的变分推理框架，旨在提升其推理能力。该框架将模型的“思维轨迹”（thinking traces）视为潜在变量，并通过变分推断进行优化。\n\n### 核心方法与创新\n\n*   **潜在变量与优化：** 框架的核心在于将语言模型在推理过程中产生的中间步骤或“思维轨迹”建模为潜在变量。这些潜在变量通过变分推断进行优化，从而指导模型生成更有效的推理路径。\n*   **多轨迹目标：** 作者将证据下界（ELBO）扩展为多轨迹目标，旨在获得更紧密的下界，从而更准确地估计模型的性能。\n*   **前向KL公式：** 提出了一种前向KL（forward-KL）公式，该公式能够稳定变分后验的训练过程，确保优化过程的鲁棒性。\n\n### 与现有方法的联系与新发现\n\n*   **统一视角：** 研究表明，拒识采样微调（rejection sampling finetuning）和二元奖励强化学习（binary-reward RL，包括GRPO）等现有方法可以被解释为局部前向KL目标。这一发现为变分推断与强化学习风格的方法提供了一个统一的概率视角。\n*   **偏见揭示：** 从推导中自然地揭示了一种由模型准确性进行的隐式加权，这进一步揭示了这些方法在训练过程中对“更容易问题”存在一种之前未被注意到的偏见。\n\n### 实验验证与贡献\n\n*   **实证验证：** 该方法在Qwen 2.5和Qwen 3模型家族上，针对广泛的推理任务进行了实证验证，证明了其有效性。\n*   **主要贡献：** 本工作提供了一个有原则的概率视角，不仅统一了变分推断与强化学习风格的方法，而且为提高语言模型的推理能力提供了稳定且有效的目标。\n\n### 资源\n\n*   相关代码已开源，可在[此链接](this https URL)获取。",
      "shortSummary": "本文提出了一种用于语言模型的变分推理框架，将思维轨迹视为潜在变量，并通过变分推断进行优化。该框架通过扩展证据下界和引入前向KL公式，稳定了训练过程。它统一了变分推断与强化学习方法，并揭示了现有方法对简单问题的偏见。实验证明，该方法能有效提升Qwen 2.5和Qwen 3等模型的推理能力，并提供了稳定的优化目标。",
      "translated_title": "语言模型的变分推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning."
    },
    {
      "title": "StateX：通过训练后状态扩展增强RNN召回能力 (原标题: StateX: Enhancing RNN Recall via Post-training State Expansion)",
      "link": "https://arxiv.org/abs/2509.22630",
      "pubDate": "Fri, 26 Sep 2025 13:55:22 GMT",
      "isoDate": "2025-09-26T13:55:22.000Z",
      "creator": "Xingyu Shen, Yingfa Chen, Zhen Leng Thai, Xu Han, Zhiyuan Liu, Maosong Sun",
      "summary": "### StateX：通过训练后状态扩展增强RNN召回能力\n\n本文介绍了StateX，一个旨在通过训练后（post-training）方法有效扩展预训练循环神经网络（RNNs）状态的训练流程，以解决RNNs在长上下文召回能力方面的不足。\n\n**背景与问题**\n*   **Transformer模型的局限性**：尽管Transformer模型在语言建模方面表现出色，但其高复杂度导致处理长上下文时成本高昂。\n*   **RNN模型的优势与挑战**：\n    *   **优势**：线性注意力模型和状态空间模型等RNN因其恒定的每token复杂度而受到青睐，在处理长上下文时效率更高。\n    *   **挑战**：这些循环模型难以准确召回长上下文中的信息，因为所有上下文信息都被压缩到一个固定大小的循环状态中。\n    *   **现有研究**：召回能力与循环状态大小呈正相关，但直接使用更大的循环状态训练RNN会导致高昂的训练成本。\n\n**StateX方法**\n*   **核心思想**：StateX提出了一种在模型训练后（post-training）阶段，高效扩展预训练RNN状态的管道。\n*   **技术实现**：针对线性注意力模型和状态空间模型这两种流行的RNN类别，StateX设计了训练后架构修改，以在不增加或仅微不足道地增加模型参数的情况下，扩大状态大小。\n\n**实验与成果**\n*   **实验范围**：在参数量高达1.3亿的模型上进行了实验。\n*   **主要发现**：\n    *   StateX能够有效增强RNN的召回能力和上下文学习能力。\n    *   该方法不会产生高昂的训练后成本。\n    *   在增强召回能力的同时，不损害模型的其他能力。\n\n**结论**\nStateX为提升RNN处理长上下文的能力提供了一个高效且经济的解决方案，使其在保持计算效率的同时，显著改善了信息召回和上下文学习表现。",
      "shortSummary": "StateX提出了一种通过训练后状态扩展来增强循环神经网络（RNNs）召回能力的方法。针对RNNs在长上下文信息压缩和召回方面的不足，StateX通过对线性注意力模型和状态空间模型进行架构修改，在不显著增加参数和成本的情况下，有效扩大了预训练RNN的状态大小。实验表明，StateX显著提升了RNN的召回和上下文学习能力，同时保持了其他性能。",
      "translated_title": "StateX：通过训练后状态扩展增强RNN召回能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities."
    },
    {
      "title": "SPARK：协同策略与奖励共同演化框架 (原标题: SPARK: Synergistic Policy And Reward Co-Evolving Framework)",
      "link": "https://arxiv.org/abs/2509.22624",
      "pubDate": "Fri, 26 Sep 2025 13:50:12 GMT",
      "isoDate": "2025-09-26T13:50:12.000Z",
      "creator": "Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
      "summary": "### SPARK：协同策略与奖励共同演化框架\n\n#### 引言\n\n近期，大型语言模型（LLMs）和大型视觉-语言模型（LVLMs）越来越多地采用强化学习（RL）进行后预训练。其中，可验证奖励强化学习（RLVR）用于客观任务，而基于人类反馈的强化学习（RLHF）则用于主观任务。然而，这两种方法都面临挑战：\n\n*   **RLHF的局限性**：成本高昂，且由于依赖人类偏好，可能导致奖励模型与策略模型之间出现不匹配。\n*   **RLVR的效率问题**：每次更新后，会丢弃rollouts和正确性信号，造成监督信息的浪费。\n\n#### SPARK框架介绍\n\n为解决上述挑战，我们提出了**协同策略与奖励共同演化框架（SPARK）**。SPARK是一个高效、on-policy且稳定的方法，它建立在RLVR的基础上，并进行了关键创新：\n\n*   **数据回收与再利用**：SPARK不再丢弃rollouts和正确性数据，而是回收并利用这些宝贵信息。\n*   **生成式奖励模型**：SPARK将模型本身训练成一个生成式奖励模型，与策略模型同步进行。这意味着模型学会了自我评估和改进其响应，从而消除了对独立奖励模型的需求。\n*   **辅助训练目标**：为了实现这一目标，辅助训练结合了多种目标，包括：\n    *   点式奖励分数（pointwise reward score）\n    *   成对比较（pairwise comparison）\n    *   基于进一步反思响应的评估（evaluation conditioned on further-reflection responses）\n*   **核心优势**：\n    *   **消除外部依赖**：无需独立的奖励模型，也无需昂贵的人类偏好数据。\n    *   **正向协同演化反馈循环**：框架创造了一个积极的反馈循环——提高的奖励准确性会产生更好的策略梯度，进而生成更高质量的rollouts，这些rollouts又会进一步完善奖励模型。\n*   **统一框架**：SPARK提供了一个统一的框架，支持通过自反思在测试时进行扩展，而无需外部奖励模型及其相关成本。\n\n#### 实验结果与性能\n\nSPARK在多个LLM和LVLM模型上，以及在推理、奖励模型和通用基准测试中，都取得了显著的性能提升，展现了其鲁棒性和广泛的泛化能力。\n\n*   **具体示例（SPARK-VL-7B）**：\n    *   在7个推理基准上，平均性能提升9.7%。\n    *   在2个奖励基准上，平均性能提升12.1%。\n    *   在8个通用基准上，平均性能提升1.5%。\n\n这些结果表明SPARK能够有效提升模型在多项任务上的表现。",
      "shortSummary": "SPARK是一个高效的协同策略与奖励共同演化框架，旨在解决大型语言模型和视觉-语言模型后预训练中RLHF成本高昂及RLVR数据浪费的问题。它通过回收rollouts和正确性数据，将模型本身训练为生成式奖励模型，从而无需独立的奖励模型和人类偏好数据。SPARK创建了一个正向反馈循环，使奖励和策略协同演化。实验证明，SPARK在多种LLM和LVLM任务上实现了显著性能提升，例如SPARK-VL-7B在推理和奖励基准上分别平均提升9.7%和12.1%。",
      "translated_title": "SPARK：协同策略与奖励共同演化框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization."
    },
    {
      "title": "LongLive：实时交互式长视频生成 (原标题: LongLive: Real-time Interactive Long Video Generation)",
      "link": "https://arxiv.org/abs/2509.22622",
      "pubDate": "Fri, 26 Sep 2025 13:48:24 GMT",
      "isoDate": "2025-09-26T13:48:24.000Z",
      "creator": "Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen",
      "summary": "# LongLive：实时交互式长视频生成框架\n\nLongLive 是一种创新的帧级自回归（AR）框架，专为实时交互式长视频生成而设计。该框架旨在解决长视频生成在效率、质量和交互性方面面临的挑战。\n\n## 现有挑战\n\n*   **效率与质量的权衡**：\n    *   扩散模型（Diffusion models）和扩散增强模型（Diffusion-Forcing models）能生成高质量视频，但由于双向注意力机制，效率较低。\n    *   因果注意力自回归模型（Causal attention AR models）支持 KV 缓存以实现更快的推理，但由于长视频训练中的内存挑战，在长视频上的质量往往会下降。\n*   **交互性需求**：\n    *   除了基于静态提示的生成，交互式功能（如流式提示输入）对于动态内容创作至关重要，使用户能够实时引导叙事。\n    *   这种交互性显著增加了复杂性，尤其是在提示转换过程中确保视觉一致性和语义连贯性。\n\n## LongLive 的核心设计\n\nLongLive 采用了因果、帧级自回归设计，并集成了以下关键机制来克服上述挑战：\n\n*   **KV-Recache 机制**：\n    *   该机制通过新提示刷新缓存状态，实现平滑、一致的提示切换，从而支持实时交互。\n*   **流式长训练（Streaming Long Tuning）**：\n    *   使模型能够进行长视频训练，并对齐训练和推理过程（train-long-test-long），确保在长视频生成中的性能。\n*   **短窗口注意力与帧级注意力槽（Frame-level Attention Sink，简称 Frame Sink）**：\n    *   结合短窗口注意力与帧级注意力槽，在实现更快生成的同时，有效保持长距离一致性。\n\n## 性能与成果\n\n*   **高效微调**：LongLive 仅用 32 个 GPU-天就将一个 1.3B 参数的短片段模型微调至可生成分钟级视频。\n*   **实时推理速度**：在单个 NVIDIA H100 GPU 上，LongLive 可持续实现 20.7 帧/秒（FPS）的推理速度。\n*   **卓越的生成质量**：在 VBench 评估中，LongLive 在短视频和长视频方面均表现出强大的性能。\n*   **长视频支持**：在单个 H100 GPU 上，LongLive 支持生成长达 240 秒（4分钟）的视频。\n*   **量化推理**：LongLive 进一步支持 INT8 量化推理，且质量损失微乎其微。\n\n## 可用性\n\nLongLive 的代码、模型和演示可在提供的链接中获取。",
      "shortSummary": "LongLive 是一种实时交互式长视频生成框架。它采用帧级自回归设计，通过 KV-recache 机制实现平滑提示切换，利用流式长训练处理长视频，并结合短窗口注意力与帧级注意力槽以保持长距离一致性并加速生成。LongLive 在单个 NVIDIA H100 上能以 20.7 FPS 生成长达 240 秒的视频，并展现出卓越的效率和质量。",
      "translated_title": "LongLive：实时交互式长视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss."
    },
    {
      "title": "用于熵安全推理的分位数优势估计 (原标题: Quantile Advantage Estimation for Entropy-Safe Reasoning)",
      "link": "https://arxiv.org/abs/2509.22611",
      "pubDate": "Fri, 26 Sep 2025 13:37:52 GMT",
      "isoDate": "2025-09-26T13:37:52.000Z",
      "creator": "Junkang Wu, Kexin Huang, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He",
      "summary": "## 用于熵安全推理的分位数优势估计 (Quantile Advantage Estimation for Entropy-Safe Reasoning)\n\n本文提出了一种名为“分位数优势估计”（Quantile Advantage Estimation, QAE）的新方法，旨在解决可验证奖励强化学习（RLVR）在大型语言模型（LLM）推理中面临的熵不稳定问题。\n\n*   **问题背景：RLVR的熵挑战**\n    *   可验证奖励强化学习（RLVR）能够增强LLM的推理能力。\n    *   然而，RLVR的训练过程常在“熵崩溃”（entropy collapse）和“熵爆炸”（entropy explosion）之间振荡。\n    *   研究发现，这两种风险都源于价值无关强化学习（如GRPO和DAPO）中使用的均值基线。在奖励异常值（reward outliers）存在的情况下，该基线会不当地惩罚负优势样本。\n\n*   **核心贡献：分位数优势估计（QAE）**\n    *   QAE通过将均值基线替换为组级K分位数基线来解决上述问题。\n    *   QAE引入了一种响应级别的双重机制：\n        *   对于“困难查询”（p <= 1 - K），它会强化罕见的成功案例。\n        *   对于“简单查询”（p > 1 - K），它则针对剩余的失败案例。\n\n*   **理论保障：双侧熵安全**\n    *   在首次序softmax更新下，QAE被证明能够实现“双侧熵安全”（two-sided entropy safety）。\n    *   这提供了单步熵变化的下限和上限，从而有效抑制熵爆炸并防止熵崩溃。\n\n*   **实验结果与发现**\n    *   **熵稳定性：** QAE这一最小的修改显著稳定了熵。\n    *   **信用分配稀疏化：** 经过调优的K值，大约80%的响应获得了零优势，表明信用分配变得更加稀疏。\n    *   **性能提升：** 在AIME 2024/2025和AMC 2023数据集上，QAE在Qwen3-8B/14B-Base模型上持续取得了pass@1的增益。\n    *   **关键机制：** 这些结果表明，“基线设计”——而非令牌级启发式方法——是扩展RLVR的主要机制。",
      "shortSummary": "本文提出分位数优势估计（QAE），以解决LLM可验证奖励强化学习（RLVR）中常见的熵崩溃和熵爆炸问题。QAE用组级K分位数基线取代传统均值基线，通过双重机制强化罕见成功并处理失败。理论上，QAE实现了双侧熵安全，抑制熵爆炸并防止熵崩溃。实验证明，QAE稳定了熵，稀疏化了信用分配，并在多个基准测试中持续提升了pass@1性能。研究强调基线设计是扩展RLVR的关键。",
      "translated_title": "用于熵安全推理的分位数优势估计",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p &lt;= 1 - K) it reinforces rare successes, while on easy queries (p &gt; 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR."
    },
    {
      "title": "掌握要领，信任成功：智能体强化学习中渐进式探索的自我模仿 (原标题: Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.22601",
      "pubDate": "Fri, 26 Sep 2025 13:20:38 GMT",
      "isoDate": "2025-09-26T13:20:38.000Z",
      "creator": "Yulei Qin, Xiaoyu Tan, Zhengbao He, Gang Li, Haojia Lin, Zongyi Li, Zihan Xu, Yuchen Shi, Siqi Cai, Renting Rui, Shaofei Cai, Yuzheng Cai, Xuan Zhang, Sheng Ye, Ke Li, Xing Sun",
      "summary": "### SPEAR：智能体强化学习中渐进式探索的自我模仿\n\n#### 1. 引言与背景\n\n*   **核心挑战：** 强化学习（RL）在处理大型语言模型（LLMs）的长周期、稀疏奖励智能体任务时，面临着探索-利用（exploration-exploitation）的根本性权衡问题。现有通过策略熵最大化来刺激探索的方法，由于多轮分布漂移，容易导致RL训练的不稳定性。\n*   **研究目标：** 本文旨在通过智能体自身的经验，实现渐进式的探索-利用平衡，从而避免熵崩溃或失控发散。\n\n#### 2. SPEAR方法概述\n\n*   **方法提出：** 提出了一种名为SPEAR（Self-imitation with Progressive Exploration for Agentic Reinforcement Learning）的课程式自我模仿学习（SIL）方法，专门用于训练智能体LLMs。\n*   **创新机制：** SPEAR扩展了传统的SIL框架。它通过在不同训练阶段逐步引导策略演化，使其熵值始终保持在良好平衡的范围内。传统的SIL框架通过回放缓冲区存储智能体自我生成的有前景的轨迹，用于离策略更新。\n\n#### 3. 渐进式探索机制\n\nSPEAR引入了一个课程来管理探索过程，主要分为两个层面：\n\n*   **技能层面探索（Skill-level exploration）：**\n    *   在训练初期，辅助工具调用奖励（作为内在奖励）发挥关键作用，促进智能体工具使用技能的积累。\n    *   这使得智能体能够广泛接触环境中不熟悉的反馈分布，并呈现出熵值上升的趋势，鼓励更广泛的探索。\n*   **动作层面探索（Action-level exploration）：**\n    *   随着训练的进行，自我模仿机制得到加强。智能体开始利用回放经验中现有的成功模式进行比较性的动作层面探索。\n    *   这种方式在不导致无限制熵增长的情况下，加速了解决方案的迭代。\n\n#### 4. 训练稳定性增强\n\n为了进一步稳定训练，SPEAR采取了以下措施：\n\n*   **优势值重新校准：** 重新校准了回放缓冲区中经验的优势值，以有效解决潜在的策略漂移问题。\n*   **正则化措施：** 引入了正则化措施，例如对概率和优势之间具有高协方差的token进行裁剪，以在轨迹层面控制熵，从而抑制智能体可能出现的过度自信（over-confidence）。\n\n#### 5. 总结\n\nSPEAR通过课程化的自我模仿学习，有效地平衡了智能体强化学习中的探索与利用。它利用内在奖励促进技能探索，并通过自我模仿加速动作探索，并通过优势值校准和正则化提升了训练的稳定性，为智能体LLMs在复杂任务中的学习提供了更稳健的范式。",
      "shortSummary": "本文提出了SPEAR，一种用于智能体强化学习的课程式自我模仿学习（SIL）方法。针对LLMs在长周期、稀疏奖励任务中探索-利用的挑战，SPEAR通过渐进式探索-利用平衡来解决现有方法的不稳定性。它利用内在奖励促进技能层面探索，并通过自我模仿加速动作层面探索。为稳定训练，SPEAR还重新校准经验优势值并引入正则化，有效避免了熵崩溃或发散，提升了智能体学习效率和稳定性。",
      "translated_title": "掌握要领，信任成功：智能体强化学习中渐进式探索的自我模仿",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence."
    },
    {
      "title": "EPO：LLM智能体强化学习的熵正则化策略优化 (原标题: EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.22576",
      "pubDate": "Fri, 26 Sep 2025 12:51:44 GMT",
      "isoDate": "2025-09-26T12:51:44.000Z",
      "creator": "Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris",
      "summary": "## EPO：LLM智能体强化学习的熵正则化策略优化\n\n本文提出了一种名为“熵正则化策略优化”（EPO）的通用框架，旨在解决大型语言模型（LLM）智能体在多轮、稀疏奖励环境中进行强化学习所面临的挑战。\n\n### 核心问题：探索-利用级联失败\n\n在需要30多轮交互才能完成单一任务的多轮、稀疏奖励环境中训练LLM智能体，对强化学习提出了根本性挑战。作者识别出一种独特的失败模式，称之为“探索-利用级联失败”，该失败模式包含两个阶段：\n\n1.  **早期策略过早收敛**：由于稀疏的反馈，智能体过早地收敛到有缺陷的、低熵策略。\n2.  **后期策略崩溃**：此时，传统的熵正则化变得适得其反，反而促进了混乱的探索，从而破坏了训练的稳定性。\n\n### 提出的解决方案：熵正则化策略优化（EPO）\n\nEPO通过以下三个协同机制打破了这种失败循环：\n\n1.  **多轮设置中的熵正则化**：在多轮环境中采用熵正则化来增强探索能力。\n2.  **熵平滑正则器**：该正则器将策略熵限制在历史平均值范围内，以防止策略熵的突然波动。\n3.  **自适应阶段加权**：在训练过程中平衡探索和利用。\n\n### 理论与性能\n\n*   **理论保证**：分析表明，EPO在保持收敛性的同时，能保证熵方差单调递减。\n*   **性能提升**：\n    *   在ScienceWorld上实现了高达152%的性能提升。\n    *   在ALFWorld上实现了高达19.8%的性能提升。\n\n### 结论\n\n这项工作表明，多轮、稀疏奖励设置需要与传统强化学习根本不同的熵控制方法，这对LLM智能体训练具有广泛的意义。",
      "shortSummary": "本文提出了“熵正则化策略优化”（EPO）框架，以解决LLM智能体在多轮、稀疏奖励环境中训练时面临的“探索-利用级联失败”问题。该失败模式包括早期策略过早收敛和后期策略崩溃。EPO通过在多轮设置中采用熵正则化、熵平滑正则器和自适应阶段加权来解决此问题。EPO在ScienceWorld上实现了高达152%的性能提升，在ALFWorld上提升了19.8%，表明多轮稀疏奖励环境需要独特的熵控制策略。",
      "translated_title": "EPO：LLM智能体强化学习的熵正则化策略优化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training."
    },
    {
      "title": "多模态大语言模型关注何处，依赖何物：解释自回归令牌生成 (原标题: Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation)",
      "link": "https://arxiv.org/abs/2509.22496",
      "pubDate": "Fri, 26 Sep 2025 11:38:42 GMT",
      "isoDate": "2025-09-26T11:38:42.000Z",
      "creator": "Ruoyu Chen, Xiaoqing Guo, Kangwei Liu, Siyuan Liang, Shiming Liu, Qunli Zhang, Hua Zhang, Xiaochun Cao",
      "summary": "本文介绍了一个名为EAGLE的轻量级黑盒框架，旨在解释多模态大语言模型（MLLMs）中的自回归令牌生成过程。\n\n**背景与挑战**\n*   MLLMs在将视觉输入与自然语言输出对齐方面表现出色。\n*   然而，生成的令牌对视觉模态的依赖程度尚不明确，这限制了模型的可解释性和可靠性。\n\n**EAGLE框架的核心功能**\n*   **令牌归因**：EAGLE能够将任何选定的令牌归因于紧凑的感知区域。\n*   **模态影响力量化**：它量化了语言先验和感知证据的相对影响力，从而揭示令牌的生成依赖于何种信息。\n*   **统一目标函数**：框架引入了一个目标函数，该函数统一了充分性（洞察力分数）和不可或缺性（必要性分数）。\n*   **优化方法**：通过对稀疏化图像区域进行贪婪搜索来优化目标函数，以实现忠实且高效的归因。\n\n**超越空间归因的分析**\n*   除了空间归因，EAGLE还执行模态感知分析，以解耦令牌所依赖的信息。\n*   这提供了对模型决策的细粒度可解释性。\n\n**实验结果与优势**\n*   在开源MLLMs上的广泛实验表明，EAGLE在忠实性、定位和幻觉诊断方面始终优于现有方法。\n*   同时，EAGLE所需的GPU内存显著减少，这凸显了其有效性和实用性。\n\n**结论**\n*   EAGLE框架为提升MLLMs的可解释性提供了一种有效且实用的方法。",
      "shortSummary": "本文提出了EAGLE，一个轻量级黑盒框架，用于解释多模态大语言模型（MLLMs）的自回归令牌生成。EAGLE能将令牌归因于感知区域，量化语言先验和视觉证据的影响力，并通过统一目标函数优化归因。它还进行模态感知分析，提升了模型决策的细粒度可解释性。实验证明，EAGLE在忠实性、定位和幻觉诊断方面优于现有方法，且显著节省GPU内存，有效提升了MLLMs的可解释性。",
      "translated_title": "多模态大语言模型关注何处，依赖何物：解释自回归令牌生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE."
    },
    {
      "title": "LucidFlux：通过大规模扩散Transformer实现无字幕通用图像修复 (原标题: LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer)",
      "link": "https://arxiv.org/abs/2509.22414",
      "pubDate": "Fri, 26 Sep 2025 10:39:08 GMT",
      "isoDate": "2025-09-26T10:39:08.000Z",
      "creator": "Song Fei, Tian Ye, Lujia Wang, Lei Zhu",
      "summary": "LucidFlux：通过大规模扩散Transformer实现无字幕通用图像修复\n\n**1. 背景与挑战**\n\n*   **通用图像修复 (UIR) 目标：** 旨在恢复被未知混合退化（如模糊、噪声、压缩等）的图像，同时保留其语义内容。\n*   **现有方法局限性：** 判别式修复器和基于UNet的扩散先验模型在处理未知退化时，常出现过平滑、幻觉或漂移等问题。\n\n**2. LucidFlux 框架介绍**\n\n*   **核心思想：** LucidFlux 提出了一种无字幕的通用图像修复框架，它在不依赖图像字幕的情况下，适应并利用了大型扩散Transformer (Flux.1) 的能力。\n*   **创新点：** 该框架的核心在于其精巧的条件作用机制，强调了在大型扩散Transformer (DiTs) 中，如何、何时以及何地进行条件作用，是实现鲁棒、无字幕通用图像修复的关键，而非简单地增加参数或依赖文本提示。\n\n**3. 关键组件与技术细节**\n\n*   **轻量级双分支条件器 (Dual-branch Conditioner)：**\n    *   **功能：** 注入来自退化输入和轻度修复代理（proxy）的信号。\n    *   **作用：** 分别用于锚定图像的几何结构和抑制伪影。\n*   **时间步和层级自适应调制调度 (Timestep- and Layer-adaptive Modulation Schedule)：**\n    *   **设计目的：** 将上述条件线索（cues）路由到骨干网络的各个层级。\n    *   **效果：** 实现从粗到精、上下文感知的更新，从而在恢复图像纹理的同时保护其全局结构。\n*   **无字幕语义对齐 (Caption-free Semantic Alignment)：**\n    *   **问题：** 避免文本提示或多模态大语言模型 (MLLM) 字幕带来的延迟和不稳定性。\n    *   **解决方案：** 通过从代理图像中提取 SigLIP 特征来强制执行语义对齐，无需依赖文本信息。\n*   **可扩展的数据筛选管道 (Scalable Curation Pipeline)：**\n    *   **目的：** 进一步筛选大规模数据，以提供结构丰富的监督信号。\n\n**4. 实验结果与验证**\n\n*   **性能表现：** 在合成数据集和真实世界基准测试中，LucidFlux 持续优于强大的开源和商业基线模型。\n*   **消融研究：** 验证了框架中每个组件的必要性，证明了其设计的有效性。\n\n**5. 结论**\n\n*   LucidFlux 表明，对于大型扩散Transformer，关键在于“何时、何地以及何种方式”进行条件作用，而非仅仅增加参数或依赖文本提示，这才是实现野外鲁棒、无字幕通用图像修复的决定性因素。",
      "shortSummary": "LucidFlux提出了一种无字幕的通用图像修复(UIR)框架，通过适应大型扩散Transformer (Flux.1)来解决未知图像退化问题。它引入了轻量级双分支条件器以锚定几何并抑制伪影，并设计了时间步和层级自适应调制调度以实现精细化修复。为避免文本提示的局限性，LucidFlux利用SigLIP特征进行无字幕语义对齐。该方法在各项基准测试中表现优异，证明了对大型DiTs而言，条件作用的时机、位置和方式是实现鲁棒UIR的关键。",
      "translated_title": "LucidFlux：通过大规模扩散Transformer实现无字幕通用图像修复",
      "images": [],
      "contentSource": "完整文章",
      "content": "Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild."
    },
    {
      "title": "HiGS：历史引导采样，即插即用地增强扩散模型 (原标题: HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models)",
      "link": "https://arxiv.org/abs/2509.22300",
      "pubDate": "Fri, 26 Sep 2025 09:01:10 GMT",
      "isoDate": "2025-09-26T09:01:10.000Z",
      "creator": "Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber",
      "summary": "### HiGS：历史引导采样，即插即用地增强扩散模型\n\n**1. 问题背景**\n\n扩散模型在图像生成领域取得了显著进展，但其输出仍可能存在以下问题：\n*   **不真实性：** 图像可能显得不够真实。\n*   **细节缺失：** 尤其是在使用较少神经函数评估（NFEs）或较低引导尺度时，图像会缺乏精细细节。\n\n**2. 解决方案：历史引导采样 (HiGS)**\n\n为解决上述问题，本文提出了一种新颖的基于动量的采样技术，称为历史引导采样（HiGS）。\n\n**3. HiGS 工作原理**\n\n*   **整合近期预测：** HiGS通过将最近的模型预测整合到每个推理步骤中，从而提高扩散采样的质量和效率。\n*   **引导采样过程：** 它利用当前预测与过去预测加权平均值之间的差异，来引导采样过程，使其生成更真实、细节更好、结构更优的输出。\n\n**4. 主要优势与特点**\n\nHiGS作为一种即插即用（plug-and-play）的增强方案，具有以下显著优势：\n*   **计算开销极低：** 几乎不引入额外的计算。\n*   **无缝集成：** 可无缝集成到现有扩散框架中，无需进行修改。\n*   **无需额外训练/微调：** 不需要额外的模型训练或微调。\n*   **性能显著提升：**\n    *   在各种模型和架构下，以及在不同的采样预算和引导尺度下，HiGS持续改进图像质量。\n    *   使用预训练的SiT模型，HiGS在256x256无引导ImageNet生成任务中，仅用30个采样步骤（而非标准的250个），就达到了1.61的全新最先进FID分数。\n\n**5. 结论**\n\nHiGS是一种高效且易于部署的标准扩散采样增强方案，能够实现更快、更高保真度的图像生成。",
      "shortSummary": "HiGS是一种新颖的基于动量的历史引导采样技术，旨在解决扩散模型在低NFE或低引导尺度下生成图像不真实、缺乏细节的问题。它通过整合近期预测来引导采样过程，几乎不增加计算开销，且无需额外训练或微调即可无缝集成。HiGS显著提升了图像质量和效率，实现了更快、更高保真度的生成，并在ImageNet生成任务中以更少步骤达到了最先进的FID分数，是一种即插即用的扩散模型增强方案。",
      "translated_title": "HiGS：历史引导采样，即插即用地增强扩散模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256times256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity."
    },
    {
      "title": "MesaTask：通过3D空间推理实现任务驱动的桌面场景生成 (原标题: MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning)",
      "link": "https://arxiv.org/abs/2509.22281",
      "pubDate": "Fri, 26 Sep 2025 08:46:00 GMT",
      "isoDate": "2025-09-26T08:46:00.000Z",
      "creator": "Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang",
      "summary": "### MesaTask：通过3D空间推理实现任务驱动的桌面场景生成\n\n**背景与问题**\n\n*   机器人要能理解人类指令并执行操作任务，需要大量任务相关的桌面场景进行训练。\n*   传统场景生成方法存在局限性：\n    *   **手动布局设计**：耗时且效率低下。\n    *   **纯随机布局**：生成的场景可能缺乏物理合理性或与特定任务不符。\n\n**新任务的提出**\n\n*   本文提出了一项新颖的任务：**任务导向的桌面场景生成**。\n*   这项任务面临显著挑战，因为高级任务指令与具体的3D桌面场景之间存在巨大的语义鸿沟。\n\n**MesaTask-10K 数据集**\n\n*   为支持这项具有挑战性的研究，作者引入了**MesaTask-10K**，一个大规模合成数据集。\n*   **规模**：包含大约10,700个合成桌面场景。\n*   **特点**：这些场景的布局经过人工精心制作，确保了：\n    *   高度的真实感。\n    *   复杂的物体间关系。\n\n**核心方法：空间推理链 (Spatial Reasoning Chain)**\n\n*   为弥合任务与场景之间的鸿沟，论文提出了一种**空间推理链**。\n*   该推理链将场景生成过程分解为以下关键步骤：\n    1.  **物体推理 (Object Inference)**：根据任务描述推断所需的物体。\n    2.  **空间相互关系推理 (Spatial Interrelation Reasoning)**：确定物体之间的空间关系。\n    3.  **场景图构建 (Scene Graph Construction)**：基于推理结果构建场景图，指导最终的3D布局。\n\n**MesaTask 框架**\n\n*   本文提出了**MesaTask**，一个基于大型语言模型（LLM）的框架。\n*   **核心机制**：MesaTask 利用上述空间推理链来指导场景生成。\n*   **增强技术**：该框架通过**DPO（Direct Preference Optimization）算法**进一步增强，以优化生成结果。\n*   **目标**：生成物理上合理且与给定任务描述高度一致的桌面场景。\n\n**实验结果与贡献**\n\n*   详尽的实验表明，MesaTask 在生成符合任务且布局真实的桌面场景方面，表现出优于基线方法的卓越性能。\n*   **主要贡献**：\n    *   定义了任务导向的桌面场景生成这一新任务。\n    *   提供了大规模、高质量的MesaTask-10K数据集。\n    *   提出了创新的空间推理链方法。\n    *   开发了高效的MesaTask LLM框架。\n\n**其他信息**\n\n*   该研究已被NeurIPS 2025接受。",
      "shortSummary": "本文提出MesaTask框架，旨在解决机器人训练中任务驱动桌面场景生成的问题。为弥合高级任务指令与3D场景间的鸿沟，研究引入了包含10,700个真实布局场景的MesaTask-10K数据集，并设计了“空间推理链”将生成过程分解为物体推理、空间关系推理和场景图构建。MesaTask是一个基于LLM并由DPO算法增强的框架，实验证明其在生成物理合理且与任务对齐的桌面场景方面，表现优于现有基线方法。",
      "translated_title": "MesaTask：通过3D空间推理实现任务驱动的桌面场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at https://mesatask.github.io/"
    },
    {
      "title": "FlashEdit: 解耦速度、结构和语义以实现精确图像编辑 (原标题: FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing)",
      "link": "https://arxiv.org/abs/2509.22244",
      "pubDate": "Fri, 26 Sep 2025 07:59:30 GMT",
      "isoDate": "2025-09-26T07:59:30.000Z",
      "creator": "Junyi Wu, Zhiteng Li, Haotong Qin, Xiaohong Liu, Linghe Kong, Yulun Zhang, Xiaokang Yang",
      "summary": "# FlashEdit：实现高保真实时图像编辑的新框架\n\n## 引言\n扩散模型驱动的文本引导图像编辑在质量上取得了显著成就，但其高昂的延迟严重阻碍了实际应用。为了解决这一问题，我们引入了FlashEdit，一个旨在实现高保真、实时图像编辑的新颖框架。\n\n## 核心创新\nFlashEdit的效率源于其三大关键创新：\n\n1.  **一步反演与编辑（One-Step Inversion-and-Editing, OSIE）流程：**\n    *   该流程绕过了传统方法中耗时的迭代过程，从而显著提升了编辑效率。\n\n2.  **背景防护罩（Background Shield, BG-Shield）技术：**\n    *   通过仅在编辑区域内选择性地修改特征，BG-Shield技术确保了背景的完整保留，有效避免了不必要的背景改动。\n\n3.  **稀疏空间交叉注意力（Sparsified Spatial Cross-Attention, SSCA）机制：**\n    *   SSCA机制通过抑制语义信息向背景区域的泄漏，保证了编辑的精确性和局部性，防止了语义信息扩散到非编辑区域。\n\n## 性能与实验结果\n广泛的实验证明，FlashEdit在保持卓越的背景一致性和结构完整性的同时，能够以极高的速度完成图像编辑：\n\n*   **编辑速度：** 在0.2秒内完成编辑。\n*   **速度提升：** 与先前的多步方法相比，速度提升超过150倍。\n\n## 代码可用性\nFlashEdit的代码将公开发布。",
      "shortSummary": "FlashEdit是一个旨在实现高保真、实时图像编辑的新框架，解决了扩散模型图像编辑速度慢的问题。它通过一步反演与编辑流程、背景防护罩技术和稀疏空间交叉注意力机制，实现了速度、结构和语义的解耦。FlashEdit能在0.2秒内完成编辑，比现有方法快150多倍，同时保持了卓越的背景一致性和结构完整性。",
      "translated_title": "FlashEdit: 解耦速度、结构和语义以实现精确图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150times speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit."
    },
    {
      "title": "MinerU2.5：一种用于高效高分辨率文档解析的解耦视觉-语言模型 (原标题: MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing)",
      "link": "https://arxiv.org/abs/2509.22186",
      "pubDate": "Fri, 26 Sep 2025 06:45:48 GMT",
      "isoDate": "2025-09-26T06:45:48.000Z",
      "creator": "Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, Conghui He",
      "summary": "## MinerU2.5 模型概述\n\nMinerU2.5 是一种参数量为 1.2B 的文档解析视觉-语言模型，旨在实现最先进的识别精度，同时保持卓越的计算效率。该模型通过创新的两阶段解析策略，在文档处理领域取得了显著进展。\n\n### 核心解析策略：粗到细的两阶段解耦方法\n\nMinerU2.5 的核心在于其独特的粗到细两阶段解析策略，该策略将全局布局分析与局部内容识别解耦开来，有效解决了高分辨率文档处理的计算挑战：\n\n1.  **第一阶段：高效的全局布局分析**\n    *   模型首先对下采样的图像进行高效的布局分析。\n    *   此阶段的目标是识别文档中的结构元素，例如段落、标题、表格和公式区域。\n    *   通过在低分辨率图像上操作，该阶段成功规避了处理高分辨率输入所带来的巨大计算开销。\n\n2.  **第二阶段：精细的目标内容识别**\n    *   在第一阶段获得的全局布局指导下，模型进入第二阶段。\n    *   此阶段对从原始图像中提取的原生分辨率裁剪区域进行有针对性的内容识别。\n    *   这种方法能够保留密集的文本、复杂的公式和表格中的细粒度细节，确保识别的准确性。\n\n### 数据引擎与训练\n\n为了支持这种高效的解析策略，研究团队开发了一个全面的数据引擎。该引擎能够生成多样化、大规模的训练语料库，用于模型的预训练和微调，从而确保 MinerU2.5 能够学习并适应各种文档结构和内容。\n\n### 性能表现\n\nMinerU2.5 展示了强大的文档解析能力，并在多个基准测试中取得了最先进的性能：\n\n*   **识别精度**：超越了通用模型和特定领域模型在各种识别任务上的表现。\n*   **计算效率**：在实现卓越性能的同时，显著降低了计算开销，体现了其高效性。",
      "shortSummary": "MinerU2.5 是一种1.2B参数的视觉-语言模型，专为高效高分辨率文档解析设计。它采用粗到细的两阶段解耦策略：首先在下采样图像上进行高效布局分析，然后根据布局指导，对原生分辨率区域进行精细内容识别。该模型通过专用数据引擎训练，在多个基准测试中实现了最先进的识别精度，并显著降低了计算开销，超越了现有通用和领域特定模型。",
      "translated_title": "MinerU2.5：一种用于高效高分辨率文档解析的解耦视觉-语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language model that achieves state-of-the-art recognition accuracy while maintaining exceptional computational efficiency. Our approach employs a coarse-to-fine, two-stage parsing strategy that decouples global layout analysis from local content recognition. In the first stage, the model performs efficient layout analysis on downsampled images to identify structural elements, circumventing the computational overhead of processing high-resolution inputs. In the second stage, guided by the global layout, it performs targeted content recognition on native-resolution crops extracted from the original image, preserving fine-grained details in dense text, complex formulas, and tables. To support this strategy, we developed a comprehensive data engine that generates diverse, large-scale training corpora for both pretraining and fine-tuning. Ultimately, MinerU2.5 demonstrates strong document parsing ability, achieving state-of-the-art performance on multiple benchmarks, surpassing both general-purpose and domain-specific models across various recognition tasks, while maintaining significantly lower computational overhead."
    }
  ],
  "lastUpdated": "2025-09-29T09:38:15.966Z"
}