{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统 (原标题: TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System)",
      "link": "https://arxiv.org/abs/2511.02832",
      "pubDate": "Tue, 04 Nov 2025 13:58:35 GMT",
      "isoDate": "2025-11-04T13:58:35.000Z",
      "creator": "Yanjie Ze, Siheng Zhao, Weizhuo Wang, Angjoo Kanazawa, Rocky Duan, Pieter Abbeel, Guanya Shi, Jiajun Wu, C. Karen Liu",
      "summary": "# TWIST2：可扩展、便携、整体式人形机器人数据采集系统\n\n## 1. 背景与挑战\n*   大规模数据已推动机器人技术取得突破，例如语言模型和双臂操作中的视觉-语言-动作模型。\n*   然而，人形机器人领域缺乏同样有效的数据采集框架。\n*   现有的人形机器人遥操作系统存在局限性：\n    *   采用解耦控制。\n    *   依赖昂贵的运动捕捉（mocap）设备。\n\n## 2. TWIST2 系统介绍\n*   **TWIST2** 是一个便携、无需运动捕捉的人形机器人遥操作和数据采集系统。\n*   它在提高可扩展性的同时，保留了完整的人形机器人全身控制能力。\n\n## 3. 核心技术与组件\n*   **全身运动获取**：利用 PICO4U VR 设备实时获取人类全身运动。\n*   **自我中心视觉**：配备定制的2自由度机器人颈部（成本约250美元），用于实现自我中心视觉。\n*   **整体控制**：实现从人类到人形机器人的整体控制。\n\n## 4. 系统性能与成果\n*   **技能演示**：成功展示了长周期、灵巧且移动的人形机器人技能。\n*   **数据采集效率**：\n    *   能够在15分钟内收集100个演示。\n    *   成功率接近100%。\n\n## 5. 分层视觉运动策略框架\n*   基于 TWIST2 采集的数据，研究人员提出了一个分层视觉运动策略框架。\n*   该框架能够基于自我中心视觉自主控制人形机器人的整个身体。\n*   **策略演示**：成功展示了全身灵巧操作和动态踢球任务。\n\n## 6. 开源与可复现性\n*   整个系统是完全可复现的，并已开源。\n*   系统代码库：[this https URL](this https URL)\n*   采集的数据集也已开源：[this https URL](this https URL)",
      "shortSummary": "TWIST2是一个创新的人形机器人数据采集系统，解决了现有系统昂贵或控制解耦的问题。它利用PICO4U VR和定制机器人颈部，实现便携、无需运动捕捉的全身遥操作，并能高效收集数据（15分钟内100个演示，成功率近100%）。基于此，TWIST2提出了一个分层视觉运动策略，使机器人能自主完成灵巧操作和动态踢球任务。系统和数据集均已开源，具有高度可复现性。",
      "translated_title": "TWIST2：可扩展、便携、整体式人形机器人数据采集系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io ."
    },
    {
      "title": "当可视化是推理的第一步：MIRA，一个视觉思维链基准 (原标题: When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought)",
      "link": "https://arxiv.org/abs/2511.02779",
      "pubDate": "Tue, 04 Nov 2025 13:00:51 GMT",
      "isoDate": "2025-11-04T13:00:51.000Z",
      "creator": "Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, Haoqi Fan, Cihang Xie, Huaxiu Yao, Qinghao Ye",
      "summary": "### MIRA：一个视觉思维链基准\n\nMIRA（When Visualizing is the First Step to Reasoning）是一个新提出的基准，旨在评估模型在需要生成中间视觉图像才能成功推理的场景中的能力。\n\n#### 核心理念与区别\n\n*   **超越传统思维链（CoT）**：与仅依赖文本的传统CoT方法不同，MIRA中的任务要求模型生成并利用草图、结构图或路径图等中间图像来指导其推理过程。\n*   **模拟人类认知**：这种设置紧密模仿了人类通过“边画边思考”来解决复杂问题的方式。\n\n#### 任务特点与数据\n\n*   **挑战性任务**：MIRA专注于本质上具有挑战性、涉及复杂结构、空间关系或难以仅通过语言表达的推理步骤的任务。\n*   **高质量数据**：基准包含546个多模态问题，这些问题都标注了中间视觉图像和最终答案，以确保评估数据的高质量。\n\n#### 评估协议\n\nMIRA提出了一个统一的评估协议，涵盖三个层级的评估输入：\n\n1.  **直接输入**：仅包含图像和问题。\n2.  **纯文本CoT输入**：包含图像和文本思考提示。\n3.  **视觉CoT输入**：同时包含标注的图像线索和文本思考提示。\n\n#### 实验结果与发现\n\n*   **现有模型表现**：实验结果表明，包括最强大的私有模型和强大的开源模型在内的现有多模态大型语言模型，在仅依赖文本提示时表现不佳。\n*   **视觉线索的关键作用**：当提供中间视觉线索时，模型性能持续提升，在所有模型和任务中平均相对增益达到33.7%。\n*   **上限探测**：通过扩展搜索空间和设计与视觉CoT对齐的文本提示来探测模型能力上限，但与视觉CoT设置相比，这些方法仅带来了有限的改进。\n*   **核心结论**：这些结果强调了“想象的视觉信息”在MIRA上实现成功推理的关键作用。",
      "shortSummary": "MIRA是一个新基准，旨在评估模型在需要生成中间视觉图像以进行推理的场景中的能力。与传统文本CoT不同，MIRA要求模型利用草图、图表等视觉线索来指导复杂问题的解决，模拟人类“边画边思考”。实验表明，现有模型在仅依赖文本提示时表现不佳，但当提供中间视觉线索时，性能平均提升33.7%。这凸显了想象的视觉信息在推理中的关键作用。",
      "translated_title": "当可视化是推理的第一步：MIRA，一个视觉思维链基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA."
    },
    {
      "title": "VCode：一个以SVG作为符号视觉表示的多模态编码基准 (原标题: VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation)",
      "link": "https://arxiv.org/abs/2511.02778",
      "pubDate": "Tue, 04 Nov 2025 13:00:18 GMT",
      "isoDate": "2025-11-04T13:00:18.000Z",
      "creator": "Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang",
      "summary": "# VCode：一个以SVG作为符号视觉表示的多模态编码基准\n\n## 引言\n在智能体时代，代码已成为进行推理和行动的精确且可执行的媒介。然而，目前的研究进展主要集中于程序合成和调试等语言中心任务，而视觉中心编码领域仍未得到充分探索。受人类通过草图进行推理的启发，本文倡导将SVG（可缩放矢量图形）代码作为一种紧凑、可解释且可执行的视觉表示形式。\n\n## VCode基准介绍\n本文引入了VCode，这是一个将多模态理解重构为代码生成任务的基准：给定一张图像，模型必须生成SVG代码，以保留其符号意义，从而支持后续的推理任务。VCode基准涵盖了三个主要领域：\n*   **通用常识：** 基于MM-Vet数据集。\n*   **专业学科：** 基于MMMU数据集。\n*   **视觉中心感知：** 基于CV-Bench数据集。\n\n## CodeVQA评估协议\n为了评估SVG代码的符号保真度，研究提出了一种新颖的评估协议——CodeVQA。在该协议中，一个策略模型需要对渲染后的SVG图像回答问题；正确的答案表明SVG代码忠实地保留了原始图像的符号信息。\n\n## 实验发现\n实证结果表明，当前的前沿视觉语言模型（VLMs）在生成忠实的SVG方面表现不佳，这揭示了语言中心编码与视觉中心编码之间存在的显著差距。\n\n## VCoder框架：弥合差距\n为了弥合这一差距，研究引入了VCoder，这是一个智能体框架，它通过两个主要方面增强了VLMs的能力：\n1.  **思考与修订 (Thinking with Revision)：** VCoder能够迭代地分析SVG代码与原始图像之间的差异，并对SVG代码进行精炼和修正。\n2.  **借助视觉工具行动 (Acting with Visual Tools)：** VCoder利用检测器和解析器等视觉工具，提供结构化的线索，如对象、形状和文本信息，这些能力超出了VLM固有的处理范围。\n\n## VCoder性能与人类研究\n*   尽管具有强大推理能力的前沿VLMs在整体上表现良好，但在专业知识和3D推理方面仍存在局限性。\n*   VCoder在基准测试中取得了显著成果，比表现最佳的Claude-4-Opus模型整体性能提升了12.3个百分点。\n*   人类研究表明，人类和VLMs在处理渲染后的SVG时表现均有所下降，但它们之间的一致性揭示了符号视觉表示的巨大潜力。\n\n## 资源可用性\nVCode基准和相关代码已公开发布，可在项目页面和GitHub上获取。",
      "shortSummary": "VCode是一个创新性的多模态编码基准，旨在通过将图像转换为SVG代码来弥补视觉中心编码的不足。它将多模态理解重构为代码生成任务，并引入CodeVQA协议评估SVG的符号保真度。鉴于现有VLMs在此任务上的局限性，研究提出了VCoder框架，通过迭代修订和视觉工具增强VLMs。VCoder显著提升了性能，比Claude-4-Opus高出12.3个百分点，揭示了SVG作为符号视觉表示的巨大潜力，并推动了视觉中心编码领域的发展。",
      "translated_title": "VCode：一个以SVG作为符号视觉表示的多模态编码基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode."
    },
    {
      "title": "VidEmo：基于情感树推理的以情感为中心的视频基础模型 (原标题: VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models)",
      "link": "https://arxiv.org/abs/2511.02712",
      "pubDate": "Tue, 04 Nov 2025 11:31:09 GMT",
      "isoDate": "2025-11-04T11:31:09.000Z",
      "creator": "Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang",
      "summary": "## VidEmo：基于情感树推理的以情感为中心的视频基础模型\n\n### 摘要\n\n本文介绍了一种名为 VidEmo 的新型视频情感基础模型，旨在解决视频中情感理解和预测的固有挑战。随着视频大语言模型（VideoLLMs）的进步，视频情感分析受到了广泛关注，但情感的动态性和对线索的依赖性使得理解复杂且不断演变的情感状态变得困难。\n\n### 核心方法：情感线索引导推理框架\n\n为了应对这些挑战，研究团队提出了一种新颖的“情感线索引导推理框架”。该框架以分阶段的方式统一了以下三个关键方面：\n\n*   **基本属性感知：** 识别视频中的基础视觉和听觉属性。\n*   **表情分析：** 深入分析面部表情、肢体语言等情感表达。\n*   **高层情感理解：** 基于感知和分析结果，进行复杂的情感状态推理。\n\n### VidEmo 模型设计与训练\n\nVidEmo 是该方法的核心，是一系列专门为情感推理和指令遵循设计的视频情感基础模型。这些模型经过一个独特的两阶段调优过程：\n\n1.  **课程情感学习（Curriculum Emotion Learning）：** 在此阶段，模型被注入基础的情感知识，逐步学习识别和理解各种情感线索。\n2.  **情感树强化学习（Affective-Tree Reinforcement Learning）：** 这一阶段侧重于情感推理能力的培养。模型通过强化学习，利用“情感树”结构进行更深层次、更具逻辑性的情感状态推断。\n\n### 基础数据基础设施：Emo-CFG 数据集\n\n为了支持 VidEmo 模型的训练和评估，研究团队建立了一个强大的基础数据基础设施，并引入了一个全新的数据集——“以情感为中心的细粒度数据集”（Emo-CFG）。\n\n*   **规模与多样性：** Emo-CFG 包含210万个多样化的基于指令的样本。\n*   **丰富的内容：** 该数据集提供了以下关键资源，对于推进情感理解任务至关重要：\n    *   **可解释的情感问答：** 包含关于视频情感的问答对，并附带解释。\n    *   **细粒度字幕：** 对视频内容进行详细的情感相关描述。\n    *   **相关推理依据：** 提供支持情感判断的理由和线索。\n\n### 实验结果与贡献\n\n实验结果表明，VidEmo 方法取得了具有竞争力的性能。它在15项面部感知任务中树立了新的里程碑，显著提升了视频情感理解和预测的能力。这标志着视频情感基础模型领域的一个重要进展。",
      "shortSummary": "VidEmo是一个新的以情感为中心的视频基础模型，旨在解决视频情感理解的挑战。它采用情感线索引导推理框架，并通过两阶段调优（课程情感学习和情感树强化学习）来注入情感知识和提升推理能力。研究团队还构建了包含210万样本的Emo-CFG数据集，用于情感问答和细粒度字幕。实验结果表明，VidEmo在15项面部感知任务中表现出色，树立了新里程碑，显著提升了视频情感理解能力。",
      "translated_title": "VidEmo：基于情感树推理的以情感为中心的视频基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks."
    },
    {
      "title": "视觉输入可以被压缩吗？大型多模态模型的视觉令牌压缩基准 (原标题: Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models)",
      "link": "https://arxiv.org/abs/2511.02650",
      "pubDate": "Tue, 04 Nov 2025 10:17:06 GMT",
      "isoDate": "2025-11-04T10:17:06.000Z",
      "creator": "Tianfan Peng, Yuntao Du, Pengzhou Ji, Shijie Dong, Kailin Jiang, Mingchuan Ma, Yijun Tian, Jinhe Bi, Qian Li, Wei Du, Feng Xiao, Lizhen Cui",
      "summary": "### UniPruneBench：大型多模态模型视觉令牌压缩的统一基准\n\n**1. 背景与问题**\n*   大型多模态模型（LMMs）因图像编码器引入的大量视觉令牌而面临严重的推理效率低下问题。\n*   尽管近期令牌压缩方法（如剪枝和合并）在减少冗余方面显示出潜力，但其评估仍然分散且不一致。\n\n**2. 提出的解决方案：UniPruneBench**\n*   本文提出了UniPruneBench，一个针对多模态LLM中视觉令牌剪枝的统一且可扩展的基准。\n*   **标准化协议**：UniPruneBench在六个能力维度和十个数据集上提供了标准化的评估协议。\n*   **覆盖范围**：\n    *   涵盖了十种具有代表性的压缩算法。\n    *   评估了三大家族的LMMs，包括LLaVA-v1.5、Intern-VL3和Qwen2.5-VL。\n*   **评估指标**：除了传统的任务准确性，UniPruneBench还纳入了系统级指标，如运行时（runtime）和预填充延迟（prefilling latency），以提供全面的视角。\n\n**3. 关键实验发现**\n*   **随机剪枝的有效性**：随机剪枝（random pruning）是一个出人意料的强大基线。\n*   **无普适最佳方法**：没有单一方法能在所有场景中始终优于其他方法。\n*   **任务敏感性差异**：剪枝敏感性在不同任务之间差异显著，其中光学字符识别（OCR）任务最为脆弱。\n*   **剪枝比例的主导作用**：剪枝比例是影响性能下降的主要因素。\n\n**4. 结论与展望**\n*   作者相信UniPruneBench将为未来高效多模态建模的研究奠定可靠的基础。",
      "shortSummary": "大型多模态模型（LMMs）因视觉令牌过多导致推理效率低下。为解决评估碎片化问题，本文提出了UniPruneBench，一个统一的视觉令牌剪枝基准。该基准在多维度、多数据集上评估了十种压缩算法和三大家族LMMs，并纳入系统级指标。研究发现，随机剪枝是强基线，无普适最佳方法，OCR任务对剪枝最敏感，且剪枝比例是影响性能的主导因素。UniPruneBench旨在为高效多模态建模研究提供可靠基础。",
      "translated_title": "视觉输入可以被压缩吗？大型多模态模型的视觉令牌压缩基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling."
    },
    {
      "title": "BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统 (原标题: BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring)",
      "link": "https://arxiv.org/abs/2511.02490",
      "pubDate": "Tue, 04 Nov 2025 06:27:03 GMT",
      "isoDate": "2025-11-04T06:27:03.000Z",
      "creator": "Rajan Das Gupta, Md Kishor Morol, Nafiz Fahad, Md Tanzib Hosain, Sumaya Binte Zilani Choya, Md Jakir Hossen",
      "summary": "## BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统\n\n### 引言\n随着全球阿尔茨海默病（AD）负担的持续增长，早期和准确的检测变得日益关键，尤其是在高级诊断工具可及性有限的地区。为应对这一挑战，研究人员提出了BRAINS（Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening）系统。\n\n### BRAINS系统概述\nBRAINS是一个新颖的系统，它利用大型语言模型（LLMs）强大的推理能力进行阿尔茨海默病的检测和监测。该系统旨在提供可扩展、可解释的早期阿尔茨海默病检测辅助工具。\n\n### 系统架构：双模块设计\nBRAINS系统采用双模块架构，包括认知诊断模块和病例检索模块：\n\n1.  **认知诊断模块 (Cognitive Diagnostic Module)**\n    *   **功能：** 利用LLMs对阿尔茨海默病风险进行结构化评估。\n    *   **技术：** 这些LLMs经过认知和神经影像数据集的微调，包括MMSE（简易精神状态检查）、CDR（临床痴呆评定量表）评分以及脑容量指标等。\n\n2.  **病例检索模块 (Case Retrieval Module)**\n    *   **功能：** 从精心策划的知识库中检索与患者档案相似的病例。\n    *   **过程：** 该模块将患者档案编码为潜在表示，然后检索相似的辅助病例。\n    *   **病例融合层 (Case Fusion Layer)：** 检索到的辅助病例通过病例融合层与输入档案进行融合，以增强对患者情况的上下文理解。\n\n### 推理与评估\n融合后的表示随后与临床提示一起进行处理，以进行最终的推断。在真实世界数据集上的评估结果表明，BRAINS系统在疾病严重程度分类和识别认知衰退早期迹象方面表现出显著的有效性。\n\n### 潜在应用\n该系统不仅展示了作为一种可扩展、可解释的早期阿尔茨海默病检测辅助工具的强大潜力，也为该领域的未来应用带来了希望。\n\n### 出版信息\n该研究已被ICMLA 2025接受发表。",
      "shortSummary": "BRAINS（生物医学检索增强型神经退行性疾病筛查智能系统）是一种利用大型语言模型（LLMs）进行阿尔茨海默病（AD）检测和监测的新型系统。它采用双模块架构：认知诊断模块使用微调的LLMs评估AD风险，病例检索模块通过融合相似病例增强上下文理解。在真实数据集上的评估显示，BRAINS能有效分类疾病严重程度并识别早期认知衰退迹象，有望成为可扩展、可解释的早期AD检测辅助工具。",
      "translated_title": "BRAINS：一种用于阿尔茨海默病检测和监测的检索增强系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "As the global burden of Alzheimer's disease (AD) continues to grow, early and accurate detection has become increasingly critical, especially in regions with limited access to advanced diagnostic tools. We propose BRAINS (Biomedical Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address this challenge. This novel system harnesses the powerful reasoning capabilities of Large Language Models (LLMs) for Alzheimer's detection and monitoring. BRAINS features a dual-module architecture: a cognitive diagnostic module and a case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain volume metrics -- to perform structured assessments of Alzheimer's risk. Meanwhile, the Case Retrieval Module encodes patient profiles into latent representations and retrieves similar cases from a curated knowledge base. These auxiliary cases are fused with the input profile via a Case Fusion Layer to enhance contextual understanding. The combined representation is then processed with clinical prompts for inference. Evaluations on real-world datasets demonstrate BRAINS effectiveness in classifying disease severity and identifying early signs of cognitive decline. This system not only shows strong potential as an assistive tool for scalable, explainable, and early-stage Alzheimer's disease detection, but also offers hope for future applications in the field."
    },
    {
      "title": "ChartM^3：一种多阶段代码驱动的管道，用于构建图表理解中的多维度、多步骤视觉推理数据 (原标题: ChartM^3: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension)",
      "link": "https://arxiv.org/abs/2511.02415",
      "pubDate": "Tue, 04 Nov 2025 04:45:34 GMT",
      "isoDate": "2025-11-04T04:45:34.000Z",
      "creator": "Duo Xu, Hao Cheng, Xin Lin, Zhen Xie, Hao Wang",
      "summary": "# ChartM^3：多维度、多步骤视觉推理数据构建管道\n\n## 摘要\n\n当前的多模态大语言模型（MLLMs）在处理复杂的图表理解任务时，面临着视觉识别和推理能力不足的挑战。现有研究对现实世界应用中普遍存在的复杂图表场景和计算密集型推理任务的覆盖有限。为解决这些局限性，本研究提出了一种自动化的多阶段代码驱动管道，用于系统地生成视觉推理数据集。\n\n## 管道设计与工作原理\n\n该管道集成了以下关键策略：\n\n*   **检索增强生成（RAG）**：用于检索专业的图表模板，确保生成图表的专业性和多样性。\n*   **思维链（CoT）策略**：用于生成推理代码，这些代码能够模拟真实的统计数据分布，从而驱动图表渲染和与问题相关的统计计算。\n\n通过这种方法，管道能够系统地生成高质量的图表及其相关的推理数据。\n\n## 管道优势\n\n*   **增强图表多样性**：通过RAG和CoT策略，管道能够生成更广泛、更多样化的图表类型。\n*   **提高数据质量**：生成的推理代码模拟真实数据分布，确保了数据的真实性和有效性。\n\n## ChartM^3 数据集\n\n基于此框架，研究团队构建了 **ChartM^3** 数据集，其特点如下：\n\n*   **规模**：包含38,000张图表和142,000个问答对，用于模型训练。\n*   **评估样本**：包含2,871个高质量的评估样本，用于实际性能评估。\n*   **特点**：这是一个多维度、多步骤的数据集，旨在测试模型在复杂图表理解中的高级推理能力。\n\n## 实验结果与影响\n\n通过监督微调（SFT）和强化学习（RL）实验，ChartM^3 数据集展现出显著的积极影响：\n\n*   **提升推理能力**：数据集显著提高了模型在图表理解中的推理能力。\n*   **增强跨领域泛化性能**：模型在不同类型和领域的图表上表现出更好的泛化能力。\n*   **赋能小型模型**：使得较小规模的模型在复杂图表理解任务中，能够达到与大型模型相当的性能水平。",
      "shortSummary": "本研究提出了一种自动化的多阶段代码驱动管道，旨在解决多模态大语言模型在复杂图表理解和计算密集型推理任务中的不足。该管道结合检索增强生成（RAG）和思维链（CoT）策略，生成模拟真实数据分布的推理代码，驱动图表渲染和统计计算。基于此，构建了ChartM^3数据集，包含38K图表和142K问答对。实验证明，ChartM^3显著提升了模型的推理能力和跨领域泛化性能，使小型模型在复杂图表理解上能媲美大型模型。",
      "translated_title": "ChartM^3：一种多阶段代码驱动的管道，用于构建图表理解中的多维度、多步骤视觉推理数据",
      "images": [],
      "contentSource": "完整文章",
      "content": "Complex chart understanding tasks demand advanced visual recognition and reasoning capabilities from multimodal large language models (MLLMs). However, current research provides limited coverage of complex chart scenarios and computation-intensive reasoning tasks prevalent in real-world applications. This study proposes an automated multi-stage code-driven pipeline for systematically generating visual reasoning datasets to address these limitations. The pipeline integrates retrieval-augmented generation (RAG) to retrieve professional chart templates and employs chain-of-thought (CoT) strategies to generate reasoning codes that simulate real data distributions, thereby driving chart rendering and question-related statistical computations. Through model-based evaluation, the pipeline enhances chart diversity and data quality. Using this framework, we construct ChartM^3, a multi-dimensional and multi-step dataset containing 38K charts and 142K Q&amp;A pairs for training, along with 2,871 high-quality evaluation samples for enabling practical performance assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL) experiments demonstrate that our dataset significantly improves reasoning capabilities and cross-domain generalization performance, enabling smaller models to achieve performance comparable to larger-scale models in complex chart comprehension."
    },
    {
      "title": "LiveSecBench：一个针对中文语境下大型语言模型的动态且文化相关的AI安全基准 (原标题: LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context)",
      "link": "https://arxiv.org/abs/2511.02366",
      "pubDate": "Tue, 04 Nov 2025 03:44:09 GMT",
      "isoDate": "2025-11-04T03:44:09.000Z",
      "creator": "Yudong Li, Zhongliang Yang, Kejiang Chen, Wenxuan Wang, Tianxin Zhang, Sifang Wan, Kecheng Wang, Haitian Li, Xu Wang, Lefan Cheng, Youdan Yang, Baocheng Chen, Ziyu Liu, Yufei Sun, Liyan Wu, Wenya Wen, Xingchi Gu, Peiru Yang",
      "summary": "## LiveSecBench：中文语境下大型语言模型的AI安全基准\n\n### 引言\nLiveSecBench是一个动态且持续更新的安全基准，专为中文大型语言模型（LLM）的应用场景设计。它旨在评估LLM在中文语境下的安全性。\n\n### 核心特点\n*   **动态性与持续更新：** LiveSecBench通过动态更新计划保持其相关性，旨在纳入新的威胁向量。例如，计划在下次更新中包含文本到图像生成安全（Text-to-Image Generation Safety）和智能体安全（Agentic Safety）。\n*   **文化相关性：** 该基准的评估维度根植于中国的法律和社会框架，确保了其在中文语境下的适用性和准确性。\n\n### 评估维度\nLiveSecBench从以下六个关键维度评估模型：\n1.  **合法性 (Legality)**\n2.  **伦理 (Ethics)**\n3.  **事实性 (Factuality)**\n4.  **隐私 (Privacy)**\n5.  **对抗性鲁棒性 (Adversarial Robustness)**\n6.  **推理安全 (Reasoning Safety)**\n\n### 当前状态\n*   LiveSecBench (v251030) 目前已评估了18个大型语言模型。\n*   它提供了一个中文语境下AI安全的全景图。\n*   评估结果的排行榜已公开，可在指定URL访问。\n\n### 图片\n文章内容中不包含有效的实际图片链接。",
      "shortSummary": "LiveSecBench是一个为中文大型语言模型（LLM）设计的动态、持续更新的AI安全基准。它基于中国的法律和社会框架，从合法性、伦理、事实性、隐私、对抗性鲁棒性和推理安全六个关键维度评估模型。该基准保持动态更新以适应新威胁，并已评估18个LLM，其排行榜已公开，旨在提供中文语境下AI安全的全面视图。",
      "translated_title": "LiveSecBench：一个针对中文语境下大型语言模型的动态且文化相关的AI安全基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we propose LiveSecBench, a dynamic and continuously updated safety benchmark specifically for Chinese-language LLM application scenarios. LiveSecBench evaluates models across six critical dimensions (Legality, Ethics, Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in the Chinese legal and social frameworks. This benchmark maintains relevance through a dynamic update schedule that incorporates new threat vectors, such as the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs, providing a landscape of AI safety in the context of Chinese language. The leaderboard is publicly accessible at https://livesecbench.intokentech.cn/."
    },
    {
      "title": "LTD-Bench：通过让大型语言模型绘图来评估它们 (原标题: LTD-Bench: Evaluating Large Language Models by Letting Them Draw)",
      "link": "https://arxiv.org/abs/2511.02347",
      "pubDate": "Tue, 04 Nov 2025 03:11:23 GMT",
      "isoDate": "2025-11-04T03:11:23.000Z",
      "creator": "Liuhao Lin, Ke Li, Zihan Xu, Yuchen Shi, Yulei Qin, Yan Zhang, Xing Sun, Rongrong Ji",
      "summary": "LTD-Bench是一个突破性的基准测试，旨在解决当前大型语言模型（LLM）评估范式中的关键盲点。目前的评估方法依赖不透明的数值指标，这些指标掩盖了LLM在空间推理方面的根本性局限，并导致报告性能与实际能力之间存在脱节，尤其是在需要理解物理世界的应用中。\n\n**LTD-Bench的创新方法：**\n*   **可视化评估：** LTD-Bench将LLM的评估从抽象分数转变为直接可观察的视觉输出。\n*   **绘图任务：** 要求模型通过点阵或可执行代码生成图画。\n*   **直观揭示局限：** 这种方法使得空间推理的局限性即使对非专业人士也一目了然，从而弥合了统计性能与直观评估之间的根本差距。\n\n**综合评估方法论：**\n*   **互补任务：**\n    *   **生成任务：** 旨在测试模型的空间想象能力。\n    *   **识别任务：** 旨在评估模型的空间感知能力。\n*   **难度分级：** 任务分为三个递进的难度级别。\n*   **双向映射评估：** 系统地评估语言与空间概念之间关键的双向映射能力。\n\n**主要实验发现：**\n*   **惊人的能力差距：** 对最先进模型的广泛实验揭示了一个令人担忧的能力差距。即使在传统基准测试中取得令人印象深刻结果的LLM，在建立语言与空间概念之间的双向映射方面也表现出深刻的不足。\n*   **根本性局限：** 这种局限性削弱了LLM作为真正“世界模型”的潜力。\n\n**额外优势：**\n*   **诊断分析：** LTD-Bench的视觉输出能够实现强大的诊断分析。\n*   **模型相似性研究：** 为研究模型相似性提供了一种潜在方法。\n\n该研究已被NeurIPS 2025接受。",
      "shortSummary": "LTD-Bench是一个创新基准，通过让大型语言模型（LLM）生成图画来评估其空间推理能力，解决了传统评估中空间局限性不透明的问题。它包含生成和识别任务，并分级评估语言与空间概念的双向映射。实验揭示，即使是先进LLM，在这一核心能力上也存在严重不足，这限制了它们作为世界模型的潜力，并为诊断分析提供了新途径。该研究已被NeurIPS 2025接受。",
      "translated_title": "LTD-Bench：通过让大型语言模型绘图来评估它们",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current evaluation paradigms for large language models (LLMs) represent a critical blind spot in AI research--relying on opaque numerical metrics that conceal fundamental limitations in spatial reasoning while providing no intuitive understanding of model capabilities. This deficiency creates a dangerous disconnect between reported performance and practical abilities, particularly for applications requiring physical world understanding. We introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation from abstract scores to directly observable visual outputs by requiring models to generate drawings through dot matrices or executable code. This approach makes spatial reasoning limitations immediately apparent even to non-experts, bridging the fundamental gap between statistical performance and intuitive assessment. LTD-Bench implements a comprehensive methodology with complementary generation tasks (testing spatial imagination) and recognition tasks (assessing spatial perception) across three progressively challenging difficulty levels, methodically evaluating both directions of the critical language-spatial mapping. Our extensive experiments with state-of-the-art models expose an alarming capability gap: even LLMs achieving impressive results on traditional benchmarks demonstrate profound deficiencies in establishing bidirectional mappings between language and spatial concept--a fundamental limitation that undermines their potential as genuine world models. Furthermore, LTD-Bench's visual outputs enable powerful diagnostic analysis, offering a potential approach to investigate model similarity."
    },
    {
      "title": "当模态冲突时：单模态推理不确定性如何支配多模态大语言模型（MLLMs）中的偏好动态 (原标题: When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs)",
      "link": "https://arxiv.org/abs/2511.02243",
      "pubDate": "Mon, 03 Nov 2025 23:11:31 GMT",
      "isoDate": "2025-11-03T23:11:31.000Z",
      "creator": "Zhuoran Zhang, Tengyue Wang, Xilin Gong, Yang Shi, Haotian Wang, Di Wang, Lijie Hu",
      "summary": "### 新框架揭示多模态大语言模型（MLLMs）如何解决模态冲突\n\n多模态大语言模型（MLLMs）在处理不同模态（如视觉和文本）提供矛盾信息时，必须解决冲突，这一过程被称为“模态跟随”。以往的研究仅通过粗略的数据集层面统计来衡量这种行为，忽略了模型在单模态推理中的置信度影响。\n\n本文引入了一个新的框架，将模态跟随分解为两个基本因素：\n*   **相对推理不确定性（Relative Reasoning Uncertainty）**：指单模态预测之间特定案例的置信度差距。\n*   **固有模态偏好（Inherent Modality Preference）**：指当不确定性平衡时，模型所展现出的稳定偏见。\n\n为了验证这一框架，研究人员构建了一个可控数据集，系统地改变视觉和文本输入的推理难度。他们使用熵作为细粒度的不确定性度量，并揭示了一个普遍规律：\n*   **普遍规律**：模型跟随某一模态的概率会随着该模态相对不确定性的增加而单调递减。\n\n在模型倾向于以相似概率跟随两种模态的相对难度水平上，研究定义了一个“平衡点”（balance point）。这个平衡点被认为是模型固有偏好的一个实用指标。与传统的宏观层面比率不同，这种衡量方法提供了一种更具原则性、更少混淆的方式来表征模态偏见，将其与单模态能力和数据集伪影区分开来。\n\n此外，通过探究模型层级间的预测，研究揭示了内部振荡机制：在接近平衡点的模糊区域，模型会在不同层级之间在模态间摇摆不定，这解释了外部观察到的犹豫不决现象。\n\n综上所述，这些发现确立了相对不确定性和固有偏好是模态跟随的两个主导原则，为MLLMs如何解决冲突信息提供了量化框架和机制洞察。",
      "shortSummary": "本文提出一个新框架，将多模态大语言模型（MLLMs）解决模态冲突（“模态跟随”）的行为分解为“相对推理不确定性”和“固有模态偏好”。研究构建可控数据集并发现，模型跟随某一模态的概率随其相对不确定性增加而单调递减。通过“平衡点”可量化固有偏好，该指标比传统方法更具原则性。此外，研究揭示了模型在模糊区域的层间振荡机制。这些发现为理解MLLMs如何处理冲突信息提供了关键洞察。",
      "translated_title": "当模态冲突时：单模态推理不确定性如何支配多模态大语言模型（MLLMs）中的偏好动态",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) must resolve conflicts when different modalities provide contradictory information, a process we term modality following. Prior work measured this behavior only with coarse dataset-level statistics, overlooking the influence of model's confidence in unimodal reasoning. In this paper, we introduce a new framework that decomposes modality following into two fundamental factors: relative reasoning uncertainty (the case-specific confidence gap between unimodal predictions) and inherent modality preference( a model's stable bias when uncertainties are balanced). To validate this framework, we construct a controllable dataset that systematically varies the reasoning difficulty of visual and textual inputs. Using entropy as a fine-grained uncertainty metric, we uncover a universal law: the probability of following a modality decreases monotonically as its relative uncertainty increases. At the relative difficulty level where the model tends to follow both modalities with comparable probability what we call the balance point, a practical indicator of the model's inherent preference. Unlike traditional macro-level ratios, this measure offers a more principled and less confounded way to characterize modality bias, disentangling it from unimodal capabilities and dataset artifacts. Further, by probing layer-wise predictions, we reveal the internal mechanism of oscillation: in ambiguous regions near the balance point, models vacillate between modalities across layers, explaining externally observed indecision. Together, these findings establish relative uncertainty and inherent preference as the two governing principles of modality following, offering both a quantitative framework and mechanistic insight into how MLLMs resolve conflicting information."
    },
    {
      "title": "TabDSR：表格数据中复杂数值推理的分解、清洗与推理 (原标题: TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data)",
      "link": "https://arxiv.org/abs/2511.02219",
      "pubDate": "Mon, 03 Nov 2025 22:13:02 GMT",
      "isoDate": "2025-11-03T22:13:02.000Z",
      "creator": "Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, Jin Zeng",
      "summary": "# TabDSR：表格数据中复杂数值推理的分解、清洗与推理\n\n## 摘要\n\n### 背景与问题\n\n在现实世界的数据分析中，对表格数据进行复杂的数值推理至关重要。然而，大型语言模型（LLMs）在此类任务中常常表现不佳，主要原因包括：\n\n*   **查询复杂性高**：用户提出的问题可能非常复杂，难以直接处理。\n*   **数据存在噪声**：表格数据中可能包含不准确、不完整或无关的信息。\n*   **数值处理能力有限**：LLMs在处理精确的数值计算和逻辑推理方面存在局限性。\n\n### 提出的方法：TabDSR 框架\n\n为解决上述挑战，本文提出了一种名为 **TabDSR** 的新框架。该框架由以下三个核心组件构成：\n\n1.  **查询分解器 (Query Decomposer)**：\n    *   负责将复杂的自然语言查询分解成更小、更易于管理和解决的子问题或步骤。\n2.  **表格清洗器 (Table Sanitizer)**：\n    *   用于识别并清洗表格中的噪声数据。\n    *   过滤掉与当前查询不相关的信息，确保推理过程基于干净、相关的表格内容。\n3.  **思维程序 (Program-of-Thoughts, PoT) 推理器**：\n    *   基于清洗后的表格数据，生成可执行的代码（例如Python代码）。\n    *   通过执行这些代码来逐步推导出最终的答案，从而实现精确的数值推理。\n\n### 新数据集：CalTab151\n\n为了确保对 TabDSR 框架进行公正且无偏的评估，并有效缓解潜在的数据泄露问题，研究者专门引入了一个新的数据集——**CalTab151**。该数据集是为复杂的表格数值推理任务精心设计的。\n\n### 实验结果与性能\n\n实验结果显著表明，TabDSR 框架在多个基准测试中持续超越现有方法，实现了最先进（SOTA）的性能：\n\n*   在 **TAT-QA** 数据集上，准确率提升了 **8.79%**。\n*   在 **TableBench** 数据集上，准确率提升了 **6.08%**。\n*   在专门设计的 **CalTab151** 数据集上，准确率更是显著提升了 **19.87%**。\n\n此外，TabDSR 框架能够与主流的大型语言模型无缝集成，为复杂的表格数值推理提供了一个强大且稳健的解决方案。\n\n### 结论\n\n这些研究发现有力地证明了 TabDSR 框架在增强大型语言模型处理复杂表格数值推理能力方面的卓越有效性。该研究已被 EMNLP 2025 Findings 接受。数据和代码可根据需求提供。",
      "shortSummary": "TabDSR框架旨在解决大型语言模型在表格数据复杂数值推理中因查询复杂、数据噪声和数值能力有限而表现不佳的问题。该框架包含查询分解器、表格清洗器和基于思维程序(PoT)的推理器。为公正评估，引入了新数据集CalTab151。实验证明，TabDSR在TAT-QA、TableBench和CalTab151上均实现了最先进(SOTA)的准确率提升，并能与主流LLMs无缝集成，有效提升了LLM的推理能力。",
      "translated_title": "TabDSR：表格数据中复杂数值推理的分解、清洗与推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Complex reasoning over tabular data is crucial in real-world data analysis, yet large language models (LLMs) often underperform due to complex queries, noisy data, and limited numerical capabilities. To address these issues, we propose \\method, a framework consisting of: (1) a query decomposer that breaks down complex questions, (2) a table sanitizer that cleans and filters noisy tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates executable code to derive the final answer from the sanitized table. To ensure unbiased evaluation and mitigate data leakage, we introduce a new dataset, CalTab151, specifically designed for complex numerical reasoning over tables. Experimental results demonstrate that \\method consistently outperforms existing methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and 19.87% accuracy improvement on TAT-QA, TableBench, and \\method, respectively. Moreover, our framework integrates seamlessly with mainstream LLMs, providing a robust solution for complex tabular numerical reasoning. These findings highlight the effectiveness of our framework in enhancing LLM performance for complex tabular numerical reasoning. Data and code are available upon request."
    },
    {
      "title": "区别对待运动分量以发展联合深度和自我运动学习 (原标题: Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning)",
      "link": "https://arxiv.org/abs/2511.01502",
      "pubDate": "Mon, 03 Nov 2025 07:14:52 GMT",
      "isoDate": "2025-11-03T07:14:52.000Z",
      "creator": "Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan",
      "summary": "## 区别对待运动分量以发展联合深度和自我运动学习\n\n### 引言与背景\n\n近年来，深度和自我运动的无监督学习作为两项基础的3D感知任务，取得了显著进展。然而，现有的大多数方法将自我运动视为辅助任务，在监督过程中要么混合所有运动类型，要么排除与深度无关的旋转运动。这种设计限制了强几何约束的融入，从而降低了在多样化条件下的可靠性和鲁棒性。\n\n### 核心贡献与方法\n\n本研究引入了一种**区别对待运动分量**的方法，利用其各自刚性流的几何规律来同时提升深度和自我运动估计的性能。\n\n该方法的核心步骤包括：\n\n1.  **相机对齐：** 给定连续的视频帧，网络输出首先对齐源相机和目标相机的光轴和成像平面。\n2.  **光流变换与几何约束：** 通过这些对齐，帧间光流被转换。随后，量化偏差，对每个自我运动分量**单独施加几何约束**，从而实现更有针对性的优化。\n3.  **联合学习重构：** 这些对齐进一步将联合学习过程重构为**同轴和共面形式**。\n4.  **深度与平移的互推导：** 在重构后的形式中，深度和每个平移分量可以通过**闭式几何关系相互推导**，引入了互补约束，显著提高了深度的鲁棒性。\n\n### DiMoDE框架\n\nDiMoDE是一个通用的深度和自我运动联合学习框架，它整合了上述所有设计。该框架旨在通过更精细地处理运动分量来克服传统方法的局限性。\n\n### 实验结果\n\nDiMoDE框架在多个公共数据集以及一个新收集的多样化真实世界数据集上，均实现了**最先进的性能**。尤其值得注意的是，该方法在**挑战性条件下**表现出卓越的鲁棒性。\n\n### 代码可用性\n\n本研究的源代码将在论文发表后公开发布。",
      "shortSummary": "该研究提出了一种区别对待运动分量的新方法，以改进无监督深度和自我运动的联合学习。通过对齐相机并对每个运动分量单独施加几何约束，该方法将联合学习重构为同轴和共面形式，使深度和翻译分量能够相互推导。由此产生的DiMoDE框架在多个数据集上实现了最先进的性能，尤其在复杂条件下表现出更高的鲁棒性，解决了现有方法几何约束不足的问题。",
      "translated_title": "区别对待运动分量以发展联合深度和自我运动学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication."
    },
    {
      "title": "更短但不更差：在数学RLVR中通过简单样本作为长度正则化器实现节俭推理 (原标题: Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR)",
      "link": "https://arxiv.org/abs/2511.01937",
      "pubDate": "Sun, 02 Nov 2025 12:29:16 GMT",
      "isoDate": "2025-11-02T12:29:16.000Z",
      "creator": "Abdelaziz Bounhar, Hadi Abdine, Evan Dufraisse, Ahmad Chamma, Amr Mohamed, Dani Bouch, Michalis Vazirgiannis, Guokan Shang",
      "summary": "## 更短但不更差：在数学RLVR中通过简单样本作为长度正则化器实现节俭推理\n\n### 摘要\n\n本文探讨了大型语言模型（LLMs）在逐步推理中普遍存在的冗长问题，并提出了一种创新的解决方案，以在不牺牲性能的情况下实现更简洁的推理。\n\n### 问题背景\n\n1.  **LLM的冗长性**：训练用于逐步推理的LLM往往会变得过于冗长，这不仅增加了推理成本，也可能降低用户体验。\n2.  **标准RLVR流程的缺陷**：\n    *   传统的“可验证奖励强化学习”（RLVR）流程为了提高训练效率，通常会过滤掉“简单”问题。\n    *   这导致模型主要在需要更长推理链的“更难”问题上进行训练。\n    *   结果是输出长度分布向上倾斜，使模型错误地将“思考更长时间”与“思考得更好”混淆。\n\n### 提出的方法\n\n本研究提出了一种“节俭推理”方法，其核心在于：\n\n*   **保留并适度提高简单样本的权重**：不再过滤掉“简单”问题，而是保留它们，并适度提高中等难度简单问题的训练权重。\n*   **隐式长度正则化**：这种做法作为一种隐式长度正则化器，引导模型生成更简洁的输出。\n\n### 机制与优势\n\n*   **约束输出分布**：通过让模型接触可解决的短链任务，可以有效约束其输出分布，从而防止过度冗长。\n*   **“免费的涌现简洁性”**：该方法使得模型能够在没有引入任何显式长度惩罚的情况下，学会解决更难的问题，同时保持输出长度不膨胀。\n\n### 实验结果\n\n*   **模型与设置**：研究人员在`Qwen3-4B-Thinking-2507`模型（具有16k令牌限制）上应用了这种RLVR方法。\n*   **性能表现**：\n    *   模型在pass@1 AIME25准确率上达到了基线水平。\n    *   同时，生成的解决方案平均长度缩短了近一半。\n*   **结论**：实验结果有力地证明了该方法在提高推理效率和简洁性方面的有效性，而无需牺牲准确性。\n\n### 资源可用性\n\n*   **代码**：相关代码已在GitHub上开源。\n*   **数据集与模型**：数据集和训练好的模型可在Hugging Face平台上获取。\n\n### 研究领域\n\n本研究主要涉及机器学习（cs.LG）、人工智能（cs.AI）和统计机器学习（stat.ML）等领域。",
      "shortSummary": "大型语言模型在逐步推理中常因冗长而增加成本。传统RLVR过滤简单问题，导致模型将“长思考”误认为“好思考”。本研究提出保留并适度提高中等难度简单问题的权重，作为隐式长度正则化器。这使得模型在没有明确长度惩罚的情况下，实现了“免费的涌现简洁性”。实验表明，该方法在保持基线准确率的同时，将解决方案长度平均缩短了近一半，有效提升了推理效率。",
      "translated_title": "更短但不更差：在数学RLVR中通过简单样本作为长度正则化器实现节俭推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) trained for step-by-step reasoning often become excessively verbose, raising inference cost. Standard Reinforcement Learning with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for training efficiency, leaving the model to train primarily on harder problems that require longer reasoning chains. This skews the output length distribution upward, resulting in a model that conflates ``thinking longer'' with ``thinking better''. In this work, we show that retaining and modestly up-weighting moderately easy problems acts as an implicit length regularizer. Exposing the model to solvable short-chain tasks constrains its output distribution and prevents runaway verbosity. The result is \\emph{emergent brevity for free}: the model learns to solve harder problems without inflating the output length,  despite the absence of any explicit length penalization. RLVR experiments using this approach on Qwen3-4B-Thinking-2507 (with a 16k token limit) achieve baseline pass@1 AIME25 accuracy while generating solutions that are, on average, nearly twice as short. The code is available at https://github.com/MBZUAI-Paris/Frugal-AI{GitHub}, with datasets and models on https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc{Hugging Face}."
    },
    {
      "title": "CodeClash：基准测试面向目标的软件工程 (原标题: CodeClash: Benchmarking Goal-Oriented Software Engineering)",
      "link": "https://arxiv.org/abs/2511.00839",
      "pubDate": "Sun, 02 Nov 2025 02:42:51 GMT",
      "isoDate": "2025-11-02T02:42:51.000Z",
      "creator": "John Yang, Kilian Lieret, Joyce Yang, Carlos E. Jimenez, Ofir Press, Ludwig Schmidt, Diyi Yang",
      "summary": "### CodeClash：基准测试面向目标的软件工程\n\n本文介绍了CodeClash，这是一个旨在评估语言模型（LMs）在面向目标软件工程中表现的新基准。\n\n**当前挑战与CodeClash的提出：**\n*   **现有基准的局限性：** 当前用于评估编码语言模型的基准主要关注具体的、明确定义的任务，例如修复特定错误或编写有针对性的测试。\n*   **真实世界软件开发的特点：** 然而，人类程序员的工作并非仅仅处理孤立的任务，而是围绕着实现高层次目标（如提高用户留存率或降低成本）展开。\n*   **未解决的问题：** 评估语言模型是否也能在没有明确指导的情况下，迭代地开发代码以更好地实现开放式目标，仍然是一个未解决的挑战。\n*   **CodeClash的解决方案：** 为解决这一问题，研究人员引入了CodeClash，一个让语言模型在多轮锦标赛中竞争，以构建最佳代码库来达成竞争性目标的基准。\n\n**CodeClash的工作机制：**\n*   **锦标赛模式：** 语言模型在多轮锦标赛中进行竞争。\n*   **两阶段回合：** 每个回合分为两个阶段：\n    1.  **代码编辑阶段：** 智能体（LMs）修改其代码。\n    2.  **代码竞技场阶段：** 修改后的代码库进行正面竞争，根据得分最大化、资源获取或生存等目标来确定胜者。\n*   **模型自主决策：** 模型必须自行决定如何改进其代码库，包括编写笔记、审查文档、分析竞争日志或创建测试套件，以提升自身表现并超越对手。\n\n**评估规模与主要发现：**\n*   **大规模评估：** 研究人员运行了1680场锦标赛（总计25,200个回合），在6个竞技场中评估了8个语言模型。\n*   **模型开发风格多样：** 结果显示，模型展现出多样化的开发风格。\n*   **战略推理的根本局限性：** 然而，模型在战略推理方面存在根本性局限。\n*   **长期代码库维护困难：** 模型还难以进行长期代码库维护，导致代码库逐渐变得混乱和冗余。\n*   **与人类专家的巨大差距：** 这些局限性非常明显：即使是顶尖模型，在与人类专家程序员的每一轮比赛中都以失败告终。\n\n**开源与未来展望：**\n*   研究人员已将CodeClash开源，旨在推动自主、面向目标的软件代码开发研究。",
      "shortSummary": "CodeClash是一个新基准，用于评估语言模型在面向目标软件工程中的表现。它通过多轮锦标赛模拟真实世界开发，模型需自主迭代代码以实现竞争目标。研究评估了8个模型在1680场锦标赛中的表现，发现模型在战略推理和长期代码维护方面存在根本性局限，且远逊于人类专家。CodeClash的开源旨在推动自主、面向目标的软件开发研究。",
      "translated_title": "CodeClash：基准测试面向目标的软件工程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current benchmarks for coding evaluate language models (LMs) on concrete, well-specified tasks such as fixing specific bugs or writing targeted tests. However, human programmers do not spend all day incessantly addressing isolated tasks. Instead, real-world software development is grounded in the pursuit of high-level goals, like improving user retention or reducing costs. Evaluating whether LMs can also iteratively develop code to better accomplish open-ended objectives without any explicit guidance remains an open challenge. To address this, we introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build the best codebase for achieving a competitive objective. Each round proceeds in two phases: agents edit their code, then their codebases compete head-to-head in a code arena that determines winners based on objectives like score maximization, resource acquisition, or survival. Whether it's writing notes, scrutinizing documentation, analyzing competition logs, or creating test suites, models must decide for themselves how to improve their codebases both absolutely and against their opponents. We run 1680 tournaments (25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal that while models exhibit diverse development styles, they share fundamental limitations in strategic reasoning. Models also struggle with long-term codebase maintenance, as repositories become progressively messy and redundant. These limitations are stark: top models lose every round against expert human programmers. We open-source CodeClash to advance the study of autonomous, goal-oriented code development."
    },
    {
      "title": "iFlyBot-VLA 技术报告 (原标题: iFlyBot-VLA Technical Report)",
      "link": "https://arxiv.org/abs/2511.01914",
      "pubDate": "Sat, 01 Nov 2025 02:24:56 GMT",
      "isoDate": "2025-11-01T02:24:56.000Z",
      "creator": "Yuan Zhang, Chenyu Xue, Wenjie Xu, Chao Ji, Jiajia wu, Jia Pan",
      "summary": "# iFlyBot-VLA 技术报告：大规模视觉-语言-动作模型的创新框架\n\n本文介绍了 iFlyBot-VLA，一个在大规模视觉-语言-动作（VLA）模型领域中，基于创新框架训练的新模型。其主要贡献和特点如下：\n\n*   **潜在动作模型 (Latent Action Model)**\n    *   该模型在大量人类和机器人操作视频上进行了彻底训练。\n    *   它能够捕捉隐式的高级意图，为VLM预测潜在动作提供基础。\n\n*   **双层动作表示框架 (Dual-level Action Representation Framework)**\n    *   在训练过程中，该框架同时监督视觉-语言模型（VLM）和动作专家。\n    *   这种双重监督机制有助于对齐语言、视觉和动作的表示空间，使VLM能够直接参与动作生成。\n\n*   **混合训练策略 (Mixed Training Strategy)**\n    *   结合了机器人轨迹数据与通用问答（QA）和空间问答数据集。\n    *   有效增强了VLM骨干网络的3D感知和推理能力。\n\n*   **VLM的动作预测机制**\n    *   VLM被训练来预测两种互补形式的动作：\n        *   **潜在动作 (Latent Actions)**：源自预训练的跨实体操作数据上的潜在动作模型，捕捉高层意图。\n        *   **结构化离散动作令牌 (Structured Discrete Action Tokens)**：通过对连续控制信号进行频域变换获得，编码显式的低层动态。\n\n*   **实验结果与性能**\n    *   在LIBERO Franka基准测试上的实验结果证明了iFlyBot-VLA框架的优越性。\n    *   真实世界评估进一步表明，iFlyBot-VLA在多样化和具有挑战性的操作任务中取得了具有竞争力的成功率。\n\n*   **未来计划**\n    *   研究团队计划开源部分自建数据集，以支持社区未来的研究工作。",
      "shortSummary": "iFlyBot-VLA 是一种大规模视觉-语言-动作（VLA）模型，通过创新框架训练。其核心贡献包括：在大量操作视频上训练的潜在动作模型；同时监督VLM和动作专家的双层动作表示框架；以及结合机器人轨迹与QA数据以增强3D感知和推理的混合训练策略。VLM预测高层潜在动作和低层离散动作令牌，实现语言、视觉与动作的对齐。实验证明，iFlyBot-VLA在LIBERO Franka基准和真实世界操作任务中表现出色，并计划开源部分数据集。",
      "translated_title": "iFlyBot-VLA 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model trained under a novel framework. The main contributions are listed as follows: (1) a latent action model thoroughly trained on large-scale human and robotic manipulation videos; (2) a dual-level action representation framework that jointly supervises both the Vision-Language Model (VLM) and the action expert during training; (3) a mixed training strategy that combines robot trajectory data with general QA and spatial QA datasets, effectively enhancing the 3D perceptual and reasoning capabilities of the VLM backbone. Specifically, the VLM is trained to predict two complementary forms of actions: latent actions, derived from our latent action model pretrained on cross-embodiment manipulation data, which capture implicit high-level intentions; and structured discrete action tokens, obtained through frequency-domain transformations of continuous control signals, which encode explicit low-level dynamics. This dual supervision aligns the representation spaces of language, vision, and action, enabling the VLM to directly contribute to action generation. Experimental results on the LIBERO Franka benchmark demonstrate the superiority of our frame-work, while real-world evaluations further show that iFlyBot-VLA achieves competitive success rates across diverse and challenging manipulation tasks. Furthermore, we plan to open-source a portion of our self-constructed dataset to support future research in the community"
    },
    {
      "title": "连续自回归语言模型 (原标题: Continuous Autoregressive Language Models)",
      "link": "https://arxiv.org/abs/2510.27688",
      "pubDate": "Fri, 31 Oct 2025 13:58:11 GMT",
      "isoDate": "2025-10-31T13:58:11.000Z",
      "creator": "Chenze Shao, Darren Li, Fandong Meng, Jie Zhou",
      "summary": "## 连续自回归语言模型 (CALM) 详细摘要\n\n### 核心问题：大型语言模型 (LLM) 的效率瓶颈\n\n当前大型语言模型 (LLM) 的效率受限于其固有的顺序性，即逐个生成词元（token）的过程。这种逐词元生成机制是LLM性能提升的主要限制因素。\n\n### 解决方案：引入连续自回归语言模型 (CALM)\n\n为了突破这一瓶颈，研究提出了一种新的LLM扩展设计轴：增加每个生成步骤的语义带宽。为此，论文引入了**连续自回归语言模型 (CALM)**，这是一种范式转变，将传统的离散“预测下一个词元”转变为**连续“预测下一个向量”**。\n\n### CALM 的工作原理\n\n1.  **高保真自编码器**：CALM 利用一个高保真自编码器，能够将K个词元（tokens）的块压缩成一个单一的连续向量。\n2.  **高精度重构**：通过这种压缩，原始词元可以从该连续向量中以超过99.9%的精度进行重构。\n3.  **语言建模范式转变**：这使得模型能够将语言建模为一系列连续向量，而非离散词元序列。\n4.  **生成步骤减少**：通过这种方式，生成步骤的数量可以减少K倍，显著提升效率。\n\n### 新的建模工具包\n\n这种范式转变要求一套全新的建模工具。因此，研究开发了一个全面的**无似然（likelihood-free）框架**，以支持在连续域中进行稳健的训练、评估和可控采样。\n\n### 实验结果与意义\n\n实验结果表明，CALM 显著改善了性能与计算成本之间的权衡。它能够在显著降低计算成本的情况下，达到与强大离散基线模型相当的性能。\n\n更重要的是，这些发现确立了“预测下一个向量”作为实现超高效语言模型的一种强大且可扩展的途径。\n\n### 资源链接\n\n*   **代码**：[this https URL](this https URL)\n*   **项目**：[this https URL](this https URL)",
      "shortSummary": "大型语言模型（LLM）的效率受限于其逐词元生成过程。为解决此问题，研究引入了**连续自回归语言模型（CALM）**，将离散的“预测下一个词元”转变为连续的“预测下一个向量”。CALM使用高保真自编码器将K个词元压缩成一个连续向量，并能以高精度重构，从而将生成步骤减少K倍。实验证明，CALM显著改善了性能与计算成本的权衡，为实现超高效语言模型提供了强大且可扩展的新途径。",
      "translated_title": "连续自回归语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autoregressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity autoencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9\\% accuracy. This allows us to model language as a sequence of continuous vectors instead of discrete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a comprehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the performance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code: https://github.com/shaochenze/calm. Project: https://shaochenze.github.io/blog/2025/CALM."
    },
    {
      "title": "分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏 (原标题: Phased DMD: Few-step Distribution Matching Distillation via Score Matching within Subintervals)",
      "link": "https://arxiv.org/abs/2510.27684",
      "pubDate": "Fri, 31 Oct 2025 13:55:10 GMT",
      "isoDate": "2025-10-31T13:55:10.000Z",
      "creator": "Xiangyu Fan, Zesong Qiu, Zhuguanyu Wu, Fanzhou Wang, Zhiqian Lin, Tianxiang Ren, Dahua Lin, Ruihao Gong, Lei Yang",
      "summary": "## 分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏\n\n### 引言\n\n分布匹配蒸馏（DMD）是一种将基于分数的生成模型蒸馏成高效一步生成器的方法，无需与教师模型的采样轨迹一一对应。然而，这种一步蒸馏模型在处理复杂生成任务（例如文本到视频生成中合成复杂的物体运动）时，由于模型容量有限，表现不佳。\n\n直接将DMD扩展到多步蒸馏会带来内存使用增加和计算深度加深的问题，从而导致模型不稳定和效率降低。尽管现有工作提出了随机梯度截断作为潜在解决方案，但研究发现，这种方法会显著降低多步蒸馏模型的生成多样性，使其降至与一步模型相同的水平。\n\n### Phased DMD 框架\n\n为了解决上述限制，本文提出了 **Phased DMD**，一个多步蒸馏框架。该框架将分阶段蒸馏（phase-wise distillation）的思想与专家混合（Mixture-of-Experts, MoE）相结合，旨在降低学习难度，同时增强模型容量。\n\nPhased DMD 基于两个关键思想构建：\n\n1.  **渐进式分布匹配 (Progressive Distribution Matching)**\n    *   模型将信噪比（SNR）范围划分为多个子区间。\n    *   通过在这些子区间内逐步细化模型，使其能够达到更高的SNR水平，从而更好地捕捉复杂分布。\n\n2.  **子区间内的分数匹配 (Score Matching within Subintervals)**\n    *   为了确保每个子区间内的训练目标准确无误，研究人员进行了严格的数学推导。\n\n### 实验验证与结果\n\n研究团队通过蒸馏最先进的图像和视频生成模型来验证Phased DMD的有效性，其中包括拥有200亿参数的Qwen-Image和280亿参数的Wan2.2。\n\n实验结果表明：\n*   Phased DMD 比传统的DMD方法更好地保留了输出多样性。\n*   同时，它保持了关键的生成能力。\n\n### 结论与展望\n\nPhased DMD 提供了一种有效的方法，可以在多步蒸馏中平衡效率、稳定性和生成多样性。研究团队计划发布相关的代码和模型，以促进社区的进一步研究和应用。\n\n### 研究领域\n\n本文属于计算机视觉与模式识别（cs.CV）领域。",
      "shortSummary": "Phased DMD 提出了一种多步蒸馏框架，旨在解决传统DMD在复杂任务中表现不佳及多步蒸馏效率低、多样性差的问题。该方法通过将信噪比范围划分为子区间进行渐进式分布匹配，并确保子区间内分数匹配的准确性，结合专家混合（MoE）来降低学习难度并增强模型容量。实验证明，Phased DMD 在保持生成能力的同时，显著提升了输出多样性，优于现有DMD方法。",
      "translated_title": "分阶段DMD：通过子区间内的分数匹配实现少步分布匹配蒸馏",
      "images": [],
      "contentSource": "完整文章",
      "content": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models."
    },
    {
      "title": "通过对比触发学习对MLLM具身决策进行视觉后门攻击 (原标题: Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning)",
      "link": "https://arxiv.org/abs/2510.27623",
      "pubDate": "Fri, 31 Oct 2025 12:50:49 GMT",
      "isoDate": "2025-10-31T12:50:49.000Z",
      "creator": "Qiusi Zhan, Hyeonjeong Ha, Rui Yang, Sirui Xu, Hanyang Chen, Liang-Yan Gui, Yu-Xiong Wang, Huan Zhang, Heng Ji, Daniel Kang",
      "summary": "### 通过对比触发学习对MLLM具身决策进行视觉后门攻击\n\n本文介绍了一种名为BEAT的框架，旨在对基于多模态大语言模型（MLLM）的具身智能体实施视觉后门攻击。\n\n**背景与问题：**\n*   MLLM通过直接感知、推理和规划任务导向的动作，推动了具身智能体的发展。\n*   然而，这种视觉驱动的具身智能体引入了一个新的攻击面：视觉后门攻击。\n*   攻击机制：智能体在正常情况下表现正常，但当场景中出现特定视觉触发器时，会持续执行攻击者指定的多步策略。\n*   挑战：与文本触发器不同，物体触发器在视角和光照方面存在广泛变化，难以可靠地植入。\n\n**BEAT框架的解决方案：**\nBEAT是首个利用环境中物体作为触发器，将视觉后门注入MLLM具身智能体的框架。它通过以下两点解决物体触发器的变异性挑战：\n\n1.  **训练集构建：** 构建一个涵盖多样化场景、任务和触发器放置的训练集，使智能体暴露于触发器的多种变体。\n2.  **两阶段训练方案：**\n    *   **第一阶段：** 应用监督微调（SFT）。\n    *   **第二阶段：** 引入新颖的对比触发学习（CTL）。\n        *   CTL将触发器判别表述为触发器存在输入和触发器不存在输入之间的偏好学习。\n        *   这明确地锐化了决策边界，以确保后门的精确激活。\n\n**实验结果与发现：**\n*   在各种具身智能体基准和MLLM上，BEAT实现了高达80%的攻击成功率。\n*   同时，它保持了强大的良性任务性能。\n*   BEAT能够可靠地泛化到分布外（out-of-distribution）的触发器放置。\n*   值得注意的是，与简单的SFT相比，在有限的后门数据下，CTL将后门激活精度提高了高达39%。\n\n**结论与影响：**\n这些发现揭示了基于MLLM的具身智能体中一个关键但尚未被探索的安全风险，强调了在实际部署之前需要开发鲁棒防御措施。",
      "shortSummary": "本文提出了BEAT框架，首次实现了对MLLM具身智能体的视觉后门攻击。通过利用环境中物体作为触发器，BEAT采用多样化训练集和两阶段训练（包括新颖的对比触发学习CTL），解决了物体触发器变异性问题。实验表明，BEAT攻击成功率高达80%，同时保持良好任务性能，且CTL显著提升了后门激活精度。这揭示了MLLM具身智能体的严重安全风险，亟需加强防御。",
      "translated_title": "通过对比触发学习对MLLM具身决策进行视觉后门攻击",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment."
    },
    {
      "title": "用于世界模型增强的视觉-语言-动作模型的双流扩散 (原标题: Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2510.27607",
      "pubDate": "Fri, 31 Oct 2025 12:32:12 GMT",
      "isoDate": "2025-10-31T12:32:12.000Z",
      "creator": "John Won, Kyungmin Lee, Huiwon Jang, Dongyoung Kim, Jinwoo Shin",
      "summary": "# DUST：用于世界模型增强视觉-语言-动作模型的双流扩散\n\n## 摘要\n\n本文提出了一种名为DUST（DUal-STream diffusion）的框架，旨在解决视觉-语言-动作（VLA）模型中联合预测下一状态观测和动作序列时存在的模态冲突问题。DUST通过其独特的双流扩散架构和解耦训练机制，显著提升了VLA在各种任务中的性能。\n\n## 背景与挑战\n\n近年来，将世界模型（world modeling）集成到视觉-语言-动作（VLA）模型中，在改进机器人策略学习方面展现出巨大潜力。然而，由于下一状态观测（视觉）和动作序列这两种模态之间固有的差异，如何有效地联合预测它们仍然是一个具有挑战性的问题。这种模态间的冲突限制了现有VLA模型的性能。\n\n## DUST方法概述\n\n为了应对上述挑战，DUST被提出作为一个世界模型增强的VLA框架。其核心创新点包括：\n\n*   **多模态扩散Transformer架构：**\n    *   DUST采用了一种新颖的多模态扩散Transformer架构。\n    *   该架构明确维护独立的模态流（例如，一个用于视觉，一个用于动作）。\n    *   同时，它通过精心设计的机制实现了有效的跨模态知识共享，确保不同模态间的信息能够互补。\n\n*   **解耦的训练机制：**\n    *   **独立噪声扰动：** DUST为每种模态引入了独立的噪声扰动，这使得模型能够更好地学习每种模态的特性。\n    *   **解耦流匹配损失：** 采用解耦的流匹配损失函数，进一步优化了训练过程。\n    *   **双向联合分布学习：** 这种设计使得模型能够以双向方式学习联合分布，而无需强制将不同模态映射到一个统一的潜在空间，从而避免了潜在的信息损失和模态冲突。\n\n*   **联合采样与测试时缩放：**\n    *   基于训练时模态的解耦特性，DUST引入了一种创新的联合采样方法。\n    *   该方法支持测试时缩放（test-time scaling），允许动作和视觉token以不同的速率异步演化，从而在推理阶段提供更大的灵活性和效率。\n\n## 实验结果与性能\n\nDUST在多个模拟和真实世界基准测试中展现出卓越的性能提升：\n\n*   **模拟基准测试：**\n    *   在RoboCasa和GR-1等模拟环境中，DUST比基线方法实现了高达6%的性能增益。\n    *   此外，DUST的测试时缩放方法额外提供了2-5%的性能提升。\n\n*   **真实世界任务：**\n    *   在与Franka Research 3机器人进行的真实世界任务中，DUST将成功率提高了13%。这有力地证实了其在模拟环境之外的实际有效性。\n\n*   **大规模预训练潜力：**\n    *   通过在BridgeV2（一个包含无动作视频的数据集）上进行预训练，DUST在RoboCasa任务上获得了显著的迁移增益。\n    *   这一结果突显了DUST在大规模VLA预训练方面的巨大潜力，为未来更通用、更强大的机器人模型奠定了基础。\n\n## 结论\n\nDUST通过其创新的双流扩散架构和解耦训练策略，成功解决了世界模型增强VLA中模态冲突的难题。它在模拟和真实世界任务中的显著性能提升，以及在大规模预训练方面的潜力，使其成为机器人策略学习领域的一个重要进展。",
      "shortSummary": "DUST是一种世界模型增强的视觉-语言-动作（VLA）框架，旨在解决VLA中视觉观测与动作序列的模态冲突。它采用双流扩散Transformer架构，独立处理并共享模态信息，通过解耦的训练机制学习联合分布。DUST在模拟基准测试（如RoboCasa）中实现了高达6%的性能提升，并在真实世界机器人任务中将成功率提高了13%。其测试时缩放和大规模预训练能力进一步增强了其有效性和应用潜力。",
      "translated_title": "用于世界模型增强的视觉-语言-动作模型的双流扩散",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing. In addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods, while our test-time scaling approach provides an additional 2-5% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST's potential for large-scale VLA pretraining."
    },
    {
      "title": "Spatial-SSRL：通过自监督强化学习增强空间理解 (原标题: Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2510.27606",
      "pubDate": "Fri, 31 Oct 2025 12:30:08 GMT",
      "isoDate": "2025-10-31T12:30:08.000Z",
      "creator": "Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang",
      "summary": "# Spatial-SSRL：通过自监督强化学习增强空间理解\n\n## 1. 研究背景与问题\n大型视觉-语言模型（LVLMs）在空间理解方面仍存在显著的弱点。现有的解决方案，如监督微调（SFT）和近期带有可验证奖励的强化学习（RLVR）方法，都面临以下挑战：\n*   **高昂的监督成本**：需要大量人工标注或专业工具。\n*   **特定环境依赖**：受限于特定或受控环境，难以大规模应用。\n\n## 2. Spatial-SSRL 方法介绍\n本文引入了 **Spatial-SSRL**，这是一种全新的自监督强化学习范式，旨在直接从普通的RGB或RGB-D图像中获取可验证的信号。其核心特点在于：\n*   **自监督机制**：无需人工或LVLM标注，自动生成监督信号。\n*   **多任务预设**：自动构建五种预设任务，以捕捉2D和3D空间结构。\n\n## 3. 五种预设任务\nSpatial-SSRL 设计了以下五种预设任务，它们提供了易于验证的真实答案：\n1.  **乱序补丁重排 (Shuffled Patch Reordering)**：模型需要将打乱的图像补丁重新排列到正确的位置。\n2.  **翻转补丁识别 (Flipped Patch Recognition)**：模型需要识别出图像中被翻转的补丁。\n3.  **裁剪补丁修复 (Cropped Patch Inpainting)**：模型需要修复图像中被裁剪掉的区域。\n4.  **区域深度排序 (Regional Depth Ordering)**：模型需要判断图像中不同区域的相对深度关系。\n5.  **相对3D位置预测 (Relative 3D Position Prediction)**：模型需要预测物体或区域在3D空间中的相对位置。\n\n这些任务的共同优势在于，它们的真实标签可以从图像本身直接推导，无需额外的人工标注或复杂的工具。\n\n## 4. 实验结果与性能提升\n通过在这些任务上进行训练，Spatial-SSRL 取得了显著的成果：\n*   **空间推理能力大幅提升**：模型在空间推理方面的表现得到显著改善。\n*   **通用视觉能力保持**：在增强空间理解的同时，模型原有的通用视觉能力并未受损。\n*   **基准测试表现**：\n    *   在图像和视频设置下的七个空间理解基准测试中，相对于Qwen2.5-VL基线模型，Spatial-SSRL 实现了平均准确率的显著提升。\n    *   对于3B模型，平均准确率提升了 **4.63%**。\n    *   对于7B模型，平均准确率提升了 **3.89%**。\n\n## 5. 结论与意义\nSpatial-SSRL 的研究结果表明，简单、内在的自监督信号能够大规模地支持可验证奖励的强化学习（RLVR），为LVLMs实现更强大的空间智能提供了一条实用且可扩展的途径。",
      "shortSummary": "Spatial-SSRL提出一种自监督强化学习范式，旨在解决大型视觉-语言模型（LVLMs）在空间理解方面的弱点。该方法通过从RGB/RGB-D图像中自动生成五种预设任务（如乱序补丁重排、区域深度排序）来获取可验证信号，无需人工标注。实验表明，Spatial-SSRL显著提升了LVLMs的空间推理能力，同时保持了通用视觉能力。在七个空间理解基准测试中，相对于Qwen2.5-VL基线，平均准确率分别提高了4.63%（3B）和3.89%（7B），为LVLMs实现更强空间智能提供了一条实用且可扩展的途径。",
      "translated_title": "Spatial-SSRL：通过自监督强化学习增强空间理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs."
    }
  ],
  "lastUpdated": "2025-11-05T09:34:31.223Z"
}