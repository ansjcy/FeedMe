{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "全身条件下的第一人称视频预测 (原标题: Whole-Body Conditioned Egocentric Video Prediction)",
      "link": "https://arxiv.org/abs/2506.21552",
      "pubDate": "Thu, 26 Jun 2025 13:59:59 GMT",
      "isoDate": "2025-06-26T13:59:59.000Z",
      "creator": "Yutong Bai, Danny Tran, Amir Bar, Yann LeCun, Trevor Darrell, Jitendra Malik",
      "summary": "## 全身条件下的第一人称视频预测 (PEVA)\n\n这项研究介绍了一种名为PEVA（Predict Ego-centric Video from human Actions）的模型，旨在根据过去的视频和由相对3D身体姿态表示的人体动作来预测第一人称视角视频。\n\n### 核心目标与方法\n*   **目标**：训练模型，使其能够模拟人类的物理动作如何从第一人称视角塑造环境。\n*   **输入条件**：模型以运动学姿态轨迹为条件，这些轨迹通过身体的关节层级结构化，从而捕捉动作的细节。\n*   **模型架构**：采用了一种自回归条件扩散Transformer模型进行训练。\n\n### 数据集\n*   **名称**：Nymeria\n*   **特点**：这是一个大规模的真实世界数据集，包含了第一人称视角视频和相应的身体姿态捕捉数据，为模型的训练提供了丰富的素材。\n\n### 评估协议\n*   **设计**：研究团队设计了一个分层的评估协议。\n*   **目的**：通过设置挑战性逐渐增加的任务，对模型的具身预测和控制能力进行全面而深入的分析。\n\n### 研究意义\n*   这项工作代表了在从人类视角建模复杂真实世界环境和具身智能体行为的视频预测领域，迈出了初步但重要的尝试。",
      "shortSummary": "这项研究提出了PEVA模型，利用自回归条件扩散Transformer，根据过去的视频和相对3D身体姿态预测第一人称视角视频。模型旨在模拟人类物理动作如何从第一人称视角塑造环境。它在大型真实数据集Nymeria上训练，并通过分层协议评估其具身预测和控制能力。这是首次尝试从人类视角对复杂真实世界环境和具身智能体行为进行视频预测。",
      "translated_title": "全身条件下的第一人称视频预测",
      "images": [],
      "contentSource": "完整文章",
      "content": "We train models to Predict Ego-centric Video from human Actions (PEVA), given the past video and an action represented by the relative 3D body pose. By conditioning on kinematic pose trajectories, structured by the joint hierarchy of the body, our model learns to simulate how physical human actions shape the environment from a first-person point of view. We train an auto-regressive conditional diffusion transformer on Nymeria, a large-scale dataset of real-world egocentric video and body pose capture. We further design a hierarchical evaluation protocol with increasingly challenging tasks, enabling a comprehensive analysis of the model's embodied prediction and control abilities. Our work represents an initial attempt to tackle the challenges of modeling complex real-world environments and embodied agent behaviors with video prediction from the perspective of a human."
    },
    {
      "title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程 (原标题: Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test)",
      "link": "https://arxiv.org/abs/2506.21551",
      "pubDate": "Thu, 26 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-26T13:59:58.000Z",
      "creator": "Ziyue Li, Chenrui Fan, Tianyi Zhou",
      "summary": "### Grokking现象在大型语言模型预训练中的发现与机制\n\n**1. 研究背景与Grokking现象**\n*   **Grokking定义：** 指的是神经网络训练中，测试性能在训练损失收敛后仍持续提升的现象。这一现象使得泛化能力和推理等新兴能力的机制变得神秘。\n*   **以往研究局限：** 多数以往研究集中于在少量玩具任务或高度特定任务上训练小型模型，且通常需要数千个epoch。\n\n**2. 本研究的创新点与发现**\n*   **首次大规模研究：** 本研究首次在7B大型语言模型（LLM）OLMoE的单次预训练检查点上，对Grokking现象进行了研究。这与以往小型模型的研究形成对比。\n*   **验证Grokking存在：** 研究首次证实，Grokking现象确实存在于大规模基础模型的预训练中，尽管不同数据可能异步进入Grokking阶段。\n*   **评估方法：** 在研究中，计算了训练损失，并在包括数学推理、代码生成以及常识/领域特定知识检索等多样化基准任务上评估了模型的泛化能力。\n\n**3. Grokking的机制解释：记忆到泛化的转变**\n*   **内部动态调查：** 通过深入调查LLM的内部动态，本研究揭示了Grokking中“泛化出现”的内在机制。\n*   **通路演变：** 训练样本的通路（即跨层专家选择）在Grokking过程中，从随机、实例特定演变为更结构化且样本间可共享。\n*   **通路复杂性降低：** 尽管训练损失已经收敛，但样本通路的复杂性却降低了。\n*   **核心结论：** 这些发现共同表明了一个从“记忆”到“泛化”的转换过程，为延迟泛化提供了机械性的解释。\n\n**4. 实用工具与理论贡献**\n*   **新型度量指标：** 本研究开发了两个新颖的度量指标，用于量化通路距离和单个通路的复杂性。\n*   **预测泛化能力：** 这些指标被证明能够有效预测模型在各种下游任务上的泛化改进。\n*   **高效实用：** 这些指标计算高效、简单，并且仅依赖于训练数据，无需额外的测试集或微调。\n*   **实际价值：** 它们对于预训练过程具有重要的实际价值，使得在不进行微调和测试的情况下，也能有效监控模型的泛化性能。\n*   **理论支持：** 理论上，研究表明更结构化的通路能够降低模型复杂性，并改善泛化界限。",
      "shortSummary": "本研究首次在7B大型语言模型（OLMoE）的预训练中证实了Grokking现象的存在。研究发现，Grokking过程中LLM内部的训练样本通路从随机演变为结构化且可共享，通路复杂性降低，这表明了从“记忆”到“泛化”的转变。为监控这一过程，研究开发了两个新颖的度量指标，它们仅依赖训练数据，能有效预测泛化改进，从而无需微调和测试即可监控模型泛化性能，具有重要的实际和理论价值。",
      "translated_title": "在LLM预训练中何处发现Grokking？无需测试即可监控记忆到泛化的过程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Grokking, i.e., test performance keeps improving long after training loss converged, has been recently witnessed in neural network training, making the mechanism of generalization and other emerging capabilities such as reasoning mysterious. While prior studies usually train small models on a few toy or highly-specific tasks for thousands of epochs, we conduct the first study of grokking on checkpoints during one-pass pretraining of a 7B large language model (LLM), i.e., OLMoE. We compute the training loss and evaluate generalization on diverse benchmark tasks, including math reasoning, code generation, and commonsense/domain-specific knowledge retrieval tasks.   Our study, for the first time, verifies that grokking still happens in the pretraining of large-scale foundation models, though different data may enter grokking stages asynchronously. We further demystify grokking's \"emergence of generalization\" by investigating LLM internal dynamics. Specifically, we find that training samples' pathways (i.e., expert choices across layers) evolve from random, instance-specific to more structured and shareable between samples during grokking. Also, the complexity of a sample's pathway reduces despite the converged loss. These indicate a memorization-to-generalization conversion, providing a mechanistic explanation of delayed generalization. In the study, we develop two novel metrics to quantify pathway distance and the complexity of a single pathway. We show their ability to predict the generalization improvement on diverse downstream tasks. They are efficient, simple to compute and solely dependent on training data. Hence, they have practical value for pretraining, enabling us to monitor the generalization performance without finetuning and test. Theoretically, we show that more structured pathways reduce model complexity and improve the generalization bound."
    },
    {
      "title": "SAM4D：在相机和激光雷达流中分割一切 (原标题: SAM4D: Segment Anything in Camera and LiDAR Streams)",
      "link": "https://arxiv.org/abs/2506.21547",
      "pubDate": "Thu, 26 Jun 2025 13:59:14 GMT",
      "isoDate": "2025-06-26T13:59:14.000Z",
      "creator": "Jianyun Xu, Song Wang, Ziqian Ni, Chunyong Hu, Sheng Yang, Jianke Zhu, Qiang Li",
      "summary": "### SAM4D：多模态与时序基础模型\n\nSAM4D是一个为相机和激光雷达流设计的可提示分割的多模态和时序基础模型。它旨在解决自动驾驶场景中动态变化环境下的鲁棒分割问题，并克服传统标注瓶颈。\n\n**核心创新与技术：**\n\n*   **统一多模态位置编码 (UMPE)**\n    *   功能：将相机和激光雷达特征对齐到共享的3D空间中。\n    *   目的：实现无缝的跨模态提示和交互。\n*   **运动感知跨模态记忆注意力 (MCMA)**\n    *   方法：利用自我运动补偿。\n    *   目的：增强时间一致性，实现长距离特征检索，确保在动态变化的自动驾驶场景中进行鲁棒分割。\n*   **多模态自动化数据引擎**\n    *   目的：避免标注瓶颈。\n    *   工作原理：\n        *   协同VFM（Video Foundation Model）驱动的视频masklets。\n        *   结合时空4D重建。\n        *   进行跨模态masklet融合。\n    *   成果：生成相机-激光雷达对齐的伪标签，其速度比人工标注快几个数量级，同时保留了VFM在点云表示中导出的语义保真度。\n\n**实验与成果：**\n\n*   在构建的Waymo-4DSeg数据集上进行了广泛实验。\n*   实验结果证明了SAM4D强大的跨模态分割能力。\n*   展示了其在数据标注方面的巨大潜力。\n\n**其他信息：**\n\n*   该研究已被ICCV2025接受。",
      "shortSummary": "SAM4D是一个多模态、时序基础模型，专为相机和激光雷达流中的可提示分割设计。它引入了统一多模态位置编码（UMPE）以对齐跨模态特征，并采用运动感知跨模态记忆注意力（MCMA）增强时间一致性。为解决标注瓶颈，SAM4D开发了一个自动化数据引擎，能高效生成高质量伪标签。在Waymo-4DSeg上的实验验证了其强大的跨模态分割能力和数据标注潜力。",
      "translated_title": "SAM4D：在相机和激光雷达流中分割一切",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present SAM4D, a multi-modal and temporal foundation model designed for promptable segmentation across camera and LiDAR streams. Unified Multi-modal Positional Encoding (UMPE) is introduced to align camera and LiDAR features in a shared 3D space, enabling seamless cross-modal prompting and interaction. Additionally, we propose Motion-aware Cross-modal Memory Attention (MCMA), which leverages ego-motion compensation to enhance temporal consistency and long-horizon feature retrieval, ensuring robust segmentation across dynamically changing autonomous driving scenes. To avoid annotation bottlenecks, we develop a multi-modal automated data engine that synergizes VFM-driven video masklets, spatiotemporal 4D reconstruction, and cross-modal masklet fusion. This framework generates camera-LiDAR aligned pseudo-labels at a speed orders of magnitude faster than human annotation while preserving VFM-derived semantic fidelity in point cloud representations. We conduct extensive experiments on the constructed Waymo-4DSeg, which demonstrate the powerful cross-modal segmentation ability and great potential in data annotation of proposed SAM4D."
    },
    {
      "title": "WorldVLA：迈向自回归动作世界模型 (原标题: WorldVLA: Towards Autoregressive Action World Model)",
      "link": "https://arxiv.org/abs/2506.21539",
      "pubDate": "Thu, 26 Jun 2025 13:55:40 GMT",
      "isoDate": "2025-06-26T13:55:40.000Z",
      "creator": "Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, Hao Chen",
      "summary": "## WorldVLA：统一动作与图像理解与生成的自回归世界模型\n\nWorldVLA是一种创新的自回归动作世界模型，旨在将动作与图像的理解和生成功能整合到一个单一框架中。该模型通过融合视觉-语言-动作（VLA）模型和世界模型，实现了对复杂环境的全面感知和交互能力。\n\n### 模型核心机制\n\n*   **世界模型（World Model）**：利用对动作和图像的理解来预测未来的图像。其主要目的是学习环境的底层物理规律，从而提升动作生成的质量和准确性。\n*   **动作模型（Action Model）**：根据图像观察生成后续动作。这不仅有助于视觉理解，反过来也支持世界模型的视觉生成能力。\n\n### 关键发现与优势\n\n*   **相互增强**：研究表明，WorldVLA的整体性能优于独立的动作模型和世界模型。这突出显示了世界模型与动作模型之间存在的相互促进和增强关系。\n*   **自回归动作生成挑战**：在自回归地生成一系列动作时，动作模型的性能会出现下降。这一现象归因于模型在动作预测方面的泛化能力有限，导致早期动作的错误会传播并累积到后续动作中。\n\n### 解决方案\n\n*   **注意力掩码策略（Attention Mask Strategy）**：为了解决自回归动作生成中的性能下降问题，研究者提出了一种注意力掩码策略。该策略在生成当前动作时，选择性地掩盖先前的动作信息。实验证明，这一策略显著提升了动作块生成任务的性能。",
      "shortSummary": "WorldVLA是一个统一动作与图像理解和生成的自回归世界模型。它将VLA模型和世界模型整合，通过世界模型预测未来图像以学习物理规律，并由动作模型生成动作以辅助视觉理解和生成。研究发现WorldVLA优于独立模型，展现了模型间的相互增强。针对自回归动作生成中性能下降的问题，WorldVLA提出了一种注意力掩码策略，通过选择性掩盖先前动作来显著提升动作块生成任务的性能。",
      "translated_title": "WorldVLA：迈向自回归动作世界模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present WorldVLA, an autoregressive action world model that unifies action and image understanding and generation. Our WorldVLA intergrates Vision-Language-Action (VLA) model and world model in one single framework. The world model predicts future images by leveraging both action and image understanding, with the purpose of learning the underlying physics of the environment to improve action generation. Meanwhile, the action model generates the subsequent actions based on image observations, aiding in visual understanding and in turn helps visual generation of the world model. We demonstrate that WorldVLA outperforms standalone action and world models, highlighting the mutual enhancement between the world model and the action model. In addition, we find that the performance of the action model deteriorates when generating sequences of actions in an autoregressive manner. This phenomenon can be attributed to the model's limited generalization capability for action prediction, leading to the propagation of errors from earlier actions to subsequent ones. To address this issue, we propose an attention mask strategy that selectively masks prior actions during the generation of the current action, which shows significant performance improvement in the action chunk generation task."
    },
    {
      "title": "MADrive：记忆增强的驾驶场景建模 (原标题: MADrive: Memory-Augmented Driving Scene Modeling)",
      "link": "https://arxiv.org/abs/2506.21520",
      "pubDate": "Thu, 26 Jun 2025 13:41:07 GMT",
      "isoDate": "2025-06-26T13:41:07.000Z",
      "creator": "Polina Karpikova, Daniil Selikhanovych, Kirill Struminsky, Ruslan Musaev, Maria Golitsyna, Dmitry Baranchuk",
      "summary": "## MADrive：记忆增强的驾驶场景建模\n\n### 引言\n\n近期，3D高斯泼溅技术在自动驾驶（AD）环境的场景重建方面取得了显著进展，实现了高度逼真的建模。然而，这些重建结果与原始观测数据紧密相关，难以支持对显著改变或全新驾驶场景进行真实感合成。\n\n### MADrive框架概述\n\n本文介绍了MADrive，一个记忆增强的重建框架。该框架旨在扩展现有场景重建方法的能力，其核心思想是通过从一个大型外部记忆库中检索视觉上相似的3D资产，来替换场景中观察到的车辆。\n\n### 关键组成部分\n\n1.  **MAD-Cars数据集**\n    *   MAD-Cars是一个精心策划的数据集，包含约7万个在野外捕获的360°汽车视频。\n2.  **检索模块**\n    *   该模块负责在记忆库中查找与目标车辆最相似的汽车实例。\n    *   接着，它从相应的视频中重建这些汽车的3D资产。\n    *   最后，通过方向对齐和重新打光（relighting）技术，将重建的3D资产无缝整合到目标场景中。\n\n### 实验结果与优势\n\nMADrive所替换的车辆提供了完整的、多视角的车辆表示。这使得该框架能够实现对大幅改变的配置进行真实感合成，其有效性已在实验中得到验证。\n\n### 相关信息\n\n*   **研究领域：** 计算机视觉与模式识别 (cs.CV)\n*   **引用信息：** arXiv:2506.21520 [cs.CV]",
      "shortSummary": "MADrive是一个记忆增强的驾驶场景建模框架，旨在解决现有3D重建方法难以合成新颖或大幅改变场景的问题。它通过从大型MAD-Cars记忆库中检索并整合视觉相似的3D车辆资产，替换原始场景中的车辆。该方法能实现车辆的完整多视图表示和真实感合成，扩展了自动驾驶环境建模的能力。",
      "translated_title": "MADrive：记忆增强的驾驶场景建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in scene reconstruction have pushed toward highly realistic modeling of autonomous driving (AD) environments using 3D Gaussian splatting. However, the resulting reconstructions remain closely tied to the original observations and struggle to support photorealistic synthesis of significantly altered or novel driving scenarios. This work introduces MADrive, a memory-augmented reconstruction framework designed to extend the capabilities of existing scene reconstruction methods by replacing observed vehicles with visually similar 3D assets retrieved from a large-scale external memory bank. Specifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg} car videos captured in the wild and present a retrieval module that finds the most similar car instances in the memory bank, reconstructs the corresponding 3D assets from video, and integrates them into the target scene through orientation alignment and relighting. The resulting replacements provide complete multi-view representations of vehicles in the scene, enabling photorealistic synthesis of substantially altered configurations, as demonstrated in our experiments. Project page: https://yandex-research.github.io/madrive/"
    },
    {
      "title": "Mind2Web 2：使用代理作为评判者评估代理式搜索 (原标题: Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge)",
      "link": "https://arxiv.org/abs/2506.21506",
      "pubDate": "Thu, 26 Jun 2025 13:32:50 GMT",
      "isoDate": "2025-06-26T13:32:50.000Z",
      "creator": "Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, Chan Hee Song, Jiaman Wu, Shijie Chen, Hanane Nour Moussa, Tianshu Zhang, Jian Xie, Yifei Li, Tianci Xue, Zeyi Liao, Kai Zhang, Boyuan Zheng, Zhaowei Cai, Viktor Rozgic, Morteza Ziyadi, Huan Sun, Yu Su",
      "summary": "## Mind2Web 2：代理式搜索评估的新基准与方法\n\n### 背景与挑战\n\n代理式搜索系统，例如深度研究系统，利用大型语言模型（LLMs）自主浏览网页、综合信息并提供带有引用的全面答案，这代表了用户与海量网络信息交互方式的重大转变。尽管代理式搜索有望提高效率并减轻认知负担，但其日益增长的复杂性和开放性已超越了现有评估基准和方法的能力。现有方法大多假设搜索范围短且答案静态，无法有效评估代理式搜索的动态和复杂性。\n\n### Mind2Web 2 基准\n\n为了应对这一挑战，本文引入了 **Mind2Web 2**，这是一个全新的评估基准，其特点如下：\n\n*   **任务数量与质量**：包含130个真实、高质量、长周期的任务。\n*   **任务要求**：这些任务需要实时网页浏览和广泛的信息综合。\n*   **构建投入**：耗费超过1,000小时的人工劳动构建而成。\n\n### 代理作为评判者（Agent-as-a-Judge）框架\n\n为了解决评估时变和复杂答案的难题，研究团队提出了一种新颖的 **代理作为评判者（Agent-as-a-Judge）框架**。该方法通过以下方式实现自动评估：\n\n*   **评判代理构建**：基于树状结构的评分标准设计，构建特定任务的评判代理。\n*   **评估内容**：自动评估答案的正确性以及来源归属。\n\n### 综合评估与发现\n\n研究团队对九个前沿的代理式搜索系统以及人类表现进行了全面评估，并进行了详细的错误分析，以期为未来的发展提供见解。评估结果显示：\n\n*   **最佳表现系统**：OpenAI Deep Research 系统。\n*   **性能对比**：该系统已能达到人类表现的50-70%。\n*   **效率优势**：同时，其耗时仅为人类的一半。\n*   **发展潜力**：这表明代理式搜索系统具有巨大的发展潜力。\n\n### 结论\n\n总而言之，Mind2Web 2 为下一代代理式搜索系统的开发和基准测试提供了坚实而严谨的基础。",
      "shortSummary": "Mind2Web 2引入了一个新的基准和“代理作为评判者”框架，以评估复杂的代理式搜索系统。该基准包含130个需实时浏览和信息综合的长周期任务。其“代理作为评判者”框架能自动评估答案正确性和来源归属。评估显示，OpenAI Deep Research等领先系统已能以一半时间达到人类表现的50-70%，展现了代理式搜索的巨大潜力。Mind2Web 2为未来代理式搜索系统的发展和基准测试奠定了基础。",
      "translated_title": "Mind2Web 2：使用代理作为评判者评估代理式搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems."
    },
    {
      "title": "学习跳过Transformer的中间层 (原标题: Learning to Skip the Middle Layers of Transformers)",
      "link": "https://arxiv.org/abs/2506.21103",
      "pubDate": "Thu, 26 Jun 2025 05:01:19 GMT",
      "isoDate": "2025-06-26T05:01:19.000Z",
      "creator": "Tim Lawson, Laurence Aitchison",
      "summary": "## 学习跳过Transformer的中间层：一种条件计算方法\n\n本文提出了一种新的Transformer架构，旨在通过动态跳过其中间层来提高计算效率。该方法基于对Transformer内部工作原理的洞察：\n\n### 背景与动机\n\n*   **条件计算**：是提高Transformer效率的常用策略。\n*   **现有方法**：通常针对单个模块（如专家混合层）或独立跳过层。\n*   **研究洞察**：可解释性研究表明，Transformer的**中间层表现出更大的冗余性**，而早期层则负责将信息聚合到token位置。\n\n### 提出的架构\n\n受上述洞察启发，作者提出了一种**从中间向外动态跳过可变数量层**的新型架构：\n\n*   **学习门控机制**：根据输入，该机制决定是否绕过对称的中心块范围。\n*   **门控注意力机制**：防止后续的token关注被跳过的token位置。\n*   **残差范数控制**：通过“三明治”（'sandwich'）或“逐层范数”（'perilayernorm'）方案来控制残差范数。\n*   **门控稀疏性**：通过自适应正则化损失来控制门控的稀疏性。\n\n### 目标与结果\n\n*   **预期目标**：旨在减少“更简单”token的计算需求，并可能促进出现多级表示层次结构。\n*   **实验结果**：在所研究的规模下，与层数较少的密集基线相比，该方法在验证交叉熵和估计FLOPs之间的权衡方面**未能实现改进**。\n\n### 代码发布\n\n*   作者已在 [this https URL](https://this.https.url/) 发布了相关代码。",
      "shortSummary": "本文提出一种新的Transformer架构，通过学习门控机制动态跳过其中间层，以提高计算效率。该方法基于中间层冗余的洞察，并结合门控注意力机制。尽管旨在减少计算量，但在实验规模下，该方法在计算效率与性能权衡方面未能超越传统的密集基线。代码已开源。",
      "translated_title": "学习跳过Transformer的中间层",
      "images": [],
      "contentSource": "完整文章",
      "content": "Conditional computation is a popular strategy to make Transformers more efficient. Existing methods often target individual modules (e.g., mixture-of-experts layers) or skip layers independently of one another. However, interpretability research has demonstrated that the middle layers of Transformers exhibit greater redundancy, and that early layers aggregate information into token positions. Guided by these insights, we propose a novel architecture that dynamically skips a variable number of layers from the middle outward. In particular, a learned gating mechanism determines whether to bypass a symmetric span of central blocks based on the input, and a gated attention mechanism prevents subsequent tokens from attending to skipped token positions. Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and gate sparsity with an adaptive regularization loss. We had aimed to reduce compute requirements for 'simpler' tokens and potentially foster an emergent multi-level representational hierarchy but, at the scales investigated, our approach does not achieve improvements in the trade-off between validation cross-entropy and estimated FLOPs compared to dense baselines with fewer layers. We release our code at https://github.com/tim-lawson/skip-middle."
    },
    {
      "title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑 (原标题: FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing)",
      "link": "https://arxiv.org/abs/2506.20911",
      "pubDate": "Wed, 25 Jun 2025 20:33:43 GMT",
      "isoDate": "2025-06-25T20:33:43.000Z",
      "creator": "Advait Gupta, Rishie Raj, Dang Nguyen, Tianyi Zhou",
      "summary": "FaSTA^* 是一种成本效益高的神经符号代理，旨在解决复杂的多轮图像编辑任务，例如“检测图像中的长凳并将其重新着色为粉红色。同时，为了更清晰的视图，移除猫并将墙壁重新着色为黄色。”\n\n*   **核心方法论**\n    *   FaSTA^* 结合了大型语言模型（LLMs）的快速、高层次子任务规划能力，以及每项子任务的慢速、准确、工具使用和局部 A* 搜索，以找到成本效益高的工具路径（即一系列对 AI 工具的调用）。\n*   **子程序挖掘与重用**\n    *   为了节省在类似子任务上进行 A* 搜索的成本，FaSTA^* 通过 LLMs 对先前成功的工具路径进行归纳推理，持续提取和优化频繁使用的子程序。\n    *   这些子程序被作为新工具重用于未来的任务中，实现了自适应的快速-慢速规划。\n    *   在规划过程中，首先探索高层次的子程序；只有当这些子程序失败时，才会激活低层次的 A* 搜索。\n*   **“快速-慢速”规划机制**\n    *   可重用的符号子程序显著节省了在应用于类似图像的相同类型子任务上的探索成本。\n    *   FaSTA^* 遵循一种“快速-慢速”工具路径代理模式：\n        *   首先，LLMs 尝试进行快速的子任务规划，并为每个子任务选择基于规则的子程序，预计能覆盖大多数任务。\n        *   只有对于新颖和具有挑战性的子任务，才会触发慢速的 A* 搜索。\n*   **性能优势**\n    *   与近期图像编辑方法相比，FaSTA^* 在计算效率上显著更高，同时在成功率方面与最先进的基线保持竞争力。",
      "shortSummary": "FaSTA^* 是一种高效的神经符号代理，用于处理复杂的多轮图像编辑任务。它结合了LLM的快速高层规划和A*搜索的慢速精确工具使用。通过LLM挖掘并重用常用子程序，FaSTA^* 显著降低了探索成本，实现了自适应的“快速-慢速”规划。该方法在计算效率上远超现有技术，同时保持了与最先进方法相当的成功率。",
      "translated_title": "FaSTA^*: 带有子程序挖掘的快速-慢速工具路径代理，用于高效多轮图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "We develop a cost-efficient neurosymbolic agent to address challenging multi-turn image editing tasks such as \"Detect the bench in the image while recoloring it to pink. Also, remove the cat for a clearer view and recolor the wall to yellow.'' It combines the fast, high-level subtask planning by large language models (LLMs) with the slow, accurate, tool-use, and local A^* search per subtask to find a cost-efficient toolpath -- a sequence of calls to AI tools. To save the cost of A^* on similar subtasks, we perform inductive reasoning on previously successful toolpaths via LLMs to continuously extract/refine frequently used subroutines and reuse them as new tools for future tasks in an adaptive fast-slow planning, where the higher-level subroutines are explored first, and only when they fail, the low-level A^* search is activated. The reusable symbolic subroutines considerably save exploration cost on the same types of subtasks applied to similar images, yielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask planning followed by rule-based subroutine selection per subtask is attempted by LLMs at first, which is expected to cover most tasks, while slow A^* search is only triggered for novel and challenging subtasks. By comparing with recent image editing approaches, we demonstrate FaSTA^* is significantly more computationally efficient while remaining competitive with the state-of-the-art baseline in terms of success rate."
    },
    {
      "title": "MMSearch-R1: 激励LMMs进行搜索 (原标题: MMSearch-R1: Incentivizing LMMs to Search)",
      "link": "https://arxiv.org/abs/2506.20670",
      "pubDate": "Wed, 25 Jun 2025 13:59:42 GMT",
      "isoDate": "2025-06-25T13:59:42.000Z",
      "creator": "Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu",
      "summary": "MMSearch-R1: 激励LMMs进行搜索\n\n**摘要**\n\n*   **背景与问题**\n    *   大型多模态模型（LMMs）在实际部署中需要访问外部知识源，因为现实世界信息复杂且动态。\n    *   现有方法（如检索增强生成RAG和提示工程搜索代理）依赖于僵化的流程，常导致搜索效率低下或过度搜索行为。\n\n*   **MMSearch-R1框架**\n    *   **创新点**：MMSearch-R1是首个端到端的强化学习（RL）框架，使LMMs能够在真实的互联网环境中执行按需、多轮搜索。\n    *   **核心机制**：\n        *   集成了图像和文本搜索工具。\n        *   模型能够根据结果导向的奖励（包含搜索惩罚）来判断何时以及如何调用这些工具。\n\n*   **数据支持**\n    *   为了支持训练，研究人员通过半自动化流程收集了一个多模态搜索VQA（视觉问答）数据集。\n    *   该数据集涵盖了多样化的视觉和文本知识需求，并策划了一个搜索平衡的子集，包含需要搜索和无需搜索的样本。\n    *   **重要性**：这个平衡的子集对于塑造高效和按需的搜索行为至关重要。\n\n*   **实验结果与发现**\n    *   在知识密集型和信息搜索VQA任务上进行了广泛实验。\n    *   **性能提升**：MMSearch-R1不仅优于相同模型大小的RAG基线模型，而且在性能上与更大的RAG基线模型相当。\n    *   **效率提升**：同时，MMSearch-R1将搜索调用次数减少了30%以上。\n    *   **洞察力**：进一步分析了关键的实证发现，为推进多模态搜索研究提供了可操作的见解。",
      "shortSummary": "MMSearch-R1是一个端到端强化学习框架，旨在解决大型多模态模型（LMMs）在搜索外部知识时效率低下和过度搜索的问题。该框架整合了图像和文本搜索工具，通过结果导向的奖励和搜索惩罚机制，使LMMs能够按需、多轮地进行搜索。通过构建多模态搜索VQA数据集进行训练，MMSearch-R1在实验中表现出色，不仅超越了同等规模的RAG基线模型，还与更大模型性能相当，同时将搜索调用次数减少了30%以上。",
      "translated_title": "MMSearch-R1: 激励LMMs进行搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Robust deployment of large multimodal models (LMMs) in real-world scenarios requires access to external knowledge sources, given the complexity and dynamic nature of real-world information. Existing approaches such as retrieval-augmented generation (RAG) and prompt engineered search agents rely on rigid pipelines, often leading to inefficient or excessive search behaviors. We present MMSearch-R1, the first end-to-end reinforcement learning framework that enables LMMs to perform on-demand, multi-turn search in real-world Internet environments. Our framework integrates both image and text search tools, allowing the model to reason about when and how to invoke them guided by an outcome-based reward with a search penalty. To support training, We collect a multimodal search VQA dataset through a semi-automated pipeline that covers diverse visual and textual knowledge needs and curate a search-balanced subset with both search-required and search-free samples, which proves essential for shaping efficient and on-demand search behavior. Extensive experiments on knowledge-intensive and info-seeking VQA tasks show that our model not only outperforms RAG-based baselines of the same model size, but also matches the performance of a larger RAG-based model while reducing search calls by over 30%. We further analyze key empirical findings to offer actionable insights for advancing research in multimodal search."
    },
    {
      "title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处 (原标题: When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs)",
      "link": "https://arxiv.org/abs/2506.20544",
      "pubDate": "Wed, 25 Jun 2025 11:37:53 GMT",
      "isoDate": "2025-06-25T11:37:53.000Z",
      "creator": "Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker",
      "summary": "### 扩展多语言大型语言模型推理计算的益处\n\n**研究背景与问题**\n\n*   近期大型语言模型（LLM）的进展已将重点转向扩展推理时计算，旨在不重新训练模型的情况下提升性能。\n*   一种常见方法是并行采样多个输出，并从中选择一个作为最终输出。\n*   然而，现有工作主要集中于英语以及数学和代码等少数领域，未能推广到开放式任务、可形式验证任务和跨语言场景。\n*   本研究旨在解决如何在多语言、多任务环境下，为开放式生成任务稳健地扩展推理时计算的问题。\n\n**主要发现与挑战**\n\n*   研究发现，基于温度变化的采样策略和选择策略都必须进行调整，以适应多样化的领域和不同的语言设置。\n*   对现有选择方法的评估表明，在英语中有效的策略往往无法推广到其他语言。\n\n**提出的新策略与成果**\n\n*   研究提出了专门针对多语言和多任务推理场景的新型采样和选择策略。\n*   这些策略在不同语言和任务中均取得了显著的性能提升。\n*   **具体成果：**\n    *   对于8B模型，结合本研究的采样和选择方法，在m-ArenaHard-v2.0提示上，与Gemini等专有模型相比，胜率平均提升了+6.8。\n    *   在更大规模的模型上，配备本研究方法的Command-A（111B模型）在相同基准测试中，仅使用五个样本，相较于单样本解码，胜率提升了+9.0，以最小的成本实现了显著的性能提升。\n\n**研究意义**\n\n*   研究结果强调了在推理时计算中采用语言和任务感知方法的必要性。\n*   这有助于在代表性不足的语言中普及性能改进，促进LLM的民主化应用。",
      "shortSummary": "这项研究旨在通过扩展推理计算来提升多语言大型语言模型（LLM）的性能，而无需重新训练。针对现有方法在跨语言和多任务场景中的局限性，研究提出了新颖的采样和选择策略。实验证明，这些优化策略显著提升了模型在多语言基准测试上的表现，例如使8B模型胜率平均提升6.8%，111B模型提升9.0%。研究强调了开发语言和任务感知型推理方法的重要性，以促进LLM在更多语言中的广泛应用。",
      "translated_title": "当生活给你样本时：扩展多语言大型语言模型推理计算的益处",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages."
    },
    {
      "title": "OctoThinker: 中期训练激励强化学习扩展 (原标题: OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling)",
      "link": "https://arxiv.org/abs/2506.20512",
      "pubDate": "Wed, 25 Jun 2025 10:58:13 GMT",
      "isoDate": "2025-06-25T10:58:13.000Z",
      "creator": "Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu",
      "summary": "### OctoThinker: 中期训练激励强化学习扩展\n\n本研究深入探讨了不同基础语言模型家族（如Llama和Qwen）在通过强化学习（RL）进行后训练时表现出的差异行为，尤其是在推理密集型任务上。理解是什么让一个基础语言模型适合强化学习，对于开发下一代可扩展的RL基础模型至关重要。\n\n**研究焦点与发现：**\n\n本研究主要调查了中期训练策略如何塑造RL动态，重点关注Qwen和Llama这两个代表性模型家族。研究揭示了以下关键洞察：\n\n1.  **高质量数学语料库的重要性：** 高质量的数学语料库，例如MegaMath-Web-Pro，能够显著提升基础模型和RL的性能。相比之下，现有的一些替代方案（如FineMath-4plus）未能达到同样的效果。\n2.  **问答式数据和长思维链（CoT）推理：** 进一步添加问答式数据，特别是长思维链（CoT）推理示例，能够增强RL结果。指令数据（instruction data）则能进一步释放这种效果。\n3.  **长CoT的权衡：** 尽管长CoT能够提高推理深度，但它也可能导致模型响应的冗长以及RL训练的不稳定性，这凸显了数据格式化的重要性。\n4.  **中期训练的扩展效应：** 扩展中期训练（scaling mid-training）能够持续带来更强的下游RL性能。\n\n**OctoThinker模型与“稳定-衰减”策略：**\n\n基于这些洞察，研究团队提出了一种名为“稳定-衰减”（Stable-then-Decay）的两阶段中期训练策略：\n\n*   **第一阶段：** 基础模型首先使用恒定学习率训练2000亿个token。\n*   **第二阶段：** 接着在三个以CoT为重点的分支上训练200亿个token，并采用学习率衰减。\n\n通过这种策略，研究团队开发了OctoThinker模型家族。OctoThinker模型展现出强大的RL兼容性，并成功缩小了与更RL友好模型家族（如Qwen）之间的性能差距。\n\n**研究意义与资源发布：**\n\n本研究旨在帮助塑造RL时代基础模型的预训练策略。为了支持进一步的研究，研究团队发布了开源模型，以及一个精心策划的、包含超过700亿个token的数学推理密集型语料库（即MegaMath-Web-Pro-Max）。",
      "shortSummary": "本研究探讨了中期训练策略对语言模型强化学习（RL）性能的影响。发现高质量数学语料库和长思维链（CoT）推理数据能显著提升RL效果，但需注意数据格式。研究提出“稳定-衰减”两阶段中期训练策略，成功开发了OctoThinker模型家族。OctoThinker展现出强大的RL兼容性，缩小了与RL友好模型的性能差距，旨在为RL时代的基础模型预训练提供指导。研究同时发布了开源模型和大型数学推理语料库。",
      "translated_title": "OctoThinker: 中期训练激励强化学习扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max)."
    },
    {
      "title": "ReCode：使用强化学习更新代码API知识 (原标题: ReCode: Updating Code API Knowledge with Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2506.20495",
      "pubDate": "Wed, 25 Jun 2025 10:41:13 GMT",
      "isoDate": "2025-06-25T10:41:13.000Z",
      "creator": "Haoze Wu, Yunzhi Yao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "## ReCode：使用强化学习更新代码API知识\n\n### 引言\n大型语言模型（LLMs）在代码生成方面展现出卓越的能力，但当外部库API频繁更新时，它们难以适应这些变化。这种局限性源于LLMs依赖其训练数据中过时的API知识，即使能够访问最新的文档，也无法在动态环境中可靠地生成代码。\n\n### ReCode框架\n为了解决这一关键问题，我们提出了ReCode（rule-based Reinforcement learning for Code Update），一个新颖的框架，旨在模仿人类程序员适应API变化的方式。\n\n### 核心机制\n*   **数据集构建：** 我们构建了一个包含大约2,000个数据条目的数据集，用于训练LLMs根据更新的信息执行版本迁移。\n*   **强化学习奖励：** 我们引入了一种修改后的字符串相似度度量作为强化学习的奖励，用于代码评估。\n\n### 实验结果与优势\n*   **性能显著提升：** 实验表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未曾见过的CodeUpdateArena任务上。\n*   **对通用能力影响小：** 与监督微调相比，ReCode对LLMs的通用代码生成能力影响较小。\n*   **广泛适用性：** 我们将ReCode应用于各种LLMs和强化学习算法（GRPO和DAPO），所有这些都实现了一致的改进。\n*   **卓越表现：** 值得注意的是，经过训练后，Qwen2.5-Coder-7B 的性能超越了相同架构的32B参数代码指令微调模型和推理模型。\n\n### 其他信息\n*   **项目状态：** 该工作目前正在进行中。\n*   **代码可用性：** 相关代码已提供。\n*   **研究领域：** 本研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、信息检索（cs.IR）、机器学习（cs.LG）和软件工程（cs.SE）等领域。",
      "shortSummary": "ReCode是一个基于强化学习的框架，旨在解决大型语言模型（LLMs）因API频繁更新而导致的过时代码生成问题。它通过构建数据集训练LLMs进行版本迁移，并使用修改后的字符串相似度作为奖励。实验证明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，且对通用能力影响较小。训练后的7B参数模型甚至超越了32B参数模型，显示出其有效性和潜力。",
      "translated_title": "ReCode：使用强化学习更新代码API知识",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode."
    },
    {
      "title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成 (原标题: HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling)",
      "link": "https://arxiv.org/abs/2506.20452",
      "pubDate": "Wed, 25 Jun 2025 09:58:37 GMT",
      "isoDate": "2025-06-25T09:58:37.000Z",
      "creator": "Tobias Vontobel, Seyedmorteza Sadat, Farnood Salehi, Romann M. Weber",
      "summary": "## HiWave：基于小波的扩散采样实现免训练高分辨率图像生成\n\n### 引言\n扩散模型已成为图像合成领域的领先方法，展现出卓越的真实感和多样性。然而，在高分辨率下训练扩散模型计算成本极高，且现有零样本生成技术在超越训练分辨率合成图像时常产生伪影，例如物体重复和空间不连贯。\n\n### HiWave 方法概述\n本文引入了 HiWave，这是一种免训练、零样本的方法，旨在利用预训练的扩散模型，显著增强超高分辨率图像合成的视觉保真度和结构连贯性。\n\n### 核心流程\nHiWave 采用一个两阶段的管道：\n\n1.  **基础图像生成与反演：**\n    *   首先，利用预训练模型生成一个基础图像。\n    *   接着，通过一个分块（patch-wise）的 DDIM 反演步骤，从该基础图像中导出初始噪声向量，以确保全局的连贯性。\n\n2.  **小波域细节增强：**\n    *   在采样过程中，HiWave 引入了一个新颖的小波域细节增强模块。\n    *   该模块的关键在于：\n        *   保留基础图像中的低频分量，以确保图像的结构一致性。\n        *   选择性地引导高频分量，以丰富图像的精细细节和纹理。\n\n### 优势与评估\n*   **免训练与零样本：** HiWave 的核心优势在于其无需额外的训练或对现有模型架构进行修改，即可实现高质量的超高分辨率图像生成。\n*   **伪影缓解：** 广泛的评估（使用 Stable Diffusion XL）表明，HiWave 能够有效缓解先前方法中常见的视觉伪影，如物体重复和空间不连贯。\n*   **卓越感知质量：** 该方法在图像合成中实现了卓越的感知质量。\n*   **用户研究验证：** 一项用户研究进一步证实了 HiWave 的性能。在超过 80% 的比较中，用户更倾向于 HiWave 而非最先进的替代方法，这充分凸显了其在高质量、超高分辨率图像合成方面的有效性。",
      "shortSummary": "HiWave 是一种免训练、零样本的方法，用于利用预训练扩散模型生成超高分辨率图像。它通过两阶段管道工作：首先生成基础图像并进行 DDIM 反演，然后通过新颖的小波域细节增强模块，保留低频结构并丰富高频细节。该方法有效缓解了伪影，实现了卓越的感知质量，并在用户研究中表现出色，无需重新训练或修改模型架构。",
      "translated_title": "HiWave：基于小波的扩散采样实现免训练高分辨率图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications."
    },
    {
      "title": "一个具有可追溯推理能力的罕见病诊断智能体系统 (原标题: An Agentic System for Rare Disease Diagnosis with Traceable Reasoning)",
      "link": "https://arxiv.org/abs/2506.20430",
      "pubDate": "Wed, 25 Jun 2025 09:42:26 GMT",
      "isoDate": "2025-06-25T09:42:26.000Z",
      "creator": "Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie",
      "summary": "# DeepRare：一个具有可追溯推理能力的罕见病诊断智能体系统\n\n## 引言\n\n罕见病在全球范围内影响超过3亿人，但及时准确的诊断仍然是一个普遍存在的挑战。这主要是由于其临床异质性、个体患病率低以及大多数临床医生对罕见病缺乏了解。\n\n## DeepRare 系统概述\n\n本文介绍了 **DeepRare**，这是首个由大型语言模型（LLM）驱动的罕见病诊断智能体系统，能够处理异构的临床输入。该系统能够：\n\n*   生成按优先级排序的罕见病诊断假设。\n*   为每个假设提供透明的推理链，将中间分析步骤与可验证的医学证据关联起来。\n\n## DeepRare 核心组件\n\nDeepRare 系统由三个关键组件构成：\n\n1.  **中央宿主 (Central Host)**：包含一个长期记忆模块，用于存储和管理信息。\n2.  **专业智能体服务器 (Specialized Agent Servers)**：\n    *   负责执行领域特定的分析任务。\n    *   集成了超过40种专业工具和网络规模的最新医学知识来源，确保获取最当前的临床信息。\n\n这种模块化和可扩展的设计使得系统能够进行复杂的诊断推理，同时保持可追溯性和适应性。\n\n## 性能评估\n\nDeepRare 在八个数据集上进行了评估，结果显示出卓越的诊断性能：\n\n*   **诊断范围**：在2,919种疾病中进行诊断。\n*   **高准确率**：对1,013种疾病实现了100%的准确率。\n*   **HPO-based 评估**：\n    *   DeepRare 显著优于其他15种方法，包括传统的生物信息学诊断工具、LLM和其他智能体系统。\n    *   平均 Recall@1 得分为 57.18%。\n    *   比第二名（Reasoning LLM）高出23.79个百分点。\n*   **多模态输入场景**：\n    *   在109个病例中，DeepRare 的 Recall@1 达到 70.60%，而 Exomiser 为 53.20%。\n*   **推理链人工验证**：临床专家对推理链的人工验证达到了95.40%的一致性。\n\n## 系统可用性\n\nDeepRare 系统已实现为一个用户友好的网络应用程序。",
      "shortSummary": "DeepRare是一个由大型语言模型驱动的罕见病诊断智能体系统，旨在解决罕见病诊断难题。它能处理异构临床输入，生成带可追溯推理链的诊断假设，并整合了40多种专业工具和最新医学知识。在评估中，DeepRare在2,919种疾病中表现出色，对1,013种疾病达到100%准确率，并在HPO和多模态评估中显著优于现有方法。临床专家对推理链的验证一致性高达95.40%。该系统已作为网络应用推出。",
      "translated_title": "一个具有可追溯推理能力的罕见病诊断智能体系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor."
    },
    {
      "title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法 (原标题: Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content)",
      "link": "https://arxiv.org/abs/2506.20331",
      "pubDate": "Wed, 25 Jun 2025 07:30:25 GMT",
      "isoDate": "2025-06-25T07:30:25.000Z",
      "creator": "Rian Touchent, Nathan Godey, Eric de la Clergerie",
      "summary": "## Biomed-Enriched：一个用于预训练和提取稀有及隐藏内容的生物医学数据集\n\nBiomed-Enriched是一个新颖的生物医学文本数据集，它通过两阶段的标注过程从PubMed构建而成。该数据集旨在为生物医学和临床自然语言处理（NLP）提供一个宝贵的资源，特别是用于预训练和提取难以获取的稀有及隐藏内容。\n\n### 数据集构建过程：\n\n1.  **第一阶段：大型语言模型（LLM）标注**\n    *   一个大型语言模型对来自PubMed科学文章的40万个段落进行标注。\n    *   标注内容包括段落的类型（综述、研究、临床病例、其他）、领域（临床、生物医学、其他）以及教育质量评分。\n    *   教育质量评分范围为1到5，用于评估段落对大学水平学习的有用性。\n2.  **第二阶段：小型语言模型（SLM）传播标签**\n    *   利用第一阶段生成的标注数据对一个小型语言模型进行微调。\n    *   该微调后的SLM将这些标签传播到整个PMC-OA语料库中。\n\n### 数据集的成果与价值：\n\n*   **精炼子集提取：** 生成的元数据使得能够提取出精炼的子集，例如：\n    *   200万个临床病例段落。\n    *   其中包含超过45万个来自具有商业使用许可文章的高质量临床病例段落。\n*   **解决数据访问难题：** 临床文本通常由于隐私限制（医院记录无法公开共享）而难以获取。Biomed-Enriched提供了一个替代方案，即一个大规模、公开可用的PubMed临床病例集合。\n*   **对NLP的价值：** 它是生物医学和临床NLP领域的一个重要资源。\n\n### 初步预训练实验结果（使用OLMo2）：\n\n*   **有针对性的改进：** 经过精心策划的子集能够实现有针对性的性能提升。\n    *   **临床上采样（Clinical Upsampling）：** 在MMLU ProfMed基准测试中，性能提升了约5%。\n    *   **教育质量过滤（Educational Quality Filtering）：** 在MedQA和MedMCQA基准测试中，性能提升了约1%。\n*   **训练效率提升：** 结合这些技术可以实现更快的收敛速度，仅用三分之一的训练令牌就能达到相同的性能。\n*   **未来潜力：** 这表明Biomed-Enriched数据集有潜力支持更高效、更有效的生物医学预训练策略。\n\n### 数据可用性：\n\n数据集链接已提供。",
      "shortSummary": "Biomed-Enriched是一个新的生物医学文本数据集，通过LLM对PubMed文章进行两阶段标注构建。它提供了200万个临床病例段落，包括45万个高质量病例，解决了临床文本隐私限制导致的访问难题。该数据集是生物医学和临床NLP的重要资源。初步实验表明，其精选子集能显著提升预训练模型的性能，如临床上采样使MMLU ProfMed提升约5%，并能加速模型收敛，实现更高效的生物医学预训练。",
      "translated_title": "Biomed-Enriched: 一个利用大型语言模型丰富生物医学数据集以进行预训练和提取稀有及隐藏内容的方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies."
    },
    {
      "title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画 (原标题: AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.19851",
      "pubDate": "Tue, 24 Jun 2025 13:59:58 GMT",
      "isoDate": "2025-06-24T13:59:58.000Z",
      "creator": "Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, Lu Sheng",
      "summary": "# AnimaX：一种创新的3D动画框架\n\nAnimaX是一个前馈式3D动画框架，旨在弥合视频扩散模型的运动先验与基于骨骼动画的可控结构之间的鸿沟。\n\n## 传统方法的局限性\n\n传统的运动合成方法存在以下限制：\n*   受限于固定的骨骼拓扑结构。\n*   需要在高维变形空间中进行昂贵的优化。\n\n## AnimaX 的核心优势与方法\n\nAnimaX通过以下创新方法克服了传统方法的局限性：\n\n*   **知识迁移：** 有效地将基于视频的运动知识迁移到3D领域。\n*   **广泛支持：** 能够支持具有任意骨骼的各种关节网格模型。\n*   **运动表示：** 将3D运动表示为多视角、多帧的2D姿态图。\n*   **联合扩散：** 实现联合视频-姿态扩散，其条件是模板渲染和文本运动提示。\n*   **对齐机制：** 引入共享位置编码和模态感知嵌入，以确保视频和姿态序列之间的时空对齐，从而将视频先验有效地转移到运动生成任务中。\n*   **3D重建：** 生成的多视角姿态序列被三角化为3D关节位置。\n*   **网格动画：** 通过逆运动学（inverse kinematics）将3D关节位置转换为最终的网格动画。\n\n## 训练与性能\n\n*   AnimaX 在一个新策展的、包含160,000个绑定序列的数据集上进行了训练。\n*   在VBench基准测试中，AnimaX在泛化能力、运动保真度和效率方面取得了最先进（state-of-the-art）的结果。\n\n## 应用前景\n\nAnimaX为类别无关的3D动画提供了一个可扩展的解决方案，具有广泛的应用潜力。\n\n**项目页面：** 提供了项目页面链接以获取更多信息。",
      "shortSummary": "AnimaX是一个创新的3D动画框架，它结合了视频扩散模型的运动先验与骨骼动画的可控性。该方法将3D运动表示为多视角2D姿态图，并通过联合视频-姿态扩散生成动画。AnimaX支持任意骨骼的网格模型，解决了传统方法的局限性。它在16万个绑定序列数据集上训练，并在泛化、运动保真度和效率方面达到了最先进水平，为类别无关的3D动画提供了可扩展的解决方案。",
      "translated_title": "AnimaX：使用联合视频-姿态扩散模型在3D中为无生命物体赋予动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present AnimaX, a feed-forward 3D animation framework that bridges the motion priors of video diffusion models with the controllable structure of skeleton-based animation. Traditional motion synthesis methods are either restricted to fixed skeletal topologies or require costly optimization in high-dimensional deformation spaces. In contrast, AnimaX effectively transfers video-based motion knowledge to the 3D domain, supporting diverse articulated meshes with arbitrary skeletons. Our method represents 3D motion as multi-view, multi-frame 2D pose maps, and enables joint video-pose diffusion conditioned on template renderings and a textual motion prompt. We introduce shared positional encodings and modality-aware embeddings to ensure spatial-temporal alignment between video and pose sequences, effectively transferring video priors to motion generation task. The resulting multi-view pose sequences are triangulated into 3D joint positions and converted into mesh animation via inverse kinematics. Trained on a newly curated dataset of 160,000 rigged sequences, AnimaX achieves state-of-the-art results on VBench in generalization, motion fidelity, and efficiency, offering a scalable solution for category-agnostic 3D animation. Project page: https://anima-x.github.io/{https://anima-x.github.io/}."
    },
    {
      "title": "统一的视觉-语言-动作模型 (原标题: Unified Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2506.19850",
      "pubDate": "Tue, 24 Jun 2025 13:59:57 GMT",
      "isoDate": "2025-06-24T13:59:57.000Z",
      "creator": "Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang",
      "summary": "## UniVLA：统一的视觉-语言-动作模型\n\n### 引言\n视觉-语言-动作（VLA）模型因其在推动机器人操作方面的巨大潜力而备受关注。然而，现有方法主要依赖视觉-语言模型（VLM）的通用理解能力来生成动作信号，这往往忽略了视觉观察中固有的丰富时序和因果结构。\n\n### UniVLA 的提出与核心机制\n本文提出了 **UniVLA**，一个统一的、原生的多模态VLA模型。UniVLA的核心创新在于将视觉、语言和动作信号自回归地建模为离散的token序列。这种独特的公式化方法带来了多项优势：\n\n*   **灵活的多模态任务学习**：它使得模型能够灵活地学习各种多模态任务，尤其擅长从大规模视频数据中进行学习。\n*   **融入世界建模**：通过在后训练阶段融入世界建模，UniVLA能够有效地从视频中捕捉因果动态。\n*   **高效的策略迁移**：捕捉到的因果动态有助于模型有效迁移到下游策略学习，这对于解决长时程任务尤为重要。\n\n### 实验结果与性能\nUniVLA在多个广泛使用的模拟基准测试中取得了新的最先进结果，显著超越了现有方法，这些基准包括：\n\n*   CALVIN\n*   LIBERO\n*   Simplenv-Bridge\n\n**具体示例**：在LIBERO基准测试中，UniVLA的平均成功率达到了95.5%，显著超越了先前方法pi0-FAST的85.5%。\n\n### 实际应用\n除了在模拟环境中的卓越表现，UniVLA的广泛适用性还在真实世界的应用中得到了验证，包括：\n\n*   ALOHA机器人操作\n*   自动驾驶",
      "shortSummary": "UniVLA是一个统一的视觉-语言-动作（VLA）模型，它将视觉、语言和动作信号自回归地建模为离散token序列。通过融入世界建模，UniVLA能从视频中捕捉因果动态，有效支持长时程任务的策略学习。该模型在CALVIN、LIBERO等多个模拟基准测试中取得了最先进结果，例如在LIBERO上成功率达95.5%。UniVLA还展示了在真实世界机器人操作和自动驾驶中的广泛适用性。",
      "translated_title": "统一的视觉-语言-动作模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action models (VLAs) have garnered significant attention for their potential in advancing robotic manipulation. However, previous approaches predominantly rely on the general comprehension capabilities of vision-language models (VLMs) to generate action signals, often overlooking the rich temporal and causal structure embedded in visual observations. In this paper, we present UniVLA, a unified and native multimodal VLA model that autoregressively models vision, language, and action signals as discrete token sequences. This formulation enables flexible multimodal tasks learning, particularly from large-scale video data. By incorporating world modeling during post-training, UniVLA captures causal dynamics from videos, facilitating effective transfer to downstream policy learning--especially for long-horizon tasks. Our approach sets new state-of-the-art results across several widely used simulation benchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly surpassing previous methods. For example, UniVLA achieves 95.5% average success rate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate its broad applicability on real-world ALOHA manipulation and autonomous driving."
    },
    {
      "title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成 (原标题: ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing)",
      "link": "https://arxiv.org/abs/2506.19848",
      "pubDate": "Tue, 24 Jun 2025 13:59:55 GMT",
      "isoDate": "2025-06-24T13:59:55.000Z",
      "creator": "Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, Jiaqi Wang, Feng Wu, Dahua Lin",
      "summary": "## ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成\n\n### 引言\n本文介绍ScaleCap，一种推理时可伸缩的图像字幕生成策略，旨在生成全面且详细的图像字幕。高质量图像字幕生成的关键挑战在于大型视觉语言模型（LVLMs）固有的偏见：\n\n*   **多模态偏见**：导致描述粒度不平衡，对某些元素描述详细，而对另一些则仅作概述。\n*   **语言偏见**：导致对不存在对象的幻觉描述。\n\n### ScaleCap：可伸缩的去偏字幕策略\n为解决上述问题，我们提出一种可伸缩的去偏字幕生成策略，该策略通过增加推理预算，持续丰富和校准字幕。该策略包含两个新颖组件：\n\n*   **启发式问答（Heuristic Question Answering）**：根据图像生成内容特定问题并回答，逐步向字幕中注入相关信息。\n*   **对比句评分（Contrastive Sentence Rating）**：采用句子级离线对比解码，有效识别并消除由语言偏见引起的幻觉。\n\n### 推理时可伸缩性\n随着推理成本的增加，ScaleCap会提出更多启发式问题，逐步捕捉额外的视觉细节，从而生成更准确、平衡且信息丰富的字幕。\n\n### 实验结果与应用\n\n*   广泛的模态对齐实验证明了ScaleCap的有效性。\n*   使用ScaleCap标注45万张图像并用于LVLM预训练，在11个广泛使用的基准测试中持续获得性能提升。\n*   此外，ScaleCap在两个额外任务中展示了生成字幕的卓越丰富性和保真度：\n    *   在VQA（视觉问答）任务中用字幕替换图像。\n    *   从字幕重建图像以评估语义覆盖范围。",
      "shortSummary": "ScaleCap是一种推理时可伸缩的图像字幕生成策略，旨在解决大型视觉语言模型（LVLMs）中的多模态和语言偏见。它通过启发式问答逐步注入细节，并利用对比句评分消除幻觉。随着推理预算增加，ScaleCap能生成更准确、平衡、信息丰富的字幕。实验证明，其能显著提升LVLM性能，并在VQA和图像重建等任务中展现出卓越的字幕质量。",
      "translated_title": "ScaleCap：通过双模态去偏实现推理时可伸缩的图像字幕生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents ScaleCap, an inference-time scalable image captioning strategy that generates comprehensive and detailed image captions. The key challenges of high-quality image captioning lie in the inherent biases of LVLMs: multimodal bias resulting in imbalanced descriptive granularity, offering detailed accounts of some elements while merely skimming over others; linguistic bias leading to hallucinated descriptions of non-existent objects. To address these issues, we propose a scalable debiased captioning strategy, which continuously enriches and calibrates the caption with increased inference budget. Specifically, we propose two novel components: heuristic question answering and contrastive sentence rating. The former generates content-specific questions based on the image and answers them to progressively inject relevant information into the caption. The latter employs sentence-level offline contrastive decoding to effectively identify and eliminate hallucinations caused by linguistic biases. With increased inference cost, more heuristic questions are raised by ScaleCap to progressively capture additional visual details, generating captions that are more accurate, balanced, and informative. Extensive modality alignment experiments demonstrate the effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them for LVLM pretraining leads to consistent performance gains across 11 widely used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity of generated captions with two additional tasks: replacing images with captions in VQA task, and reconstructing images from captions to assess semantic coverage. Code is available at https://github.com/Cooperx521/ScaleCap."
    },
    {
      "title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线 (原标题: SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution)",
      "link": "https://arxiv.org/abs/2506.19838",
      "pubDate": "Tue, 24 Jun 2025 13:57:26 GMT",
      "isoDate": "2025-06-24T13:57:26.000Z",
      "creator": "Liangbin Xie, Yu Li, Shian Du, Menghan Xia, Xintao Wang, Fanghua Yu, Ziyan Chen, Pengfei Wan, Jiantao Zhou, Chao Dong",
      "summary": "# SimpleGVR：潜在级联视频超分辨率的简单基线\n\n## 核心问题与背景\n当前，潜在扩散模型已成为高效视频生成领域的主导范式。然而，随着用户对更高分辨率输出的期望不断提高，仅依赖潜在计算已无法满足需求。一个有前景的方法是将视频生成过程解耦为两个阶段：语义内容生成和细节合成。前者利用计算密集型的基础模型在较低分辨率下生成内容，而后者则借助轻量级的级联视频超分辨率（VSR）模型实现高分辨率输出。\n\n## 本文研究重点\n本研究专注于探讨级联VSR模型的关键设计原则，这在当前领域中尚未得到充分探索。\n\n## 主要贡献与设计原则\n\n### 1. 降级策略\n文章提出了两种降级策略，用于生成训练对。这些策略旨在更好地模拟基础模型的输出特性，从而确保VSR模型与其上游生成器之间的高度对齐。\n\n### 2. VSR模型行为分析\n通过系统分析，本文提供了关于VSR模型行为的关键见解，具体包括：\n*   **时间步采样策略**：研究了不同时间步采样策略对模型性能的影响。\n*   **低分辨率（LR）输入上的噪声增强效果**：分析了噪声增强对LR输入的影响。\n这些发现直接指导了模型架构和训练方法的创新。\n\n### 3. 效率创新\n为了实现高效的训练和推理，并大幅降低计算开销，SimpleGVR引入了：\n*   **交错时间单元（interleaving temporal unit）**\n*   **稀疏局部注意力（sparse local attention）**\n\n## 实验结果与结论\n广泛的实验证明，SimpleGVR框架优于现有方法。消融研究也证实了每个设计选择的有效性。这项工作为级联视频超分辨率生成建立了一个简单而有效的基线，为未来高效级联合成系统的发展提供了实用的见解。",
      "shortSummary": "SimpleGVR提出了一种用于潜在级联视频超分辨率（VSR）的简单有效基线。它通过将视频生成解耦为内容生成和细节合成来解决高分辨率输出问题。主要贡献包括：优化训练对的降级策略、深入分析VSR模型行为（时间步采样、噪声增强），以及引入交错时间单元和稀疏局部注意力以提高效率。实验证明其优越性，为未来高效级联合成系统提供了实用见解。",
      "translated_title": "SimpleGVR：一种用于潜在级联视频超分辨率的简单基线",
      "images": [],
      "contentSource": "完整文章",
      "content": "Latent diffusion models have emerged as a leading paradigm for efficient video generation. However, as user expectations shift toward higher-resolution outputs, relying solely on latent computation becomes inadequate. A promising approach involves decoupling the process into two stages: semantic content generation and detail synthesis. The former employs a computationally intensive base model at lower resolutions, while the latter leverages a lightweight cascaded video super-resolution (VSR) model to achieve high-resolution output. In this work, we focus on studying key design principles for latter cascaded VSR models, which are underexplored currently. First, we propose two degradation strategies to generate training pairs that better mimic the output characteristics of the base model, ensuring alignment between the VSR model and its upstream generator. Second, we provide critical insights into VSR model behavior through systematic analysis of (1) timestep sampling strategies, (2) noise augmentation effects on low-resolution (LR) inputs. These findings directly inform our architectural and training innovations. Finally, we introduce interleaving temporal unit and sparse local attention to achieve efficient training and inference, drastically reducing computational overhead. Extensive experiments demonstrate the superiority of our framework over existing methods, with ablation studies confirming the efficacy of each design choice. Our work establishes a simple yet effective baseline for cascaded video super-resolution generation, offering practical insights to guide future advancements in efficient cascaded synthesis systems."
    },
    {
      "title": "KnowRL: 探索用于事实性的知识增强强化学习 (原标题: KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality)",
      "link": "https://arxiv.org/abs/2506.19807",
      "pubDate": "Tue, 24 Jun 2025 13:17:17 GMT",
      "isoDate": "2025-06-24T13:17:17.000Z",
      "creator": "Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang",
      "summary": "本文提出了一种名为KnowRL的知识增强强化学习方法，旨在解决大型语言模型（LLMs）在推理过程中由于无法准确识别知识边界而产生的严重幻觉问题，尤其是在慢思考模型中。\n\n**主要观点：**\n\n*   **问题：** 大型语言模型（LLMs）的慢思考模型常常出现幻觉，输出不正确的内容，因为它们无法准确识别知识边界。\n*   **方法：** KnowRL通过将基于知识验证的事实性奖励整合到强化学习（RL）训练过程中，引导模型执行基于事实的慢思考，帮助它们识别知识边界。\n*   **机制：** KnowRL在RL训练期间提供有针对性的事实输入，使模型能够学习和内化基于事实的推理策略。通过直接奖励推理步骤中对事实的遵守，KnowRL培养了更可靠的思考过程。\n*   **实验结果：** 在三个幻觉评估数据集和两个推理评估数据集上的实验结果表明，KnowRL有效地缓解了慢思考模型中的幻觉，同时保持了其原有的强大推理能力。\n*   **代码：** 论文代码已开源，地址为：[this https URL](this https URL)。\n\n**关键词：**\n\n*   大型语言模型 (LLMs)\n*   强化学习 (RL)\n*   幻觉\n*   知识验证\n*   事实性奖励\n*   慢思考模型\n*   知识边界\n\n**研究领域：**\n\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)\n*   机器学习 (cs.LG)\n*   多智能体系统 (cs.MA)",
      "shortSummary": "本文提出KnowRL，一种知识增强强化学习方法，旨在解决大型语言模型（LLMs）慢思考模型中普遍存在的幻觉问题。KnowRL通过将基于知识验证的事实性奖励整合到强化学习训练中，引导模型进行基于事实的慢思考，从而帮助模型识别知识边界。实验结果表明，KnowRL能有效缓解慢思考模型中的幻觉，同时保持其推理能力。论文代码已开源。",
      "translated_title": "KnowRL: 探索用于事实性的知识增强强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL."
    }
  ],
  "lastUpdated": "2025-06-27T09:32:38.936Z"
}