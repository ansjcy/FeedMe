{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "GLM-4.5：智能体、推理与编码（ARC）基础模型 (原标题: GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models)",
      "link": "https://arxiv.org/abs/2508.06471",
      "pubDate": "Fri, 08 Aug 2025 13:21:06 GMT",
      "isoDate": "2025-08-08T13:21:06.000Z",
      "creator": "GLM-4. 5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang",
      "summary": "## GLM-4.5：智能体、推理与编码（ARC）基础模型\n\n### 引言\n\n本文介绍了GLM-4.5，一个开源的混合专家（MoE）大型语言模型，旨在推动智能体（Agentic）、推理（Reasoning）和编码（Coding）（简称ARC）任务领域的研究进展。\n\n### 模型概述\n\n*   **模型类型**：GLM-4.5是一个开源的混合专家（MoE）大型语言模型。\n*   **参数规模**：\n    *   总参数量：3550亿（355B）。\n    *   激活参数量：320亿（32B）。\n*   **推理方法**：采用混合推理方法，支持“思考”（thinking）模式和“直接响应”（direct response）模式。\n*   **训练过程**：\n    *   经过多阶段训练，使用了23万亿（23T）个tokens。\n    *   通过专家模型迭代和强化学习进行全面的后期训练。\n\n### 性能表现\n\nGLM-4.5在智能体、推理和编码（ARC）任务中展现出强大的性能：\n\n*   **TAU-Bench**：得分70.1%。\n*   **AIME 24**：得分91.0%。\n*   **SWE-bench Verified**：得分64.2%。\n\n尽管参数量远少于一些竞争对手，GLM-4.5在所有评估模型中总体排名第三，在智能体基准测试中排名第二。\n\n### 发布版本\n\n为了促进推理和智能体AI系统的研究，GLM-4.5团队发布了两个版本：\n\n*   **GLM-4.5**：包含3550亿参数的完整版本。\n*   **GLM-4.5-Air**：一个更紧凑的版本，包含1060亿（106B）参数。\n\n### 可用性\n\n模型的代码、模型文件及更多详细信息已在线提供。",
      "shortSummary": "GLM-4.5是一个开源的混合专家（MoE）大型语言模型，总参数3550亿，激活参数320亿。它采用混合推理方法，在智能体、推理和编码（ARC）任务中表现出色，尽管参数量较少，仍位居评估模型前列。团队发布了GLM-4.5及其紧凑版GLM-4.5-Air，旨在推动AI研究。代码和模型已在线提供。",
      "translated_title": "GLM-4.5：智能体、推理与编码（ARC）基础模型",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5."
    },
    {
      "title": "Memp：探索智能体的程序记忆 (原标题: Memp: Exploring Agent Procedural Memory)",
      "link": "https://arxiv.org/abs/2508.06433",
      "pubDate": "Fri, 08 Aug 2025 12:20:56 GMT",
      "isoDate": "2025-08-08T12:20:56.000Z",
      "creator": "Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
      "summary": "Memp：探索智能体的程序记忆\n\n**1. 背景与问题**\n\n*   大型语言模型（LLM）驱动的智能体在执行各种任务方面表现出色。\n*   然而，它们面临一个关键挑战：程序记忆（procedural memory）脆弱，通常是手动设计或固化在静态参数中，难以学习和更新。\n\n**2. 核心贡献：Memp 框架**\n\n*   本文提出 Memp，旨在为智能体提供一种可学习、可更新且终身持续的程序记忆机制。\n*   Memp 的核心思想是将智能体过去的轨迹（trajectories）提炼成两种形式的知识：\n    *   **细粒度、分步的指令**：详细描述每一步的操作。\n    *   **高层次、脚本式的抽象**：概括任务的整体流程和策略。\n\n**3. 记忆的构建、检索与更新策略**\n\n*   研究探讨了构建（Build）、检索（Retrieval）和更新（Update）程序记忆的不同策略。\n*   Memp 结合了一个动态机制，持续地更新、纠正和废弃其内容，确保记忆库随着新经验的获取而不断演进。\n\n**4. 实验评估与成果**\n\n*   **评估任务**：在 TravelPlanner 和 ALFWorld 任务上进行了实证评估。\n*   **主要发现**：\n    *   随着记忆库的不断完善，智能体在类似任务上的成功率稳步提高，效率也显著提升。\n    *   由更强模型构建的程序记忆具有持久价值：将这些程序记忆迁移到较弱的模型上，能够带来显著的性能提升。\n\n**5. 项目状态**\n\n*   该研究目前处于进行中（Work in progress）的状态。",
      "shortSummary": "Memp 提出了一种为大型语言模型（LLM）智能体构建可学习、可更新程序记忆的方法。它通过提炼智能体过去的经验，生成细粒度指令和高层次脚本，并采用动态机制持续更新记忆。在 TravelPlanner 和 ALFWorld 任务上的评估显示，Memp 显著提高了智能体的成功率和效率。此外，由强模型构建的记忆迁移到弱模型上也能带来显著性能提升。",
      "translated_title": "Memp：探索智能体的程序记忆",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains."
    },
    {
      "title": "修剪不意外：通过首词惊奇度实现高效代码推理 (原标题: Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal)",
      "link": "https://arxiv.org/abs/2508.05988",
      "pubDate": "Thu, 07 Aug 2025 23:46:21 GMT",
      "isoDate": "2025-08-07T23:46:21.000Z",
      "creator": "Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, Xiaodong Gu",
      "summary": "# ASAP：一种高效代码推理的CoT压缩框架\n\n## 引言：大型推理模型（LRMs）与思维链（CoT）的挑战\n\n近年来，大型推理模型（LRMs）通过扩展思维链（CoT）的长度，在代码推理方面展现出卓越的能力。然而，过长的推理轨迹带来了巨大的挑战，包括高昂的训练成本、推理延迟以及部署可行性。为了解决这一问题，尽管出现了多种CoT压缩方法，但它们面临固有的权衡：\n\n*   **Token级方法**：通常会破坏语法和逻辑连贯性。\n*   **步级方法**：基于困惑度（perplexity）的方法无法可靠地捕捉到逻辑上关键的推理步骤。\n\n## ASAP框架：锚点引导与基于惊奇度的剪枝\n\n本文提出了一种名为ASAP（Anchor-guided, Surprisal-based Pruning）的新型粗粒度到细粒度CoT压缩框架。ASAP旨在克服现有方法的局限性，实现高效的代码推理。\n\nASAP框架的核心步骤包括：\n\n1.  **锚点引导剪枝（Anchor-guided Pruning）**\n    *   ASAP首先执行锚点引导剪枝，以保留核心推理结构。\n    *   这一步骤能够高效地减少后续处理的搜索空间。\n\n2.  **基于首词惊奇度的逻辑感知剪枝（Logic-aware Pruning via First-Token Surprisal）**\n    *   在锚点引导剪枝的基础上，ASAP通过选择逻辑上必要的推理步骤来实现逻辑感知剪枝。\n    *   这一选择是基于一种新颖的“首词惊奇度”（first-token surprisal）指标。\n\n3.  **推理时自主生成与利用（Autonomous Generation and Leveraging at Inference Time）**\n    *   最后，ASAP训练模型在推理时自主生成并利用这些简洁的CoT。\n    *   这使得在编码任务中能够实现高效的推理。\n\n## 实验结果与性能\n\n实验结果表明，ASAP在多个代码生成基准测试中实现了最先进的准确性，同时显著降低了训练和推理成本。\n\n*   **整体表现**：ASAP在代码生成任务中表现出色，实现了高准确性和成本效益。\n*   **LiveCodeBench v4_v5基准测试**：\n    *   在具有挑战性的LiveCodeBench v4_v5基准测试中，ASAP相较于最强的基线方法，将token生成量减少了23.5%。\n    *   推理延迟降低了43.5%。\n    *   同时，在Pass@1指标上取得了36.19%的竞争性准确率。\n\n## 结论\n\nASAP的成果突显了构建强大且高效的LRMs的一个有前景的方向。该方法通过智能地压缩CoT，解决了LRMs在实际应用中面临的效率瓶颈，为未来的代码推理模型发展提供了新的思路。",
      "shortSummary": "大型推理模型（LRMs）的过长思维链（CoT）导致高成本和延迟。本文提出ASAP框架，通过锚点引导和基于首词惊奇度的逻辑感知剪枝来压缩CoT。ASAP使模型在推理时自主生成简洁CoT，实现了高效代码推理。实验表明，ASAP在代码生成任务中达到最先进的准确率，并显著降低训练和推理成本。在LiveCodeBench上，ASAP减少23.5%的token生成和43.5%的推理延迟，同时保持高准确率。",
      "translated_title": "修剪不意外：通过首词惊奇度实现高效代码推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs."
    },
    {
      "title": "Genie Envisioner：一个用于机器人操作的统一世界基础平台 (原标题: Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation)",
      "link": "https://arxiv.org/abs/2508.05635",
      "pubDate": "Thu, 07 Aug 2025 13:59:44 GMT",
      "isoDate": "2025-08-07T13:59:44.000Z",
      "creator": "Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren",
      "summary": "## Genie Envisioner：机器人操作的统一世界基础平台\n\nGenie Envisioner (GE) 是一个为机器人操作设计的统一世界基础平台，它将策略学习、评估和仿真整合到一个单一的视频生成框架中。\n\n### 核心组件与功能\n\n1.  **GE-Base：核心视频扩散模型**\n    *   **定位**：GE平台的核心。\n    *   **类型**：一个大规模、指令条件（instruction-conditioned）的视频扩散模型。\n    *   **能力**：能够捕获真实世界机器人交互的空间、时间及语义动态。\n    *   **输出**：将这些动态信息编码到一个结构化的潜在空间中。\n\n2.  **GE-Act：从潜在表示到可执行动作**\n    *   **基础**：构建在GE-Base之上。\n    *   **机制**：通过一个轻量级的流匹配解码器（flow-matching decoder），将GE-Base生成的潜在表示映射到可执行的动作轨迹。\n    *   **优势**：\n        *   实现精确且泛化能力强的策略推理。\n        *   适用于多种不同的机器人实体（embodiments）。\n        *   所需监督（supervision）极少。\n\n3.  **GE-Sim：动作条件神经仿真器**\n    *   **作用**：作为一种动作条件（action-conditioned）的神经仿真器。\n    *   **支持**：为可扩展的评估和训练提供支持。\n    *   **输出**：生成高保真度的推演（rollouts），用于闭环策略的开发。\n\n### 评估与基准\n\n*   **EWMBench**：平台配备了一个名为EWMBench的标准化基准套件。\n    *   **衡量指标**：用于衡量视觉保真度（visual fidelity）、物理一致性（physical consistency）以及指令-动作对齐（instruction-action alignment）。\n\n### 平台愿景与可用性\n\n*   **目标**：Genie Envisioner的各个组件共同建立了一个可扩展且实用的基础，旨在实现指令驱动的通用具身智能（general-purpose embodied intelligence）。\n*   **发布**：所有代码、模型和基准都将公开发布。",
      "shortSummary": "Genie Envisioner (GE) 是一个统一的机器人操作平台，集成了策略学习、评估和仿真。其核心是GE-Base，一个指令条件视频扩散模型，用于捕获机器人交互动态。GE-Act将潜在表示映射为可执行动作，实现精确泛化。GE-Sim作为神经仿真器，支持可扩展训练和评估。平台还包含EWMBench基准套件。GE旨在成为指令驱动型通用具身智能的可扩展基础，所有资源将公开发布。",
      "translated_title": "Genie Envisioner：一个用于机器人操作的统一世界基础平台",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly."
    },
    {
      "title": "MOSEv2：一个更具挑战性的复杂场景视频目标分割数据集 (原标题: MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes)",
      "link": "https://arxiv.org/abs/2508.05630",
      "pubDate": "Thu, 07 Aug 2025 13:59:27 GMT",
      "isoDate": "2025-08-07T13:59:27.000Z",
      "creator": "Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai",
      "summary": "## MOSEv2：一个更具挑战性的复杂场景视频目标分割数据集\n\n### 引言：视频目标分割 (VOS) 及其现有挑战\n\n视频目标分割（VOS）旨在在整个视频中分割出指定的目标物体。尽管目前最先进的方法在DAVIS和YouTube-VOS等现有基准测试中取得了令人印象深刻的性能（例如，超过90%的J&F分数），但这些数据集主要包含突出、主导和孤立的物体，这限制了它们在真实世界场景中的泛化能力。为了推动VOS在更真实的复杂环境中发展，研究人员引入了MOSEv1（复杂视频目标分割）数据集。\n\n### MOSEv2 数据集概述\n\n*   **背景与目的**：MOSEv2建立在MOSEv1的优势和局限性之上，是一个显著更具挑战性的数据集，旨在进一步推动VOS方法在真实世界条件下的发展。\n*   **数据集规模与内容**：\n    *   包含5,024个视频。\n    *   拥有超过701,976个高质量掩码。\n    *   涵盖10,074个物体。\n    *   涉及200个类别。\n\n### MOSEv2 引入的复杂性与新挑战\n\n与MOSEv1相比，MOSEv2引入了显著更高的场景复杂性，具体包括：\n\n*   **场景复杂性**：\n    *   更频繁的物体消失和重新出现。\n    *   严重的遮挡和拥挤。\n    *   更小的物体。\n*   **环境条件**：\n    *   恶劣天气（例如，雨、雪、雾）。\n    *   低光照场景（例如，夜间、水下）。\n*   **特殊目标与场景**：\n    *   多镜头序列。\n    *   伪装物体。\n    *   非物理目标（例如，阴影、反射）。\n    *   需要外部知识的场景。\n\n### 基准测试结果\n\n研究人员在5种不同设置下对20种代表性VOS方法进行了基准测试，并观察到性能普遍下降。例如，SAM2在MOSEv1上的性能从76.4%下降到MOSEv2上的50.9%。此外，对9种视频目标跟踪方法进行评估也发现了类似的性能下降，这表明MOSEv2对不同任务都提出了挑战。\n\n### 结论与意义\n\n这些结果突出表明，尽管现有VOS方法在现有数据集上表现出高精度，但它们在面对真实世界的复杂性时仍然面临困难。MOSEv2的发布旨在促进VOS领域的研究，以开发出更鲁棒、更适用于真实世界场景的算法。\n\n### 数据集可用性\n\nMOSEv2数据集已公开可用。",
      "shortSummary": "MOSEv2是一个为复杂场景视频目标分割（VOS）设计的更具挑战性的数据集，旨在弥补现有数据集在真实世界复杂性方面的不足。它包含5024个视频和超过70万个高质量掩码，引入了物体消失重现、严重遮挡、小物体、恶劣天气、低光照及非物理目标等新挑战。基准测试显示，现有VOS方法在MOSEv2上的性能显著下降，凸显了在真实复杂场景下提升VOS能力的必要性。",
      "translated_title": "MOSEv2：一个更具挑战性的复杂场景视频目标分割数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&amp;F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video."
    },
    {
      "title": "关于SFT的泛化性：一个带有奖励修正的强化学习视角 (原标题: On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification)",
      "link": "https://arxiv.org/abs/2508.05629",
      "pubDate": "Thu, 07 Aug 2025 13:59:04 GMT",
      "isoDate": "2025-08-07T13:59:04.000Z",
      "creator": "Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang",
      "summary": "### SFT泛化性改进：动态微调（DFT）\n\n本文提出了一种名为“动态微调”（Dynamic Fine-Tuning, DFT）的方法，旨在解决大型语言模型（LLM）中监督微调（SFT）相对于强化学习（RL）泛化能力有限的问题。\n\n**核心问题与理论洞察：**\n\n*   **SFT的局限性：** 标准SFT在泛化能力方面不如强化学习。\n*   **数学分析揭示：** 通过数学分析，研究人员发现标准SFT的梯度隐式地编码了一种有问题的奖励结构，这严重限制了模型的泛化能力。\n\n**提出的解决方案：动态微调（DFT）**\n\n*   **方法：** DFT通过动态地根据每个标记的概率重新调整目标函数，从而稳定了梯度更新。\n*   **实现：** 这一改进仅需一行代码的修改。\n\n**实验结果与优势：**\n\n*   **显著提升：** DFT在多个具有挑战性的基准测试和基础模型上显著优于标准SFT，展现出极大的泛化能力提升。\n*   **离线RL表现：** 此外，该方法在离线强化学习（offline RL）设置中也表现出竞争力，提供了一种有效但更简单的替代方案。\n\n**意义：**\n\n*   这项工作将理论洞察与实际解决方案相结合，显著提升了SFT的性能。",
      "shortSummary": "本文提出动态微调（DFT）以解决监督微调（SFT）在大型语言模型中泛化能力有限的问题。研究发现标准SFT梯度隐含了有问题的奖励结构。DFT通过动态调整目标函数来稳定梯度更新，仅需一行代码修改。实验证明，DFT在多个基准测试中显著优于标准SFT，并提升了泛化能力，同时在离线强化学习中也表现出色，为SFT性能带来了实质性进展。",
      "translated_title": "关于SFT的泛化性：一个带有奖励修正的强化学习视角",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT."
    },
    {
      "title": "学习事实性推理 (原标题: Learning to Reason for Factuality)",
      "link": "https://arxiv.org/abs/2508.05618",
      "pubDate": "Thu, 07 Aug 2025 13:57:09 GMT",
      "isoDate": "2025-08-07T13:57:09.000Z",
      "creator": "Xilun Chen, Ilia Kulikov, Vincent-Pierre Berges, Barlas Oğuz, Rulin Shao, Gargi Ghosh, Jason Weston, Wen-tau Yih",
      "summary": "# 学习事实性推理\n\n本文探讨了推理大型语言模型（R-LLMs）在事实性方面的挑战，并提出了一种新的解决方案。\n\n## 核心问题\n\n*   **R-LLMs的事实性困境**: 尽管R-LLMs在复杂推理任务上取得了显著进展，但在长篇事实性基准测试中，它们比非推理模型更容易产生幻觉，导致事实性表现不佳。\n*   **在线强化学习的挑战**: 将在线强化学习（RL）应用于长篇事实性设置面临独特挑战，主要是因为缺乏可靠的验证方法来评估事实性。\n\n## 现有方法的局限性\n\n*   **离线RL与FActScore**: 之前的研究利用FActScore等自动事实性评估框架在离线RL设置中整理偏好数据。\n*   **奖励欺骗（Reward Hacking）**: 然而，研究发现直接将这些方法作为在线RL的奖励会导致“奖励欺骗”问题，例如模型会生成细节不足或相关性较低的响应，以最大化奖励。\n\n## 提出的解决方案\n\n*   **新型奖励函数**: 本文提出了一种新颖的奖励函数，该函数在在线RL中同时考虑以下三个关键因素：\n    1.  **事实精确性（Factual Precision）**: 确保生成内容的准确性。\n    2.  **响应细节水平（Response Detail Level）**: 鼓励模型提供更详细、全面的信息。\n    3.  **答案相关性（Answer Relevance）**: 确保响应与问题高度相关。\n*   **目标**: 通过这种多维度奖励函数，模型能够学习高质量的事实性推理。\n\n## 实验结果\n\n*   **评估基准**: 模型在六个长篇事实性基准测试上进行了评估。\n*   **显著改进**:\n    *   平均幻觉率降低了23.1个百分点。\n    *   答案细节水平提高了23%。\n    *   整体响应的有用性没有下降。",
      "shortSummary": "推理大型语言模型（R-LLMs）在事实性方面存在高幻觉率问题。将在线强化学习（RL）应用于此领域因缺乏可靠验证而面临挑战，且现有离线RL方法易导致奖励欺骗。本文提出一种新型在线RL奖励函数，同时考虑事实精确性、响应细节和答案相关性。实验结果显示，该方法在六个事实性基准测试上将幻觉率平均降低23.1%，答案细节水平提高23%，且不损害整体有用性，有效提升了R-LLMs的事实性推理能力。",
      "translated_title": "学习事实性推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness."
    },
    {
      "title": "Hi3DEval：通过分层有效性推进3D生成评估 (原标题: Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity)",
      "link": "https://arxiv.org/abs/2508.05609",
      "pubDate": "Thu, 07 Aug 2025 13:50:13 GMT",
      "isoDate": "2025-08-07T13:50:13.000Z",
      "creator": "Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu",
      "summary": "## Hi3DEval：推进3D生成评估的分层框架\n\n### 挑战\n尽管3D内容生成技术取得了快速进展，但对所生成3D资产的质量评估仍然充满挑战。现有方法主要依赖于基于图像的度量，并且仅在对象级别进行操作，这限制了它们捕捉空间一致性、材料真实性和高保真局部细节的能力。\n\n### Hi3DEval 框架\n为了解决这些挑战，研究人员引入了 **Hi3DEval**，一个专为3D生成内容设计的分层评估框架。该框架具有以下特点：\n\n*   **分层评估**：结合了对象级别和部件级别的评估，从而能够进行多维度的整体评估以及细粒度的质量分析。\n*   **扩展纹理评估**：将纹理评估扩展到美学外观之外，明确评估材料的真实性，重点关注反照率（albedo）、饱和度（saturation）和金属度（metallicness）等属性。\n\n### Hi3DBench 数据集与自动化评分系统\n为支持Hi3DEval框架，研究人员构建了以下关键组件：\n\n*   **Hi3DBench 数据集**：一个大规模数据集，包含多样化的3D资产和高质量的标注，并辅以可靠的多代理标注流程。\n*   **3D感知自动化评分系统**：基于混合3D表示提出，具体包括：\n    *   **视频表示**：用于对象级别和材料主题评估，以增强时空一致性建模。\n    *   **预训练3D特征**：用于部件级别的感知。\n\n### 实验结果\n广泛的实验表明，Hi3DEval 方法在建模3D特性方面优于现有的基于图像的度量，并实现了与人类偏好更高的对齐度，为手动评估提供了一种可扩展的替代方案。",
      "shortSummary": "Hi3DEval是一个分层评估框架，旨在解决3D内容生成质量评估的挑战。它结合了对象级和部件级评估，并扩展了对材料真实性的评估。该框架由大规模的Hi3DBench数据集和基于混合3D表示的自动化评分系统支持。实验证明，Hi3DEval在建模3D特性方面优于现有方法，并与人类偏好高度一致，为3D生成评估提供了可扩展的解决方案。",
      "translated_title": "Hi3DEval：通过分层有效性推进3D生成评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/."
    },
    {
      "title": "InfiGUI-G1：通过自适应探索策略优化推进GUI接地 (原标题: InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization)",
      "link": "https://arxiv.org/abs/2508.05731",
      "pubDate": "Thu, 07 Aug 2025 13:49:56 GMT",
      "isoDate": "2025-08-07T13:49:56.000Z",
      "creator": "Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, Fei Wu",
      "summary": "## InfiGUI-G1：通过自适应探索策略优化推进GUI接地\n\n### 引言\n\n随着多模态大型语言模型（MLLMs）的兴起，能够纯粹依靠视觉输入在图形用户界面（GUIs）上操作的自主智能体得到了显著发展。然而，一个核心挑战在于如何鲁棒地将自然语言指令进行“接地”（grounding）。这不仅需要精确的空间对齐（准确找到每个UI元素的坐标），更关键的是需要正确的语义对齐（将指令与功能上合适的UI元素匹配）。\n\n### 现有问题\n\n尽管可验证奖励强化学习（RLVR）已被证明能有效改善MLLMs的空间对齐，但研究发现，低效的探索策略限制了语义对齐的学习，阻碍了模型学习复杂的语义关联。\n\n### 解决方案：自适应探索策略优化（AEPO）\n\n为了解决这一探索问题，本文提出了一个新的策略优化框架——自适应探索策略优化（AEPO）。AEPO通过以下两种机制来增强探索能力：\n\n1.  **多答案生成策略**：强制模型进行更广泛的探索，生成多种可能的响应。\n2.  **自适应探索奖励（AER）函数**：通过一个基于效率第一性原理（η=U/C）推导出的理论基础奖励函数来指导探索过程。\n\n### 实验结果\n\n经过AEPO训练的模型，包括InfiGUI-G1-3B和InfiGUI-G1-7B，在多个具有挑战性的GUI接地基准测试中取得了新的最先进（SOTA）结果。与朴素的RLVR基线相比，这些模型在旨在测试泛化能力和语义理解的基准测试中，实现了高达9.0%的显著相对改进。\n\n### 结论\n\nInfiGUI-G1及其核心的AEPO框架有效解决了GUI接地中语义对齐的探索效率瓶颈，显著提升了MLLMs在GUI操作任务中的性能。",
      "shortSummary": "InfiGUI-G1引入了自适应探索策略优化（AEPO）框架，旨在解决多模态大型语言模型（MLLMs）在GUI接地任务中语义对齐的探索效率问题。现有RLVR方法在语义对齐上存在瓶颈。AEPO通过多答案生成和基于效率原则的自适应探索奖励（AER）函数，增强了探索能力。InfiGUI-G1-3B和InfiGUI-G1-7B模型在GUI接地基准测试中取得了最先进结果，相较于RLVR基线，性能提升高达9.0%，尤其在泛化和语义理解方面表现突出。",
      "translated_title": "InfiGUI-G1：通过自适应探索策略优化推进GUI接地",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1."
    },
    {
      "title": "无标签视觉-语言模型自适应：一项全面综述 (原标题: Adapting Vision-Language Models Without Labels: A Comprehensive Survey)",
      "link": "https://arxiv.org/abs/2508.05547",
      "pubDate": "Thu, 07 Aug 2025 12:27:37 GMT",
      "isoDate": "2025-08-07T12:27:37.000Z",
      "creator": "Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink",
      "summary": "# 无标签视觉-语言模型自适应：一项全面综述\n\n## 摘要\n\n视觉-语言模型（VLMs）在广泛任务中展现出卓越的泛化能力。然而，在未进行特定任务自适应的情况下，直接应用于特定下游场景时，其性能往往不尽理想。为了在提高实用性的同时保持数据效率，近期研究日益关注不依赖标注数据的无监督自适应方法。\n\n## 研究动机与目标\n\n尽管该领域兴趣日益增长，但目前仍缺乏针对无监督VLM自适应的统一、面向任务的综述。本论文旨在弥补这一空白，提供一个全面且结构化的领域概述。\n\n## 提出的分类体系\n\n本综述提出了一种基于无标签视觉数据可用性和性质的分类法，将现有方法归类为四大关键范式：\n\n1.  **无数据迁移 (Data-Free Transfer)**：指在没有可用数据的情况下进行模型自适应。\n2.  **无监督域迁移 (Unsupervised Domain Transfer)**：指在拥有大量无标签数据的情况下进行域适应。\n3.  **分批测试时自适应 (Episodic Test-Time Adaptation)**：指在处理批量数据时进行的自适应。\n4.  **在线测试时自适应 (Online Test-Time Adaptation)**：指在处理流式数据时进行的实时自适应。\n\n## 分析内容与贡献\n\n*   在上述框架内，论文分析了与每种范式相关的核心方法论和自适应策略，旨在建立对该领域的系统性理解。\n*   此外，综述还回顾了跨不同应用的代表性基准。\n*   最后，论文强调了该领域的开放性挑战和未来研究的有前景方向。",
      "shortSummary": "视觉-语言模型（VLMs）在特定任务上表现不佳，促使研究转向无标签的无监督自适应方法。本综述旨在填补该领域统一概述的空白，提出一个基于无标签数据可用性的分类体系。该体系将现有方法分为无数据迁移、无监督域迁移、分批测试时自适应和在线测试时自适应四种范式。论文分析了核心方法、自适应策略，并探讨了基准、挑战及未来方向，以系统理解无标签VLM自适应领域。",
      "translated_title": "无标签视觉-语言模型自适应：一项全面综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs."
    },
    {
      "title": "PRvL：量化大型语言模型在个人身份信息（PII）匿名化方面的能力与风险 (原标题: PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction)",
      "link": "https://arxiv.org/abs/2508.05545",
      "pubDate": "Thu, 07 Aug 2025 12:22:49 GMT",
      "isoDate": "2025-08-07T12:22:49.000Z",
      "creator": "Leon Garza, Anantaa Kotal, Aritran Piplai, Lavanya Elluri, Prajit Das, Aman Chadha",
      "summary": "### PRvL：大型语言模型在个人身份信息（PII）匿名化方面的能力与风险分析\n\n**引言**\n在受监管领域，从非结构化文本中匿名化个人身份信息（PII）对于确保数据隐私至关重要。传统方法，如基于规则的系统和领域特定的命名实体识别（NER）模型，在泛化性方面表现不佳，难以适应不同的格式和上下文。\n\n**大型语言模型（LLMs）的潜力与挑战**\n近期大型语言模型（LLMs）的进展为PII匿名化提供了有前景的替代方案。LLMs在需要上下文语言理解的任务中表现出色，包括自由文本中的PII匿名化。先前的研究表明，经过适当调整，LLMs可以成为有效的上下文隐私学习器。然而，LLM的架构和训练选择对匿名化性能的影响尚未得到充分探索。\n\n**本研究的贡献与分析**\n本研究对LLMs作为隐私保护的PII匿名化系统进行了全面分析。研究评估了一系列LLM架构和训练策略在PII匿名化方面的有效性。分析衡量了以下关键指标：\n\n*   **匿名化性能**：模型识别和匿名化PII的准确性。\n*   **语义保留**：匿名化后文本的原始含义和上下文是否得到有效保留。\n*   **PII泄露**：匿名化过程中PII信息意外暴露的风险。\n*   **延迟和计算成本**：系统运行效率和资源消耗。\n\n研究结果为配置准确、高效且注重隐私的基于LLM的匿名化器提供了实用指导。\n\n**PRvL：开源工具套件的发布**\n为了支持研究的可复现性和实际部署，本研究发布了PRvL，一个开源的微调模型和评估工具套件，专为通用PII匿名化设计。\n\n**PRvL 的特点与优势**\n\n*   **基于开源LLMs**：PRvL完全基于开源大型语言模型构建。\n*   **灵活的推理设置**：支持多种推理设置，以满足不同的灵活性和合规性需求。\n*   **高度可定制**：易于针对不同领域进行定制。\n*   **安全自管理环境**：PRvL设计为可在安全的、自管理的（on-premise）环境中完全操作。这意味着数据所有者无需依赖第三方服务，也无需将敏感内容暴露在自身基础设施之外，即可执行匿名化操作，从而最大程度地保障数据安全和隐私。\n\n**结论**\nPRvL提供了一个强大的框架，用于量化和优化LLMs在PII匿名化中的能力和风险，为企业和组织在保护数据隐私方面提供了实用的解决方案。",
      "shortSummary": "本研究全面分析了大型语言模型（LLMs）在个人身份信息（PII）匿名化方面的能力与风险。通过评估不同LLM架构和训练策略的匿名化性能、语义保留、PII泄露、延迟和计算成本，研究提供了配置高效、隐私感知匿名化器的实用指导。为支持实际应用，研究发布了PRvL，一个开源的微调模型和评估工具套件，允许数据所有者在安全的自管理环境中进行PII匿名化，无需依赖第三方服务。",
      "translated_title": "PRvL：量化大型语言模型在个人身份信息（PII）匿名化方面的能力与风险",
      "images": [],
      "contentSource": "完整文章",
      "content": "Redacting Personally Identifiable Information (PII) from unstructured text is critical for ensuring data privacy in regulated domains. While earlier approaches have relied on rule-based systems and domain-specific Named Entity Recognition (NER) models, these methods fail to generalize across formats and contexts. Recent advances in Large Language Models (LLMs) offer a promising alternative, yet the effect of architectural and training choices on redaction performance remains underexplored. LLMs have demonstrated strong performance in tasks that require contextual language understanding, including the redaction of PII in free-form text. Prior work suggests that with appropriate adaptation, LLMs can become effective contextual privacy learners. However, the consequences of architectural and training choices for PII Redaction remain underexplored. In this work, we present a comprehensive analysis of LLMs as privacy-preserving PII Redaction systems. We evaluate a range of LLM architectures and training strategies for their effectiveness in PII Redaction. Our analysis measures redaction performance, semantic preservation, and PII leakage, and compares these outcomes against latency and computational cost. The results provide practical guidance for configuring LLM-based redactors that are accurate, efficient, and privacy-aware. To support reproducibility and real-world deployment, we release PRvL, an open-source suite of fine-tuned models, and evaluation tools for general-purpose PII Redaction. PRvL is built entirely on open-source LLMs and supports multiple inference settings for flexibility and compliance. It is designed to be easily customized for different domains and fully operable within secure, self-managed environments. This enables data owners to perform redactions without relying on third-party services or exposing sensitive content beyond their own infrastructure."
    },
    {
      "title": "MELLA：弥合低资源语言多模态大语言模型的语言能力与文化基础 (原标题: MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs)",
      "link": "https://arxiv.org/abs/2508.05502",
      "pubDate": "Thu, 07 Aug 2025 11:36:24 GMT",
      "isoDate": "2025-08-07T11:36:24.000Z",
      "creator": "Yufei Gao, Jiaying Fei, Nuo Chen, Ruirui Chen, Guohang Yan, Yunshi Lan, Botian Shi",
      "summary": "## MELLA：弥合低资源语言多模态大语言模型的语言能力与文化基础\n\n### 引言\n\n*   **现有问题**：多模态大语言模型（MLLMs）在高资源语言中表现出色，但在低资源语言环境中效果显著下降。\n*   **当前方法局限**：现有的多语言增强方法通常仅限于文本模态或完全依赖机器翻译。这些方法虽然有助于模型获得基本的语言能力并产生“薄描述”（thin descriptions），但却忽视了多模态信息丰富性和文化基础的重要性，而这两者对于有效服务低资源语言用户至关重要。\n\n### 研究目标\n\n*   **弥合差距**：为了解决上述问题，本研究确定了在低资源语言环境中实现真正有效MLLM的两个重要目标：\n    1.  **语言能力**：确保模型具备处理和理解低资源语言的能力。\n    2.  **文化基础**：特别强调文化意识，使模型能够理解并生成符合当地文化背景的内容。\n\n### 解决方案：双源策略\n\n*   **数据收集**：为实现上述双重目标，研究提出了一种双源策略，指导数据收集以满足每个目标的需求：\n    *   **文化基础数据**：来源于原生网络alt-text（图像替代文本），以捕捉丰富的文化背景信息。\n    *   **语言能力数据**：来源于MLLM生成的字幕（captions），以增强模型的语言表达和理解能力。\n\n### 具体实现：MELLA数据集\n\n*   **数据集介绍**：作为具体实现，研究引入了MELLA，一个多模态、多语言的数据集。\n\n### 实验结果\n\n*   **性能提升**：在MELLA数据集上进行微调后，实验结果显示，八种语言在各种MLLM骨干网络上的性能普遍得到提升，模型能够生成更具信息量的“厚描述”（thick descriptions）。\n*   **增益验证**：研究验证了性能提升来源于文化知识增强和语言能力增强的双重作用。\n\n### 数据集可用性\n\n*   MELLA数据集可供研究人员使用。",
      "shortSummary": "多模态大语言模型（MLLMs）在低资源语言中表现受限，现有方法忽视了文化基础和多模态信息。为解决此问题，MELLA研究提出提升低资源语言MLLM的语言能力和文化基础。通过双源策略，MELLA数据集结合原生网络alt-text和MLLM生成字幕。实验表明，在MELLA上微调可显著提升八种语言的MLLM性能，使其能生成更具文化意识的“厚描述”，验证了文化知识和语言能力增强的有效性。",
      "translated_title": "MELLA：弥合低资源语言多模态大语言模型的语言能力与文化基础",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce \"thin descriptions\", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing \"thick descriptions\". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus."
    },
    {
      "title": "InfiAlign：一个可扩展且样本高效的LLM对齐框架，用于增强推理能力 (原标题: InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities)",
      "link": "https://arxiv.org/abs/2508.05496",
      "pubDate": "Thu, 07 Aug 2025 11:34:06 GMT",
      "isoDate": "2025-08-07T11:34:06.000Z",
      "creator": "Shuo Cai, Su Lu, Qi Zhou, Kejing Yang, Zhijie Sang, Congkai Xie, Hongxia Yang",
      "summary": "# InfiAlign: 可扩展且样本高效的LLM对齐框架，用于增强推理能力\n\n## 引言\n大型语言模型（LLMs）在复杂任务中展现出强大的推理能力。然而，通过后期训练（post-training）来增强这些能力通常资源密集，尤其是在数据和计算成本方面。现有提高样本效率的方法常依赖启发式或任务特定策略，限制了其可扩展性。\n\n## InfiAlign 框架介绍\n本文引入了 **InfiAlign**，一个可扩展且样本高效的后期训练框架，它通过整合监督微调（SFT）和直接偏好优化（DPO）来对齐LLMs，以增强其推理能力。\n\n### 核心组成\nInfiAlign 的核心是一个强大的数据选择管道。该管道利用多维质量指标，从开源推理数据集中自动筛选出高质量的对齐数据。\n\n### 关键优势\n*   **显著的性能提升**：通过精选数据实现卓越性能。\n*   **大幅降低数据需求**：显著减少了训练所需的数据量。\n*   **可扩展性**：该管道可扩展到新的数据源。\n\n## 实验结果与性能\n研究团队将 InfiAlign 应用于 **Qwen2.5-Math-7B-Base** 模型，并取得了显著成果：\n\n*   **SFT 阶段表现**：\n    *   InfiAlign 的 SFT 模型实现了与 DeepSeek-R1-Distill-Qwen-7B 相当的性能。\n    *   它仅使用了 DeepSeek-R1-Distill-Qwen-7B 大约 **12%** 的训练数据，极大地提高了数据效率。\n    *   模型在多样化的推理任务中展现出强大的泛化能力。\n*   **DPO 阶段提升**：\n    *   通过应用 DPO，模型获得了额外的性能提升，尤其在数学推理任务中表现出显著进步。\n    *   在 AIME 24/25 基准测试中，模型平均提升了 **3.89%**。\n\n## 结论与意义\n研究结果强调了将原则性数据选择与全阶段后期训练相结合的有效性。InfiAlign 为以可扩展和数据高效的方式对齐大型推理模型提供了一个实用的解决方案。\n\n## 资源可用性\n模型的检查点可在指定链接获取。",
      "shortSummary": "InfiAlign是一个可扩展且样本高效的LLM对齐框架，旨在增强推理能力。它结合了SFT和DPO，并采用强大的数据选择管道，从开源数据中自动筛选高质量数据。该框架在Qwen2.5-Math-7B-Base模型上，仅用12%的数据就达到了与DeepSeek-R1-Distill-Qwen-7B相当的SFT性能，并展现出强大的泛化能力。DPO进一步提升了数学推理表现，在AIME基准上平均提升3.89%。InfiAlign为LLM对齐提供了一个可扩展且数据高效的实用方案。",
      "translated_title": "InfiAlign：一个可扩展且样本高效的LLM对齐框架，用于增强推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT."
    },
    {
      "title": "DeepPHY：基准测试智能体视觉语言模型在物理推理方面的表现 (原标题: DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning)",
      "link": "https://arxiv.org/abs/2508.05405",
      "pubDate": "Thu, 07 Aug 2025 09:58:19 GMT",
      "isoDate": "2025-08-07T09:58:19.000Z",
      "creator": "Xinrun Xu, Pi Bu, Ye Wang, Börje F. Karlsson, Ziming Wang, Tengtao Song, Qi Zhu, Jun Song, Zhiming Ding, Bo Zheng",
      "summary": "# DeepPHY：基准测试智能体视觉语言模型在物理推理方面的表现\n\n## 引言与背景\n*   **视觉语言模型（VLMs）的现状与挑战：**\n    *   尽管视觉语言模型（VLMs）展现出强大的感知能力和令人印象深刻的视觉推理能力，但在处理复杂、动态环境时，它们在细节关注和精确行动规划方面仍面临挑战，导致性能不佳。\n    *   现实世界中的任务通常需要复杂的交互、高级空间推理、长期规划以及持续的策略优化，而这些能力往往依赖于对目标场景物理规则的理解。\n*   **评估难题：**\n    *   在现实世界场景中评估VLMs的这些能力通常成本极高，难以实现。\n\n## DeepPHY：新型基准测试框架\n*   **目的：**\n    *   为了弥补现有评估方法的不足，本研究引入了DeepPHY，一个新颖的基准测试框架。\n    *   DeepPHY旨在系统地评估视觉语言模型对基本物理原理的理解和推理能力。\n*   **设计与构成：**\n    *   DeepPHY整合了多个难度级别各异的物理推理模拟环境。\n    *   该框架还包含了细粒度的评估指标，以提供更精确的性能衡量。\n\n## 主要发现\n*   **性能局限性：**\n    *   通过DeepPHY进行的评估发现，即使是目前最先进的视觉语言模型，也难以将描述性的物理知识转化为精确的、预测性的控制行为。这表明VLMs在将抽象物理理解应用于具体行动规划方面仍有显著不足。\n\n## 其他信息\n*   **文档详情：** 本研究报告共48页。\n*   **研究领域：** 属于人工智能（cs.AI）领域。\n*   **引用信息：** 可通过arXiv:2508.05405 [cs.AI] 或 DOI: 10.48550/arXiv.2508.05405 进行引用。\n*   **提交日期：** 该论文于2025年8月7日提交。",
      "shortSummary": "视觉语言模型（VLMs）在复杂动态环境中的物理推理和精确行动规划方面表现不足，且现实世界评估成本高昂。为解决此问题，研究引入了DeepPHY基准框架。DeepPHY通过整合多难度模拟环境和细粒度评估指标，系统评估VLMs对基本物理原理的理解和推理能力。评估结果显示，即使是先进的VLMs也难以将描述性物理知识转化为精确的预测性控制，揭示了其在应用物理理解方面的局限性。",
      "translated_title": "DeepPHY：基准测试智能体视觉语言模型在物理推理方面的表现",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control."
    },
    {
      "title": "注意力盆地：为什么上下文位置在大型语言模型中很重要 (原标题: Attention Basin: Why Contextual Position Matters in Large Language Models)",
      "link": "https://arxiv.org/abs/2508.05128",
      "pubDate": "Thu, 07 Aug 2025 04:08:08 GMT",
      "isoDate": "2025-08-07T04:08:08.000Z",
      "creator": "Zihao Yi, Delong Zeng, Zhenqing Ling, Haohao Luo, Zhe Xu, Wei Liu, Jian Luan, Wanxia Cao, Ying Shen",
      "summary": "# 注意力盆地：大型语言模型中上下文位置的重要性\n\n大型语言模型（LLMs）的性能对输入信息在上下文中的位置表现出显著的敏感性。为了深入探究这种位置偏差背后的机制，研究人员进行了广泛的实验，并揭示了一个被称为“注意力盆地”（attention basin）的一致现象。\n\n## 注意力盆地现象\n\n当模型接收到一系列结构化项目（例如，检索到的文档或少样本示例）时，它们会系统性地将更高的注意力分配给序列的开头和结尾处的项目，而忽略中间部分。\n\n## 关键洞察\n\n分析进一步揭示，将更高的注意力分配给关键信息是提升模型性能的关键。\n\n## 解决方案：注意力驱动重排序（AttnRank）\n\n基于上述洞察，研究人员引入了“注意力驱动重排序”（AttnRank）框架。这是一个模型无关、无需训练且即插即用的两阶段方法，计算开销极小。\n\n### AttnRank 的两阶段流程：\n\n1.  **估计模型偏好**：使用一个小型校准集来估计模型固有的位置注意力偏好。\n2.  **内容重排序**：根据估计出的偏好，对检索到的文档或少样本示例进行重新排序，使最显著的内容与模型的高注意力位置对齐。\n\n## 实验结果\n\n在多跳问答（multi-hop QA）和少样本上下文学习（few-shot in-context learning）任务上的实验表明，AttnRank 在10个不同架构和规模的大型语言模型上均实现了显著的性能提升，且无需修改模型参数或训练过程。",
      "shortSummary": "大型语言模型性能受上下文位置影响。研究发现“注意力盆地”现象：模型更关注序列开头和结尾的信息，忽略中间。关键在于将注意力分配给重要信息。为此，研究提出AttnRank框架，通过两阶段重排序，将关键内容与模型高注意力位置对齐。AttnRank无需训练，模型无关，显著提升了多任务下10个大型语言模型的性能。",
      "translated_title": "注意力盆地：为什么上下文位置在大型语言模型中很重要",
      "images": [],
      "contentSource": "完整文章",
      "content": "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures."
    },
    {
      "title": "R-Zero：从零数据自进化的推理LLM (原标题: R-Zero: Self-Evolving Reasoning LLM from Zero Data)",
      "link": "https://arxiv.org/abs/2508.05004",
      "pubDate": "Wed, 06 Aug 2025 23:38:16 GMT",
      "isoDate": "2025-08-06T23:38:16.000Z",
      "creator": "Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu",
      "summary": "# R-Zero：从零数据自进化的推理LLM\n\n## 核心问题\n现有的自进化大型语言模型（LLMs）虽然提供了通向超智能的可扩展路径，但它们在训练过程中仍严重依赖大量人工整理的任务和标签（通常通过微调或强化学习）。这构成了AI系统能力超越人类智能的根本瓶颈。\n\n## R-Zero 框架介绍\n为了克服这一限制，研究人员引入了 **R-Zero**，这是一个完全自主的框架，能够从零开始生成自己的训练数据。\n\n## R-Zero 的工作机制\nR-Zero 的核心机制涉及两个独立模型的协同进化：\n*   **初始化**：从一个基础LLM开始，R-Zero 初始化两个具有不同角色的独立模型：一个“挑战者”（Challenger）和一个“解决者”（Solver）。\n*   **独立优化与协同进化**：\n    *   **挑战者**：其目标是提出接近解决者能力边缘的任务。挑战者因成功提出这些挑战性任务而获得奖励。\n    *   **解决者**：其目标是解决挑战者提出的日益复杂的任务。解决者因成功解决这些任务而获得奖励。\n*   **自生成课程**：通过这种持续的交互和优化过程，R-Zero 能够生成一个有针对性的、自我改进的课程，而无需任何预先存在的任务和标签。\n\n## 实验结果\nR-Zero 在经验上显著提升了不同骨干LLM的推理能力：\n*   在数学推理基准测试中，将 Qwen3-4B-Base 的性能提升了 **+6.49**。\n*   在通用领域推理基准测试中，将 Qwen3-4B-Base 的性能提升了 **+7.54**。\n\n## 总结\nR-Zero 提供了一种全新的、完全自主的LLM训练范式，摆脱了对人工数据的依赖，为实现超越人类智能的AI系统开辟了道路。",
      "shortSummary": "R-Zero是一个创新的、完全自主的框架，旨在从零数据训练自进化的推理LLM。它通过初始化“挑战者”和“解决者”两个模型，让挑战者提出任务，解决者解决任务，从而实现模型间的协同进化和自我改进。这种方法克服了传统LLM对人工数据的依赖，并在数学和通用推理基准测试中显著提升了模型性能，为AI超越人类智能提供了新路径。",
      "translated_title": "R-Zero：从零数据自进化的推理LLM",
      "images": [],
      "contentSource": "完整文章",
      "content": "Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks."
    },
    {
      "title": "使用高保真解码器引导一步扩散模型实现快速图像压缩 (原标题: Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression)",
      "link": "https://arxiv.org/abs/2508.04979",
      "pubDate": "Wed, 06 Aug 2025 22:24:03 GMT",
      "isoDate": "2025-08-06T22:24:03.000Z",
      "creator": "Zheng Chen, Mingde Zhou, Jinpei Guo, Jiale Yuan, Yifei Ji, Yulun Zhang",
      "summary": "# SODEC：使用高保真解码器引导一步扩散模型实现快速图像压缩\n\n## 概述\n\n本文介绍了一种名为SODEC（Single-step Diffusion image Compression）的新型一步扩散图像压缩模型。该模型旨在解决当前基于扩散的图像压缩方法所面临的两大核心挑战：解码延迟过高和图像保真度不足。\n\n## 现有扩散压缩模型的局限性\n\n尽管基于扩散的图像压缩在感知性能方面表现出色，但其应用受限于以下关键缺陷：\n\n*   **解码效率低下**：由于需要执行多步采样过程，导致解码时间过长。\n*   **保真度欠佳**：过度依赖生成先验知识，使得输出图像与原始图像的保真度难以保证。\n\n## SODEC模型的核心创新\n\nSODEC模型基于一个关键洞察：在图像压缩任务中，如果潜在表示（latent representation）本身已足够信息丰富，则无需进行多步迭代细化。基于此，SODEC采用了以下创新设计：\n\n*   **单步解码机制**\n    *   **信息丰富的潜在表示**：SODEC利用预训练的基于VAE（变分自编码器）的模型来生成具有高信息量的潜在表示。\n    *   **取代迭代去噪**：通过将传统的迭代去噪过程替换为一步解码，显著提升了图像解码速度。\n\n*   **高保真引导模块（Fidelity Guidance Module）**\n    *   为解决保真度不足的问题，SODEC引入了高保真引导模块。\n    *   该模块通过引导模型输出更忠实于原始图像的结果，有效提升了压缩图像的保真度。\n\n*   **码率退火训练策略（Rate Annealing Training Strategy）**\n    *   为了确保模型在极低比特率下也能进行有效训练，SODEC设计了一种独特的码率退火训练策略。\n    *   这使得模型在压缩比极高的情况下依然能保持优异性能。\n\n## 实验结果与性能优势\n\n广泛的实验验证了SODEC模型的卓越性能：\n\n*   **性能超越现有方法**：SODEC在码率-失真-感知（rate-distortion-perception）性能方面显著优于现有图像压缩方法。\n*   **解码速度大幅提升**：与以往的基于扩散的压缩模型相比，SODEC的解码速度提高了20倍以上。\n\n## 结论\n\nSODEC通过其创新的单步解码机制、高保真引导模块和码率退火训练策略，成功克服了扩散模型在图像压缩领域的主要挑战，为实现快速、高质量的图像压缩提供了一个高效且实用的解决方案。",
      "shortSummary": "SODEC是一种新型一步扩散图像压缩模型，旨在解决现有扩散模型解码慢、保真度差的问题。它通过利用信息丰富的潜在表示、单步解码、高保真引导模块和码率退火训练策略，显著提升了图像压缩速度（20倍以上）和码率-失真-感知性能。SODEC为快速高质量图像压缩提供了高效方案。",
      "translated_title": "使用高保真解码器引导一步扩散模型实现快速图像压缩",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20times. Code is released at: https://github.com/zhengchen1999/SODEC."
    },
    {
      "title": "REINA：基于正则化熵信息的损失函数，用于高效的同声语音翻译 (原标题: REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation)",
      "link": "https://arxiv.org/abs/2508.04946",
      "pubDate": "Wed, 06 Aug 2025 20:25:58 GMT",
      "isoDate": "2025-08-06T20:25:58.000Z",
      "creator": "Nameer Hirschkind, Joseph Liu, Mahesh Kumar Nandwana, Xiao Yu",
      "summary": "## REINA：高效同声语音翻译的正则化熵信息损失函数\n\n### 摘要\n\n*   **背景与挑战**\n    *   同声语音翻译（SimulST）系统在接收音频流的同时输出翻译文本或语音。\n    *   这类系统面临的核心挑战是如何平衡翻译质量与延迟之间的矛盾。\n\n*   **REINA 方法介绍**\n    *   **核心策略：** 仅当通过等待更多输入能够获取信息增益时才等待。\n    *   **新颖损失函数：** 引入了“正则化熵信息适应”（REINA）作为一种新型损失函数，用于训练自适应策略。\n    *   **理论基础：** REINA 是基于信息论原理推导而来的，旨在利用现有的非流式翻译模型进行训练。\n\n*   **性能与成果**\n    *   **帕累托前沿改进：** REINA 有助于将延迟/质量权衡的帕累托前沿推向超越现有工作的水平。\n    *   **最先进（SOTA）结果：** 利用 REINA 训练的 SimulST 模型在法语、西班牙语和德语与英语之间的互译任务上，仅使用开源或合成数据，就实现了与同等规模模型相比的最先进流式翻译结果。\n    *   **效率度量与提升：**\n        *   文章引入了一个新的流式传输效率度量标准。\n        *   定量分析表明，与现有方法相比，REINA 在延迟/质量权衡方面提高了高达21%（相对于非流式基线BLEU分数进行归一化）。\n\n*   **研究领域**\n    *   机器学习 (cs.LG)\n    *   计算与语言 (cs.CL)\n    *   音频与语音处理 (eess.AS)",
      "shortSummary": "REINA是一种基于正则化熵信息的新型损失函数，旨在优化同声语音翻译（SimulST）系统的翻译质量与延迟之间的权衡。其核心策略是仅在获取信息增益时才等待更多输入。REINA基于信息论原理，能将延迟/质量帕累托前沿推向新高。在法、西、德语与英语互译任务中，REINA实现了最先进的流式翻译结果，并使延迟/质量权衡效率提升高达21%。",
      "translated_title": "REINA：基于正则化熵信息的损失函数，用于高效的同声语音翻译",
      "images": [],
      "contentSource": "完整文章",
      "content": "Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores."
    },
    {
      "title": "我思，故我能力不足？一个评估大型语言模型在招聘评估中语言试金石检测的基准 (原标题: I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations)",
      "link": "https://arxiv.org/abs/2508.04939",
      "pubDate": "Wed, 06 Aug 2025 19:51:03 GMT",
      "isoDate": "2025-08-06T19:51:03.000Z",
      "creator": "Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah",
      "summary": "## 大型语言模型招聘评估中的语言偏见检测基准\n\n### 研究背景与目的\n\n本论文提出了一个全面的基准，旨在评估大型语言模型（LLMs）如何响应“语言试金石”（linguistic shibboleths）。“语言试金石”是指那些微妙的语言标记，它们可能无意中揭示说话者的性别、社会阶层或地域背景等人口统计学属性。\n\n### 研究方法\n\n*   通过精心构建的面试模拟进行评估。\n*   使用了100对经过验证的问题-回答对。\n*   该基准能够生成受控的语言变体，从而在保持语义等效性的前提下，隔离并精确测量特定语言现象对自动化评估系统中的人口统计学偏见的影响。\n\n### 主要发现\n\n*   研究表明，LLMs系统性地惩罚某些语言模式，尤其是“对冲语言”（hedging language），即使这些回答的内容质量与非对冲性回答相同。\n*   经验证，使用对冲语言的回答平均得分降低了25.6%。\n*   该基准在识别模型特定偏见方面表现出有效性。\n\n### 意义与应用\n\n*   这项工作为检测和衡量人工智能系统中的语言歧视奠定了基础框架。\n*   其研究成果在自动化决策制定（如招聘、贷款审批等）的公平性方面具有广泛的应用价值。",
      "shortSummary": "该研究引入了一个基准，用于评估大型语言模型（LLMs）在招聘评估中对“语言试金石”的反应。研究发现，LLMs系统性地惩罚某些语言模式，特别是“对冲语言”，即使内容质量相同，也会导致评分显著降低。该基准能够有效识别并量化AI系统中的语言歧视，为自动化决策的公平性提供了重要工具。",
      "translated_title": "我思，故我能力不足？一个评估大型语言模型在招聘评估中语言试金石检测的基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts."
    },
    {
      "title": "Voost：一种用于双向虚拟试穿和试脱的统一可扩展扩散Transformer (原标题: Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off)",
      "link": "https://arxiv.org/abs/2508.04825",
      "pubDate": "Wed, 06 Aug 2025 15:10:58 GMT",
      "isoDate": "2025-08-06T15:10:58.000Z",
      "creator": "Seungyong Lee, Jeong-gi Kwak",
      "summary": "## Voost：一种用于双向虚拟试穿和试脱的统一可扩展扩散Transformer\n\n### 摘要\n\n虚拟试穿技术旨在合成一个人穿着目标服装的逼真图像。然而，准确地建模服装与身体之间的对应关系仍然是一个持续的挑战，尤其是在姿势和外观发生变化时。\n\n### Voost 框架\n\n本文提出了 **Voost**，一个统一且可扩展的框架，它通过单个扩散Transformer模型共同学习虚拟试穿（try-on）和试脱（try-off）两个任务。\n\n*   **双向学习优势**：\n    *   通过联合建模这两个任务，Voost 使得每对服装-人物组合能够对两个方向（试穿和试脱）进行监督。\n    *   支持对生成方向和服装类别进行灵活的条件设置。\n    *   增强了服装与身体之间的关系推理能力，而无需使用任务特定的网络、辅助损失或额外的标签。\n\n### 推理时技术\n\nVoost 引入了两种推理时技术，以进一步提升性能和鲁棒性：\n\n*   **注意力温度缩放（Attention Temperature Scaling）**：增强模型对分辨率或遮罩变化的鲁棒性。\n*   **自校正采样（Self-Corrective Sampling）**：利用任务间的双向一致性进行自我校正。\n\n### 实验结果\n\n广泛的实验表明，Voost 在试穿和试脱基准测试中均取得了最先进的（state-of-the-art）结果。它在对齐准确性、视觉保真度和泛化能力方面，持续优于强大的基线模型。",
      "shortSummary": "Voost 是一种统一且可扩展的扩散Transformer框架，用于同时学习虚拟试穿和试脱。它通过双向建模增强了服装与身体的关系推理，无需额外组件。该框架引入了注意力温度缩放和自校正采样等推理技术，显著提升了对齐准确性、视觉保真度和泛化能力。实验证明，Voost 在试穿和试脱任务上均达到了最先进的性能。",
      "translated_title": "Voost：一种用于双向虚拟试穿和试脱的统一可扩展扩散Transformer",
      "images": [],
      "contentSource": "完整文章",
      "content": "Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization."
    }
  ],
  "lastUpdated": "2025-08-11T09:38:11.505Z"
}