{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑 (原标题: VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space)",
      "link": "https://arxiv.org/abs/2508.19247",
      "pubDate": "Tue, 26 Aug 2025 13:59:47 GMT",
      "isoDate": "2025-08-26T13:59:47.000Z",
      "creator": "Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng",
      "summary": "# VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑\n\n## 摘要\n\n本文介绍了VoxHammer，一种新颖的免训练方法，旨在解决3D局部编辑中现有方法面临的挑战，即难以精确保留未编辑区域并保持整体连贯性。VoxHammer受结构化3D生成模型的启发，通过在原生3D潜在空间中直接进行编辑，实现了对指定区域的精确且连贯的修改。\n\n## 核心问题\n\n*   **3D局部编辑的重要性**：在游戏产业和机器人交互中，对3D模型的特定区域进行局部编辑至关重要。\n*   **现有方法的局限性**：\n    *   通常通过编辑渲染的多视图图像，然后重建3D模型。\n    *   面临的主要挑战是难以精确地保留未编辑区域，并确保编辑后的模型整体连贯性。\n\n## VoxHammer 方法\n\nVoxHammer提出了一种在3D潜在空间中进行编辑的训练无关方法，其核心流程分为两个阶段：\n\n### 1. 反演阶段 (Inversion)\n\n*   **输入**：一个给定的3D模型。\n*   **过程**：VoxHammer首先预测该模型的反演轨迹。\n*   **输出**：在每个时间步，获取其反演的潜在变量（inverted latents）和键值token（key-value tokens）。\n\n### 2. 去噪与编辑阶段 (Denoising and Editing)\n\n*   **核心操作**：在此阶段，VoxHammer将保留区域的去噪特征替换为在反演阶段获得的相应反演潜在变量和缓存的键值token。\n*   **机制**：通过保留这些上下文特征，VoxHammer能够确保：\n    *   未编辑的保留区域能够一致地重建。\n    *   编辑过的部分能够与模型其余部分连贯地集成。\n\n## 评估与实验\n\n*   **数据集**：为了评估保留区域的一致性，研究团队构建了**Edit3D-Bench**数据集。\n    *   这是一个人工标注的数据集，包含数百个样本。\n    *   每个样本都仔细标注了3D编辑区域。\n*   **实验结果**：实验证明，VoxHammer在以下两个关键指标上显著优于现有方法：\n    *   保留区域的3D一致性。\n    *   整体编辑质量。\n\n## 潜在影响与未来展望\n\n*   VoxHammer方法有望用于合成高质量的编辑配对数据。\n*   这将为上下文3D生成（in-context 3D generation）奠定坚实的数据基础。\n\n## 项目页面\n\n更多详情请访问项目页面：this https URL",
      "shortSummary": "VoxHammer是一种免训练的3D编辑方法，旨在解决现有方法在保留区域一致性和整体连贯性方面的挑战。它通过在3D潜在空间中直接编辑，利用反演潜在变量和键值token精确保留未编辑区域，并确保编辑部分的连贯集成。实验表明，VoxHammer在3D一致性和整体质量上显著优于现有方法，为高质量3D编辑和上下文3D生成提供了新途径。",
      "translated_title": "VoxHammer：在原生3D空间中进行免训练的精确连贯3D编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/."
    },
    {
      "title": "自回归通用视频分割模型 (原标题: Autoregressive Universal Video Segmentation Model)",
      "link": "https://arxiv.org/abs/2508.19242",
      "pubDate": "Tue, 26 Aug 2025 13:59:13 GMT",
      "isoDate": "2025-08-26T13:59:13.000Z",
      "creator": "Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma",
      "summary": "## 自回归通用视频分割模型（AUSM）\n\n### 背景与挑战\n\n*   **现有问题：** 尽管像SAM2这样的最新视频基础模型在提示式视频分割（即根据外部提示进行分割）方面表现出色，但许多实际应用场景需要无提示分割。无提示分割的目标是在没有外部线索的情况下，自动检测并跟踪视频中的所有对象。\n*   **当前局限：** 目前的无提示视频分割领域被各种任务特定模型和管道所碎片化，缺乏一个统一的解决方案。\n\n### AUSM的核心贡献\n\n*   **统一架构：** 本文引入了自回归通用分割模型（Autoregressive Universal Segmentation Model, AUSM），这是一种单一架构，能够同时处理提示式和无提示式视频分割任务。\n*   **创新方法：** AUSM将流式视频分割重新定义为序列掩码预测问题，其概念类似于语言建模中的序列预测。\n\n### AUSM的技术特点\n\n1.  **基于状态空间模型：** AUSM构建在最新的状态空间模型之上，这使其能够有效地处理序列数据。\n2.  **固定大小空间状态：** 模型维护一个固定大小的空间状态，使其能够扩展到任意长度的视频流，而不会因视频长度增加而导致计算复杂度急剧上升。\n3.  **并行训练设计：** AUSM的所有组件都设计为支持跨帧并行训练，这与传统的迭代训练方法相比，显著提高了训练速度。\n\n### 性能表现\n\n*   **超越现有方法：** 在多个标准基准测试（包括DAVIS17、YouTube-VOS 2018 & 2019、MOSE、YouTube-VIS 2019 & 2021以及OVIS）上，AUSM的性能均优于先前的通用流式视频分割方法。\n*   **训练效率提升：** 在16帧序列的训练中，AUSM的训练速度最高可提高2.5倍，显示出其在效率方面的显著优势。",
      "shortSummary": "自回归通用分割模型（AUSM）提出了一种统一架构，将流式视频分割重新定义为序列掩码预测，从而整合了提示式和无提示式视频分割。AUSM基于状态空间模型，维护固定大小空间状态，并支持跨帧并行训练，有效处理任意长度视频流。它在多个标准基准测试中超越了现有通用方法，并显著提高了训练速度，为视频理解提供了一个高效且通用的解决方案。",
      "translated_title": "自回归通用视频分割模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent video foundation models such as SAM2 excel at prompted video segmentation by treating masks as a general-purpose primitive. However, many real-world settings require unprompted segmentation that aims to detect and track all objects in a video without external cues, leaving today's landscape fragmented across task-specific models and pipelines. We recast streaming video segmentation as sequential mask prediction, analogous to language modeling, and introduce the Autoregressive Universal Segmentation Model (AUSM), a single architecture that unifies both prompted and unprompted video segmentation. Built on recent state-space models, AUSM maintains a fixed-size spatial state and scales to video streams of arbitrary length. Furthermore, all components of AUSM are designed for parallel training across frames, yielding substantial speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS 2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prior universal streaming video segmentation methods and achieves up to 2.5x faster training on 16-frame sequences."
    },
    {
      "title": "OmniHuman-1.5：通过认知模拟为虚拟形象注入活跃思维 (原标题: OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation)",
      "link": "https://arxiv.org/abs/2508.19209",
      "pubDate": "Tue, 26 Aug 2025 13:15:26 GMT",
      "isoDate": "2025-08-26T13:15:26.000Z",
      "creator": "Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, Mingyuan Gao",
      "summary": "## OmniHuman-1.5：通过认知模拟生成具有活跃思维的虚拟形象\n\n### 挑战：现有虚拟形象模型的局限性\n\n当前视频虚拟形象模型能够生成流畅的人体动画，但它们往往停留在物理相似性层面，难以捕捉角色的真实本质。它们的动作通常仅与音频节奏等低级线索同步，缺乏对情感、意图或上下文的深层语义理解。\n\n### 解决方案：OmniHuman-1.5 框架\n\n为了弥补这一差距，研究人员提出了一个名为 **OmniHuman-1.5** 的框架，旨在生成不仅在物理上合理，而且在语义上连贯和富有表现力的角色动画。该模型建立在两项关键技术贡献之上：\n\n1.  **利用多模态大语言模型 (MLLMs) 进行高级语义指导：**\n    *   模型利用多模态大语言模型来合成结构化的文本表示，提供高层次的语义指导。\n    *   这种指导使运动生成器超越了简单的节奏同步，能够产生与上下文和情感产生共鸣的动作。\n\n2.  **引入带有新型伪最后一帧设计的专用多模态 DiT 架构：**\n    *   为了确保有效融合多模态输入并减轻模态间冲突，模型引入了一种专门的多模态 DiT 架构，其中包含新颖的“伪最后一帧”（Pseudo Last Frame）设计。\n\n### 协同作用与性能\n\n这些组件的协同作用使得 OmniHuman-1.5 模型能够准确解释音频、图像和文本的联合语义，从而生成与角色、场景和语言内容深度连贯的动作。\n\n广泛的实验表明，该模型在以下方面取得了领先性能：\n\n*   唇语同步准确性\n*   视频质量\n*   动作自然度\n*   与文本提示的语义一致性\n\n此外，该方法还展现出卓越的扩展性，能够应用于多人物和非人类主体等复杂场景。",
      "shortSummary": "OmniHuman-1.5 框架旨在通过认知模拟为虚拟形象注入活跃思维，解决现有模型缺乏深层语义理解的问题。它利用多模态大语言模型提供高层次语义指导，并引入带有新型伪最后一帧设计的专用多模态 DiT 架构，有效融合多模态输入。该模型能生成物理合理、语义连贯且富有表现力的动画，在唇语同步、视频质量、动作自然度和语义一致性等指标上表现出色，并具有良好的场景扩展性。",
      "translated_title": "OmniHuman-1.5：通过认知模拟为虚拟形象注入活跃思维",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing video avatar models can produce fluid human animations, yet they struggle to move beyond mere physical likeness to capture a character's authentic essence. Their motions typically synchronize with low-level cues like audio rhythm, lacking a deeper semantic understanding of emotion, intent, or context. To bridge this gap, we propose a framework designed to generate character animations that are not only physically plausible but also semantically coherent and expressive. Our model, OmniHuman-1.5, is built upon two key technical contributions. First, we leverage Multimodal Large Language Models to synthesize a structured textual representation of conditions that provides high-level semantic guidance. This guidance steers our motion generator beyond simplistic rhythmic synchronization, enabling the production of actions that are contextually and emotionally resonant. Second, to ensure the effective fusion of these multimodal inputs and mitigate inter-modality conflicts, we introduce a specialized Multimodal DiT architecture with a novel Pseudo Last Frame design. The synergy of these components allows our model to accurately interpret the joint semantics of audio, images, and text, thereby generating motions that are deeply coherent with the character, scene, and linguistic content. Extensive experiments demonstrate that our model achieves leading performance across a comprehensive set of metrics, including lip-sync accuracy, video quality, motion naturalness and semantic consistency with textual prompts. Furthermore, our approach shows remarkable extensibility to complex scenarios, such as those involving multi-person and non-human subjects. Homepage: https://omnihuman-lab.github.io/v1_5/"
    },
    {
      "title": "VibeVoice 技术报告 (原标题: VibeVoice Technical Report)",
      "link": "https://arxiv.org/abs/2508.19205",
      "pubDate": "Tue, 26 Aug 2025 13:09:12 GMT",
      "isoDate": "2025-08-26T13:09:12.000Z",
      "creator": "Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei",
      "summary": "# VibeVoice 技术报告\n\n本报告介绍了VibeVoice，一个旨在通过“下一词元扩散”（next-token diffusion）技术合成多说话者长篇语音的新颖模型。\n\n## 核心技术与创新\n\n*   **下一词元扩散（Next-token Diffusion）**：VibeVoice采用了一种统一的方法，通过自回归生成潜在向量来建模连续数据，从而实现语音合成。\n*   **新型连续语音词元分析器（Novel Continuous Speech Tokenizer）**：\n    *   该模型引入了一种创新的连续语音词元分析器。\n    *   与流行的Encodec模型相比，它将数据压缩率提高了80倍，同时保持了可比的性能。\n    *   该词元分析器能够有效保留音频保真度，并显著提升处理长序列时的计算效率。\n\n## VibeVoice的性能与能力\n\n*   **长篇语音合成**：VibeVoice能够合成长达90分钟的长篇语音（在64K的上下文窗口长度下）。\n*   **多说话者支持**：该模型最多可以合成4位说话者的语音。\n*   **真实对话氛围**：VibeVoice能够捕捉真实的对话“氛围”（conversational \"vibe\"）。\n*   **卓越性能**：其性能超越了现有的开源和专有对话模型。\n\n## 研究领域\n\n*   计算与语言 (cs.CL)\n*   人工智能 (cs.AI)\n*   声音 (cs.SD)",
      "shortSummary": "VibeVoice是一个利用“下一词元扩散”技术合成多说话者长篇语音的新模型。它引入了一种新型连续语音词元分析器，将数据压缩率提高80倍，同时保持音频保真度和计算效率。VibeVoice能合成长达90分钟、最多4位说话者的语音，捕捉真实对话氛围，并超越现有对话模型。",
      "translated_title": "VibeVoice 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models."
    },
    {
      "title": "通过探究知识和推理揭示大型语言模型中的科学问题解决能力 (原标题: Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning)",
      "link": "https://arxiv.org/abs/2508.19202",
      "pubDate": "Tue, 26 Aug 2025 13:04:23 GMT",
      "isoDate": "2025-08-26T13:04:23.000Z",
      "creator": "Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan",
      "summary": "## 通过探究知识和推理揭示大型语言模型中的科学问题解决能力\n\n### 引言\n大型语言模型（LLMs）在科学问题解决方面面临独特的挑战，这不仅需要深厚的领域知识，还需要通过复杂的推理来应用这些知识。尽管自动化科学推理器在辅助人类科学家方面前景广阔，但目前缺乏一个被广泛采用的整体基准来评估科学推理能力，也很少有方法系统地解耦知识和推理在这些任务中的不同作用。\n\n### 现有问题与解决方案\n为了解决这些空白，研究人员提出了以下方案：\n\n*   **SciReas 基准套件**：引入了一个多样化的现有科学推理任务基准套件，名为 `SciReas`。\n*   **SciReas-Pro 子集**：从 `SciReas` 中筛选出一个子集 `SciReas-Pro`，该子集需要更复杂的推理能力。\n    *   通过对 `SciReas` 和 `SciReas-Pro` 进行整体评估，研究揭示了仅依赖单个基准时无法发现的关于科学推理性能的深刻见解。\n*   **KRUX 探究框架**：提出了一个名为 `KRUX` 的探究框架，用于研究知识和推理在科学任务中的不同作用。\n\n### 主要发现\n结合 `SciReas`、`SciReas-Pro` 和 `KRUX`，研究人员进行了深入分析，得出了以下几个关键发现：\n\n1.  **知识检索瓶颈**：从模型参数中检索任务相关知识是LLMs在科学推理中的一个关键瓶颈。\n2.  **外部知识的益处**：在推理增强的基础上，通过上下文添加外部知识对推理模型有持续的益处。\n3.  **口头化推理的增强作用**：增强口头化推理（verbalized reasoning）能够提高LLMs浮现任务相关知识的能力。\n\n### 其他贡献\n*   研究还进行了一项轻量级分析，将其科学领域的数据构成与当前关于长链式思维（CoT）监督微调（SFT）的研究进行了比较。\n*   发布了 `SciLit01`，这是一个强大的8B科学推理基线模型。",
      "shortSummary": "本研究旨在揭示大型语言模型（LLMs）在科学问题解决中的知识与推理机制。针对缺乏整体评估基准和知识推理解耦方法的现状，研究引入了`SciReas`基准套件及其复杂推理子集`SciReas-Pro`，并提出了`KRUX`探究框架。主要发现包括：从模型参数中检索知识是LLMs的瓶颈；外部知识对推理模型有益；增强口头化推理能提升知识浮现能力。研究还发布了`SciLit01`科学推理基线模型。",
      "translated_title": "通过探究知识和推理揭示大型语言模型中的科学问题解决能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning."
    },
    {
      "title": "FastMesh：通过组件解耦实现高效艺术网格生成 (原标题: FastMesh:Efficient Artistic Mesh Generation via Component Decoupling)",
      "link": "https://arxiv.org/abs/2508.19188",
      "pubDate": "Tue, 26 Aug 2025 12:51:02 GMT",
      "isoDate": "2025-08-26T12:51:02.000Z",
      "creator": "Jeonghwan Kim, Yushi Lan, Armando Fortes, Yongwei Chen, Xingang Pan",
      "summary": "# FastMesh：通过组件解耦实现高效艺术网格生成\n\n本文介绍了一种名为 FastMesh 的高效框架，旨在解决现有艺术网格生成方法中存在的效率低下和冗余问题。\n\n## 现有方法的局限性\n*   **冗余的令牌序列**：当前网格生成方法通常将三角网格令牌化为序列，并使用自回归模型进行生成。\n*   **顶点重复利用**：由于每个顶点被多个面共享，导致令牌序列中顶点信息被多次重复利用。\n*   **效率低下**：这种冗余导致令牌序列过长，进而使得生成过程效率低下。\n\n## FastMesh 提出的解决方案：组件解耦\nFastMesh 通过将顶点和面分开处理，显著减少了冗余，从而实现高效的艺术网格生成。\n\n### 核心机制\n1.  **顶点生成**：\n    *   采用一个**自回归模型**专门用于顶点生成。\n    *   将令牌数量减少到现有最紧凑分词器所需数量的约 **23%**，大幅降低了冗余。\n2.  **网格补全（面生成）**：\n    *   利用**双向 Transformer** 在**单一步骤**内完成网格。\n    *   该 Transformer 捕捉顶点间的关系，并构建定义网格面的邻接矩阵。\n3.  **质量提升组件**：\n    *   **保真度增强器 (Fidelity Enhancer)**：用于优化顶点位置，使其排列更自然。\n    *   **后处理框架 (Post-processing Framework)**：用于移除不理想的边连接。\n\n## 实验结果\n*   **速度提升**：与现有最先进的方法相比，FastMesh 在网格生成速度上实现了**超过 8 倍**的提升。\n*   **质量提高**：同时，FastMesh 能够生成**更高质量**的网格。",
      "shortSummary": "FastMesh 提出一种高效的艺术网格生成框架，通过将顶点和面解耦处理来解决现有方法的冗余问题。它使用自回归模型生成顶点，然后通过双向 Transformer 在单一步骤内完成面生成，并辅以保真度增强器和后处理。实验表明，FastMesh 比现有技术快 8 倍以上，并能生成更高质量的网格，显著减少了令牌冗余。",
      "translated_title": "FastMesh：通过组件解耦实现高效艺术网格生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23\\% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8times faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality."
    },
    {
      "title": "MovieCORE：电影中的认知推理 (原标题: MovieCORE: COgnitive REasoning in Movies)",
      "link": "https://arxiv.org/abs/2508.19026",
      "pubDate": "Tue, 26 Aug 2025 09:43:45 GMT",
      "isoDate": "2025-08-26T09:43:45.000Z",
      "creator": "Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu",
      "summary": "MovieCORE是一个新颖的视频问答（VQA）数据集，旨在深入探究对电影内容的认知理解，超越现有数据集的表面理解。\n\n**主要特点与贡献：**\n\n*   **深入认知理解**：\n    *   MovieCORE专注于需要“系统2思维”的问题，这些问题要求更深层次的推理和理解，同时仍与视频材料具体相关。\n    *   这与现有VQA数据集形成对比，后者通常侧重于表层或事实性理解。\n\n*   **数据生成方法**：\n    *   本文提出了一种创新的“智能体式头脑风暴”方法。\n    *   该方法利用多个大型语言模型（LLM）作为思维智能体，协同生成和完善高质量的问答对，确保数据集的深度和复杂性。\n\n*   **数据集质量评估**：\n    *   为评估MovieCORE数据集的质量，研究人员开发了一套认知测试。\n    *   这些测试用于评估数据集问题的深度、启发思考的潜力以及句法复杂性。\n\n*   **VQA模型评估方案**：\n    *   论文还提出了一项全面的评估方案，专门用于评估VQA模型在处理更深层次认知任务时的性能。\n\n*   **模型增强模块**：\n    *   为解决现有视频-语言模型（VLM）在处理复杂认知任务时的局限性，研究引入了“智能体选择增强”（Agentic Choice Enhancement, ACE）模块。\n    *   ACE模块能够在模型训练后，将其推理能力提高高达25%，显著提升了VLM的性能。\n\n*   **研究意义**：\n    *   MovieCORE的工作有助于推动人工智能系统在电影理解领域的发展。\n    *   它为当前VQA模型在面对更具挑战性、更细致的电影内容问题时的能力和局限性，提供了宝贵的见解。\n\n**可用性与接受情况：**\n\n*   项目页面、数据集和代码可在提供的URL获取。\n*   该论文已被EMNLP'2025主会议接受。",
      "shortSummary": "MovieCORE是一个新颖的视频问答（VQA）数据集，旨在促进AI对电影内容的深层认知理解，而非表面理解。它采用多LLM智能体式头脑风暴生成高质量问答对，并引入“智能体选择增强”（ACE）模块，可将视频-语言模型（VLM）的推理能力提升高达25%。该工作为评估和提升VQA模型处理复杂电影内容的能力提供了新方法，并已被EMNLP'2025接受。",
      "translated_title": "MovieCORE：电影中的认知推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html."
    },
    {
      "title": "ThinkDial：一种控制大型语言模型推理工作量的开放方案 (原标题: ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models)",
      "link": "https://arxiv.org/abs/2508.18773",
      "pubDate": "Tue, 26 Aug 2025 03:57:28 GMT",
      "isoDate": "2025-08-26T03:57:28.000Z",
      "creator": "Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen",
      "summary": "### ThinkDial：一种控制大型语言模型推理工作量的开放方案\n\n**引言与背景**\n大型语言模型（LLMs）结合思维链（chain-of-thought）推理展现出卓越的问题解决能力。然而，在实际部署中，如何有效控制其计算工作量仍然是一个重大挑战。尽管OpenAI的gpt-oss系列等专有系统已经引入了离散的操作模式来实现直观的推理控制，但开源社区在实现类似能力方面仍显不足。\n\n**ThinkDial 框架介绍**\n本文提出了 ThinkDial，这是首个开放方案的端到端框架，成功实现了类似 gpt-oss 的、通过离散操作模式进行可控推理的能力。\n\n**核心功能与模式**\nThinkDial 系统支持在三种不同的推理模式之间无缝切换，以平衡性能与计算资源：\n*   **高模式 (High mode)**：提供完整的推理能力。\n*   **中模式 (Medium mode)**：实现 50% 的 token 减少，同时性能下降低于 10%。\n*   **低模式 (Low mode)**：实现 75% 的 token 减少，同时性能下降低于 15%。\n\n**实现方法**\nThinkDial 通过一个端到端的训练范式实现这些能力，该范式在整个流程中整合了预算模式控制：\n1.  **预算模式监督微调 (Budget-mode supervised fine-tuning)**：将可控推理能力直接嵌入到学习过程中。\n2.  **两阶段预算感知强化学习 (Two-phase budget-aware reinforcement learning)**：结合自适应奖励塑形。\n\n**实验结果与泛化能力**\n广泛的实验证明，ThinkDial 实现了目标压缩-性能权衡，显著减少了响应长度，同时保持了性能阈值。该框架还在分布外（out-of-distribution）任务上展现出强大的泛化能力。",
      "shortSummary": "ThinkDial是一个开放的端到端框架，旨在解决大型语言模型推理工作量控制的挑战。它通过离散操作模式（高、中、低）实现类似gpt-oss的可控推理，允许用户在计算成本和性能之间进行权衡。例如，低模式可减少75%的token，性能下降低于15%。ThinkDial采用预算模式监督微调和两阶段强化学习进行训练，实验证明其能有效减少响应长度并保持性能，同时具有强大的泛化能力。",
      "translated_title": "ThinkDial：一种控制大型语言模型推理工作量的开放方案",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with &lt;10 percent performance degradation), and Low mode (75 percent token reduction with &lt;15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks."
    },
    {
      "title": "UltraMemV2: 内存网络扩展至1200亿参数，具备卓越的长上下文学习能力 (原标题: UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning)",
      "link": "https://arxiv.org/abs/2508.18756",
      "pubDate": "Tue, 26 Aug 2025 03:33:11 GMT",
      "isoDate": "2025-08-26T03:33:11.000Z",
      "creator": "Zihao Huang, Yu Bao, Qiyang Min, Siyan Chen, Ran Guo, Hongzhi Huang, Defa Zhu, Yutao Zeng, Banggu Wu, Xun Zhou, Siyuan Qiao",
      "summary": "## UltraMemV2：扩展至1200亿参数并具备卓越长上下文学习能力的内存网络\n\n本文介绍了UltraMemV2，一种重新设计的内存层架构，旨在解决现有稀疏模型（如专家混合模型MoE）的内存访问成本问题，并提升内存层架构的性能，使其与最先进的MoE模型相媲美。\n\n### 背景与挑战：\n*   **专家混合模型（MoE）**：通过仅激活部分参数实现显著效率，但在推理过程中存在高内存访问成本。\n*   **现有内存层架构（如UltraMem）**：虽然内存访问成本极低，但性能仅能与2专家MoE模型匹敌，远低于最先进的8专家配置。\n\n### UltraMemV2 的核心改进：\nUltraMemV2 引入了五项关键改进，以弥补性能差距：\n1.  **内存层集成**：将内存层集成到每个Transformer块中。\n2.  **简化值扩展**：通过单一线性投影简化值扩展过程。\n3.  **基于FFN的值处理**：采用来自PEER的基于前馈网络（FFN）的值处理方法。\n4.  **原则性参数初始化**：实施有原则的参数初始化策略。\n5.  **内存与FFN计算比重再平衡**：重新平衡内存层与FFN之间的计算比重。\n\n### 性能评估与结果：\n*   **性能对等**：在相同的计算量和参数量下，UltraMemV2 实现了与8专家MoE模型相当的性能，但内存访问成本显著降低。\n*   **内存密集型任务的卓越表现**：\n    *   长上下文记忆能力提升 +1.6 点。\n    *   多轮记忆能力提升 +6.2 点。\n    *   上下文学习能力提升 +7.9 点。\n*   **大规模验证**：该方法在总参数量高达1200亿、激活参数量达25亿的模型上进行了验证。\n*   **关键发现**：激活密度对性能的影响大于总稀疏参数数量。\n\n### 结论：\nUltraMemV2 使内存层架构的性能达到了与最先进MoE模型相当的水平，为高效稀疏计算提供了一个极具吸引力的替代方案。",
      "shortSummary": "UltraMemV2 提出了一种新的内存层架构，旨在解决专家混合模型（MoE）高内存访问成本的问题，并超越现有内存层架构的性能限制。通过五项关键改进，UltraMemV2 在相同计算和参数下，实现了与8专家MoE模型相当的性能，同时显著降低了内存访问。它在长上下文记忆、多轮记忆和上下文学习等内存密集型任务上表现出卓越性能，提升了高达7.9点。UltraMemV2 为高效稀疏计算提供了一个有力的替代方案。",
      "translated_title": "UltraMemV2: 内存网络扩展至1200亿参数，具备卓越的长上下文学习能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Mixture of Experts (MoE) models achieve remarkable efficiency by activating only subsets of parameters, they suffer from high memory access costs during inference. Memory-layer architectures offer an appealing alternative with very few memory access, but previous attempts like UltraMem have only matched the performance of 2-expert MoE models, falling significantly short of state-of-the-art 8-expert configurations. We present UltraMemV2, a redesigned memory-layer architecture that closes this performance gap. Our approach introduces five key improvements: integrating memory layers into every transformer block, simplifying value expansion with single linear projections, adopting FFN-based value processing from PEER, implementing principled parameter initialization, and rebalancing memory-to-FFN computation ratios. Through extensive evaluation, we demonstrate that UltraMemV2 achieves performance parity with 8-expert MoE models under same computation and parameters but significantly low memory access. Notably, UltraMemV2 shows superior performance on memory-intensive tasks, with improvements of +1.6 points on long-context memorization, +6.2 points on multi-round memorization, and +7.9 points on in-context learning. We validate our approach at scale with models up to 2.5B activated parameters from 120B total parameters, and establish that activation density has greater impact on performance than total sparse parameter count. Our work brings memory-layer architectures to performance parity with state-of-the-art MoE models, presenting a compelling alternative for efficient sparse computation."
    },
    {
      "title": "用于推理任务的专家混合语言模型的最佳稀疏度 (原标题: Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks)",
      "link": "https://arxiv.org/abs/2508.18672",
      "pubDate": "Tue, 26 Aug 2025 00:31:28 GMT",
      "isoDate": "2025-08-26T00:31:28.000Z",
      "creator": "Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota",
      "summary": "## 用于推理任务的专家混合语言模型的最佳稀疏度研究\n\n### 研究背景与目的\n\n大型语言模型（LLMs）的演进一直由经验性缩放定律驱动，但这些定律的系数会随着模型架构或数据管道的变化而调整。专家混合（Mixture-of-Experts, MoE）模型作为当前最先进系统中的标准配置，引入了一个新的稀疏度维度，而这一维度在当前密集的模型研究中常被忽视。本文旨在深入探究MoE模型的稀疏度如何影响两种截然不同的能力机制：**记忆（memorization）**和**推理（reasoning）**。\n\n### 研究方法\n\n为了系统性地研究MoE稀疏度的影响，研究团队采取了以下方法：\n\n*   **模型训练**：训练了一系列MoE Transformer模型家族。\n*   **参数与路由变化**：在保持计算预算固定的前提下，系统性地改变了模型的总参数量（total parameters）、活跃参数量（active parameters）以及top-k路由策略。\n*   **数据记录**：对每个训练好的模型，详细记录了其预训练损失（pre-training loss）、下游任务损失（downstream task loss）和任务准确率（task accuracy）。\n*   **差距分析**：通过这些数据，研究能够清晰地区分训练-测试泛化差距（train-test generalization gap）与损失-准确率差距（loss-accuracy gap）。\n\n### 主要发现\n\n研究结果揭示了MoE稀疏度对记忆和推理能力的不同影响：\n\n*   **记忆能力**：记忆基准测试的性能表现出随总参数量增加而单调提升的趋势，这与训练损失的持续改善相吻合。\n*   **推理能力**：与记忆能力形成鲜明对比的是，推理性能在总参数量和训练损失持续获得增益的情况下，会达到饱和点，甚至出现退步。\n*   **Top-k路由的影响**：当模型的活跃参数量保持不变时，仅仅改变top-k路由策略对模型性能的影响微乎其微。\n*   **经典超参数的作用**：学习率和初始化等经典的超参数，其对泛化差距的调节方向与稀疏度对泛化差距的调节方向一致。\n*   **后训练干预的局限性**：无论是通过后训练强化学习（如GRPO）还是增加测试时的计算资源，都无法有效弥补过度稀疏模型在推理任务上所表现出的不足。\n\n### 资源公开\n\n为了促进研究的透明度和可复现性，本研究的模型检查点、代码和日志均已开源，可在[https://this.https/URL](https://this.https/URL)获取。\n\n### 会议与主题\n\n本研究成果已在ICML的第二届AI for Math Workshop上发表，其研究领域涵盖机器学习（cs.LG）、人工智能（cs.AI）和计算与语言（cs.CL）。",
      "shortSummary": "本文研究了专家混合（MoE）语言模型稀疏度对记忆和推理能力的影响。研究发现，记忆能力随总参数量增加而单调提升，与训练损失一致。然而，推理性能在总参数量和训练损失持续改进的情况下会饱和甚至退步。仅改变top-k路由影响不大，且后训练强化学习或额外计算无法弥补过度稀疏模型在推理上的不足。这表明MoE模型在推理任务上存在最佳稀疏度。",
      "translated_title": "用于推理任务的专家混合语言模型的最佳稀疏度",
      "images": [],
      "contentSource": "完整文章",
      "content": "Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-k routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-k alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity."
    },
    {
      "title": "Wan-S2V：音频驱动的电影级视频生成 (原标题: Wan-S2V: Audio-Driven Cinematic Video Generation)",
      "link": "https://arxiv.org/abs/2508.18621",
      "pubDate": "Mon, 25 Aug 2025 22:51:31 GMT",
      "isoDate": "2025-08-25T22:51:31.000Z",
      "creator": "Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo",
      "summary": "### Wan-S2V：音频驱动的电影级视频生成\n\n**1. 背景与挑战**\n*   当前最先进的（SOTA）音频驱动角色动画方法在处理语音和歌唱场景时表现良好。\n*   然而，这些方法在更复杂的电影和电视制作中存在局限性，因为影视制作需要精细的角色互动、逼真的身体动作和动态的摄像机运镜等复杂元素。\n*   实现电影级角色动画是一个长期存在的挑战。\n\n**2. 提出的解决方案：Wan-S2V 模型**\n*   为解决上述挑战并实现电影级角色动画，研究人员提出了一个名为 Wan-S2V 的音频驱动模型。\n*   该模型基于 Wan 框架构建。\n*   Wan-S2V 在电影级语境下，相比现有方法，显著增强了表达能力和逼真度。\n\n**3. 实验与性能评估**\n*   研究团队进行了广泛的实验，将 Wan-S2V 与 Hunyuan-Avatar 和 Omnihuman 等尖端模型进行了基准测试。\n*   实验结果一致表明，Wan-S2V 的表现显著优于这些现有解决方案。\n\n**4. 模型的通用性与应用**\n*   Wan-S2V 模型还展示了其在多种应用场景中的通用性。\n*   具体应用包括长视频生成和精确的视频唇形同步编辑。\n\n**5. 研究领域**\n*   该研究属于计算机视觉与模式识别（cs.CV）领域。",
      "shortSummary": "Wan-S2V 是一种音频驱动的电影级视频生成模型，旨在解决现有方法在复杂影视制作中，对精细角色互动、逼真动作和动态运镜表现不足的问题。该模型基于 Wan 框架，显著提升了电影语境下的表达力和逼真度。实验证明，Wan-S2V 性能优于 Hunyuan-Avatar 和 Omnihuman 等现有先进模型。此外，它还可应用于长视频生成和精确的视频唇形同步编辑。",
      "translated_title": "Wan-S2V：音频驱动的电影级视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current state-of-the-art (SOTA) methods for audio-driven character animation demonstrate promising performance for scenarios primarily involving speech and singing. However, they often fall short in more complex film and television productions, which demand sophisticated elements such as nuanced character interactions, realistic body movements, and dynamic camera work. To address this long-standing challenge of achieving film-level character animation, we propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan. Our model achieves significantly enhanced expressiveness and fidelity in cinematic contexts compared to existing approaches. We conducted extensive experiments, benchmarking our method against cutting-edge models such as Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate that our approach significantly outperforms these existing solutions. Additionally, we explore the versatility of our method through its applications in long-form video generation and precise video lip-sync editing."
    },
    {
      "title": "训练语言模型代理以使用 CTF-Dojo 查找漏洞 (原标题: Training Language Model Agents to Find Vulnerabilities with CTF-Dojo)",
      "link": "https://arxiv.org/abs/2508.18370",
      "pubDate": "Mon, 25 Aug 2025 14:02:23 GMT",
      "isoDate": "2025-08-25T14:02:23.000Z",
      "creator": "Terry Yue Zhuo, Dingmin Wang, Hantian Ding, Varun Kumar, Zijian Wang",
      "summary": "### CTF-Dojo：用于训练语言模型代理查找漏洞的执行环境\n\n**背景与挑战**\n大型语言模型（LLMs）在可执行运行时环境中进行训练时，尤其是在通过验证反馈循环进行软件工程任务方面，展现出卓越的能力。然而，可扩展且通用化的执行驱动环境仍然稀缺，这限制了训练更强大机器学习代理的进展。\n\n**CTF-Dojo 介绍**\n为解决这一挑战，研究人员引入了 CTF-Dojo，这是首个专为训练 LLMs 而设计的、具有可验证反馈的大规模可执行运行时环境。\n*   **规模与内容**：CTF-Dojo 包含 658 个功能齐全的“夺旗赛”（Capture-The-Flag, CTF）风格挑战。\n*   **技术实现**：所有挑战均通过 Docker 容器化，确保了高度的复现性。\n\n**CTF-Forge：自动化环境生成**\n为了实现无需人工干预的快速扩展，研究团队开发了 CTF-Forge。\n*   **功能**：CTF-Forge 是一个自动化管道，能够在数分钟内将公开可用的工件转换为即用型执行环境。\n*   **效率提升**：这一创新消除了传统上需要数周专家配置的时间，极大地提高了效率。\n\n**LLM 代理训练与成果**\n研究人员在 CTF-Dojo 中使用了 486 条高质量、经过执行验证的轨迹来训练基于 LLM 的代理。\n*   **性能提升**：与强大的基线模型相比，这些代理在性能上取得了高达 11.6% 的绝对增益。\n*   **基准测试**：代理在三个竞争性基准测试中进行了评估，包括 InterCode-CTF、NYU CTF Bench 和 Cybench。\n*   **最先进成果**：表现最佳的 32B 模型达到了 31.9% 的 Pass@1 分数，确立了新的开源模型最先进水平，其性能可与 DeepSeek-V3-0324 和 Gemini-2.5-Flash 等前沿模型相媲美。\n\n**研究意义**\n通过将 CTF 风格的任务作为可执行代理学习的基准，CTF-Dojo 证明了：\n*   **执行驱动训练的有效性**：执行驱动的训练信号不仅有效，而且对于推进高性能机器学习代理至关重要。\n*   **降低依赖**：该方法有助于减少对昂贵专有系统的依赖。",
      "shortSummary": "CTF-Dojo 是首个大规模可执行运行时环境，旨在通过 658 个 Docker 容器化的 CTF 挑战训练语言模型代理查找漏洞。结合自动化管道 CTF-Forge，它解决了可扩展训练环境稀缺的问题。研究人员利用 CTF-Dojo 训练的 LLM 代理在多个基准测试中取得了高达 11.6% 的性能提升，其最佳 32B 模型达到了 31.9% Pass@1，树立了新的开源模型最先进水平，并证明了执行驱动训练对高性能机器学习代理的关键作用。",
      "translated_title": "训练语言模型代理以使用 CTF-Dojo 查找漏洞",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems."
    },
    {
      "title": "ObjFiller-3D：基于视频扩散模型的一致多视角三维修复 (原标题: ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models)",
      "link": "https://arxiv.org/abs/2508.18271",
      "pubDate": "Mon, 25 Aug 2025 13:59:40 GMT",
      "isoDate": "2025-08-25T13:59:40.000Z",
      "creator": "Haitang Feng, Jie Liu, Jie Tang, Gangshan Wu, Beiqi Chen, Jianhuang Lai, Guangcong Wang",
      "summary": "## ObjFiller-3D：基于视频扩散模型的一致多视角三维修复\n\n### 1. 研究背景与问题\n\n*   **传统三维修复的局限性：** 现有的三维修复方法通常依赖于多视角二维图像修复技术。\n*   **跨视角不一致性：** 这种方法固有的跨不同修复视角的不一致性会导致以下问题：\n    *   纹理模糊\n    *   空间不连续性\n    *   分散的视觉伪影\n*   **挑战：** 这些不一致性对实现高保真、结构连贯的三维物体精确逼真修复构成了重大挑战，尤其是在对精度要求高的应用中。\n\n### 2. 提出的解决方案：ObjFiller-3D\n\n*   **核心目标：** ObjFiller-3D 是一种新颖的方法，旨在实现高质量、一致的三维物体补全和编辑。\n*   **创新方法：** 与传统的二维图像修复模型不同，ObjFiller-3D 利用精选的先进视频编辑模型来填充三维物体的遮罩区域。\n\n### 3. 关键技术与贡献\n\n*   **视频模型适应性：**\n    *   分析了三维数据与视频数据之间的表示差距。\n    *   提出了一种将视频修复模型应用于三维场景修复的改编方法。\n*   **参考基三维修复：** 引入了一种基于参考的三维修复方法，以进一步增强重建质量。\n\n### 4. 实验结果与优势\n\n*   **性能提升：** 在多样化数据集上的实验表明，与现有方法相比，ObjFiller-3D 能够生成更忠实、更精细的重建结果。\n*   **量化指标对比：**\n    *   **PSNR：** ObjFiller-3D 达到 26.6，显著高于 NeRFiller 的 15.9。\n    *   **LPIPS：** ObjFiller-3D 为 0.19，优于 Instant3dit 的 0.25。\n*   **实际应用潜力：** 该方法在实际三维编辑应用中展现出强大的部署潜力。\n\n### 5. 项目资源\n\n*   项目页面：this https URL\n*   代码：this https URL",
      "shortSummary": "ObjFiller-3D 提出了一种新颖的三维修复方法，通过利用视频扩散模型解决传统多视角二维修复中存在的跨视角不一致性问题。该方法将先进的视频编辑模型应用于三维场景修复，并引入了基于参考的修复策略。实验证明，ObjFiller-3D 在重建质量上显著优于现有方法，生成更忠实、精细的三维物体，展现出在实际三维编辑应用中的巨大潜力。",
      "translated_title": "ObjFiller-3D：基于视频扩散模型的一致多视角三维修复",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D ."
    },
    {
      "title": "通过模块社区揭示大型语言模型的认知模式 (原标题: Unraveling the cognitive patterns of Large Language Models through module communities)",
      "link": "https://arxiv.org/abs/2508.18192",
      "pubDate": "Mon, 25 Aug 2025 12:49:38 GMT",
      "isoDate": "2025-08-25T12:49:38.000Z",
      "creator": "Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao",
      "summary": "# 揭示大型语言模型的认知模式\n\n## 引言\n大型语言模型（LLMs）在科学、工程和社会领域取得了显著进展，其应用范围从科学发现、医学诊断到聊天机器人，极大地改变了我们的世界。然而，尽管LLMs无处不在且功能强大，其底层机制仍隐藏在数十亿参数和复杂结构中，使得其内部架构和认知过程难以理解。\n\n## 研究方法\n本研究旨在解决LLMs认知机制的理解鸿沟，通过以下方法：\n*   **借鉴生物学方法**：采纳了生物学中理解新兴认知的方法。\n*   **开发网络化框架**：构建了一个基于网络的框架，该框架将认知技能、LLM架构和数据集联系起来。\n*   **范式转变**：这标志着基础模型分析领域的一次范式转变，为深入理解LLMs提供了新视角。\n\n## 关键发现\n研究通过分析模块社区中的技能分布，揭示了LLMs认知模式的独特之处：\n*   **与生物系统的对比**：\n    *   LLMs并未严格遵循特定生物系统中观察到的焦点化专业分工。\n    *   它们展现出独特的模块社区，其涌现的技能模式部分反映了在鸟类和小型哺乳动物大脑中看到的分布式但相互连接的认知组织。\n*   **与生物系统的主要分歧**：\n    *   LLMs的技能习得显著受益于动态的、跨区域的交互和神经可塑性。这是LLMs与生物系统的一个关键区别。\n\n## 研究意义与建议\n本研究的发现具有重要的理论和实践意义：\n*   **提高可解释性**：该框架为LLM的可解释性提供了新的见解，有助于我们更好地理解这些复杂模型的内部运作。\n*   **优化微调策略**：研究建议，有效的微调策略应利用分布式学习动态，而非僵化的模块化干预，以充分发挥LLMs的潜力。\n\n## 研究领域\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   机器学习 (cs.LG)",
      "shortSummary": "本研究通过开发一个连接认知技能、LLM架构和数据集的网络化框架，深入探讨了大型语言模型（LLMs）的认知模式。研究发现，LLMs的模块社区展现出分布式但相互连接的认知组织，与鸟类和小型哺乳动物大脑有相似之处，但其技能习得更依赖于动态的跨区域交互和神经可塑性。这为LLM的可解释性提供了新见解，并建议微调策略应侧重于分布式学习动态。",
      "translated_title": "通过模块社区揭示大型语言模型的认知模式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions."
    },
    {
      "title": "CMPhysBench：凝聚态物理中大型语言模型评估基准 (原标题: CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics)",
      "link": "https://arxiv.org/abs/2508.18124",
      "pubDate": "Mon, 25 Aug 2025 11:32:22 GMT",
      "isoDate": "2025-08-25T11:32:22.000Z",
      "creator": "Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Yunqi Cai, Xi Dai, Shufei Zhang, Lei Bai, Jinguang Cheng, Zhong Fang, Hongming Weng",
      "summary": "CMPhysBench是一个新颖的基准，旨在评估大型语言模型（LLMs）在凝聚态物理领域的熟练程度。该基准的详细信息和主要发现如下：\n\n### 1. CMPhysBench的介绍与构成\n*   **目的**：评估LLMs在凝聚态物理领域的知识和问题解决能力。\n*   **问题数量与难度**：包含超过520个经过精心策划的研究生级别问题。\n*   **覆盖范围**：涵盖凝聚态物理的代表性子领域和基础理论框架，例如磁性、超导性、强关联系统等。\n*   **问题类型**：专注于计算问题，要求LLMs独立生成全面的解决方案，以确保对问题解决过程的深入理解。\n\n### 2. 评估方法：可伸缩表达式编辑距离（SEED）分数\n*   **核心机制**：利用表达式的树状表示来引入SEED分数。\n*   **评估优势**：\n    *   提供细粒度（非二元）的部分分数，而非简单的对错判断。\n    *   能够更准确地评估模型预测与标准答案之间的相似性。\n\n### 3. 主要评估结果\n*   **模型表现**：即使是表现最佳的模型Grok-4，在CMPhysBench上的平均SEED分数也仅达到36分，准确率仅为28%。\n*   **能力差距**：这一结果凸显了LLMs在凝聚态物理这一实用且前沿领域存在显著的能力差距，尤其与传统物理领域相比。\n\n### 4. 数据集与代码可用性\n*   **公开可用**：CMPhysBench的代码和数据集已公开发布，方便研究人员进行进一步的探索和验证。\n\n本文提及“7 figures”，但未提供实际的图片链接，因此摘要中不包含任何图片。",
      "shortSummary": "CMPhysBench是一个新颖的基准，用于评估大型语言模型（LLMs）在凝聚态物理领域的表现。它包含520多个研究生级别的计算问题，涵盖多个子领域。该基准引入了可伸缩表达式编辑距离（SEED）分数，以提供细粒度的评估。结果显示，即使是最佳模型Grok-4，平均SEED分数也仅为36分，准确率28%，揭示了LLMs在该领域存在显著的能力差距。代码和数据集已公开。",
      "translated_title": "CMPhysBench：凝聚态物理中大型语言模型评估基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench."
    },
    {
      "title": "Spacer：迈向工程化的科学灵感 (原标题: Spacer: Towards Engineered Scientific Inspiration)",
      "link": "https://arxiv.org/abs/2508.17661",
      "pubDate": "Mon, 25 Aug 2025 00:49:16 GMT",
      "isoDate": "2025-08-25T00:49:16.000Z",
      "creator": "Minhyeong Lee, Suyoung Hwang, Seunghyun Moon, Geonho Nah, Donghyun Koh, Youngjun Cho, Johyun Park, Hojin Yoo, Jiho Park, Haneul Choi, Sungbin Moon, Taehoon Hwang, Seungwon Kim, Jaeyeong Kim, Seongjun Kim, Juneau Jung",
      "summary": "# Spacer：迈向工程化的科学灵感\n\n## 引言与背景\n当前，大型语言模型（LLMs）在自动化科学研究方面取得了进展，但其应用范围受限于狭窄的任务或有限的创造力。为解决这一问题，研究人员提出了Spacer系统，旨在实现无需外部干预的、具有创造性且基于事实的科学发现。\n\n## Spacer系统概述\nSpacer的核心方法是“刻意去语境化”（deliberate decontextualization）。该方法将信息分解为原子单元——关键词，并通过探索这些关键词之间未被发现的连接来激发创造力。Spacer系统主要由两个核心组件构成：Nuri（灵感引擎）和Manifesting Pipeline（概念具现化流程）。\n\n## Spacer的组成部分\n\n### 1. Nuri：灵感引擎\n*   **功能：** 负责构建关键词集合。\n*   **数据源：** 从包含180,000篇生物学领域学术出版物的关键词图中提取。\n*   **目标：** 识别新颖且具有高潜力的关键词集合。\n\n### 2. Manifesting Pipeline：概念具现化流程\n*   **功能：** 将Nuri生成的关键词集合精炼成详细的科学陈述。\n*   **步骤：**\n    *   寻找关键词之间的关联。\n    *   分析其逻辑结构。\n    *   验证其合理性。\n    *   最终起草原创的科学概念。\n\n## 实验与评估结果\n\n### Nuri的性能\n*   Nuri的评估指标能够准确分类高影响力出版物。\n*   **AUROC分数：** 0.737。\n\n### Manifesting Pipeline的性能\n*   该流程能够仅凭关键词集合成功重建最新顶级期刊文章的核心概念。\n*   **LLM评分系统估计：** 超过85%的重建是合理的。\n\n### 与现有技术的比较\n*   通过嵌入空间分析，Spacer的输出与领先出版物的高度相似性显著高于现有最先进的LLMs（SOTA LLMs）。\n\n## 研究领域\n该研究涉及人工智能（cs.AI）、机器学习（cs.LG）和神经网络与进化计算（cs.NE）等领域。",
      "shortSummary": "Spacer是一个旨在实现自动化、创造性科学发现的系统，通过“刻意去语境化”方法，将信息分解为关键词并探索其未发现的连接。它包含Nuri（生成高潜力关键词集）和Manifesting Pipeline（将关键词精炼为科学概念）。实验表明，Nuri能有效识别高影响力出版物，Manifesting Pipeline能成功重建核心概念，且Spacer的输出与领先出版物的相似度显著高于现有LLMs，展现了其在科学灵感工程方面的潜力。",
      "translated_title": "Spacer：迈向工程化的科学灵感",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in LLMs have made automated scientific research the next frontline in the path to artificial superintelligence. However, these systems are bound either to tasks of narrow scope or the limited creative capabilities of LLMs. We propose Spacer, a scientific discovery system that develops creative and factually grounded concepts without external intervention. Spacer attempts to achieve this via 'deliberate decontextualization,' an approach that disassembles information into atomic units - keywords - and draws creativity from unexplored connections between them. Spacer consists of (i) Nuri, an inspiration engine that builds keyword sets, and (ii) the Manifesting Pipeline that refines these sets into elaborate scientific statements. Nuri extracts novel, high-potential keyword sets from a keyword graph built with 180,000 academic publications in biological fields. The Manifesting Pipeline finds links between keywords, analyzes their logical structure, validates their plausibility, and ultimately drafts original scientific concepts. According to our experiments, the evaluation metric of Nuri accurately classifies high-impact publications with an AUROC score of 0.737. Our Manifesting Pipeline also successfully reconstructs core concepts from the latest top-journal articles solely from their keyword sets. An LLM-based scoring system estimates that this reconstruction was sound for over 85% of the cases. Finally, our embedding space analysis shows that outputs from Spacer are significantly more similar to leading publications compared with those from SOTA LLMs."
    },
    {
      "title": "TreePO：通过启发式树状建模弥合策略优化、有效性与推理效率之间的鸿沟 (原标题: TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling)",
      "link": "https://arxiv.org/abs/2508.17445",
      "pubDate": "Sun, 24 Aug 2025 12:52:37 GMT",
      "isoDate": "2025-08-24T12:52:37.000Z",
      "creator": "Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang",
      "summary": "### TreePO：弥合策略优化、有效性与推理效率的鸿沟\n\n本文介绍了TreePO，一个旨在解决大型语言模型（LLM）通过强化学习进行对齐时面临的挑战的创新框架。尽管RL-based LLM对齐在解决复杂推理问题上取得了显著进步，但其缺点在于昂贵的在线策略（on-policy）回滚和对多样化推理路径探索的限制。\n\n#### 核心问题\n\n*   **昂贵的在线策略回滚**：每次更新都需要大量的计算资源。\n*   **探索受限**：难以充分探索多样化的推理路径，可能导致次优解。\n\n#### TreePO的解决方案\n\nTreePO将序列生成视为一个**树状搜索过程**，并引入了一个**自引导回滚算法**。其核心机制包括：\n\n*   **动态树采样策略与定长片段解码**：结合使用这两种方法来构建和遍历推理树。\n*   **利用局部不确定性生成额外分支**：在推理过程中，当遇到不确定性时，TreePO能够智能地创建新的分支，以探索更多潜在的推理路径。\n*   **计算分摊与路径剪枝**：通过分摊共同前缀的计算，并提前剪枝低价值的路径，TreePO显著降低了每次更新的计算负担，同时保持或增强了探索的多样性。\n\n#### 主要贡献\n\nTreePO的贡献体现在以下几个关键方面：\n\n1.  **片段式采样算法**：\n    *   通过使用连续片段，有效减轻了KV缓存的负担。\n    *   结合早期停止机制，能够生成新的分支，提高探索效率。\n2.  **基于树的片段级优势估计**：\n    *   该方法综合考虑了全局和局部的近端策略优化（PPO）因素，使得优势估计更加准确和全面。\n3.  **动态分歧与回退策略分析**：\n    *   对概率和质量驱动的动态分歧与回退策略的有效性进行了深入分析，为算法的鲁棒性提供了理论支持。\n\n#### 实验验证与效率提升\n\n研究人员通过实验验证了TreePO的性能和效率：\n\n*   **性能提升**：在多个推理基准测试集上，TreePO展现了显著的性能提升。\n*   **GPU小时节省**：对于训练模型，TreePO的采样设计能够节省22%至43%的GPU小时。\n*   **计算量减少**：\n    *   对于现有模型，轨迹级采样计算量减少了高达40%。\n    *   令牌级采样计算量减少了35%。\n\n#### 结论\n\nTreePO在提供推理效率的同时，为以更少样本和更少计算量扩展基于强化学习的后训练提供了一条实用路径，有望推动LLM对齐技术的发展。",
      "shortSummary": "TreePO是一种新颖的算法，旨在解决大型语言模型（LLM）通过强化学习对齐时面临的昂贵在线策略回滚和探索受限问题。它将序列生成视为树状搜索，通过动态采样、片段解码和路径剪枝，显著降低了计算负担并增强了探索多样性。实验证明，TreePO在推理任务上提高了性能，并大幅节省了GPU计算资源（22%-43%的GPU小时，40%轨迹级，35%令牌级计算）。TreePO为高效扩展基于RL的LLM后训练提供了实用方案。",
      "translated_title": "TreePO：通过启发式树状建模弥合策略优化、有效性与推理效率之间的鸿沟",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\\% up to 43\\% of the sampling design for the trained models, meanwhile showing up to 40\\% reduction at trajectory-level and 35\\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO."
    },
    {
      "title": "QueryBandits用于幻觉缓解：利用语义特征实现无悔重写 (原标题: QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting)",
      "link": "https://arxiv.org/abs/2508.16697",
      "pubDate": "Thu, 21 Aug 2025 21:41:49 GMT",
      "isoDate": "2025-08-21T21:41:49.000Z",
      "creator": "Nicole Cho, William Watson, Alec Koppel, Sumitra Ganesh, Manuela Veloso",
      "summary": "# QueryBandits用于幻觉缓解：利用语义特征实现无悔重写\n\n## 摘要\n\n本文介绍了一种名为QueryBandits的新型多臂赌博机（bandit）框架，旨在通过主动重写输入查询来缓解大型语言模型（LLMs）中的幻觉问题。与大多数专注于事后过滤的现有方法不同，QueryBandits致力于在LLM生成响应之前，通过优化查询本身来预防幻觉的发生。\n\n## 核心概念与方法\n\n*   **问题背景：** 尽管LLMs的推理能力日益增强，但幻觉现象也随之增多。现有缓解策略多为事后处理，未能从源头解决问题。\n*   **QueryBandits框架：**\n    *   **目标：** 设计并选择最优的查询重写策略，以最大化一个奖励模型。\n    *   **奖励模型：** 该模型根据输入查询的17个语言特征的敏感性来量化幻觉倾向。通过评估这些特征，QueryBandits能够预测并避免可能导致幻觉的查询形式。\n    *   **主动干预：** 框架通过重写查询，主动引导LLMs远离生成幻觉性内容。\n\n## 实验与结果\n\n*   **实验设置：**\n    *   在13个多样化的问答（QA）基准数据集上进行了广泛测试。\n    *   每个数据集包含了1,050个经过词汇扰动的查询，以模拟真实世界中查询的多样性。\n*   **关键发现：**\n    *   **卓越性能：** 表现最佳的上下文QueryBandit（采用Thompson Sampling策略）相对于“不重写”的基线方法，取得了高达87.5%的胜率。\n    *   **超越静态提示：** 与零样本静态提示（如“释义”或“扩展”指令）相比，QueryBandits的性能分别提升了42.6%和60.3%。这表明QueryBandits的动态、自适应重写策略远优于固定的提示工程方法。\n    *   **静态重写的潜在危害：** 研究发现，某些静态提示策略（在现有查询重写文献中占据相当比例）的累积遗憾值甚至高于“不重写”基线。这有力地证明了不当的静态重写不仅无益，反而可能加剧LLM的幻觉问题。\n    *   **无通用最优策略：** 通过分析收敛后的每个重写策略（“臂”）的回归特征权重向量，研究揭示了一个重要结论：不存在一个对所有查询都普遍最优的单一重写策略。这强调了动态、上下文感知的重写方法的必要性。\n*   **机制优势：** QueryBandits通过利用语义特征进行引导式重写，能够通过前向传播机制显著改变LLM的输出行为，从而避免了耗时的模型重新训练或复杂的基于梯度的适应过程。\n\n## 结论\n\nQueryBandits框架通过其创新的多臂赌博机方法和对语义特征的利用，为LLM幻觉缓解提供了一种高效且无悔的解决方案。它不仅在实验中展现出显著的性能提升，还揭示了静态重写策略的局限性，并强调了动态、自适应查询重写在预防LLM幻觉方面的巨大潜力。",
      "shortSummary": "本文提出QueryBandits框架，通过利用输入查询的17个语言特征，主动重写查询以缓解大型语言模型（LLMs）的幻觉问题。该框架采用多臂赌博机方法，旨在最大化一个评估幻觉倾向的奖励模型。实验表明，QueryBandits在13个QA基准上，相对于无重写基线取得了87.5%的胜率，并显著优于静态提示。研究强调，静态重写可能加剧幻觉，且没有单一重写策略对所有查询最优，证实了QueryBandits动态重写的有效性。",
      "translated_title": "QueryBandits用于幻觉缓解：利用语义特征实现无悔重写",
      "images": [],
      "contentSource": "完整文章",
      "content": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting (\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation."
    },
    {
      "title": "CineScale：高分辨率电影级视觉生成中的免费午餐 (原标题: CineScale: Free Lunch in High-Resolution Cinematic Visual Generation)",
      "link": "https://arxiv.org/abs/2508.15774",
      "pubDate": "Thu, 21 Aug 2025 13:59:57 GMT",
      "isoDate": "2025-08-21T13:59:57.000Z",
      "creator": "Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu",
      "summary": "# CineScale：高分辨率电影级视觉生成\n\n## 摘要\n\n本文介绍了 **CineScale**，一种新颖的推理范式，旨在解决现有视觉扩散模型在生成高分辨率电影级视觉内容时面临的挑战。尽管视觉扩散模型取得了显著进展，但由于缺乏高分辨率数据和计算资源限制，它们通常在有限分辨率下进行训练，这阻碍了其生成高保真图像或视频的能力。\n\n## 现有问题\n\n*   **分辨率限制**：视觉扩散模型通常在有限分辨率下训练，难以生成高保真高分辨率内容。\n*   **现有方法缺陷**：尽管有免调优策略尝试利用预训练模型的潜力，但它们仍易于产生带有重复模式的低质量视觉内容。\n*   **根本障碍**：当模型生成超出其训练分辨率的视觉内容时，高频信息不可避免地增加，导致累积误差并产生不希望的重复模式。\n\n## CineScale 方法概述\n\nCineScale 提出了一种新颖的推理范式，以实现更高分辨率的视觉生成。为了解决不同视频生成架构引入的各种问题，CineScale 提出了针对每种架构量身定制的专用变体。\n\n## 核心贡献与优势\n\n*   **广泛适用性**：与现有基线方法仅限于高分辨率文本到图像（T2I）和文本到视频（T2V）生成不同，CineScale 扩展了范围，支持高分辨率图像到视频（I2V）和视频到视频（V2V）合成。\n*   **基于先进框架**：CineScale 构建于最先进的开源视频生成框架之上。\n*   **卓越性能**：\n    *   **图像生成**：无需任何微调即可实现 **8k 图像生成**。\n    *   **视频生成**：仅需少量 LoRA 微调即可实现 **4k 视频生成**。\n\n## 实验验证\n\n广泛的实验验证了 CineScale 范式在扩展图像和视频模型更高分辨率视觉生成能力方面的优越性。\n\n## 项目资源\n\n*   生成的视频样本可在项目网站上查看。\n*   CineScale 是 FreeScale (ICCV 2025) 的扩展工作。\n*   项目页面和代码库信息已提供（请注意，文章中提供的链接是占位符，并非实际可点击的图片链接）。",
      "shortSummary": "CineScale 提出了一种新颖的推理范式，旨在解决视觉扩散模型在生成高分辨率图像和视频时遇到的质量问题，特别是重复模式。该方法通过为不同视频生成架构定制变体，扩展了高分辨率图像到视频（I2V）和视频到视频（V2V）合成能力。CineScale 无需微调即可实现8k图像生成，并以少量LoRA微调实现4k视频生成，显著提升了高分辨率视觉内容的生成质量和范围。",
      "translated_title": "CineScale：高分辨率电影级视觉生成中的免费午餐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/."
    },
    {
      "title": "用于指令引导图像编辑的视觉自回归建模 (原标题: Visual Autoregressive Modeling for Instruction-Guided Image Editing)",
      "link": "https://arxiv.org/abs/2508.15772",
      "pubDate": "Thu, 21 Aug 2025 13:59:32 GMT",
      "isoDate": "2025-08-21T13:59:32.000Z",
      "creator": "Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei",
      "summary": "# VAREdit：用于指令引导图像编辑的视觉自回归框架\n\n## 引言与背景\n*   **扩散模型的局限性：** 近期，扩散模型在指令引导图像编辑中展现出卓越的视觉保真度。然而，其固有的全局去噪过程将编辑区域与整个图像上下文纠缠在一起，导致意外的虚假修改，并损害了对编辑指令的遵循。\n*   **自回归模型的优势：** 相比之下，自回归模型通过将图像合成表述为离散视觉令牌的顺序过程，提供了一种独特的范式。其因果和组合机制自然地规避了基于扩散方法在遵循指令方面的挑战。\n\n## VAREdit 框架\n*   **核心理念：** 本文提出了 VAREdit，一个视觉自回归（VAR）框架，它将图像编辑重新定义为一个“下一尺度预测”问题。\n*   **工作原理：** VAREdit 以源图像特征和文本指令为条件，生成多尺度目标特征以实现精确编辑。\n\n## 关键挑战与解决方案\n*   **挑战：** 在此范式中，一个核心挑战是如何有效地条件化源图像令牌。研究观察到，最精细尺度的源特征无法有效指导更粗尺度目标特征的预测。\n*   **解决方案：** 为弥合这一差距，研究引入了“尺度对齐参考（SAR）模块”。该模块将尺度匹配的条件信息注入到第一个自注意力层中。\n\n## 性能与效率\n*   **编辑遵循度与效率的显著提升：** VAREdit 在编辑遵循度和效率方面均展现出显著进步。\n*   **超越扩散模型：** 在标准基准测试中，VAREdit 的 GPT-Balance 分数比领先的基于扩散的方法高出 30% 以上。\n*   **高速编辑：** 此外，VAREdit 可以在 1.2 秒内完成 512x512 图像的编辑，比同等大小的 UltraEdit 快 2.2 倍。\n\n## 资源可用性\n*   模型的源代码和模型已公开。",
      "shortSummary": "VAREdit 是一种视觉自回归（VAR）框架，旨在解决扩散模型在指令引导图像编辑中存在的虚假修改和指令遵循度差的问题。它将图像编辑重构为下一尺度预测任务，通过引入尺度对齐参考（SAR）模块有效处理多尺度特征条件化。VAREdit 在编辑遵循度和效率上均有显著提升，其GPT-Balance分数比现有扩散方法高出30%以上，且编辑速度比UltraEdit快2.2倍，可在1.2秒内完成512x512图像编辑。",
      "translated_title": "用于指令引导图像编辑的视觉自回归建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a 512times512 editing in 1.2 seconds, making it 2.2times faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit."
    }
  ],
  "lastUpdated": "2025-08-27T09:35:59.858Z"
}