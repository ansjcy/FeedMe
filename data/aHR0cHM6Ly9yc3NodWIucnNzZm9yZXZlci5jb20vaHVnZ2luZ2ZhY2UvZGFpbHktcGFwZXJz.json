{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "AutoMind：用于自动化数据科学的自适应知识型智能体 (原标题: AutoMind: Adaptive Knowledgeable Agent for Automated Data Science)",
      "link": "https://arxiv.org/abs/2506.10974",
      "pubDate": "Thu, 12 Jun 2025 13:59:32 GMT",
      "isoDate": "2025-06-12T13:59:32.000Z",
      "creator": "Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, Ningyu Zhang",
      "summary": "# AutoMind：用于自动化数据科学的自适应知识型智能体\n\n## 摘要\n\n大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力，有望自动化整个机器学习流程。然而，现有框架存在局限性，其依赖于僵化、预定义的工作流和不灵活的编码策略，导致它们仅在相对简单、经典的问题上表现出色，而无法捕捉人类实践者在处理复杂、创新任务时所具备的经验知识。\n\n## AutoMind 框架介绍\n\n本文引入了 **AutoMind**，一个自适应、知识型的 LLM 智能体框架，旨在克服上述缺陷。AutoMind 通过以下三个关键进展实现了其目标：\n\n1.  **精选专家知识库：**\n    *   该知识库将智能体建立在领域专家知识的基础上，为智能体提供坚实的基础，使其能够理解和应用专业的经验和最佳实践。\n\n2.  **智能体知识型树搜索算法：**\n    *   此算法能够策略性地探索可能的解决方案，通过结构化的搜索过程，智能体可以更有效地发现和评估不同的方法，从而找到最优解。\n\n3.  **自适应编码策略：**\n    *   这种策略能够根据任务的复杂性动态调整代码生成。这意味着智能体可以根据具体需求灵活地生成定制化的代码，而不是遵循一成不变的模式，从而提高解决复杂问题的能力。\n\n## 性能评估\n\nAutoMind 在两个自动化数据科学基准测试上进行了评估，结果表明其性能优于现有最先进的基线方法。\n\n## 额外分析与结论\n\n进一步的分析证实了 AutoMind 在以下方面的优势：\n\n*   **有效性：** 解决方案的质量和准确性。\n*   **效率：** 解决问题所需的时间和资源。\n*   **定性解决方案质量：** 生成代码和方法的实用性和优越性。\n\n这些结果突出表明，AutoMind 是迈向完全自动化数据科学的有效且稳健的一步。",
      "shortSummary": "AutoMind 是一个自适应、知识型的LLM智能体框架，旨在克服现有数据科学自动化工具的局限性。它通过整合精选专家知识库、智能体知识型树搜索算法和自适应编码策略，解决了传统框架僵化、缺乏经验知识的问题。在基准测试中，AutoMind 表现出优越的性能、效率和解决方案质量，是实现完全自动化数据科学的重要进展。",
      "translated_title": "AutoMind：用于自动化数据科学的自适应知识型智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science."
    },
    {
      "title": "ChineseHarm-Bench：一个中文有害内容检测基准 (原标题: ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark)",
      "link": "https://arxiv.org/abs/2506.10960",
      "pubDate": "Thu, 12 Jun 2025 13:57:05 GMT",
      "isoDate": "2025-06-12T13:57:05.000Z",
      "creator": "Kangwei Liu, Siyuan Cheng, Bozhong Tian, Xiaozhuan Liang, Yuyang Yin, Meng Han, Ningyu Zhang, Bryan Hooi, Xi Chen, Shumin Deng",
      "summary": "## ChineseHarm-Bench：一个中文有害内容检测基准\n\n### 引言\n\n大型语言模型（LLMs）在自动化有害内容检测任务中扮演着越来越重要的角色，它们能够协助内容审核员识别违规内容，从而提高内容审核的整体效率和准确性。然而，当前有害内容检测的现有资源主要集中在英文领域，中文数据集则相对稀缺且范围有限，这限制了LLMs在中文有害内容检测方面的应用和发展。\n\n### ChineseHarm-Bench：中文有害内容检测基准\n\n为了解决中文有害内容检测数据集稀缺的问题，本文提出了一个名为“ChineseHarm-Bench”的综合性、专业标注的中文内容有害性检测基准。该基准具有以下特点：\n\n*   **数据来源**：完全基于真实世界数据构建，确保了数据的实用性和代表性。\n*   **覆盖范围**：涵盖了六个具有代表性的有害内容类别，提供了广泛的检测范围。\n*   **标注质量**：经过专业标注，保证了数据集的准确性和可靠性。\n\n### 知识规则库\n\n在ChineseHarm-Bench的标注过程中，研究人员还额外产出了一个**知识规则库**。这个规则库提供了明确的专家知识，旨在辅助LLMs更有效地进行中文有害内容检测，弥补了模型在处理复杂中文语境时可能存在的知识空白。\n\n### 知识增强基线模型\n\n此外，文章还提出了一种**知识增强基线模型**。该模型创新性地整合了人工标注的知识规则和大型语言模型中蕴含的隐式知识。通过这种结合，即使是规模较小的模型也能够实现与当前最先进（state-of-the-art, SOTA）LLMs相媲美的性能，这为资源受限的环境下进行高效有害内容检测提供了新的可能性。\n\n### 资源可用性与项目状态\n\n*   **代码与数据**：相关代码和数据已公开可用，方便研究人员进行复现和进一步研究。\n*   **项目状态**：该工作目前仍在进行中（Work in progress）。\n*   **相关领域**：该研究涉及计算与语言（cs.CL）、人工智能（cs.AI）、密码学与安全（cs.CR）、信息检索（cs.IR）和机器学习（cs.LG）等多个学科领域。",
      "shortSummary": "针对中文有害内容检测数据集稀缺的问题，本文提出了“ChineseHarm-Bench”——一个综合性、专业标注的中文有害内容检测基准。该基准基于真实世界数据构建，涵盖六个代表性类别。研究还产出了一个知识规则库，并提出了一种知识增强基线模型，该模型结合人工规则和LLM隐式知识，使小型模型也能达到SOTA LLM的性能。代码和数据已公开。",
      "translated_title": "ChineseHarm-Bench：一个中文有害内容检测基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench."
    },
    {
      "title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂 (原标题: SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks)",
      "link": "https://arxiv.org/abs/2506.10954",
      "pubDate": "Thu, 12 Jun 2025 13:54:17 GMT",
      "isoDate": "2025-06-12T13:54:17.000Z",
      "creator": "Lianghong Guo, Yanlin Wang, Caihua Li, Pengyu Yang, Jiachi Chen, Wei Tao, Yingtian Zou, Duyu Tang, Zibin Zheng",
      "summary": "## SWE-Factory：自动化GitHub问题解决数据集构建\n\n### 引言\n\n为大型语言模型（LLMs）构建用于GitHub问题解决任务的大规模数据集，对于训练和评估其软件工程能力至关重要。然而，传统的基准创建过程面临巨大挑战且劳动密集，尤其是在设置评估环境、测试结果评分和验证任务实例等阶段。\n\n### SWE-Factory 解决方案\n\n本文提出 **SWE-Factory**，一个旨在解决这些挑战的自动化流程。该流程集成了三个核心自动化组件：\n\n1.  **SWE-Builder：** 一个多智能体系统，用于自动化评估环境的构建。它包含四个专业智能体，以协作、迭代循环的方式工作，并利用环境内存池来提高效率。\n2.  **标准化、基于退出码的评分方法：** 这种方法消除了手动编写自定义解析器的需要，简化了评分过程。\n3.  **自动化 fail2pass 验证：** 利用可靠的退出码信号，实现任务实例的自动化验证。\n\n### 实验与结果\n\n研究团队对四种编程语言的671个问题进行了实验，结果表明 SWE-Factory 流程能够有效地构建有效的任务实例：\n\n*   使用 GPT-4.1-mini 时，SWE-Builder 能够构建269个有效实例，每个实例的成本为0.045美元。\n*   使用 Gemini-2.5-flash 时，其性能与GPT-4.1-mini相当，但成本更低，每个实例仅为0.024美元。\n\n此外，实验还验证了 SWE-Factory 各组件的准确性：\n\n*   基于退出码的评分方法与人工检查相比，达到了100%的准确率。\n*   自动化 fail2pass 验证的精确度为0.92，召回率为1.00。\n\n### 结论与展望\n\nSWE-Factory 有望加速大规模、高质量GitHub问题解决数据集的收集，从而促进LLM在软件工程领域的训练和评估。相关代码和数据集已发布。",
      "shortSummary": "SWE-Factory是一个自动化流程，旨在解决为大型语言模型构建GitHub问题解决训练数据和评估基准的挑战。它通过SWE-Builder自动化评估环境构建，采用基于退出码的标准化评分方法，并实现自动化fail2pass验证。实验证明，SWE-Factory能有效构建高质量任务实例，显著降低成本，并实现高准确率，从而加速大规模数据集的收集。",
      "translated_title": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂",
      "images": [],
      "contentSource": "完整文章",
      "content": "Constructing large-scale datasets for the GitHub issue resolution task is crucial for both training and evaluating the software engineering capabilities of Large Language Models (LLMs). However, the traditional process for creating such benchmarks is notoriously challenging and labor-intensive, particularly in the stages of setting up evaluation environments, grading test outcomes, and validating task instances. In this paper, we propose SWE-Factory, an automated pipeline designed to address these challenges. To tackle these issues, our pipeline integrates three core automated components. First, we introduce SWE-Builder, a multi-agent system that automates evaluation environment construction, which employs four specialized agents that work in a collaborative, iterative loop and leverages an environment memory pool to enhance efficiency. Second, we introduce a standardized, exit-code-based grading method that eliminates the need for manually writing custom parsers. Finally, we automate the fail2pass validation process using these reliable exit code signals. Experiments on 671 issues across four programming languages show that our pipeline can effectively construct valid task instances; for example, with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per instance, while with Gemini-2.5-flash, it achieves comparable performance at the lowest cost of 0.024 per instance. We also demonstrate that our exit-code-based grading achieves 100% accuracy compared to manual inspection, and our automated fail2pass validation reaches a precision of 0.92 and a recall of 1.00. We hope our automated pipeline will accelerate the collection of large-scale, high-quality GitHub issue resolution datasets for both training and evaluation. Our code and datasets are released at https://github.com/DeepSoftwareAnalytics/swe-factory."
    },
    {
      "title": "为智能体构建网络，而非为网络构建智能体 (原标题: Build the web for agents, not agents for the web)",
      "link": "https://arxiv.org/abs/2506.10953",
      "pubDate": "Thu, 12 Jun 2025 13:53:58 GMT",
      "isoDate": "2025-06-12T13:53:58.000Z",
      "creator": "Xing Han Lù, Gaurav Kamath, Marius Mosbach, Siva Reddy",
      "summary": "### 核心问题：当前网络接口与AI智能体能力不匹配\n\n*   **背景**：大型语言模型（LLMs）及多模态模型的发展，激发了开发网络智能体（Web Agents）的巨大兴趣。这些AI系统旨在自主导航和完成网络环境中的任务。\n*   **挑战**：现有方法面临重大挑战，因为人类设计的网络接口与LLM的能力之间存在根本性不匹配。具体困难包括：\n    *   处理庞大的DOM树。\n    *   依赖截图并辅以额外信息。\n    *   通过API交互完全绕过用户界面。\n\n### 提出的解决方案：范式转变——为智能体设计网络接口\n\n*   **核心主张**：本文倡导网络智能体研究的范式转变。与其强迫网络智能体适应为人类设计的接口，不如开发一种专门为智能体能力优化的新型交互范式。\n*   **概念引入**：引入“智能体网络接口”（Agentic Web Interface, AWI）的概念。AWI是一种专门为智能体导航网站而设计的接口。\n\n### AWI设计原则与目标\n\n*   **六项指导原则**：为AWI设计建立了六项指导原则，强调：\n    *   安全性（Safety）\n    *   效率（Efficiency）\n    *   标准化（Standardization）\n*   **利益相关者**：这些原则旨在兼顾所有主要利益相关者的利益。\n*   **预期成果**：这种重新定位旨在克服现有接口的根本性局限，为设计更高效、可靠和透明的网络智能体铺平道路。\n\n### 协作努力\n\n*   **社区参与**：实现这一目标将是一项协作努力，需要更广泛的机器学习（ML）社区的参与。",
      "shortSummary": "当前网络接口与AI智能体能力不匹配，导致网络智能体开发面临挑战。本文提出范式转变，倡导为智能体设计专门的“智能体网络接口”（AWI）。AWI将遵循安全性、效率和标准化等原则，旨在克服现有接口局限，实现更高效、可靠、透明的网络智能体设计，这将是机器学习社区的协作成果。",
      "translated_title": "为智能体构建网络，而非为网络构建智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community."
    },
    {
      "title": "Domain2Vec：向量化数据集以无需训练找到最优数据混合 (原标题: Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training)",
      "link": "https://arxiv.org/abs/2506.10952",
      "pubDate": "Thu, 12 Jun 2025 13:53:51 GMT",
      "isoDate": "2025-06-12T13:53:51.000Z",
      "creator": "Mozhi Zhang, Howe Tissue, Lu Wang, Xipeng Qiu",
      "summary": "# Domain2Vec：无需训练寻找最优数据混合的向量化数据集方法\n\n## 核心概念与方法\n*   **Domain2Vec 介绍**：Domain2Vec 是一种新颖的方法，旨在将任何数据集分解为多个“元域”（meta-domains）的线性组合。元域是一个新概念，用于捕捉数据集的关键底层特征。\n*   **元域词汇表与分类器**：Domain2Vec 维护一个元域词汇表，并使用分类器将给定数据集分解为域向量。这些域向量对应于元域词汇表上的一个分布。\n\n## 应用与优势\n*   **优化语言模型预训练数据混合**：\n    *   通过域向量，Domain2Vec 能够在无需训练的情况下，识别出语言模型（LM）预训练的最佳数据混合。\n    *   这一过程基于“**分布对齐假设**”（DA²），即当训练集和验证集的数据分布对齐程度越高时，验证损失越低。\n*   **提升效率与可扩展性**：Domain2Vec 可以无缝集成到现有工作中，以建模域向量与 LM 性能之间的关系，从而显著提高现有方法的效率和可扩展性。\n*   **计算开销最小化**：该方法以最小的计算开销增强下游任务性能。\n\n## 实验结果\n*   **计算效率提升**：在 Pile-CC 数据集上，Domain2Vec 仅使用原始 The Pile 数据集混合训练所需计算量的 51.5%，就达到了相同的验证损失。\n*   **下游任务性能提升**：在同等计算预算下，Domain2Vec 平均将下游任务性能提高了 2.83%。\n\n## 其他信息\n*   **接受会议**：该研究已被 ICML2025 接受。\n*   **研究领域**：计算与语言 (cs.CL)、人工智能 (cs.AI)、机器学习 (cs.LG)。",
      "shortSummary": "Domain2Vec 是一种无需训练即可优化语言模型数据混合的新方法。它通过将数据集分解为“元域”并生成“域向量”，利用“分布对齐假设”来识别最佳数据组合。实验表明，Domain2Vec 能显著减少计算量（如在 Pile-CC 上节省 51.5% 计算），同时平均提升下游任务性能 2.83%，从而提高效率和可扩展性。",
      "translated_title": "Domain2Vec：向量化数据集以无需训练找到最优数据混合",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce~Domain2Vec, a novel approach that decomposes any dataset into a linear combination of several meta-domains, a new concept designed to capture the key underlying features of datasets. Domain2Vec maintains a vocabulary of meta-domains and uses a classifier to decompose any given dataset into a domain vector that corresponds to a distribution over this vocabulary. These domain vectors enable the identification of the optimal data mixture for language model (LM) pretraining in a training-free manner under the \\textbf{Distribution Alignment Assumption} (DA^{2}), which suggests that when the data distributions of the training set and the validation set are better aligned, a lower validation loss is achieved. Moreover, Domain2vec can be seamlessly integrated into previous works to model the relationship between domain vectors and LM performance, greatly enhancing the efficiency and scalability of previous methods. Extensive experiments demonstrate that Domain2Vec helps find the data mixture that enhances downstream task performance with minimal computational overhead. Specifically, Domain2Vec achieves the same validation loss on Pile-CC using only 51.5% of the computation required when training on the original mixture of The Pile dataset. Under equivalent compute budget, Domain2Vec improves downstream performance by an average of 2.83%."
    },
    {
      "title": "Magistral",
      "link": "https://arxiv.org/abs/2506.10910",
      "pubDate": "Thu, 12 Jun 2025 13:22:37 GMT",
      "isoDate": "2025-06-12T13:22:37.000Z",
      "creator": "Mistral-AI, Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, Adam Yang, Alexander H. Liu, Alexandre Sablayrolles, Amélie Héliou, Amélie Martin, Andy Ehrenberg, Anmol Agarwal, Antoine Roux, Arthur Darcet, Arthur Mensch, Baptiste Bout, Baptiste Rozière, Baudouin De Monicault, Chris Bamford, Christian Wallenwein, Christophe Renaudin, Clémence Lanfranchi, Darius Dabert, Devon Mizelle, Diego de las Casas, Elliot Chane-Sane, Emilien Fugier, Emma Bou Hanna, Gauthier Delerce, Gauthier Guinet, Georgii Novikov, Guillaume Martin, Himanshu Jaju, Jan Ludziejewski, Jean-Hadrien Chabran, Jean-Malo Delignon, Joachim Studnia, Jonas Amar, Josselin Somerville Roberts, Julien Denize, Karan Saxena, Kush Jain, Lingxiao Zhao, Louis Martin, Luyu Gao, Lélio Renard Lavaud, Marie Pellat, Mathilde Guillaumin, Mathis Felardos, Maximilian Augustin, Mickaël Seznec, Nikhil Raghuraman, Olivier Duchenne, Patricia Wang, Patrick von Platen, Patryk Saffer, Paul Jacob, Paul Wambergue, Paula Kurylowicz, Pavankumar Reddy Muddireddy, Philomène Chagniot, Pierre Stock, Pravesh Agrawal, Romain Sauvestre, Rémi Delacourt, Sanchit Gandhi, Sandeep Subramanian, Shashwat Dalal, Siddharth Gandhi, Soham Ghosh, Srijan Mishra, Sumukh Aithal, Szymon Antoniak, Thibault Schueller, Thibaut Lavril, Thomas Robert, Thomas Wang, Timothée Lacroix, Valeriia Nemychnikova, Victor Paltz, Virgile Richard, Wen-Ding Li, William Marshall, Xuanyu Zhang, Yunhao Tang",
      "summary": "### Magistral：Mistral-AI 的首个推理模型与可扩展强化学习管线\n\nMistral-AI 团队隆重推出了 **Magistral**，这不仅是他们首个专注于推理能力的模型，更标志着他们自主研发的可扩展强化学习（RL）管线的诞生。\n\n#### 核心方法与创新\n\nMagistral 项目的核心在于其“从零开始”（ground-up）的独特方法。与以往依赖现有实现或从先前模型蒸馏 RL 轨迹不同，该项目完全基于 Mistral-AI 自己的模型和基础设施进行构建。这一创新方法使得团队能够：\n\n*   **探索纯 RL 训练的极限**：他们展示了一个能够深入探索大型语言模型（LLMs）纯粹强化学习训练潜力的技术栈。\n*   **强制推理语言**：提出了一种简单而有效的方法，能够引导模型使用特定的推理语言。\n\n#### 关键发现与成果\n\n研究结果令人鼓舞，表明仅在文本数据上进行强化学习训练，不仅能够保持初始检查点的大部分能力，甚至在以下关键领域有所提升：\n\n*   **多模态理解**：模型对多种数据形式的理解能力得到维持或增强。\n*   **指令遵循**：模型执行用户指令的准确性和效率得到保持或改善。\n*   **函数调用**：模型在调用外部函数方面的能力得到维持或提升。\n\n#### 模型发布\n\nMistral-AI 公布了两款 Magistral 系列模型：\n\n*   **Magistral Medium**：该模型在 Mistral Medium 3 的基础上，完全通过强化学习进行推理训练，展现了纯 RL 在复杂任务上的潜力。\n*   **Magistral Small**：该模型已根据 Apache 2.0 许可开源，并且进一步包含了来自 Magistral Medium 的冷启动数据，为社区提供了可访问的资源。\n\n#### 研究领域\n\n本研究属于计算与语言（cs.CL）领域。",
      "shortSummary": "Magistral 是 Mistral-AI 推出的首个推理模型，并引入了其自研的可扩展强化学习（RL）管线。该项目采用“从零开始”的方法，完全基于自有模型和基础设施。研究发现，仅通过文本数据上的 RL 训练，模型能保持并提升多模态理解、指令遵循和函数调用能力。Mistral-AI 发布了基于纯 RL 训练的 Magistral Medium，并开源了包含冷启动数据的 Magistral Small 模型。",
      "translated_title": "Magistral",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium."
    },
    {
      "title": "CreatiPoster：迈向可编辑和可控的多层平面设计生成 (原标题: CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation)",
      "link": "https://arxiv.org/abs/2506.10890",
      "pubDate": "Thu, 12 Jun 2025 12:54:39 GMT",
      "isoDate": "2025-06-12T12:54:39.000Z",
      "creator": "Zhao Zhang, Yutao Cheng, Dexiang Hong, Maoke Yang, Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, Xinglong Wu",
      "summary": "### CreatiPoster：可编辑和可控的多层平面设计生成框架\n\n**1. 引言与背景**\n\n*   **挑战：** 高质量、可编辑且美观的平面设计创作耗时耗力，尤其对初学者而言。现有AI工具虽能自动化部分流程，但在整合用户资产、保持可编辑性及实现专业视觉效果方面表现不佳。商业系统（如Canva Magic Design）依赖庞大的模板库，难以复制。\n\n**2. CreatiPoster 框架介绍**\n\n*   **核心目标：** 引入CreatiPoster，一个能够从自然语言指令或用户资产生成可编辑、多层设计作品的框架。\n*   **工作原理：** 采用双模型方法：\n    *   **协议模型（RGBA大型多模态模型）：** 首先生成一个JSON规范，详细定义每个图层（文本或资产）的精确布局、层级、内容和样式，并提供一个简洁的背景提示。\n    *   **条件背景模型：** 随后根据渲染出的前景图层合成一个连贯的背景。\n\n**3. 性能评估与贡献**\n\n*   **基准测试：** 构建了一个包含自动化指标的平面设计生成基准。\n*   **性能表现：** CreatiPoster 在该基准测试中超越了领先的开源方法和专有商业系统。\n*   **数据发布：** 为促进进一步研究，项目发布了一个包含100,000个多层设计的无版权语料库。\n\n**4. 多样化应用**\n\n*   CreatiPoster 支持多种应用场景，包括：\n    *   画布编辑\n    *   文本叠加\n    *   响应式尺寸调整\n    *   多语言适配\n    *   动态海报制作\n\n**5. 愿景**\n\n*   该框架旨在推动AI辅助平面设计的普及化和民主化。",
      "shortSummary": "CreatiPoster是一个创新的AI框架，旨在生成可编辑、多层的平面设计作品。它通过一个双模型系统，根据自然语言指令或用户资产生成详细的图层规范和背景。该框架在性能上超越了现有系统，并发布了一个包含10万个设计的无版权语料库。CreatiPoster支持多种应用，如画布编辑、文本叠加和动态海报，致力于推动AI辅助平面设计的普及。",
      "translated_title": "CreatiPoster：迈向可编辑和可控的多层平面设计生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphic design plays a crucial role in both commercial and personal contexts, yet creating high-quality, editable, and aesthetically pleasing graphic compositions remains a time-consuming and skill-intensive task, especially for beginners. Current AI tools automate parts of the workflow, but struggle to accurately incorporate user-supplied assets, maintain editability, and achieve professional visual appeal. Commercial systems, like Canva Magic Design, rely on vast template libraries, which are impractical for replicate. In this paper, we introduce CreatiPoster, a framework that generates editable, multi-layer compositions from optional natural-language instructions or assets. A protocol model, an RGBA large multimodal model, first produces a JSON specification detailing every layer (text or asset) with precise layout, hierarchy, content and style, plus a concise background prompt. A conditional background model then synthesizes a coherent background conditioned on this rendered foreground layers. We construct a benchmark with automated metrics for graphic-design generation and show that CreatiPoster surpasses leading open-source approaches and proprietary commercial systems. To catalyze further research, we release a copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports diverse applications such as canvas editing, text overlay, responsive resizing, multilingual adaptation, and animated posters, advancing the democratization of AI-assisted graphic design. Project homepage: https://github.com/graphic-design-ai/creatiposter"
    },
    {
      "title": "VRBench：长篇叙事视频多步推理基准 (原标题: VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos)",
      "link": "https://arxiv.org/abs/2506.10857",
      "pubDate": "Thu, 12 Jun 2025 12:17:17 GMT",
      "isoDate": "2025-06-12T12:17:17.000Z",
      "creator": "Jiashuo Yu, Yue Wu, Meng Chu, Zhifei Ren, Zizheng Huang, Pei Chu, Ruijie Zhang, Yinan He, Qirui Li, Songze Li, Zhenxiang Li, Zhongying Tu, Conghui He, Yu Qiao, Yali Wang, Yi Wang, Limin Wang",
      "summary": "# VRBench：长篇叙事视频多步推理基准\n\n**引言**\n\nVRBench是首个专为评估大型模型在长篇叙事视频中多步推理能力而设计的基准。它旨在弥补现有评估在时间推理和程序有效性方面的不足。\n\n**基准构成与数据策展**\n\n*   **视频数据：** 包含1,010个长视频，平均时长1.6小时。\n*   **问答对：** 拥有9,468个人工标注的多步问答对。\n*   **推理步骤：** 包含30,292个带有时间戳的推理步骤。\n*   **数据策展流程：** 视频内容通过多阶段过滤过程进行精心策划，包括专家交叉评审，以确保情节的连贯性。\n\n**推理链生成框架**\n\n*   VRBench开发了一个人机协作框架，用于生成连贯的推理链。\n*   每个推理链都要求多个基于时间定位的步骤。\n*   推理类型涵盖七种，例如事件归因和隐式推理。\n\n**多阶段评估流程**\n\n*   该基准设计了一个多阶段评估流程，从结果和过程两个层面全面评估模型。\n*   **结果评估：** 采用多项选择题（MCQs）来评估最终结果。\n*   **过程评估：** 提出了一种由大型语言模型（LLM）引导的进度级别评分指标，用于多维度地综合评估推理链的质量。\n\n**实验与发现**\n\n*   研究人员在VRBench上对12个大型语言模型（LLMs）和16个视觉语言模型（VLMs）进行了广泛评估。\n*   通过深入分析，该研究为多步推理领域提供了宝贵的见解，推动了该领域的发展。\n\n**其他信息**\n\n*   该工作以技术报告形式发布。\n*   相关研究领域包括计算机视觉与模式识别（cs.CV）、人工智能（cs.AI）和多媒体（cs.MM）。",
      "shortSummary": "VRBench是首个用于评估大型模型在长篇叙事视频中多步推理能力的基准。它包含1,010个平均1.6小时的长视频，以及9,468个人工标注的多步问答对和30,292个带时间戳的推理步骤。该基准通过人机协作框架生成连贯的推理链，并设计了多阶段评估流程，从结果和过程层面全面评估模型。对12个LLM和16个VLM的广泛评估提供了多步推理领域的宝贵见解，填补了现有评估在时间推理和程序有效性方面的空白。",
      "translated_title": "VRBench：长篇叙事视频多步推理基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 1,010 long videos (with an average duration of 1.6 hours), along with 9,468 human-labeled multi-step question-answering pairs and 30,292 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning."
    },
    {
      "title": "VideoDeepResearch：使用智能体工具进行长视频理解 (原标题: VideoDeepResearch: Long Video Understanding With Agentic Tool Using)",
      "link": "https://arxiv.org/abs/2506.10821",
      "pubDate": "Thu, 12 Jun 2025 11:39:10 GMT",
      "isoDate": "2025-06-12T11:39:10.000Z",
      "creator": "Huaying Yuan, Zheng Liu, Junjie Zhou, Ji-Rong Wen, Zhicheng Dou",
      "summary": "VideoDeepResearch：使用智能体工具进行长视频理解\n\n**摘要**\n\n长视频理解（LVU）对当前的多模态大型语言模型（MLLM）构成了重大挑战，这主要是由于任务本身的复杂性以及上下文窗口的限制。传统观点认为，解决LVU任务需要具备扩展上下文窗口、强大视觉感知能力和专业领域知识的基础MLLM。\n\n**VideoDeepResearch 框架**\n\n*   **挑战传统观念**：本文提出VideoDeepResearch，一个新颖的智能体框架，旨在挑战上述传统观念。该框架表明，LVU任务不一定需要复杂的MLLM。\n*   **核心构成**：\n    *   **纯文本大型推理模型（LRM）**：VideoDeepResearch 仅依赖一个纯文本的LRM作为核心推理引擎。\n    *   **模块化多模态工具包**：LRM与一个模块化的多模态工具包结合使用，该工具包包含多模态检索器和视觉感知器。这些工具在实践中均可轻易获得。\n*   **工作原理**：\n    *   对于每个LVU任务，系统通过推理制定问题解决策略。\n    *   系统选择性地访问并利用必要的视频内容，通过工具使用来实现这一目标。\n\n**实验结果与性能**\n\n*   **实验基准**：研究团队在流行的LVU基准测试上进行了广泛实验，包括MLVU、Video-MME和LVBench。\n*   **显著提升**：VideoDeepResearch 在现有MLLM基线上取得了显著的性能提升：\n    *   在MLVU（测试集）上超越现有最佳水平9.6%。\n    *   在LVBench上超越现有最佳水平6.6%。\n    *   在LongVideoBench上超越现有最佳水平3.9%。\n\n**结论**\n\n这些发现强调了智能体系统在克服长视频理解问题中关键挑战方面的巨大潜力。",
      "shortSummary": "VideoDeepResearch 提出一种新颖的智能体框架，用于长视频理解（LVU）。该框架挑战了LVU必须依赖复杂多模态大型语言模型（MLLM）的传统观念，转而使用纯文本大型推理模型（LRM）结合模块化多模态工具包。系统通过推理制定策略并利用工具访问视频内容。实验结果显示，VideoDeepResearch 在多个LVU基准测试上显著超越现有MLLM基线，证明了智能体系统在解决LVU问题上的巨大潜力。",
      "translated_title": "VideoDeepResearch：使用智能体工具进行长视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task's inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems."
    },
    {
      "title": "PosterCraft：统一框架下高质量美学海报生成的再思考 (原标题: PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework)",
      "link": "https://arxiv.org/abs/2506.10741",
      "pubDate": "Thu, 12 Jun 2025 10:28:12 GMT",
      "isoDate": "2025-06-12T10:28:12.000Z",
      "creator": "SiXiang Chen, Jianyu Lai, Jialin Gao, Tian Ye, Haoyu Chen, Hengyu Shi, Shitong Shao, Yunlong Lin, Song Fei, Zhaohu Xing, Yeying Jin, Junfeng Luo, Xiaoming Wei, Lei Zhu",
      "summary": "## PosterCraft：统一框架下高质量美学海报生成\n\n### 引言\n生成美学海报比简单的图像设计更具挑战性，因为它不仅需要精确的文本渲染，还需要抽象艺术内容、引人注目的布局和整体风格和谐的无缝整合。\n\n### PosterCraft 框架概述\n为了应对这些挑战，研究人员提出了 PosterCraft，一个统一的框架。该框架摒弃了以往模块化的管道和僵硬、预定义的布局，允许模型自由探索连贯且视觉上引人注目的构图。\n\n### 核心工作流程\nPosterCraft 采用精心设计的级联工作流程来优化高美学海报的生成，具体包括以下四个阶段：\n\n1.  **大规模文本渲染优化**：在研究团队新引入的 Text-Render-2M 数据集上进行大规模文本渲染优化。\n2.  **区域感知监督微调**：在 HQ-Poster100K 数据集上进行区域感知监督微调。\n3.  **美学文本强化学习**：通过“最佳之N”偏好优化（best-of-n preference optimization）进行美学文本强化学习。\n4.  **视觉-语言联合反馈细化**：通过视觉和语言的联合反馈进行精炼。\n\n### 数据与训练\n每个阶段都由一个完全自动化的数据构建管道支持，该管道根据其特定需求量身定制，从而在不进行复杂架构修改的情况下实现稳健的训练。\n\n### 性能评估\nPosterCraft 在多项实验中进行了评估，结果显示其在渲染准确性、布局连贯性和整体视觉吸引力方面显著优于开源基线，其质量已接近最先进的商业系统。",
      "shortSummary": "PosterCraft 是一个统一的框架，旨在解决高质量美学海报生成中的复杂挑战。它通过放弃传统模块化设计，采用级联工作流，包括文本渲染优化、区域感知微调、美学文本强化学习和视觉-语言反馈细化。该框架利用自动化数据构建，显著提升了海报的渲染准确性、布局连贯性和视觉吸引力，性能超越开源基线并接近顶级商业系统。",
      "translated_title": "PosterCraft：统一框架下高质量美学海报生成的再思考",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft"
    },
    {
      "title": "AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作 (原标题: AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation)",
      "link": "https://arxiv.org/abs/2506.10540",
      "pubDate": "Thu, 12 Jun 2025 06:06:21 GMT",
      "isoDate": "2025-06-12T06:06:21.000Z",
      "creator": "Haoyuan Shi, Yunxin Li, Xinyu Chen, Longyue Wang, Baotian Hu, Min Zhang",
      "summary": "# AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作\n\n## 引言\n\n尽管视频生成模型取得了快速进展，但生成跨越多个场景和角色的连贯故事视频仍然充满挑战。现有方法通常将预生成的关键帧僵硬地转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着即使一个低质量的片段也可能显著降低整个输出动画的逻辑连贯性和视觉连续性。\n\n## AniMaker框架\n\n为了克服这些障碍，我们引入了AniMaker，一个多智能体框架，旨在实现高效的多候选片段生成和故事感知片段选择，从而仅从文本输入创建全局一致且故事连贯的动画。\n\n该框架围绕以下专业智能体构建：\n\n*   **导演智能体 (Director Agent)：** 负责故事板生成。\n*   **摄影智能体 (Photography Agent)：** 负责视频片段生成。\n*   **评审智能体 (Reviewer Agent)：** 负责评估。\n*   **后期制作智能体 (Post-Production Agent)：** 负责编辑和配音。\n\n## 关键技术组件\n\nAniMaker方法的核心是两个关键技术组件：\n\n1.  **MCTS-Gen (在摄影智能体中)：**\n    *   这是一种受蒙特卡洛树搜索 (MCTS) 启发的策略。\n    *   它能够智能地探索候选空间，生成高潜力的片段，同时优化资源使用。\n2.  **AniEval (在评审智能体中)：**\n    *   这是第一个专门为多镜头动画评估设计的框架。\n    *   它通过考虑每个片段与其前后片段的上下文关系，评估故事层面的连贯性、动作完成度以及动画特定特征。\n\n## 实验结果\n\n实验表明，AniMaker在VBench和我们提出的AniEval框架等流行指标上均取得了卓越的质量，同时显著提高了多候选生成的效率，推动了AI生成的动画故事创作更接近生产标准。\n\n## 研究领域\n\n*   多智能体系统 (cs.MA)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "AniMaker是一个多智能体框架，旨在通过文本输入自动化生成连贯的动画故事。它解决了现有视频生成中叙事脱节和质量不稳定的问题。框架包含导演、摄影、评审和后期制作智能体。核心技术包括MCTS-Gen（高效片段生成）和AniEval（多镜头动画评估）。实验证明，AniMaker在质量和效率上均优于现有方法，使AI动画更接近生产标准。",
      "translated_title": "AniMaker：基于MCTS驱动片段生成的自动化多智能体动画故事创作",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards."
    },
    {
      "title": "通过因果表示学习发现语言模型的层次化潜在能力 (原标题: Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning)",
      "link": "https://arxiv.org/abs/2506.10378",
      "pubDate": "Thu, 12 Jun 2025 02:07:42 GMT",
      "isoDate": "2025-06-12T02:07:42.000Z",
      "creator": "Jikai Jin, Vasilis Syrgkanis, Sham Kakade, Hanlin Zhang",
      "summary": "# 通过因果表示学习发现语言模型的层次化潜在能力\n\n## 引言\n对语言模型（LLM）能力的忠实评估对于获取可指导模型开发的实用见解至关重要。然而，在该领域进行严格的因果评估面临显著的方法学挑战，包括复杂的混杂效应和高昂的计算成本（如广泛的再训练）。\n\n## 提出的框架\n为了解决这些挑战，本文提出了一种**因果表示学习框架**。\n*   该框架将观察到的基准性能建模为少数潜在能力因素的线性变换。\n*   关键在于，这些潜在因素在适当控制基础模型作为共同混杂因素后，被识别为具有因果关联。\n\n## 应用与发现\n*   **数据集：** 该方法被应用于一个综合数据集，该数据集包含来自Open LLM Leaderboard的超过1500个模型，并在六个基准上进行了评估。\n*   **核心发现：** 识别出一个简洁的**三节点线性因果结构**，该结构能够可靠地解释观察到的性能变化。\n\n## 科学洞察\n*   对这一因果结构的进一步解释提供了超越简单数值排名的重要科学见解。\n*   具体而言，研究揭示了一个清晰的因果方向：\n    1.  从**通用问题解决能力**开始。\n    2.  通过**指令遵循熟练度**进一步发展。\n    3.  最终达到**数学推理能力**。\n\n## 结论\n研究结果强调了在评估过程中仔细控制基础模型变异的重要性，这是准确揭示模型潜在能力之间潜在因果关系的关键一步。",
      "shortSummary": "本文提出一种因果表示学习框架，旨在解决语言模型能力评估中存在的混杂效应和高计算成本问题。该框架将基准性能建模为潜在能力因素的线性变换，并在控制基础模型后识别其因果关系。研究对1500多个模型应用此方法，发现了一个三节点因果结构，揭示了从通用问题解决到指令遵循再到数学推理的清晰能力发展路径。结果强调了在评估中控制基础模型变异的重要性。",
      "translated_title": "通过因果表示学习发现语言模型的层次化潜在能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities."
    },
    {
      "title": "Optimus-3：迈向具有可扩展任务专家的通用多模态Minecraft智能体 (原标题: Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts)",
      "link": "https://arxiv.org/abs/2506.10357",
      "pubDate": "Thu, 12 Jun 2025 01:29:40 GMT",
      "isoDate": "2025-06-12T01:29:40.000Z",
      "creator": "Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, Liqiang Nie",
      "summary": "### Optimus-3：迈向通用多模态Minecraft智能体\n\n#### 背景与挑战\n\n*   **现有进展：** 近期，基于多模态大语言模型（MLLMs）的智能体在多个领域取得了显著进展。\n*   **开放世界环境的挑战：** 然而，在Minecraft等开放世界环境中构建一个具备感知、规划、行动、接地和反思能力的通用智能体仍面临多重挑战：\n    *   领域特定数据不足。\n    *   异构任务之间的相互干扰。\n    *   开放世界设置中视觉多样性带来的复杂性。\n\n#### 主要贡献与方法\n\n为了应对上述挑战，本文提出了三项关键贡献：\n\n1.  **知识增强型数据生成管道：** 提出了一种知识增强型数据生成管道，旨在为智能体开发提供可扩展且高质量的训练数据。\n2.  **专家混合（MoE）架构：** 引入了一种带有任务级别路由的专家混合（MoE）架构，以有效减轻异构任务之间的干扰。\n3.  **多模态推理增强型强化学习：** 开发了一种多模态推理增强型强化学习方法，以增强智能体在Minecraft复杂视觉环境中的推理能力。\n\n#### Optimus-3智能体\n\n*   基于这些创新，本文提出了 **Optimus-3**，一个专为Minecraft设计的通用智能体。\n\n#### 实验结果\n\n*   广泛的实验结果表明，Optimus-3在Minecraft环境中的各种任务上，均超越了现有的通用多模态大语言模型和最先进的智能体。",
      "shortSummary": "Optimus-3是一个为Minecraft设计的通用多模态智能体，旨在解决开放世界环境中数据不足、任务干扰和视觉多样性等挑战。该研究通过知识增强数据生成、引入带有任务级别路由的专家混合（MoE）架构，以及开发多模态推理增强型强化学习方法来提升智能体能力。实验证明，Optimus-3在Minecraft的广泛任务中表现优于现有最先进的智能体和通用多模态大语言模型。",
      "translated_title": "Optimus-3：迈向具有可扩展任务专家的通用多模态Minecraft智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, agents based on multimodal large language models (MLLMs) have achieved remarkable progress across various domains. However, building a generalist agent with capabilities such as perception, planning, action, grounding, and reflection in open-world environments like Minecraft remains challenges: insufficient domain-specific data, interference among heterogeneous tasks, and visual diversity in open-world settings. In this paper, we address these challenges through three key contributions. 1) We propose a knowledge-enhanced data generation pipeline to provide scalable and high-quality training data for agent development. 2) To mitigate interference among heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture with task-level routing. 3) We develop a Multimodal Reasoning-Augmented Reinforcement Learning approach to enhance the agent's reasoning ability for visual diversity in Minecraft. Built upon these innovations, we present Optimus-3, a general-purpose agent for Minecraft. Extensive experimental results demonstrate that Optimus-3 surpasses both generalist multimodal large language models and existing state-of-the-art agents across a wide range of tasks in the Minecraft environment. Project page: https://cybertronagent.github.io/Optimus-3.github.io/"
    },
    {
      "title": "基于扩散模型的文本感知图像修复 (原标题: Text-Aware Image Restoration with Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.09993",
      "pubDate": "Wed, 11 Jun 2025 13:59:46 GMT",
      "isoDate": "2025-06-11T13:59:46.000Z",
      "creator": "Jaewon Min, Jin Hyeon Kim, Paul Hyunbin Cho, Jaeeun Lee, Jihye Park, Minkyu Park, Sangpil Kim, Hyunhee Park, Seungryong Kim",
      "summary": "## 文本感知图像修复 (TAIR)\n\n### 引言\n\n图像修复旨在恢复退化图像。尽管现有基于扩散模型的修复方法在自然图像修复方面取得了巨大成功，但它们在忠实重建退化图像中的文本区域时常常遇到困难。这些方法经常生成看似合理但实际上不正确的文本模式，这种现象被称为“文本图像幻觉”。\n\n### 新任务：文本感知图像修复 (TAIR)\n\n本文引入了“文本感知图像修复 (TAIR)”，这是一项新颖的修复任务，要求同时恢复图像的视觉内容和文本的保真度。\n\n### 提出的基准数据集：SA-Text\n\n为了解决TAIR任务，研究者提出了SA-Text，这是一个大规模的基准数据集。SA-Text包含10万张高质量的场景图像，这些图像密集标注了多样且复杂的文本实例。\n\n### 提出的多任务扩散框架：TeReDiff\n\n本文提出了一个名为TeReDiff的多任务扩散框架，以应对TAIR任务。TeReDiff的核心机制包括：\n\n*   **特征整合：** 将扩散模型内部提取的特征整合到一个文本定位模块中。\n*   **联合训练：** 这种整合使得扩散模型和文本定位模块能够进行联合训练，从而使两个组件相互受益。\n*   **文本表示作为提示：** 该框架能够提取丰富的文本表示，这些表示随后被用作去噪步骤中的提示，引导修复过程。\n\n### 实验结果\n\n广泛的实验结果表明，TeReDiff方法持续优于现有最先进的修复方法，并在文本识别准确性方面取得了显著提升。",
      "shortSummary": "本文提出了“文本感知图像修复 (TAIR)”新任务，旨在解决现有扩散模型在图像修复中对文本区域处理不佳、易产生“文本图像幻觉”的问题。为支持此任务，研究者构建了包含10万张带文本标注图像的大规模基准数据集SA-Text。同时，提出了多任务扩散框架TeReDiff，该框架将扩散模型特征与文本定位模块结合，通过联合训练和文本表示作为去噪提示，显著提升了文本识别准确性，并优于现有最先进的修复方法。",
      "translated_title": "基于扩散模型的文本感知图像修复",
      "images": [],
      "contentSource": "完整文章",
      "content": "Image restoration aims to recover degraded images. However, existing diffusion-based restoration methods, despite great success in natural image restoration, often struggle to faithfully reconstruct textual regions in degraded images. Those methods frequently generate plausible but incorrect text-like patterns, a phenomenon we refer to as text-image hallucination. In this paper, we introduce Text-Aware Image Restoration (TAIR), a novel restoration task that requires the simultaneous recovery of visual contents and textual fidelity. To tackle this task, we present SA-Text, a large-scale benchmark of 100K high-quality scene images densely annotated with diverse and complex text instances. Furthermore, we propose a multi-task diffusion framework, called TeReDiff, that integrates internal features from diffusion models into a text-spotting module, enabling both components to benefit from joint training. This allows for the extraction of rich text representations, which are utilized as prompts in subsequent denoising steps. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art restoration methods, achieving significant gains in text recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/"
    },
    {
      "title": "Resa：通过稀疏自编码器实现透明推理模型 (原标题: Resa: Transparent Reasoning Models via SAEs)",
      "link": "https://arxiv.org/abs/2506.09967",
      "pubDate": "Wed, 11 Jun 2025 13:44:01 GMT",
      "isoDate": "2025-06-11T13:44:01.000Z",
      "creator": "Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, Willie Neiswanger",
      "summary": "### Resa：通过稀疏自编码器（SAE）实现透明推理模型\n\n本文介绍了 Resa，一个由1.5B推理模型组成的家族，这些模型通过一种新颖且高效的稀疏自编码器调优（SAE-Tuning）程序进行训练。该研究旨在探讨如何以高成本效益的方式，通过利用语言模型的底层表示来激发其强大的推理能力。\n\n#### SAE-Tuning 方法概述\n\nSAE-Tuning 是一种两阶段的方法，用于在目标模型中激发推理能力，而无需使用推理轨迹，仅依赖经过验证的问答数据：\n\n1.  **SAE 训练**：首先训练一个稀疏自编码器（SAE），以从源模型中捕获推理能力。\n2.  **监督微调指导**：然后，利用训练好的 SAE 来指导标准的监督微调（SFT）过程，从而在目标模型中激发这些推理能力。\n\n#### 关键成果与效益\n\nSAE-Tuning 在成本效益和性能方面展现出显著优势：\n\n*   **成本与时间效率**：\n    *   当应用于某些基础模型（在进一步的强化学习（RL）后训练之前）时，SAE-Tuning 能够保留其 RL 训练对应模型超过97%的推理性能。\n    *   同时，训练成本降低了2000倍以上，降至约1美元。\n    *   训练时间缩短了450倍以上，降至约20分钟。\n*   **在轻度 RL 训练模型上的表现**：\n    *   当应用于经过轻度 RL 训练的模型（例如，在2个GPU上训练1小时内）时，SAE-Tuning 仅需额外约1美元的成本，即可实现强大的推理性能。\n    *   例如，在 AIME24 上达到43.33%的 Pass@1 成绩。\n    *   在 AMC23 上达到90%的 Pass@1 成绩。\n\n#### 意外发现：推理能力的通用性和模块化\n\n研究发现，通过 SAE 提取的推理能力可能具有通用性和模块化特性：\n\n*   **通用性**：从一个数据集中提取的能力仍然能够提升在更大且有重叠语料库上的性能。\n*   **模块化**：从 Qwen 或 Qwen-Math 模型中提取的能力可以在测试时直接附加到 R1-Distill 模型上，无需任何重新训练，并能产生可比较的性能提升。\n\n#### 结论与开放资源\n\n广泛的消融实验验证了这些发现。所有相关的人工制品（artifacts）均已完全开源。",
      "shortSummary": "Resa 是一种1.5B推理模型家族，通过创新的稀疏自编码器调优（SAE-Tuning）方法训练。该方法通过SAE从源模型捕获推理能力，并指导目标模型的监督微调。SAE-Tuning显著降低了训练成本（>2000倍）和时间（>450倍），同时保持了高性能。研究还发现，通过SAE提取的推理能力具有通用性和模块化，可跨数据集和模型应用。所有成果均已开源。",
      "translated_title": "Resa：通过稀疏自编码器实现透明推理模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains &gt;97% of its RL-trained counterpart's reasoning performance while reducing training costs by &gt;2000x to roughly \\1 and training time by &gt;450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around 1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."
    },
    {
      "title": "UniPre3D：基于跨模态高斯泼溅的3D点云模型统一预训练 (原标题: UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting)",
      "link": "https://arxiv.org/abs/2506.09952",
      "pubDate": "Wed, 11 Jun 2025 13:23:21 GMT",
      "isoDate": "2025-06-11T13:23:21.000Z",
      "creator": "Ziyi Wang, Yanran Zhang, Jie Zhou, Jiwen Lu",
      "summary": "UniPre3D：基于跨模态高斯泼溅的3D点云模型统一预训练\n\n**1. 研究背景与挑战**\n*   **问题核心：** 点云数据的尺度多样性给开发统一的3D视觉表示学习技术带来了显著挑战。\n*   **现有局限：** 当前统一的3D模型数量稀少，且现有预训练方法无法同时高效应用于物体级和场景级点云。\n\n**2. UniPre3D 方法介绍**\n*   **创新点：** UniPre3D 是首个能够无缝应用于任意尺度点云和任意3D模型架构的统一预训练方法。\n*   **预训练任务：** 预测高斯基元（Gaussian primitives）。\n*   **核心机制：**\n    *   采用可微分高斯泼溅（differentiable Gaussian splatting）来渲染图像。\n    *   实现精确的像素级监督和端到端优化。\n*   **任务优化：**\n    *   整合来自预训练图像模型的2D特征，以引入成熟的纹理知识。\n    *   此举旨在调节预训练任务的复杂性，并引导模型关注几何结构。\n\n**3. 实验验证与成果**\n*   **通用有效性：** 通过在多种物体级和场景级任务上进行广泛实验，并使用不同的点云模型作为骨干网络，验证了UniPre3D的普适有效性。\n\n**4. 其他信息**\n*   **论文状态：** 已被CVPR 2025接收。\n*   **研究领域：** 计算机视觉与模式识别（cs.CV）、人工智能（cs.AI）。",
      "shortSummary": "UniPre3D是一种创新的统一预训练方法，专为解决3D点云数据尺度多样性挑战而设计。它通过预测高斯基元并利用可微分高斯泼溅进行像素级监督，实现对任意尺度点云和3D模型架构的无缝应用。该方法还整合了2D图像特征以增强几何结构学习。实验证明，UniPre3D在物体级和场景级任务中均表现出普适有效性，并已被CVPR 2025接收。",
      "translated_title": "UniPre3D：基于跨模态高斯泼溅的3D点云模型统一预训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "The scale diversity of point cloud data presents significant challenges in developing unified representation learning techniques for 3D vision. Currently, there are few unified 3D models, and no existing pre-training method is equally effective for both object- and scene-level point clouds. In this paper, we introduce UniPre3D, the first unified pre-training method that can be seamlessly applied to point clouds of any scale and 3D models of any architecture. Our approach predicts Gaussian primitives as the pre-training task and employs differentiable Gaussian splatting to render images, enabling precise pixel-level supervision and end-to-end optimization. To further regulate the complexity of the pre-training task and direct the model's focus toward geometric structures, we integrate 2D features from pre-trained image models to incorporate well-established texture knowledge. We validate the universal effectiveness of our proposed method through extensive experiments across a variety of object- and scene-level tasks, using diverse point cloud models as backbones. Code is available at https://github.com/wangzy22/UniPre3D."
    },
    {
      "title": "VerIF：指令遵循中强化学习的验证工程 (原标题: VerIF: Verification Engineering for Reinforcement Learning in Instruction Following)",
      "link": "https://arxiv.org/abs/2506.09942",
      "pubDate": "Wed, 11 Jun 2025 13:10:36 GMT",
      "isoDate": "2025-06-11T13:10:36.000Z",
      "creator": "Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li",
      "summary": "### VerIF：指令遵循中强化学习的验证工程\n\n*   **背景与挑战**\n    *   可验证奖励强化学习（RLVR）是增强大型语言模型（LLMs）的关键技术，其中验证工程扮演核心角色。\n    *   然而，指令遵循领域中强化学习的最佳实践仍未得到充分探索。\n    *   本文旨在探索指令遵循中强化学习的验证挑战。\n\n*   **提出的方法：VerIF**\n    *   本文提出了一种名为 VerIF 的验证方法。\n    *   VerIF 结合了基于规则的代码验证和基于大型推理模型（如 QwQ-32B）的 LLM 验证。\n\n*   **构建高质量数据集：VerInstruct**\n    *   为支持 VerIF 方法，研究人员构建了一个高质量的指令遵循数据集 VerInstruct。\n    *   该数据集包含约 22,000 个实例，并附带了相关的验证信号。\n\n*   **实验与成果**\n    *   研究人员将 VerIF 应用于两个模型的强化学习训练。\n    *   在多个代表性的指令遵循基准测试中，模型性能取得了显著提升。\n    *   训练后的模型在同等规模模型中达到了最先进（SOTA）的性能。\n    *   模型对未见过的约束条件表现出良好的泛化能力。\n    *   进一步观察表明，模型的通用能力未受影响，这表明 VerIF 可以整合到现有的强化学习方案中，以提升整体模型性能。\n\n*   **资源发布**\n    *   为促进未来的研究，研究人员已发布了数据集、代码和模型。",
      "shortSummary": "本文提出了 VerIF，一种用于指令遵循中强化学习的验证方法。VerIF 结合了基于规则的代码验证和基于大型语言模型的验证。为支持该方法，研究人员构建了包含约22,000个实例的高质量数据集 VerInstruct。将 VerIF 应用于模型训练后，在多个指令遵循基准测试中取得了显著改进，达到了同等规模模型的最新水平，并展现出良好的泛化能力，同时不影响模型的通用能力。相关数据集、代码和模型已公开发布。",
      "translated_title": "VerIF：指令遵循中强化学习的验证工程",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF."
    },
    {
      "title": "ReasonMed：一个用于推进医学推理的37万多智能体生成数据集 (原标题: ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning)",
      "link": "https://arxiv.org/abs/2506.09513",
      "pubDate": "Wed, 11 Jun 2025 04:36:55 GMT",
      "isoDate": "2025-06-11T04:36:55.000Z",
      "creator": "Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang, Qifeng Bai, Tingyang Xu",
      "summary": "ReasonMed：一个用于推进医学推理的37万多智能体生成数据集\n\n**1. 背景与问题**\n*   尽管大型语言模型（LLMs）在数学和编程等领域展现出卓越的推理能力，但其在知识密集型医学问答领域的潜力尚未得到充分探索。\n\n**2. 解决方案：ReasonMed数据集**\n*   本文引入了ReasonMed，这是目前最大的医学推理数据集。\n*   **数据集规模与来源：**\n    *   包含37万个高质量示例。\n    *   这些示例是从170万个由各种LLM生成的初始推理路径中提炼而来。\n\n**3. 数据集构建方法**\n*   ReasonMed通过一个**多智能体验证和精炼过程**构建。\n*   设计了一个**错误精炼器（Error Refiner）**，该精炼器能够识别并纠正由验证器标记的易出错步骤，从而增强推理路径的质量。\n\n**4. 研究发现与最佳实践**\n*   研究人员利用ReasonMed系统性地调查了训练医学推理模型的最佳实践。\n*   发现将详细的**思维链（Chain-of-Thought, CoT）推理**与**简洁的答案摘要**相结合，能够产生最有效的微调策略。\n\n**5. 模型训练与性能**\n*   基于上述最佳策略，研究人员训练了**ReasonMed-7B**模型。\n*   ReasonMed-7B为100亿参数以下模型设定了新的基准。\n*   其性能比之前最佳模型高出4.17%。\n*   在PubMedQA数据集上，ReasonMed-7B甚至超越了LLaMA3.1-70B模型4.60%。",
      "shortSummary": "ReasonMed是目前最大的医学推理数据集，包含37万个高质量示例，通过多智能体验证和错误精炼过程构建。研究发现，结合详细的思维链推理和简洁的答案摘要是训练医学推理模型的最佳策略。基于此策略训练的ReasonMed-7B模型，为100亿参数以下模型设定了新基准，并在PubMedQA上超越了LLaMA3.1-70B模型。",
      "translated_title": "ReasonMed：一个用于推进医学推理的37万多智能体生成数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a multi-agent verification and refinement process, where we design an Error Refiner to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%."
    },
    {
      "title": "Ming-Omni：一个用于感知和生成的统一多模态模型 (原标题: Ming-Omni: A Unified Multimodal Model for Perception and Generation)",
      "link": "https://arxiv.org/abs/2506.09344",
      "pubDate": "Tue, 10 Jun 2025 22:50:49 GMT",
      "isoDate": "2025-06-10T22:50:49.000Z",
      "creator": "Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, Kaiyou Song, Kaimeng Ren, Libin Wang, Lixiang Ru, Lele Xie, Longhua Tan, Lyuxin Xue, Lan Wang, Mochen Bai, Ning Gao, Pei Chen, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Sirui Gao, Tinghao Liu, Taisong Li, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaoxue Chen, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yunxiao Sun, Yipeng Chen, Yifei Wu, Yongjie Lyu, Ziping Ma, Zipeng Feng, Zhijiang Fang, Zhihao Qiu, Ziyuan Huang, Zhengyu He",
      "summary": "# Ming-Omni：一个用于感知和生成的统一多模态模型\n\nMing-Omni 是一个由 Inclusion AI 提出的统一多模态模型，旨在处理多种模态数据并支持感知与生成任务。\n\n## 核心能力与特点\n\n*   **统一多模态处理**：Ming-Omni 能够处理图像、文本、音频和视频等多种模态的数据。\n*   **强大的生成能力**：该模型在语音和图像生成方面表现出强大的能力。\n\n## 模型架构\n\nMing-Omni 采用了以下关键设计：\n\n*   **专用编码器**：为不同模态数据配备了专用的编码器，用于提取各自的特征标记（tokens）。\n*   **Ling 架构**：提取出的标记随后由名为“Ling”的 MoE（Mixture-of-Experts）架构进行处理。Ling 架构集成了新提出的模态特定路由器（modality-specific routers）。\n\n## 设计优势\n\n这种统一的架构设计带来了显著优势：\n\n*   **高效处理与融合**：单个模型即可高效处理和融合多模态输入。\n*   **任务多样性支持**：无需单独的模型、任务特定的微调或结构重新设计，即可支持多种多样的任务。\n\n## 扩展的生成功能\n\nMing-Omni 超越了传统的感知型多模态模型，通过集成先进组件支持生成任务：\n\n*   **音频生成**：集成了先进的音频解码器，能够生成自然流畅的语音。\n*   **图像生成**：通过整合 Ming-Lite-Uni 模块，实现高质量的图像生成。\n\n## 应用场景\n\n凭借其多模态处理和生成能力，Ming-Omni 能够支持多种实际应用：\n\n*   **上下文感知聊天**：进行理解上下文的多模态对话。\n*   **文本到语音转换**：将文本内容转换为自然语音。\n*   **多功能图像编辑**：执行各种图像编辑操作。\n\n## 实验成果与开源状态\n\n*   **卓越性能**：实验结果表明，Ming-Omni 为跨所有模态的统一感知和生成提供了一个强大的解决方案。\n*   **首个开源对标 GPT-4o 的模型**：据作者所知，Ming-Omni 是首个在模态支持方面与 GPT-4o 匹敌的开源模型。\n*   **完全开源**：所有代码和模型权重均已发布，以鼓励社区的进一步研究和开发。",
      "shortSummary": "Ming-Omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并擅长语音和图像生成。它采用专用的模态编码器和基于 MoE 的 Ling 架构，实现了高效的多模态输入处理与融合。该模型支持上下文感知聊天、文本到语音转换和图像编辑等多样化任务，无需单独模型或微调。Ming-Omni 是已知首个在模态支持上对标 GPT-4o 的开源模型，其代码和模型权重已全部发布，旨在推动社区研究。",
      "translated_title": "Ming-Omni：一个用于感知和生成的统一多模态模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We propose Ming-Omni, a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-Omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-Omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-Omni offers a powerful solution for unified perception and generation across all modalities. Notably, our proposed Ming-Omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community."
    },
    {
      "title": "扩散模型的Token扰动引导 (原标题: Token Perturbation Guidance for Diffusion Models)",
      "link": "https://arxiv.org/abs/2506.10036",
      "pubDate": "Tue, 10 Jun 2025 17:25:46 GMT",
      "isoDate": "2025-06-10T17:25:46.000Z",
      "creator": "Javad Rajabi, Soroush Mehraban, Seyedmorteza Sadat, Babak Taati",
      "summary": "## 扩散模型的Token扰动引导 (Token Perturbation Guidance for Diffusion Models)\n\n### 摘要\n\n本文提出了一种名为Token扰动引导（Token Perturbation Guidance, TPG）的新方法，旨在解决现有分类器无关引导（Classifier-free Guidance, CFG）在扩散模型中存在的局限性，即需要特定的训练过程且仅限于条件生成。\n\n### 核心问题与TPG的提出\n\n*   **CFG的局限性**：尽管CFG已成为现代扩散模型中提升生成质量和对齐输入条件的关键组件，但它要求特定的训练程序，并且只能应用于条件生成任务。\n*   **TPG的解决方案**：TPG是一种新颖的方法，它直接将扰动矩阵应用于扩散网络内部的中间token表示。通过采用一种保持范数的洗牌操作，TPG能够提供有效且稳定的引导信号，从而在不改变模型架构的情况下提高生成质量。\n\n### TPG的关键特性与优势\n\n*   **无需训练**：TPG不需要额外的训练过程，可以直接应用于现有模型。\n*   **与输入条件无关**：TPG对输入条件是不可知的，这意味着它既适用于条件生成，也适用于无条件生成，极大地扩展了其适用范围。\n*   **无需架构修改**：该方法不要求对扩散模型的网络架构进行任何更改。\n*   **引导机制**：TPG提供的引导项在采样过程中，其效果比现有的其他无需训练的引导技术更接近CFG。\n\n### 实验验证与结果\n\n*   **实验设置**：研究人员在SDXL和Stable Diffusion 2.1模型上进行了广泛的实验。\n*   **无条件生成性能**：对于无条件生成任务，TPG在FID（Fréchet Inception Distance）指标上相对于SDXL基线实现了近2倍的提升。\n*   **提示对齐能力**：在提示对齐方面，TPG的表现与CFG非常接近。\n\n### 结论\n\n这些实验结果表明，TPG是一种通用且与条件无关的引导方法，能够为更广泛的扩散模型带来类似CFG的优势。该方法的代码已公开可用。",
      "shortSummary": "Token扰动引导（TPG）是一种无需训练、与条件无关的扩散模型新方法。它通过扰动中间token表示来提供引导信号，无需改变模型架构。TPG解决了分类器无关引导（CFG）的训练和条件限制，能同时应用于条件和无条件生成。实验显示，TPG在无条件生成上将FID性能提升近2倍，并在提示对齐上媲美CFG，为扩散模型带来了CFG般的广泛益处。",
      "translated_title": "扩散模型的Token扰动引导",
      "images": [],
      "contentSource": "完整文章",
      "content": "Classifier-free guidance (CFG) has become an essential component of modern diffusion models to enhance both generation quality and alignment with input conditions. However, CFG requires specific training procedures and is limited to conditional generation. To address these limitations, we propose Token Perturbation Guidance (TPG), a novel method that applies perturbation matrices directly to intermediate token representations within the diffusion network. TPG employs a norm-preserving shuffling operation to provide effective and stable guidance signals that improve generation quality without architectural changes. As a result, TPG is training-free and agnostic to input conditions, making it readily applicable to both conditional and unconditional generation. We further analyze the guidance term provided by TPG and show that its effect on sampling more closely resembles CFG compared to existing training-free guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1 show that TPG achieves nearly a 2times improvement in FID for unconditional generation over the SDXL baseline, while closely matching CFG in prompt alignment. These results establish TPG as a general, condition-agnostic guidance method that brings CFG-like benefits to a broader class of diffusion models. The code is available at https://github.com/TaatiTeam/Token-Perturbation-Guidance"
    }
  ],
  "lastUpdated": "2025-06-13T09:35:04.650Z"
}