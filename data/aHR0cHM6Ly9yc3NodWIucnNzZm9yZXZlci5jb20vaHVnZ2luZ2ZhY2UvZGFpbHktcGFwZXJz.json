{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "用于指令引导图像编辑的视觉自回归建模 (原标题: Visual Autoregressive Modeling for Instruction-Guided Image Editing)",
      "link": "https://arxiv.org/abs/2508.15772",
      "pubDate": "Thu, 21 Aug 2025 13:59:32 GMT",
      "isoDate": "2025-08-21T13:59:32.000Z",
      "creator": "Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei",
      "summary": "# VAREdit：用于指令引导图像编辑的视觉自回归框架\n\n## 引言与背景\n*   **扩散模型的局限性：** 近期，扩散模型在指令引导图像编辑中展现出卓越的视觉保真度。然而，其固有的全局去噪过程将编辑区域与整个图像上下文纠缠在一起，导致意外的虚假修改，并损害了对编辑指令的遵循。\n*   **自回归模型的优势：** 相比之下，自回归模型通过将图像合成表述为离散视觉令牌的顺序过程，提供了一种独特的范式。其因果和组合机制自然地规避了基于扩散方法在遵循指令方面的挑战。\n\n## VAREdit 框架\n*   **核心理念：** 本文提出了 VAREdit，一个视觉自回归（VAR）框架，它将图像编辑重新定义为一个“下一尺度预测”问题。\n*   **工作原理：** VAREdit 以源图像特征和文本指令为条件，生成多尺度目标特征以实现精确编辑。\n\n## 关键挑战与解决方案\n*   **挑战：** 在此范式中，一个核心挑战是如何有效地条件化源图像令牌。研究观察到，最精细尺度的源特征无法有效指导更粗尺度目标特征的预测。\n*   **解决方案：** 为弥合这一差距，研究引入了“尺度对齐参考（SAR）模块”。该模块将尺度匹配的条件信息注入到第一个自注意力层中。\n\n## 性能与效率\n*   **编辑遵循度与效率的显著提升：** VAREdit 在编辑遵循度和效率方面均展现出显著进步。\n*   **超越扩散模型：** 在标准基准测试中，VAREdit 的 GPT-Balance 分数比领先的基于扩散的方法高出 30% 以上。\n*   **高速编辑：** 此外，VAREdit 可以在 1.2 秒内完成 512x512 图像的编辑，比同等大小的 UltraEdit 快 2.2 倍。\n\n## 资源可用性\n*   模型的源代码和模型已公开。",
      "shortSummary": "VAREdit 是一种视觉自回归（VAR）框架，旨在解决扩散模型在指令引导图像编辑中存在的虚假修改和指令遵循度差的问题。它将图像编辑重构为下一尺度预测任务，通过引入尺度对齐参考（SAR）模块有效处理多尺度特征条件化。VAREdit 在编辑遵循度和效率上均有显著提升，其GPT-Balance分数比现有扩散方法高出30%以上，且编辑速度比UltraEdit快2.2倍，可在1.2秒内完成512x512图像编辑。",
      "translated_title": "用于指令引导图像编辑的视觉自回归建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a 512times512 editing in 1.2 seconds, making it 2.2times faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit."
    },
    {
      "title": "SceneGen：单图像一次前向传播3D场景生成 (原标题: SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass)",
      "link": "https://arxiv.org/abs/2508.15769",
      "pubDate": "Thu, 21 Aug 2025 13:59:16 GMT",
      "isoDate": "2025-08-21T13:59:16.000Z",
      "creator": "Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie",
      "summary": "# SceneGen：单图像一次前向传播3D场景生成\n\n## 摘要\n本文介绍了一种名为SceneGen的新型框架，旨在解决从单个场景图像中合成多个3D资产的挑战性任务。该研究旨在推动VR/AR和具身AI等领域中3D内容生成技术的发展。\n\n## 主要贡献\n\nSceneGen框架的贡献主要体现在以下四个方面：\n\n1.  **新型框架SceneGen**：\n    *   **输入**：接收单个场景图像及其对应的对象掩码。\n    *   **输出**：同时生成具有几何形状和纹理的多个3D资产。\n    *   **核心特点**：无需优化或资产检索过程，实现高效生成。\n\n2.  **创新的特征聚合模块**：\n    *   **功能**：在特征提取模块内部，整合来自视觉和几何编码器的局部及全局场景信息。\n    *   **机制**：结合位置头部（position head），使得系统能够通过一次前向传播，同时生成3D资产及其相对空间位置。\n\n3.  **多图像输入场景的扩展性**：\n    *   **训练方式**：尽管仅使用单图像输入进行训练。\n    *   **设计优势**：其架构设计使其能够直接扩展到多图像输入场景，并在这种情况下展现出更优的生成性能。\n\n4.  **全面的评估与验证**：\n    *   **评估方法**：通过广泛的定量和定性评估。\n    *   **结果**：确认了该方法的效率和鲁棒的生成能力。\n\n## 潜在影响\n研究人员认为，SceneGen范式为高质量3D内容生成提供了一种新颖的解决方案，有望推动其在下游任务中的实际应用。\n\n## 资源可用性\n项目的代码和模型将公开发布。",
      "shortSummary": "SceneGen是一个新型框架，旨在通过一次前向传播，从单个场景图像和对象掩码中同时生成多个具有几何形状和纹理的3D资产。它无需优化或资产检索，并引入了创新的特征聚合模块，能同时确定3D资产及其相对空间位置。尽管主要基于单图像训练，SceneGen也能有效扩展到多图像输入，并展现出高效和鲁棒的生成能力，为高质量3D内容生成提供了新方案。",
      "translated_title": "SceneGen：单图像一次前向传播3D场景生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen."
    },
    {
      "title": "ATLAS: 解耦骨骼与形状参数以实现富有表现力的参数化人体建模 (原标题: ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling)",
      "link": "https://arxiv.org/abs/2508.15767",
      "pubDate": "Thu, 21 Aug 2025 13:58:56 GMT",
      "isoDate": "2025-08-21T13:58:56.000Z",
      "creator": "Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar",
      "summary": "## ATLAS：解耦骨骼与形状参数以实现富有表现力的参数化人体建模\n\n### 现有挑战\n\n当前的参数化人体模型在捕捉多样姿态和形状的精细变化方面面临显著挑战，主要原因包括：\n*   **数据多样性不足：** 训练数据有限，导致模型难以泛化到更广泛的人体形态。\n*   **建模假设限制：** 现有模型通常基于限制性假设，影响了其表现力。\n*   **骨骼与软组织依赖：** 常见的建模范式是先使用线性基优化外部身体表面，然后从表面顶点回归内部骨骼关节。这种方法在内部骨骼和外部软组织之间引入了问题性依赖，限制了对身高和骨骼长度等身体属性的直接控制。\n\n### ATLAS模型介绍\n\n为了解决上述问题，我们提出了ATLAS，一个高保真的人体模型。ATLAS通过学习60万个高分辨率扫描数据构建而成，这些数据使用240个同步摄像头捕获，确保了数据的丰富性和准确性。\n\n### ATLAS的关键创新：骨骼与形状参数解耦\n\n与以往方法不同，ATLAS的核心创新在于明确地解耦了形状基和骨骼基。我们通过将网格表示直接基于人体骨骼来实现这一目标。这种解耦是ATLAS模型能够超越现有方法的关键。\n\n### 解耦带来的优势\n\n骨骼与形状参数的解耦为ATLAS带来了多项显著优势：\n*   **增强的形状表现力：** 模型能够更准确、更细致地捕捉人体形状的复杂变化，从而实现更富有表现力的3D人体表示。\n*   **细粒度身体属性定制：** 允许用户对身高、骨骼长度等身体属性进行更直接、更精细的控制，提高了模型的灵活性和实用性。\n*   **独立的关键点拟合：** 使得关键点拟合独立于外部软组织特征，提高了拟合的准确性和鲁棒性。\n\n### 性能评估\n\n定量评估结果表明，ATLAS在拟合未见过的、处于多样姿态的个体方面，表现优于现有方法，准确性更高。此外，ATLAS的非线性姿态校正器比线性模型能更有效地捕捉复杂姿态，进一步验证了其在处理复杂人体姿态方面的优越性。",
      "shortSummary": "ATLAS是一种高保真参数化人体模型，旨在解决现有模型在捕捉精细人体变化和骨骼-软组织依赖方面的不足。它通过将网格表示基于人体骨骼，明确解耦了形状和骨骼参数。这种创新方法显著增强了形状表现力，实现了细粒度身体属性定制，并支持独立的关键点拟合。ATLAS在拟合多样姿态的未见个体方面表现优异，其非线性姿态校正器能更有效地捕捉复杂姿态，超越了现有方法。",
      "translated_title": "ATLAS: 解耦骨骼与形状参数以实现富有表现力的参数化人体建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models."
    },
    {
      "title": "Intern-S1：一个科学多模态基础模型 (原标题: Intern-S1: A Scientific Multimodal Foundation Model)",
      "link": "https://arxiv.org/abs/2508.15763",
      "pubDate": "Thu, 21 Aug 2025 13:58:00 GMT",
      "isoDate": "2025-08-21T13:58:00.000Z",
      "creator": "Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou",
      "summary": "### Intern-S1：一个科学多模态基础模型\n\n**1. 引言与背景**\n近年来，尽管开源基础模型在许多热门领域取得了显著进展，性能已接近闭源模型，但在高价值但更具挑战性的科学专业领域，通用基础模型的进展明显滞后。这些领域仍高度依赖专家模型，且开源模型与闭源模型之间存在巨大差距，远不足以变革科学研究。\n\n**2. Intern-S1 的目标与定位**\n为弥合这一差距并进一步探索通用人工智能（AGI），我们引入了 Intern-S1。Intern-S1 是一个专业的通用模型，具备通用的理解和推理能力，并专长于分析多种科学模态数据。\n\n**3. 模型架构与规模**\nIntern-S1 采用多模态专家混合（Mixture-of-Experts, MoE）架构，其主要参数如下：\n*   **激活参数：** 280 亿\n*   **总参数：** 2410 亿\n\n**4. 训练过程**\nIntern-S1 的训练过程分为两个主要阶段：\n\n*   **持续预训练：**\n    *   在 5 万亿（5T）个 token 上进行持续预训练。\n    *   其中超过 2.5 万亿（2.5T）个 token 来自科学领域数据。\n\n*   **后训练（强化学习）：**\n    *   在 InternBootCamp 平台进行离线和在线强化学习（RL）。\n    *   提出并采用了“奖励混合（Mixture-of-Rewards, MoR）”方法，以协同方式同时对 1000 多个任务进行强化学习训练。\n    *   通过算法、数据和训练系统方面的综合创新，Intern-S1 在在线强化学习中取得了顶尖性能。\n\n**5. 性能表现**\n在综合评估基准测试中，Intern-S1 展现出卓越的性能：\n\n*   **通用推理任务：** 在开源模型中表现出竞争力。\n*   **科学领域任务：** 显著优于其他开源模型。\n*   **专业任务：** 在分子合成规划、反应条件预测、晶体热力学稳定性预测等专业任务上，超越了闭源的最新（state-of-the-art, SOTA）模型。\n\n**6. 可用性**\nIntern-S1 模型已开源，可在 [https://interns1.internlm.com](https://interns1.internlm.com) 获取。\n\n**7. 研究领域**\n该研究涉及以下主要领域：\n*   机器学习 (cs.LG)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "Intern-S1是一个科学多模态基础模型，旨在弥合通用模型与科学专业领域之间的差距。它采用2410亿总参数的多模态专家混合（MoE）架构，在包含2.5T科学数据的5T token上进行预训练，并通过创新的奖励混合强化学习方法在1000多个任务上进行后训练。Intern-S1在通用推理任务上具有竞争力，并在分子合成规划、反应条件预测等科学专业任务上显著超越开源模型，甚至超越了闭源SOTA模型。",
      "translated_title": "Intern-S1：一个科学多模态基础模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1."
    },
    {
      "title": "Waver：挥舞你的方式，生成栩栩如生的视频 (原标题: Waver: Wave Your Way to Lifelike Video Generation)",
      "link": "https://arxiv.org/abs/2508.15761",
      "pubDate": "Thu, 21 Aug 2025 13:56:10 GMT",
      "isoDate": "2025-08-21T13:56:10.000Z",
      "creator": "Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng",
      "summary": "# Waver：统一的图像与视频生成基础模型\n\nWaver是一个高性能的基础模型，旨在实现统一的图像和视频生成。它能够直接生成时长为5到10秒的视频，原生分辨率为720p，并可后续升级至1080p。该模型在一个集成框架内同时支持文本到视频（T2V）、图像到视频（I2V）以及文本到图像（T2I）的生成。\n\n## 核心技术与创新\n\n*   **混合流DiT架构**：Waver引入了一种混合流DiT（Hybrid Stream DiT）架构，以增强模态对齐并加速训练收敛。\n*   **高质量数据策展**：为确保训练数据质量，研究团队建立了一个全面的数据策展流程。他们手动标注并训练了一个基于MLLM（多模态大型语言模型）的视频质量模型，用于筛选出最高质量的样本。\n*   **详细的训练与推理指南**：该项目还提供了详细的训练和推理指南，旨在帮助社区更高效地生成高质量视频。\n\n## 卓越性能表现\n\n*   **复杂运动捕捉与时间一致性**：Waver在捕捉复杂运动方面表现出色，在视频合成中实现了卓越的运动幅度和时间一致性。\n*   **行业领先地位**：截至2025年7月30日，Waver在Artificial Analysis的T2V和I2V排行榜上均位列前三。它持续超越现有的开源模型，并与最先进的商业解决方案持平或更优。\n\n## 社区贡献与展望\n\n该技术报告旨在帮助社区更高效地训练高质量视频生成模型，从而加速视频生成技术的发展。",
      "shortSummary": "Waver是一个高性能的基础模型，能统一生成逼真的图像和视频。它直接生成5-10秒的720p（可升至1080p）视频，支持文本到视频、图像到视频及文本到图像。Waver采用混合流DiT架构和高质量数据策展，在复杂运动捕捉和时间一致性上表现卓越。在T2V和I2V排行榜上，Waver位列前三，超越开源模型并媲美商业方案，旨在推动视频生成技术发展。",
      "translated_title": "Waver：挥舞你的方式，生成栩栩如生的视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver."
    },
    {
      "title": "LiveMCP-101：在挑战性查询上对启用MCP的智能体进行压力测试和诊断 (原标题: LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries)",
      "link": "https://arxiv.org/abs/2508.15760",
      "pubDate": "Thu, 21 Aug 2025 13:55:54 GMT",
      "isoDate": "2025-08-21T13:55:54.000Z",
      "creator": "Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song",
      "summary": "# LiveMCP-101：在挑战性查询上对启用MCP的智能体进行压力测试和诊断\n\n## 摘要\n\n本文介绍了LiveMCP-101，一个旨在评估AI智能体在复杂多步任务中利用多种模型上下文协议（MCP）工具能力的基准测试。该基准测试通过模拟真实世界的动态场景，揭示了当前前沿大型语言模型在工具编排和效率方面的不足，并为未来的模型改进指明了方向。\n\n## 背景与问题\n\nAI智能体通过工具调用与现实世界交互并解决复杂任务的能力已成为一项关键能力。模型上下文协议（MCP）为此提供了一个强大的标准化框架，用于工具集成。然而，在基准测试AI智能体如何有效利用多样化的MCP工具在真实、动态场景中解决多步任务方面，目前存在显著的空白。\n\n## LiveMCP-101：解决方案\n\n为了弥补这一空白，研究人员提出了LiveMCP-101，这是一个全面的基准测试：\n\n*   **基准内容**：LiveMCP-101包含101个经过精心策划的真实世界查询。这些查询通过迭代的大型语言模型（LLM）重写和人工审查进行完善，确保其复杂性和真实性。\n*   **工具要求**：这些查询要求智能体协调使用多种MCP工具，包括但不限于：\n    *   网页搜索\n    *   文件操作\n    *   数学推理\n    *   数据分析\n*   **新颖的评估方法**：LiveMCP-101引入了一种新颖的评估方法。它利用“真实执行计划”（ground-truth execution plans）进行评估，而非仅仅依赖原始API输出。这种方法能够更好地反映真实世界环境的演变特性，提供更准确的性能衡量。\n\n## 实验结果与发现\n\n实验结果揭示了当前AI智能体在工具编排方面的重大挑战：\n\n*   **成功率低**：即使是前沿的大型语言模型（LLMs），其成功率也低于60%。这突出表明了在工具协调和任务执行方面存在的主要挑战。\n*   **失败模式与效率低下**：详细的消融研究和错误分析进一步揭示了智能体独特的失败模式以及在令牌使用上的低效率。这些分析为理解模型局限性提供了深入见解。\n*   **未来方向**：这些发现为改进当前模型、提升其工具使用能力指明了具体的方向。\n\n## 意义\n\nLiveMCP-101为评估真实世界智能体的能力设定了严格的标准，对于推动能够通过工具使用可靠执行复杂任务的自主AI系统向前发展具有重要意义。",
      "shortSummary": "LiveMCP-101是一个新的基准测试，旨在评估AI智能体在复杂多步任务中利用多种模型上下文协议（MCP）工具的能力。它包含101个真实的查询，要求智能体协调使用网页搜索、文件操作、数学推理和数据分析等工具。通过采用基于真实执行计划的评估方法，研究发现即使是前沿大型语言模型，成功率也低于60%，揭示了工具编排和令牌使用效率方面的重大挑战。LiveMCP-101为推动自主AI系统发展提供了严格的评估标准。",
      "translated_title": "LiveMCP-101：在挑战性查询上对启用MCP的智能体进行压力测试和诊断",
      "images": [],
      "contentSource": "完整文章",
      "content": "Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use."
    },
    {
      "title": "咖啡馆入口看起来无障碍吗？门在哪里？——迈向地理空间AI智能体以进行视觉查询 (原标题: \"Does the cafe entrance look accessible? Where is the door?\" Towards Geospatial AI Agents for Visual Inquiries)",
      "link": "https://arxiv.org/abs/2508.15752",
      "pubDate": "Thu, 21 Aug 2025 13:49:52 GMT",
      "isoDate": "2025-08-21T13:49:52.000Z",
      "creator": "Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane",
      "summary": "### 迈向地理空间AI智能体以进行视觉查询\n\n**1. 引言与问题背景**\n*   **数字地图的局限性：** 交互式数字地图虽然彻底改变了人们的出行和世界认知方式，但它们主要依赖于地理信息系统（GIS）数据库中预先存在的结构化数据（例如，道路网络、兴趣点索引）。\n*   **未解决的挑战：** 这种依赖性限制了数字地图回答与“世界看起来如何”相关的地理视觉问题的能力，例如“咖啡馆入口是否无障碍？”或“门在哪里？”。\n\n**2. 核心愿景：Geo-Visual Agents（地理视觉智能体）**\n*   **概念提出：** 本文提出了“地理视觉智能体”的愿景。\n*   **智能体特性：** 这些智能体是多模态AI智能体，旨在理解并响应关于世界的细致入微的视觉空间查询。\n*   **工作原理：** 它们通过分析大规模的地理空间图像库（包括街景图像、基于地点的照片和航空图像，如卫星照片）并结合传统的GIS数据源来实现这一目标。\n\n**3. 研究内容与结构**\n*   **愿景定义：** 文章详细定义了地理视觉智能体的愿景。\n*   **方法描述：** 描述了实现这些智能体的感知和交互方法。\n*   **案例展示：** 提供了三个示例来具体说明其应用潜力。\n*   **未来展望：** 阐述了未来工作的关键挑战和机遇。\n\n**4. 学术背景与主题**\n*   **会议接受：** 该研究已被ICCV'25研讨会“视觉基础模型和生成式AI在可访问性方面的挑战与机遇”接受。\n*   **涉及学科：** 主要涉及人机交互（cs.HC）、人工智能（cs.AI）和计算机视觉与模式识别（cs.CV）等领域。",
      "shortSummary": "本文提出了“地理视觉智能体”的愿景，旨在解决传统数字地图因依赖结构化数据而无法回答视觉空间查询的问题。这些多模态AI智能体将通过分析大规模地理空间图像（如街景、航拍图）并结合GIS数据，来理解并响应关于世界外观的细致查询，从而增强地图的可访问性和信息深度。该工作已被ICCV'25研讨会接受。",
      "translated_title": "咖啡馆入口看起来无障碍吗？门在哪里？——迈向地理空间AI智能体以进行视觉查询",
      "images": [],
      "contentSource": "完整文章",
      "content": "Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work."
    },
    {
      "title": "何时何事：基于扩散的视频大语言模型结合实体感知分割用于长视频理解 (原标题: When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding)",
      "link": "https://arxiv.org/abs/2508.15641",
      "pubDate": "Thu, 21 Aug 2025 11:12:14 GMT",
      "isoDate": "2025-08-21T11:12:14.000Z",
      "creator": "Pengcheng Fang, Yuxia Chen, Rui Guo",
      "summary": "## Grounded VideoDiT：提升长视频理解中的时间与实体感知\n\n### 引言\n\n理解视频不仅需要回答开放式问题，更要求能够精确识别事件发生的时间（“何时”）以及实体在时间维度上的交互方式（“何事”）。尽管近期视频大语言模型（Video LLMs）在整体推理方面取得了显著进展，但在时间感知方面仍显粗糙，主要存在以下局限：\n\n*   时间戳仅被隐式编码。\n*   帧级特征在捕捉时间连续性方面表现不足。\n*   语言与视觉的对齐常常偏离所关注的实体。\n\n### Grounded VideoDiT 的创新点\n\n本文提出了 Grounded VideoDiT，一个旨在克服上述局限的视频大语言模型。它通过引入三项关键创新来增强视频理解能力：\n\n1.  **扩散时间潜在（Diffusion Temporal Latent, DTL）编码器：**\n    *   该编码器能够增强模型对时间边界的敏感性。\n    *   同时，它有助于保持时间上的一致性，确保事件的连贯性。\n\n2.  **对象接地表示（Object Grounded Representations）：**\n    *   这种表示方法将查询实体与局部视觉证据明确绑定。\n    *   从而显著加强了语言与视觉之间的对齐，确保模型能够准确识别和关注视频中的特定实体。\n\n3.  **混合令牌方案与离散时间令牌（Mixed Token Scheme with Discrete Temporal Tokens）：**\n    *   该方案提供了显式的时间戳建模能力。\n    *   使得模型能够进行细粒度的时间推理，精确判断事件发生的具体时刻。\n\n### 成果与验证\n\n这些设计共同赋予了 Grounded VideoDiT 强大的接地能力，使其能够更准确地理解视频中的“何时”和“何事”。该模型在以下基准测试中取得了最先进（state-of-the-art）的结果：\n\n*   Charades STA\n*   NExT GQA\n*   多个 VideoQA 基准测试",
      "shortSummary": "Grounded VideoDiT 是一种新型视频大语言模型，旨在解决现有Video LLM在长视频理解中时间感知和实体交互的局限。它通过引入扩散时间潜在编码器增强时间一致性，利用对象接地表示强化语言-视觉对齐，并采用离散时间令牌实现细粒度时间推理。该模型在Charades STA、NExT GQA和多个VideoQA基准测试中取得了最先进的性能，显著提升了视频的“何时何事”理解能力。",
      "translated_title": "何时何事：基于扩散的视频大语言模型结合实体感知分割用于长视频理解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks."
    },
    {
      "title": "LLaSO：大型语言和语音模型可复现研究的基础框架 (原标题: LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model)",
      "link": "https://arxiv.org/abs/2508.15418",
      "pubDate": "Thu, 21 Aug 2025 06:20:00 GMT",
      "isoDate": "2025-08-21T06:20:00.000Z",
      "creator": "Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen",
      "summary": "## LLaSO：大型语言和语音模型可复现研究的基础框架\n\n### 背景与问题\n大型语音语言模型（LSLMs）的发展一直受到架构碎片化和透明度不足的阻碍，这严重影响了研究的系统比较和复现性。与视觉-语言领域不同，LSLM领域普遍存在只发布模型权重而缺乏相应训练数据和配置的实践，进一步加剧了这些问题。\n\n### LLaSO框架介绍\n为了解决这些关键空白，研究人员推出了LLaSO，这是首个完全开放、端到端的大规模语音语言建模框架。LLaSO旨在为社区提供一套统一的资源，以促进可复现的研究和协作。\n\n### LLaSO提供的核心资源\nLLaSO为社区提供了三项重要的资源：\n\n1.  **LLaSO-Align：** 一个包含1200万实例的语音-文本对齐语料库，为语音和文本数据的精确关联提供了基础。\n2.  **LLaSO-Instruct：** 一个包含1350万实例的多任务指令微调数据集，支持模型在多种任务上进行指令遵循学习。\n3.  **LLaSO-Eval：** 一个可复现的基准，用于标准化评估LSLMs的性能，确保评估结果的公平性和可比性。\n\n### LLaSO-Base参考模型\n为了验证LLaSO框架的有效性，研究人员构建并发布了LLaSO-Base，这是一个拥有38亿参数的参考模型。该模型完全基于LLaSO提供的公共数据进行训练，并在标准化评估中取得了0.72的分数，建立了一个强大且可复现的基线，其性能超越了同类可比较的模型。\n\n### 研究分析与发现\n对LLaSO-Base的分析揭示了以下关键发现：\n\n*   更广泛的训练覆盖范围能够显著提升模型性能。\n*   然而，在未见过的任务上，特别是纯音频场景中，仍然存在显著的泛化差距，这表明模型在处理全新音频任务时仍面临挑战。\n\n### LLaSO的意义与影响\n通过发布完整的数据栈、基准和模型，LLaSO建立了一个开放的基础标准，旨在统一LSLM领域的研究工作，并加速社区驱动的进展。相关代码、数据集、预训练模型和结果已在线发布，供研究人员使用和贡献。",
      "shortSummary": "LLaSO是一个开放、端到端的大规模语音语言模型（LSLM）研究框架，旨在解决该领域碎片化和复现性差的问题。它提供了1200万实例的语音-文本对齐语料库、1350万实例的多任务指令微调数据集以及一个可复现的评估基准。LLaSO-Base是一个38亿参数的参考模型，基于这些公共数据训练，建立了强大的可复现基线。该框架通过提供完整的数据、基准和模型栈，旨在统一研究并加速LSLM领域的进展。",
      "translated_title": "LLaSO：大型语言和语音模型可复现研究的基础框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO."
    },
    {
      "title": "大型语言模型基准测试综述 (原标题: A Survey on Large Language Model Benchmarks)",
      "link": "https://arxiv.org/abs/2508.15361",
      "pubDate": "Thu, 21 Aug 2025 04:43:35 GMT",
      "isoDate": "2025-08-21T04:43:35.000Z",
      "creator": "Shiwen Ni, Guhong Chen, Shuaimin Li, Xuanang Chen, Siyi Li, Bingli Wang, Qiyao Wang, Xingjian Wang, Yifan Zhang, Liyang Fan, Chengming Li, Ruifeng Xu, Le Sun, Min Yang",
      "summary": "## 大型语言模型基准测试综述\n\n### 引言\n\n近年来，随着大型语言模型（LLM）能力深度和广度的快速发展，相应的评估基准测试（benchmarks）也日益增多。基准测试作为量化评估模型性能的工具，不仅是衡量模型能力的核心手段，也是指导模型发展方向和促进技术创新的关键要素。\n\n### 研究范围与分类\n\n本文首次系统地回顾了大型语言模型基准测试的现状与发展，并将283个具有代表性的基准测试归纳为三大类别：\n\n1.  **通用能力基准测试：** 涵盖核心语言学、知识和推理等方面，旨在评估模型的基础和广泛能力。\n2.  **领域特定基准测试：** 专注于特定领域，如自然科学、人文社会科学和工程技术等，以评估模型在专业领域的表现。\n3.  **目标特定基准测试：** 关注特定目标或特性，例如风险评估、可靠性验证以及智能体（agents）行为等。\n\n### 当前基准测试存在的问题\n\n文章指出，当前的基准测试存在以下几个主要问题：\n\n*   **数据污染导致分数虚高：** 由于训练数据可能包含测试数据，导致模型在基准测试上的得分被夸大，未能真实反映其泛化能力。\n*   **文化和语言偏见导致评估不公：** 现有基准测试可能存在固有的文化或语言偏见，使得对不同文化背景或语言模型的评估不够公平。\n*   **缺乏对过程可信度和动态环境的评估：** 现有基准测试未能充分评估模型在复杂、动态环境中的表现，也未能有效评估其决策过程的可信度和透明度。\n\n### 未来展望\n\n针对上述问题，文章为未来的基准测试创新提供了一个可供参考的设计范式，旨在推动开发更公平、更全面、更能反映模型真实能力的评估工具。",
      "shortSummary": "本文首次系统综述了大型语言模型（LLM）基准测试的现状与发展，将283个代表性基准测试分为通用能力、领域特定和目标特定三大类。文章指出当前基准测试存在数据污染导致分数虚高、文化语言偏见导致评估不公以及缺乏对过程可信度和动态环境评估等问题。最后，作者为未来的基准测试创新提出了一个可参考的设计范式。",
      "translated_title": "大型语言模型基准测试综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation."
    },
    {
      "title": "深度思考与置信度 (原标题: Deep Think with Confidence)",
      "link": "https://arxiv.org/abs/2508.15260",
      "pubDate": "Thu, 21 Aug 2025 01:48:38 GMT",
      "isoDate": "2025-08-21T01:48:38.000Z",
      "creator": "Yichao Fu, Xuewei Wang, Yuandong Tian, Jiawei Zhao",
      "summary": "# 深度思考与置信度 (DeepConf)\n\n本文介绍了一种名为“深度思考与置信度”（DeepConf）的新方法，旨在解决大型语言模型（LLMs）在推理任务中通过自洽性（self-consistency）与多数投票等测试时扩展方法所面临的效率和性能挑战。\n\n## 现有挑战\n*   **准确性回报递减**：LLMs通过自洽性进行推理时，准确性提升往往面临边际效益递减的问题。\n*   **高计算开销**：这些方法通常需要生成大量的推理轨迹，导致显著的计算成本。\n\n## DeepConf 方法介绍\nDeepConf 是一种简单而有效的方法，旨在提高测试时的推理效率和性能。\n*   **核心机制**：它利用模型内部的置信度信号，在生成过程中或生成后动态地过滤掉低质量的推理轨迹。\n*   **优势**：\n    *   **无需额外训练**：DeepConf 不需要额外的模型训练。\n    *   **无需超参数调优**：它也不需要进行超参数调优。\n    *   **无缝集成**：可以轻松地集成到现有的服务框架中。\n\n## 评估与结果\n研究人员在多种推理任务和最新的开源模型（包括 Qwen 3 和 GPT-OSS 系列）上对 DeepConf 进行了评估。\n*   **显著成果**：在具有挑战性的 AIME 2025 基准测试中，DeepConf@512 相较于完全并行思考，实现了高达 99.9% 的准确率，并将生成的 token 数量减少了 84.7%。\n\n## 结论\nDeepConf 提供了一种有效且高效的解决方案，通过利用模型内部置信度信号来优化LLMs的推理过程，显著提升了性能并降低了计算成本。",
      "shortSummary": "Deep Think with Confidence (DeepConf) 是一种新方法，旨在提高大型语言模型（LLMs）的推理效率和性能。它通过利用模型内部置信度信号，动态过滤低质量推理轨迹。DeepConf 无需额外训练或调优，可无缝集成。在 AIME 2025 等基准测试中，DeepConf@512 实现了高达 99.9% 的准确率，并减少了 84.7% 的生成 token，显著优于传统自洽性方法。",
      "translated_title": "深度思考与置信度",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking."
    },
    {
      "title": "Fin-PRM：一种用于大型语言模型金融推理的领域专用过程奖励模型 (原标题: Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models)",
      "link": "https://arxiv.org/abs/2508.15202",
      "pubDate": "Wed, 20 Aug 2025 23:31:11 GMT",
      "isoDate": "2025-08-20T23:31:11.000Z",
      "creator": "Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang",
      "summary": "Fin-PRM：一种用于大型语言模型金融推理的领域专用过程奖励模型\n\n**1. 背景与问题**\n\n*   **现有PRM的局限性：** 过程奖励模型（PRMs）是监督大型语言模型（LLMs）中间推理的有效框架，但现有PRMs主要在通用或科学、技术、工程和数学（STEM）领域进行训练。\n*   **金融领域的特殊性：** 在金融等领域，推理更为结构化、符号化，且对事实和监管的正确性高度敏感，现有PRM在此类领域表现不足。\n\n**2. Fin-PRM的引入与特点**\n\n*   **模型名称：** 引入 **Fin-PRM**，一个领域专用、轨迹感知的过程奖励模型。\n*   **核心目标：** 专门用于评估金融任务中的中间推理步骤。\n*   **关键创新：** Fin-PRM集成了步级（step-level）和轨迹级（trajectory-level）的奖励监督，从而能够对与金融逻辑对齐的推理轨迹进行细粒度评估。\n\n**3. Fin-PRM的应用场景**\n\nFin-PRM 可在离线和在线奖励学习设置中应用，支持以下三个关键应用：\n\n*   **高质量推理轨迹选择：** 用于基于蒸馏的监督微调（Supervised Fine-Tuning, SFT），以选择高质量的推理轨迹。\n*   **密集过程级奖励提供：** 为强化学习（Reinforcement Learning, RL）提供密集的、过程级别的奖励信号。\n*   **奖励引导的最佳-N推理：** 在测试时（test time）指导奖励信息引导的最佳-N（Best-of-N）推理策略。\n\n**4. 实验结果与性能提升**\n\n*   **基准测试：** 在CFLUE和FinQA等金融推理基准上进行了实验。\n*   **轨迹选择质量：** 实验结果表明，Fin-PRM在轨迹选择质量方面始终优于通用PRMs和强大的领域基线模型。\n*   **下游模型性能提升：** 使用Fin-PRM训练的下游模型在基线上取得了显著改进：\n    *   **监督学习：** 性能提升12.9%。\n    *   **强化学习：** 性能提升5.2%。\n    *   **测试时性能：** 性能提升5.1%。\n\n**5. 结论**\n\n这些发现强调了领域专用奖励建模对于使大型语言模型与专家级金融推理对齐的巨大价值。",
      "shortSummary": "Fin-PRM是一种领域专用过程奖励模型，旨在解决现有PRM在金融推理中的不足。它通过集成步级和轨迹级奖励监督，评估大型语言模型在金融任务中的中间推理步骤。实验表明，Fin-PRM在金融基准测试中显著优于通用PRM和领域基线，并在监督学习、强化学习和测试时性能方面分别带来了12.9%、5.2%和5.1%的提升，证明了其在对齐LLM与专家级金融推理方面的价值。",
      "translated_title": "Fin-PRM：一种用于大型语言模型金融推理的领域专用过程奖励模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin."
    },
    {
      "title": "Mobile-Agent-v3：GUI自动化基础智能体 (原标题: Mobile-Agent-v3: Foundamental Agents for GUI Automation)",
      "link": "https://arxiv.org/abs/2508.15144",
      "pubDate": "Wed, 20 Aug 2025 20:39:12 GMT",
      "isoDate": "2025-08-20T20:39:12.000Z",
      "creator": "Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan",
      "summary": "# Mobile-Agent-v3：GUI自动化基础智能体\n\n本文介绍了GUI-Owl模型和Mobile-Agent-v3框架，它们在图形用户界面（GUI）自动化领域取得了显著进展，并为开源GUI智能体设定了新的技术标准。\n\n## GUI-Owl：基础GUI智能体模型\n\nGUI-Owl是一个基础的GUI智能体模型，在桌面和移动环境的十个GUI基准测试中，其端到端开源模型性能达到了最先进水平。这些基准测试涵盖了定位、问答、规划、决策制定和程序知识等多个方面。\n\n*   **性能表现**：\n    *   GUI-Owl-7B在AndroidWorld上取得了66.4分。\n    *   在OSWorld上取得了29.4分。\n\n*   **三大核心创新**：\n    1.  **大规模环境基础设施**：\n        *   构建了一个基于云的虚拟环境，支持Android、Ubuntu、macOS和Windows等操作系统。\n        *   该基础设施支持“自演进GUI轨迹生成”框架，通过自动化查询生成和正确性验证来生成高质量的交互数据。\n        *   利用GUI-Owl迭代优化轨迹，形成一个自我改进的循环。\n        *   支持多样化的数据管道，并减少了手动标注的需求。\n    2.  **多样化的基础智能体能力**：\n        *   整合了UI定位、规划、动作语义和推理模式。\n        *   支持端到端的决策制定。\n        *   可以作为多智能体系统中的模块化组件。\n    3.  **可扩展的环境强化学习（RL）**：\n        *   开发了一个可扩展的强化学习框架，采用完全异步训练以实现与真实世界的对齐。\n        *   引入了“轨迹感知相对策略优化（TRPO）”用于在线强化学习，在OSWorld上实现了34.9分。\n\n## Mobile-Agent-v3：通用GUI智能体框架\n\nMobile-Agent-v3在GUI-Owl的基础上进一步发展，是一个通用的GUI智能体框架，其性能得到了显著提升。\n\n*   **性能提升**：\n    *   在AndroidWorld上的性能提升至73.3分。\n    *   在OSWorld上的性能提升至37.7分。\n*   **行业地位**：Mobile-Agent-v3为开源GUI智能体框架树立了新的最先进标准。\n\n## 开源信息\n\nGUI-Owl和Mobile-Agent-v3均已开源。\n\n## 作者与主题\n\n*   **作者**：Jiabo Ye、Xi Zhang、Haiyang Xu等。\n*   **主题**：人工智能（cs.AI）。",
      "shortSummary": "本文介绍了GUI-Owl和Mobile-Agent-v3，这是GUI自动化领域的两个基础智能体模型和框架。GUI-Owl在多项GUI基准测试中表现出色，其核心创新包括大规模环境基础设施、多样化智能体能力和可扩展强化学习。Mobile-Agent-v3在此基础上进一步提升性能，在AndroidWorld和OSWorld上均创下开源GUI智能体框架的新高，为GUI自动化提供了强大的通用解决方案。两者均已开源。",
      "translated_title": "Mobile-Agent-v3：GUI自动化基础智能体",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent."
    },
    {
      "title": "aiXiv：由AI科学家生成的下一代开放获取科学发现生态系统 (原标题: aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists)",
      "link": "https://arxiv.org/abs/2508.15126",
      "pubDate": "Wed, 20 Aug 2025 19:16:41 GMT",
      "isoDate": "2025-08-20T19:16:41.000Z",
      "creator": "Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu",
      "summary": "## aiXiv：由AI科学家生成的下一代开放获取科学发现生态系统\n\n### 引言：AI生成研究内容的挑战\n\n随着大型语言模型（LLMs）的最新进展，AI代理已具备自主生成科学提案、进行实验、撰写论文以及执行同行评审的能力。然而，这种由AI生成的研究内容洪流与当前碎片化且大多封闭的出版生态系统之间存在显著冲突。\n\n*   **传统出版模式的局限性：**\n    *   传统期刊和会议依赖人工同行评审，这使得它们难以扩展以应对日益增长的AI生成内容。\n    *   它们通常对接受AI生成的研究内容持谨慎态度。\n*   **现有预印本服务器的不足：**\n    *   如arXiv等现有预印本服务器缺乏严格的质量控制机制，难以确保AI生成内容的可靠性。\n*   **结果：** 大量高质量的AI生成研究缺乏合适的传播渠道，这严重阻碍了其推动科学进步的潜力。\n\n### aiXiv平台介绍\n\n为了应对这些挑战，研究人员引入了 **aiXiv**，一个面向人类和AI科学家的下一代开放获取平台。\n\n*   **核心架构：** aiXiv采用多智能体架构，允许研究提案和论文由人类和AI科学家共同提交、评审并进行迭代完善。\n*   **无缝集成：** 平台提供API和MCP（Multi-Agent Communication Protocol）接口，以实现异构人类和AI科学家的无缝集成。\n*   **愿景：** 旨在创建一个可扩展、可扩展的自主科学发现生态系统。\n\n### 实验验证与成果\n\n通过广泛的实验，研究人员证明了aiXiv是一个可靠且稳健的平台。\n\n*   **质量提升：** 经过在aiXiv上进行迭代修订和评审后，AI生成的研究提案和论文的质量得到了显著提升。\n\n### 意义与展望\n\n这项工作为AI科学家构建下一代开放获取生态系统奠定了基础，将显著加速高质量AI生成研究内容的出版和传播。",
      "shortSummary": "AI科学家生成的研究内容因现有出版系统（传统期刊扩展性差，预印本缺乏质量控制）而面临传播挑战。aiXiv是一个新的开放获取平台，旨在解决此问题。它采用多智能体架构，支持人类和AI科学家共同提交、评审和完善研究提案及论文。通过API和MCP接口，aiXiv构建了一个可扩展的自主科学发现生态系统。实验证明，aiXiv能显著提升AI生成研究的质量，加速其传播。",
      "translated_title": "aiXiv：由AI科学家生成的下一代开放获取科学发现生态系统",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8."
    },
    {
      "title": "Snap-Snap：利用两张图像在毫秒级重建3D人体高斯模型 (原标题: Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds)",
      "link": "https://arxiv.org/abs/2508.14892",
      "pubDate": "Wed, 20 Aug 2025 13:59:11 GMT",
      "isoDate": "2025-08-20T13:59:11.000Z",
      "creator": "Jia Lu, Taoran Yi, Jiemin Fang, Chen Yang, Chuiyun Wu, Wei Shen, Wenyu Liu, Qi Tian, Xinggang Wang",
      "summary": "## Snap-Snap：利用两张图像在毫秒级重建3D人体高斯模型\n\n### 研究背景与挑战\n*   **目标：** 从稀疏视角重建3D人体，以拓宽相关应用。\n*   **核心任务：** 本文提出了一项具有挑战性但极具价值的任务——仅利用两张图像（正面和背面）重建人体，旨在大幅降低用户创建3D数字人体的门槛。\n*   **主要挑战：**\n    *   从高度稀疏的输入中建立3D一致性。\n    *   恢复缺失信息。\n\n### 方法论\n*   **几何重建模型：**\n    *   重新设计了一个基于基础重建模型的几何重建模型。\n    *   即使输入图像重叠稀少，也能预测一致的点云。\n    *   通过大量人体数据进行训练。\n*   **颜色信息增强：**\n    *   应用增强算法补充缺失的颜色信息。\n    *   获得带有颜色的完整人体点云。\n*   **3D高斯模型转换：**\n    *   将完整的带颜色人体点云直接转换为3D高斯模型。\n    *   旨在实现更好的渲染质量。\n\n### 实验结果与性能\n*   **重建速度：** 在单个NVIDIA RTX 4090显卡上，使用两张1024x1024分辨率的图像，可在190毫秒内完成整个人体的重建。\n*   **性能表现：** 在THuman2.0和跨域数据集上展现了最先进的性能。\n*   **设备兼容性：** 即使使用低成本移动设备捕获的图像也能完成人体重建，降低了数据采集要求。\n\n### 可用性\n*   演示和代码可在项目页面获取。",
      "shortSummary": "本文提出“Snap-Snap”方法，旨在仅利用两张（正面和背面）图像在毫秒级重建3D人体高斯模型。该方法通过重新设计的几何重建模型和颜色增强算法，解决了稀疏输入下的3D一致性和信息恢复挑战。实验表明，它能在190毫秒内完成高质量人体重建，并在THuman2.0等数据集上达到最先进水平，同时支持低成本移动设备图像，显著降低了3D数字人创建门槛。",
      "translated_title": "Snap-Snap：利用两张图像在毫秒级重建3D人体高斯模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/."
    },
    {
      "title": "Puppeteer: 绑定和动画化你的3D模型 (原标题: Puppeteer: Rig and Animate Your 3D Models)",
      "link": "https://arxiv.org/abs/2508.10898",
      "pubDate": "Thu, 14 Aug 2025 13:59:31 GMT",
      "isoDate": "2025-08-14T13:59:31.000Z",
      "creator": "Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang",
      "summary": "# Puppeteer：3D模型绑定与动画框架\n\n## 概述\n现代交互式应用对动态3D内容的需求日益增长，然而，将静态3D模型转换为可动画资产是内容创作流程中的一个显著瓶颈。尽管生成式AI在静态3D模型创建方面取得了革命性进展，但绑定（rigging）和动画制作仍严重依赖于专业人员的干预。本文提出了Puppeteer，一个全面的框架，旨在解决各种3D对象的自动绑定和动画问题。\n\n## Puppeteer框架介绍\nPuppeteer系统通过以下三个主要阶段实现其功能：\n\n### 1. 骨骼结构预测\nPuppeteer首先通过一个自回归Transformer来预测合理的骨骼结构。该Transformer引入了：\n*   **基于关节的标记化策略**：用于紧凑表示骨骼信息。\n*   **具有随机扰动的分层排序方法**：增强双向学习能力。\n\n### 2. 蒙皮权重推断\n在骨骼结构预测之后，系统通过一个基于注意力的架构推断蒙皮权重。该架构整合了：\n*   **拓扑感知关节注意力**：根据骨骼图距离明确编码关节间的关系。\n\n### 3. 动画生成管线\n最后，Puppeteer通过一个可微分的基于优化的动画管线来补充上述绑定进展。该管线能够：\n*   生成稳定、高保真的动画。\n*   在计算效率上优于现有方法。\n\n## 性能与优势\n在多个基准测试中进行的广泛评估表明，Puppeteer方法在骨骼预测精度和蒙皮质量方面均显著优于最先进的技术。该系统能够稳健地处理多样化的3D内容，包括专业设计的游戏资产和AI生成的形状，并生成时间上连贯的动画，有效消除了现有方法中常见的抖动问题。",
      "shortSummary": "Puppeteer是一个创新的框架，旨在自动化3D模型的绑定和动画过程。它通过自回归Transformer预测骨骼结构，利用注意力机制推断蒙皮权重，并采用可微分优化管线生成稳定、高保真的动画。该系统在骨骼预测精度和蒙皮质量上超越现有技术，能处理多样化3D内容，并消除动画抖动，显著提升了3D内容创作效率。",
      "translated_title": "Puppeteer: 绑定和动画化你的3D模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods."
    },
    {
      "title": "STream3R：基于因果Transformer的可扩展序列3D重建 (原标题: STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer)",
      "link": "https://arxiv.org/abs/2508.10893",
      "pubDate": "Thu, 14 Aug 2025 13:58:05 GMT",
      "isoDate": "2025-08-14T13:58:05.000Z",
      "creator": "Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan",
      "summary": "STream3R是一种新颖的3D重建方法，它将点图预测重新定义为一个仅解码器Transformer问题。该方法旨在解决现有最先进的多视角重建方法的局限性，这些方法要么依赖于昂贵的全局优化，要么依赖于简单的内存机制，导致在序列长度增加时扩展性差。\n\n**STream3R的核心创新与优势：**\n\n*   **流式处理框架：** STream3R引入了一个流式处理框架，借鉴了现代语言模型领域的进展，利用因果注意力机制高效处理图像序列。\n*   **Transformer架构：** 将点图预测重构为仅解码器Transformer问题，利用其强大的序列建模能力。\n*   **几何先验学习：** 通过从大规模3D数据集中学习几何先验，STream3R能够很好地泛化到各种复杂场景，包括传统方法难以处理的动态场景。\n*   **卓越的性能：** 广泛的实验表明，STream3R在静态和动态场景基准测试中均持续优于现有工作。\n*   **兼容LLM训练基础设施：** STream3R本质上与大型语言模型（LLM）风格的训练基础设施兼容，这使得其能够高效地进行大规模预训练和针对各种下游3D任务的微调。\n\n**影响与展望：**\n\nSTream3R的成果强调了因果Transformer模型在在线3D感知方面的巨大潜力，为流媒体环境中的实时3D理解铺平了道路。",
      "shortSummary": "STream3R是一种创新的3D重建方法，它将点图预测重构为基于因果Transformer的流式问题。该方法借鉴语言模型技术，通过高效的因果注意力机制处理图像序列。STream3R能够从大规模3D数据中学习几何先验，有效处理动态场景，并在性能上超越现有方法。它兼容LLM训练，为实时3D感知和理解提供了可扩展的解决方案。",
      "translated_title": "STream3R：基于因果Transformer的可扩展序列3D重建",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r."
    },
    {
      "title": "ToonComposer：通过生成式关键帧后处理简化卡通制作 (原标题: ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing)",
      "link": "https://arxiv.org/abs/2508.10881",
      "pubDate": "Thu, 14 Aug 2025 13:50:11 GMT",
      "isoDate": "2025-08-14T13:50:11.000Z",
      "creator": "Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan",
      "summary": "## ToonComposer：通过生成式关键帧后处理简化卡通制作\n\n### 挑战：传统卡通制作的痛点\n\n传统的卡通和动漫制作流程涉及关键帧绘制、中间帧生成和上色等多个阶段，这些阶段都需要大量的人工投入。尽管人工智能在近年来取得了显著进展，但现有方法通常独立处理这些阶段，导致错误累积和伪影产生。例如，中间帧生成方法难以处理大幅度运动，而上色方法则需要密集的逐帧草图。\n\n### 解决方案：ToonComposer 模型\n\n为了解决上述问题，我们引入了 ToonComposer，一个生成模型，它将中间帧生成和上色统一到一个单一的“关键帧后处理”阶段。ToonComposer 的核心创新点包括：\n\n*   **稀疏草图注入机制**：该机制允许通过关键帧草图提供精确控制。ToonComposer 能够以稀疏输入高效工作，最少只需一张草图和一个彩色参考帧。同时，它也支持在任何时间位置使用多张草图，以实现更精确的运动控制。\n*   **卡通领域适应方法**：模型采用空间低秩适配器（spatial low-rank adapter）来调整现代视频基础模型，使其适应卡通领域，同时保持其固有的时间先验。\n\n### 优势与应用\n\nToonComposer 的双重能力显著减少了手动工作量，并提高了制作的灵活性，从而赋能艺术家在实际场景中更高效地进行创作。它为AI辅助的卡通制作提供了一个卓越且更灵活的解决方案。\n\n### 评估与性能\n\n为了评估 ToonComposer 的性能，我们创建了 PKBench，这是一个包含人工绘制草图的基准测试，旨在模拟真实世界的使用场景。我们的评估结果表明，ToonComposer 在视觉质量、运动一致性和生产效率方面均优于现有方法。",
      "shortSummary": "ToonComposer 是一种生成模型，旨在简化卡通制作流程。它将传统的中间帧生成和上色阶段统一到单一的“关键帧后处理”阶段，从而减少人工投入。该模型通过稀疏草图注入提供精确控制，并利用空间低秩适配器将视频基础模型适应卡通领域。ToonComposer 显著提高了制作效率和灵活性，并在视觉质量和运动一致性方面超越了现有方法，为AI辅助卡通制作提供了更优解决方案。",
      "translated_title": "ToonComposer：通过生成式关键帧后处理简化卡通制作",
      "images": [],
      "contentSource": "完整文章",
      "content": "Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production."
    },
    {
      "title": "扩散语言模型综述 (原标题: A Survey on Diffusion Language Models)",
      "link": "https://arxiv.org/abs/2508.10875",
      "pubDate": "Thu, 14 Aug 2025 13:47:22 GMT",
      "isoDate": "2025-08-14T13:47:22.000Z",
      "creator": "Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen",
      "summary": "# 扩散语言模型综述\n\n## 引言\n\n扩散语言模型（Diffusion Language Models, DLMs）正迅速崛起，成为主导的自回归（Autoregressive, AR）范式的强大且有前景的替代方案。DLMs通过迭代去噪过程并行生成令牌，具有以下固有优势：\n\n*   **降低推理延迟**：显著提升生成速度。\n*   **捕获双向上下文**：能够理解和利用文本的双向信息。\n*   **实现精细控制**：对生成过程提供更细致的控制能力。\n\n近期进展表明，DLMs在实现数倍加速的同时，性能已可与自回归模型相媲美，使其成为各种自然语言处理任务的有力选择。\n\n## 综述范围与贡献\n\n本综述对当前DLM领域的整体概况进行了全面概述，具体贡献包括：\n\n*   **演变与关系追溯**：\n    *   追溯DLM的演变历程。\n    *   探讨其与自回归模型和掩码语言模型等其他范式的关系。\n*   **核心原理与模型**：\n    *   涵盖DLM的基础原理。\n    *   介绍最先进的DLM模型。\n*   **分类与技术分析**：\n    *   提供最新、全面的DLM分类法。\n    *   深入分析现有技术，包括从预训练策略到高级后训练方法。\n*   **推理策略与优化**：\n    *   详细回顾DLM的推理策略和优化方法。\n    *   具体包括解码并行性、缓存机制和生成质量的改进。\n*   **多模态扩展与应用**：\n    *   重点介绍DLM的最新多模态扩展。\n    *   阐述其在各种实际应用场景中的具体应用。\n*   **局限性、挑战与未来方向**：\n    *   讨论DLM面临的局限性和挑战，例如效率问题、长序列处理能力以及基础设施要求。\n    *   展望并勾勒出未来研究方向，以促进该快速发展领域的持续进步。\n\n## 其他信息\n\n*   **主题**：计算与语言（cs.CL）、人工智能（cs.AI）、机器学习（cs.LG）。\n*   **引用信息**：arXiv:2508.10875。\n*   **DOI**：10.48550/arXiv.2508.10875。",
      "shortSummary": "本综述全面介绍了扩散语言模型（DLMs），它们作为自回归模型的有力替代品，通过并行生成令牌，在降低推理延迟、捕获双向上下文和实现精细控制方面具有固有优势。文章涵盖了DLM的演变、核心原理、最先进模型、推理策略、多模态扩展及其应用。此外，综述还讨论了DLM的局限性、挑战，并指出了未来的研究方向。",
      "translated_title": "扩散语言模型综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
    },
    {
      "title": "从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估 (原标题: From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms)",
      "link": "https://arxiv.org/abs/2508.10860",
      "pubDate": "Thu, 14 Aug 2025 13:31:18 GMT",
      "isoDate": "2025-08-14T13:31:18.000Z",
      "creator": "Zhaokun Jiang, Ziyin Zhang",
      "summary": "## 从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估\n\n### 研究背景与问题\n\n*   **现有不足**：尽管机器学习在自动化口译质量评估方面取得了进展，但现有研究存在以下局限：\n    *   对语言使用质量的检查不足。\n    *   由于数据稀缺和不平衡，模型建模效果不尽如人意。\n    *   缺乏对模型预测的解释性。\n\n### 提出的解决方案\n\n*   **多维建模框架**：为解决上述问题，本研究提出一个多维建模框架，该框架整合了：\n    *   特征工程（Feature Engineering）。\n    *   数据增强（Data Augmentation）。\n    *   可解释机器学习（Explainable Machine Learning）。\n\n### 核心方法与原则\n\n*   **优先可解释性**：该方法优先考虑可解释性而非“黑箱”预测。\n*   **透明特征**：仅使用与构建相关的、透明的特征。\n*   **SHAP分析**：通过Shapley值（SHAP）分析来解释模型预测。\n\n### 研究成果与发现\n\n*   **预测性能**：在新的英汉同声传译数据集上展示了强大的预测性能。\n*   **关键预测特征识别**：\n    *   **忠实度（Fidelity）**：BLEURT和CometKiwi分数被确定为最强的预测特征。\n    *   **流利度（Fluency）**：停顿相关特征是关键预测因素。\n    *   **语言使用（Language Use）**：中文特有的短语多样性指标对语言使用质量具有重要预测作用。\n\n### 研究意义与贡献\n\n*   **替代方案**：通过特别强调可解释性，本研究提出了一种可扩展、可靠且透明的替代传统人工评估的方法。\n*   **诊断反馈**：有助于为学习者提供详细的诊断反馈。\n*   **自主学习**：支持自动化分数单独无法提供的自主学习优势。",
      "shortSummary": "针对现有自动化口译评估在语言质量、数据和可解释性方面的不足，本文提出一个多维建模框架。该框架整合特征工程、数据增强和可解释机器学习，通过Shapley值分析优先实现透明预测。研究在英汉口译数据集上表现出色，识别出忠实度、流利度和语言使用的关键预测特征。这提供了一种可扩展、可靠且透明的替代人工评估的方法，有助于提供诊断反馈并促进自主学习。",
      "translated_title": "从黑箱到透明：利用可解释人工智能增强大学课堂中的自动化口译评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation."
    }
  ],
  "lastUpdated": "2025-08-24T09:27:10.138Z"
}