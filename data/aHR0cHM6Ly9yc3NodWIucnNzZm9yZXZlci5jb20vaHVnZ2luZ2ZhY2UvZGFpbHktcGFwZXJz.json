{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准 (原标题: FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark)",
      "link": "https://arxiv.org/abs/2509.09680",
      "pubDate": "Thu, 11 Sep 2025 13:59:59 GMT",
      "isoDate": "2025-09-11T13:59:59.000Z",
      "creator": "Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, Hongsheng Li",
      "summary": "## FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准\n\n### 引言\n\n当前，开源文本到图像（T2I）模型的发展面临挑战，主要原因在于缺乏大规模、专注于推理能力的数据集以及全面的评估基准。这导致开源模型与领先的闭源系统之间存在显著的性能差距。\n\n### FLUX-Reason-6M 数据集\n\n为解决上述问题，研究团队引入了FLUX-Reason-6M数据集，旨在教授模型复杂的推理能力。\n\n*   **规模与内容：**\n    *   包含600万张由FLUX模型生成的高质量图像。\n    *   附带2000万条双语（英语和中文）描述，这些描述经过精心设计以支持复杂推理任务。\n*   **组织与结构：**\n    *   图像根据六个关键特征进行组织：想象力（Imagination）、实体（Entity）、文本渲染（Text rendering）、风格（Style）、情感（Affection）和构图（Composition）。\n    *   设计了明确的“生成思维链”（Generation Chain-of-Thought, GCoT），为图像生成步骤提供详细的分解，从而帮助模型理解和执行复杂指令。\n*   **资源投入：** 整个数据整理过程耗费了15,000个A100 GPU天，为社区提供了一个此前仅在大型工业实验室才能获得的宝贵资源。\n\n### PRISM-Bench 评估基准\n\n同时推出的PRISM-Bench（Precise and Robust Image Synthesis Measurement Benchmark）提供了一个新颖且全面的评估标准。\n\n*   **评估轨道：** 包含七个不同的评估轨道，其中包括一个极具挑战性的“长文本挑战”，该挑战利用GCoT来评估模型处理复杂长文本提示的能力。\n*   **评估方法：**\n    *   通过精心设计的提示词，利用先进的视觉-语言模型（VLM）进行评估。\n    *   实现对提示词-图像对齐（prompt-image alignment）和图像美学（image aesthetics）进行细致的、与人类评估高度一致的评估。\n\n### 实验与发现\n\n研究团队对19个领先的T2I模型在PRISM-Bench上进行了广泛评估。结果揭示了这些模型在推理能力上的关键性能差距，并明确指出了需要改进的具体领域。\n\n### 贡献与开放资源\n\nFLUX-Reason-6M数据集、PRISM-Bench基准以及相关的评估代码均已公开发布。此举旨在催化下一波面向推理的T2I生成研究，推动开源社区在这一领域取得突破。项目页面：this https URL",
      "shortSummary": "为解决开源文本到图像（T2I）模型在推理能力和评估方面的不足，本文提出了FLUX-Reason-6M数据集和PRISM-Bench基准。FLUX-Reason-6M包含600万张图像和2000万条双语描述，通过“生成思维链”（GCoT）教授复杂推理。PRISM-Bench提供七个评估轨道，包括长文本挑战，利用先进视觉-语言模型进行细致评估。对19个模型的评估揭示了性能差距。该工作发布了数据集、基准和代码，旨在推动面向推理的T2I生成发展。",
      "translated_title": "FLUX-Reason-6M与PRISM-Bench：百万级文本到图像推理数据集与综合基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ ."
    },
    {
      "title": "SpatialVID：一个带有空间标注的大规模视频数据集 (原标题: SpatialVID: A Large-Scale Video Dataset with Spatial Annotations)",
      "link": "https://arxiv.org/abs/2509.09676",
      "pubDate": "Thu, 11 Sep 2025 13:59:31 GMT",
      "isoDate": "2025-09-11T13:59:31.000Z",
      "creator": "Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, Yao Yao",
      "summary": "# SpatialVID：一个带有空间标注的大规模视频数据集\n\n## 摘要\n本文介绍了 **SpatialVID**，一个大规模的视频数据集，旨在解决当前空间智能模型（包括空间重建和世界探索）在可扩展性和真实世界保真度方面受限于高质量训练数据稀缺的问题。现有数据集在规模、多样性和标注丰富性方面存在局限，尤其缺乏真实世界动态场景中的地面真值相机运动信息。\n\n## SpatialVID 数据集概述\n*   **目标**：提供一个大规模、高质量的视频数据集，以促进空间智能领域的发展。\n*   **构成**：包含大量“野外”视频，场景和相机运动多样。\n*   **核心标注**：提供密集的3D标注，如每帧相机姿态、深度信息和运动指令。\n\n## 数据收集与处理\n1.  **原始数据收集**：收集了超过21,000小时的原始视频。\n2.  **分层过滤**：通过一个分层过滤管道，将原始视频处理成270万个视频片段。\n3.  **动态内容总量**：最终数据集包含7,089小时的动态内容。\n\n## 详细标注流程\n在视频片段处理完成后，通过后续的标注管道，为这些片段丰富了详细的空间和语义信息，具体包括：\n*   **相机姿态（Camera Poses）**：每帧的相机位置和方向。\n*   **深度图（Depth Maps）**：场景中物体的深度信息。\n*   **动态掩码（Dynamic Masks）**：识别和标注视频中的动态对象。\n*   **结构化字幕（Structured Captions）**：对视频内容进行描述。\n*   **序列化运动指令（Serialized Motion Instructions）**：描述相机或场景的运动序列。\n\n## 数据意义与贡献\n*   **丰富性和多样性**：SpatialVID的数据统计分析表明其具有丰富的多样性。\n*   **促进模型泛化**：这种丰富性直接有助于提高模型的泛化能力和性能。\n*   **重要资产**：SpatialVID被定位为视频和3D视觉研究社区的关键资产。",
      "shortSummary": "SpatialVID是一个大规模视频数据集，旨在解决空间智能模型训练数据不足的问题。它包含7,089小时的“野外”视频，具有多样化的场景和相机运动。数据集提供密集的3D标注，包括每帧相机姿态、深度图、动态掩码、结构化字幕和运动指令。SpatialVID的丰富性和多样性有望显著提升视频和3D视觉模型的泛化能力和性能，是该研究领域的重要资源。",
      "translated_title": "SpatialVID：一个带有空间标注的大规模视频数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw video, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly foster improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community."
    },
    {
      "title": "SimpleVLA-RL：通过强化学习扩展VLA训练 (原标题: SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.09674",
      "pubDate": "Thu, 11 Sep 2025 13:59:17 GMT",
      "isoDate": "2025-09-11T13:59:17.000Z",
      "creator": "Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding",
      "summary": "## SimpleVLA-RL：通过强化学习扩展VLA训练\n\n### VLA模型面临的挑战\n视觉-语言-动作 (VLA) 模型已成为机器人操作领域的强大范式。然而，它们面临两个基本挑战：\n*   **数据稀缺与高成本**：用于监督微调 (SFT) 的大规模人工操作机器人轨迹数据稀缺且成本高昂，限制了SFT的扩展。\n*   **泛化能力有限**：对涉及分布偏移的任务，VLA模型的泛化能力有限。\n\n### 强化学习的启发\n大型推理模型 (LRM) 的最新突破表明，强化学习 (RL) 可以显著增强逐步推理能力。这引发了一个关键问题：RL能否同样改进VLA模型的长周期逐步动作规划能力？\n\n### 引入SimpleVLA-RL框架\n本文提出了SimpleVLA-RL，一个专为VLA模型量身定制的高效强化学习框架。该框架在veRL的基础上，引入了以下VLA特定的改进措施，以提升训练效率和性能：\n*   **VLA特定的轨迹采样**：优化了数据收集和利用方式。\n*   **可扩展的并行化**：实现了高效的计算资源利用。\n*   **多环境渲染**：支持在多个模拟环境中进行训练，提高数据多样性。\n*   **优化的损失计算**：提升了模型学习的效率和稳定性。\n\n### 关键成果与性能\nSimpleVLA-RL在多个基准测试和真实世界任务中展现出卓越性能：\n*   **最先进性能**：应用于OpenVLA-OFT时，SimpleVLA-RL在LIBERO数据集上取得了最先进 (SoTA) 的性能。\n*   **超越基线**：通过引入探索增强策略，SimpleVLA-RL在RoboTwin 1.0和2.0上甚至超越了$\\pi_0$。\n*   **降低数据依赖与增强泛化**：SimpleVLA-RL不仅显著减少了对大规模数据的依赖，还实现了鲁棒的泛化能力。\n*   **超越SFT**：在真实世界任务中，SimpleVLA-RL的表现显著超越了传统的监督微调 (SFT) 方法。\n\n### 新现象“Pushcut”\n研究中还识别出一个新颖的现象，称为“pushcut”。在RL训练过程中，策略能够发现以前训练过程中未曾见过的模式，这表明RL具有超越现有数据分布的探索和发现能力。",
      "shortSummary": "SimpleVLA-RL是一个为视觉-语言-动作 (VLA) 模型设计的高效强化学习框架，旨在解决VLA模型在数据稀缺和泛化能力上的挑战。它通过引入VLA特定轨迹采样、可扩展并行化等改进，在LIBERO上实现了最先进性能，并在RoboTwin上超越了基线。SimpleVLA-RL显著减少了对大规模数据的依赖，增强了泛化能力，并在真实世界任务中表现优于监督微调。研究还发现策略在RL训练中能发现新模式。",
      "translated_title": "SimpleVLA-RL：通过强化学习扩展VLA训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL"
    },
    {
      "title": "理解与生成能否真正互利共赢——抑或仅仅是共存？ (原标题: Can Understanding and Generation Truly Benefit Together -- or Just Coexist?)",
      "link": "https://arxiv.org/abs/2509.09666",
      "pubDate": "Thu, 11 Sep 2025 13:57:59 GMT",
      "isoDate": "2025-09-11T13:57:59.000Z",
      "creator": "Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, Xinyan Xiao, Jingdong Wang, Haifeng Wang, Li Yuan",
      "summary": "## UAE：通过统一框架实现理解与生成的互利共赢\n\n本文提出了一种创新的范式，通过自动编码器（Auto-Encoder）的视角，将图像理解（Image-to-Text, I2T）视为编码器，负责将图像压缩成文本；将图像生成（Text-to-Image, T2I）视为解码器，负责从文本重建图像。以重建保真度作为统一的训练目标，该方法强制理解与生成过程之间实现连贯的双向信息流，从而带来相互增益。\n\n### 核心框架：UAE\n为实现这一目标，作者提出了UAE（Unified Auto-Encoder），一个用于统一多模态学习的新颖框架。\n\n*   **解码器预训练：** 首先，使用大规模、长上下文的图像描述对解码器进行预训练，以捕捉细粒度的语义和复杂的空间关系。\n\n*   **统一的GRPO强化学习：** 随后，通过强化学习（RL）引入了Unified-GRPO，该过程涵盖三个阶段：\n    1.  **冷启动阶段：** 使用语义重建损失温和地初始化编码器和解码器。\n    2.  **生成促进理解：** 在此阶段，训练编码器生成信息丰富的描述，以最大化解码器的重建质量，从而增强编码器的视觉理解能力。\n    3.  **理解促进生成：** 在此阶段，解码器被精炼以从这些描述中进行重建，这迫使解码器利用每一个细节，从而提高其对长上下文指令的遵循能力和生成保真度。\n\n### 评估与发现\n为了评估多模态统一模型（UMMs）的统一程度，作者引入了Unified-Bench，这是首个为此目的量身定制的基准。\n\n一个令人惊讶的“顿悟时刻”出现在多模态学习领域：随着强化学习的进展，编码器自主地生成了更具描述性的文本，而解码器同时展现出深刻理解这些复杂描述的能力，最终实现了惊人的高保真度重建。\n\n### 关键贡献\n*   提出了通过Auto-Encoder视角统一理解与生成的范式。\n*   引入了UAE框架和Unified-GRPO强化学习策略。\n*   创建了Unified-Bench基准来评估统一多模态模型。\n*   展示了理解与生成之间通过重建目标实现的互利共赢。",
      "shortSummary": "本文提出UAE框架，通过自动编码器视角统一图像理解（I2T编码器）和生成（T2I解码器）。以重建保真度为统一目标，并结合Unified-GRPO强化学习，实现理解与生成间的双向信息流。实验表明，随着训练进展，编码器能生成更具描述性的文本，解码器则能实现高保真度重建，证明了理解与生成可相互促进，实现互利共赢。",
      "translated_title": "理解与生成能否真正互利共赢——抑或仅仅是共存？",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we introduce an insightful paradigm through the Auto-Encoder lens-understanding as the encoder (I2T) that compresses images into text, and generation as the decoder (T2I) that reconstructs images from that text. Using reconstruction fidelity as the unified training objective, we enforce the coherent bidirectional information flow between the understanding and generation processes, bringing mutual gains. To implement this, we propose UAE, a novel framework for unified multimodal learning. We begin by pre-training the decoder with large-scale long-context image captions to capture fine-grained semantic and complex spatial relationships. We then propose Unified-GRPO via reinforcement learning (RL), which covers three stages: (1) A cold-start phase to gently initialize both encoder and decoder with a semantic reconstruction loss; (2) Generation for Understanding, where the encoder is trained to generate informative captions that maximize the decoder's reconstruction quality, enhancing its visual understanding; (3) Understanding for Generation, where the decoder is refined to reconstruct from these captions, forcing it to leverage every detail and improving its long-context instruction following and generation fidelity. For evaluation, we introduce Unified-Bench, the first benchmark tailored to assess the degree of unification of the UMMs. A surprising \"aha moment\" arises within the multimodal learning domain: as RL progresses, the encoder autonomously produces more descriptive captions, while the decoder simultaneously demonstrates a profound ability to understand these intricate descriptions, resulting in reconstructions of striking fidelity."
    },
    {
      "title": "LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试 (原标题: LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering)",
      "link": "https://arxiv.org/abs/2509.09614",
      "pubDate": "Thu, 11 Sep 2025 12:55:04 GMT",
      "isoDate": "2025-09-11T12:55:04.000Z",
      "creator": "Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang",
      "summary": "# LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试\n\n## 引言\n\n随着上下文窗口扩展到数百万个token的长上下文语言模型（LLM）的出现，为复杂的代码理解和软件开发评估带来了新的机遇。然而，现有的代码评估基准主要关注单函数完成或短上下文任务，这在评估LLM的长上下文能力方面存在显著空白。这些能力包括理解整个代码库、跨多个文件进行推理以及在大规模软件系统中保持架构一致性。\n\n## LoCoBench 概述\n\n为解决这一评估差距，我们提出了 LoCoBench，这是一个专门设计用于在真实、复杂的软件开发场景中评估长上下文LLM的综合性基准测试。\n\n## LoCoBench 的主要特点与构成\n\nLoCoBench 具有以下关键特性：\n\n*   **广泛的评估场景：** 提供了 8,000 个系统生成的评估场景。\n*   **多语言支持：** 涵盖 10 种编程语言。\n*   **可变上下文长度：** 上下文长度从 10K 扩展到 1M token，这种 100 倍的变化范围使得能够精确评估长上下文性能在实际软件开发环境中的下降情况。\n*   **八大任务类别：** LoCoBench 引入了 8 种任务类别，旨在捕捉 LLM 在长上下文环境下的核心能力：\n    1.  **架构理解：** 评估模型对软件系统整体架构的把握。\n    2.  **跨文件重构：** 考察模型在多个文件间进行代码重构的能力。\n    3.  **多会话开发：** 模拟持续开发过程中对上下文的维护和利用。\n    4.  **错误调查：** 评估模型在复杂代码库中定位和诊断错误的能力。\n    5.  **功能实现：** 衡量模型根据需求实现新功能的能力。\n    6.  **代码理解：** 评估模型对现有代码逻辑和意图的理解。\n    7.  **集成测试：** 考察模型在系统集成层面的测试能力。\n    8.  **安全分析：** 评估模型识别和分析代码中安全漏洞的能力。\n*   **高质量场景生成：** 通过一个 5 阶段的流程，我们创建了多样化、高质量的场景，这些场景旨在挑战 LLM 在前所未有的规模上对复杂代码库进行推理。\n\n## 评估框架\n\nLoCoBench 引入了一个全面的评估框架，该框架：\n\n*   包含 4 个维度下的 17 项指标，其中包括 8 项新的评估指标。\n*   所有指标最终综合为一个 LoCoBench 分数 (LCBS)，提供了一个统一的性能衡量标准。\n\n## 研究发现与结论\n\n我们对当前最先进的长上下文模型进行了评估，结果揭示了它们之间存在显著的性能差距。这表明，在复杂软件开发中实现有效且可靠的长上下文理解仍然是一个重大的未解决挑战，需要学术界和工业界投入更多关注和研究。\n\n## 可用性\n\nLoCoBench 基准测试已公开发布。",
      "shortSummary": "LoCoBench是一个综合性基准测试，旨在评估长上下文大型语言模型在复杂软件工程场景中的能力。它通过8000个跨10种编程语言、上下文长度达1M token的场景，弥补了现有基准在理解整个代码库和跨文件推理方面的不足。LoCoBench涵盖架构理解、错误调查等8类核心任务，并引入包含17项指标的评估框架。初步评估显示，当前模型在长上下文理解方面存在显著性能差距，表明这是一个亟待解决的挑战。LoCoBench已公开发布。",
      "translated_title": "LoCoBench：复杂软件工程中长上下文大型语言模型的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench."
    },
    {
      "title": "Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成 (原标题: Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis)",
      "link": "https://arxiv.org/abs/2509.09595",
      "pubDate": "Thu, 11 Sep 2025 12:34:57 GMT",
      "isoDate": "2025-09-11T12:34:57.000Z",
      "creator": "Yikang Ding, Jiwen Liu, Wenyuan Zhang, Zekun Wang, Wentao Hu, Liyuan Cui, Mingming Lao, Yingchao Shao, Hui Liu, Xiaohan Li, Ming Chen, Xiaoqiang Liu, Yu-Shen Liu, Pengfei Wan",
      "summary": "# Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成\n\n## 引言与背景\n*   **现有问题：** 当前的音频驱动虚拟形象视频生成方法在音视频真实感方面有所提升，但它们将指令条件视为仅由声学或视觉线索驱动的低级跟踪，未能建模指令所传达的交流目的。这导致叙事连贯性和角色表现力不足。\n\n## Kling-Avatar 解决方案\n*   **核心创新：** 引入Kling-Avatar，一个新颖的级联框架，它将多模态指令理解与逼真肖像生成相结合。\n*   **两阶段流水线：**\n    1.  **第一阶段：多模态大语言模型（MLLM）导演**\n        *   根据多样化的指令信号生成“蓝图视频”（blueprint video）。\n        *   负责控制高级语义，如角色动作和情感。\n    2.  **第二阶段：并行子剪辑生成**\n        *   以蓝图关键帧为指导。\n        *   采用“首尾帧策略”（first-last frame strategy）并行生成多个子剪辑。\n\n## 框架优势\n*   **全局到局部：** 这种全局到局部的框架设计能够保留精细细节，同时忠实地编码多模态指令背后更高层次的意图。\n*   **并行架构：** 支持快速稳定地生成长时程视频，适用于数字人直播和视频博客等实际应用。\n\n## 评估与成果\n*   **基准构建：** 构建了一个包含375个精心策划样本的基准，涵盖了多样化的指令和具有挑战性的场景。\n*   **实验结果：**\n    *   Kling-Avatar能够生成生动、流畅、长时程的视频，分辨率高达1080p，帧率达48 fps。\n    *   在以下方面表现出卓越性能：\n        *   唇部同步准确性\n        *   情感和动态表现力\n        *   指令可控性\n        *   身份保持\n        *   跨领域泛化能力\n*   **行业地位：** 这些结果确立了Kling-Avatar作为语义驱动、高保真音频驱动虚拟形象合成新基准的地位。",
      "shortSummary": "Kling-Avatar是一个创新的级联框架，旨在解决现有音频驱动虚拟形象生成方法在叙事连贯性和角色表现力方面的不足。它通过结合多模态指令理解与逼真肖像生成，采用两阶段流水线：首先，MLLM导演根据指令生成蓝图视频以控制高级语义；其次，并行生成子剪辑。该方法能快速稳定地生成长时程、高保真视频（最高1080p，48fps），在唇部同步、情感表达、指令可控性、身份保持和泛化能力上表现卓越，为语义驱动的虚拟形象合成树立了新基准。",
      "translated_title": "Kling-Avatar：多模态指令驱动的级联长时程虚拟形象动画合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in audio-driven avatar video generation have significantly enhanced audio-visual realism. However, existing methods treat instruction conditioning merely as low-level tracking driven by acoustic or visual cues, without modeling the communicative purpose conveyed by the instructions. This limitation compromises their narrative coherence and character expressiveness. To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that unifies multimodal instruction understanding with photorealistic portrait generation. Our approach adopts a two-stage pipeline. In the first stage, we design a multimodal large language model (MLLM) director that produces a blueprint video conditioned on diverse instruction signals, thereby governing high-level semantics such as character motion and emotions. In the second stage, guided by blueprint keyframes, we generate multiple sub-clips in parallel using a first-last frame strategy. This global-to-local framework preserves fine-grained details while faithfully encoding the high-level intent behind multimodal instructions. Our parallel architecture also enables fast and stable generation of long-duration videos, making it suitable for real-world applications such as digital human livestreaming and vlogging. To comprehensively evaluate our method, we construct a benchmark of 375 curated samples covering diverse instructions and challenging scenarios. Extensive experiments demonstrate that Kling-Avatar is capable of generating vivid, fluent, long-duration videos at up to 1080p and 48 fps, achieving superior performance in lip synchronization accuracy, emotion and dynamic expressiveness, instruction controllability, identity preservation, and cross-domain generalization. These results establish Kling-Avatar as a new benchmark for semantically grounded, high-fidelity audio-driven avatar synthesis."
    },
    {
      "title": "ObjectReact：学习用于视觉导航的物体相对控制 (原标题: ObjectReact: Learning Object-Relative Control for Visual Navigation)",
      "link": "https://arxiv.org/abs/2509.09594",
      "pubDate": "Thu, 11 Sep 2025 12:34:17 GMT",
      "isoDate": "2025-09-11T12:34:17.000Z",
      "creator": "Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid",
      "summary": "# ObjectReact：学习用于视觉导航的物体相对控制\n\n## 1. 背景与问题\n\n*   **视觉导航的兴起：** 近年来，仅使用单个摄像头和拓扑地图进行视觉导航，已成为替代需要额外传感器和3D地图方法的有吸引力选择。\n*   **传统方法的局限性：** 这种方法通常采用“图像相对”的方式，从当前观测图像和子目标图像对中估计控制。然而，图像级别的世界表示存在局限性，因为图像严格受限于智能体的姿态和具体形态。\n\n## 2. 提出的新范式：“物体相对”控制\n\n*   **核心思想：** 与图像不同，物体是地图的固有属性，提供了一种与具体形态和轨迹无关的世界表示。\n*   **“物体相对”控制的优势：**\n    *   **a) 路径遍历灵活性：** 可以在不严格模仿先前经验的情况下遍历新路线。\n    *   **b) 解耦问题：** 将控制预测问题与图像匹配问题解耦。\n    *   **c) 跨形态部署不变性：** 在跨形态部署中实现高度不变性，适用于训练-测试和建图-执行设置中的各种变化。\n\n## 3. 技术实现\n\n*   **拓扑度量地图表示：** 提出了一种以“相对”3D场景图形式存在的拓扑度量地图表示。\n    *   **目的：** 用于获取更具信息量的物体级别全局路径规划成本。\n*   **局部控制器“ObjectReact”：**\n    *   **设计：** 直接以高级别的“WayObject Costmap”表示为条件进行训练。\n    *   **特点：** 消除了对显式RGB输入的需要。\n\n## 4. 实验与成果\n\n*   **性能优势：** 证明了学习“物体相对”控制相对于“图像相对”控制的优势。\n    *   **测试场景：** 涵盖传感器高度变化和挑战底层空间理解能力的多项导航任务（例如，沿反方向遍历地图轨迹）。\n*   **泛化能力：** 进一步表明，仅在模拟环境中训练的策略能够很好地泛化到真实的室内环境。\n\n## 5. 资源\n\n*   **项目页面：** 代码和补充材料可通过项目页面获取。",
      "shortSummary": "本文提出了一种名为“ObjectReact”的新型“物体相对”控制范式，用于单摄像头视觉导航。该方法通过利用物体作为与智能体姿态和形态无关的世界表示，克服了传统“图像相对”控制的局限性。它采用“相对”3D场景图进行路径规划，并训练一个基于“WayObject Costmap”的局部控制器，无需显式RGB输入。实验证明，“ObjectReact”在传感器高度变化和复杂导航任务中优于传统方法，并能从模拟环境泛化到真实世界。",
      "translated_title": "ObjectReact：学习用于视觉导航的物体相对控制",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/"
    },
    {
      "title": "VLA-Adapter：一种用于微型视觉-语言-动作模型的有效范式 (原标题: VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model)",
      "link": "https://arxiv.org/abs/2509.09372",
      "pubDate": "Thu, 11 Sep 2025 07:42:21 GMT",
      "isoDate": "2025-09-11T07:42:21.000Z",
      "creator": "Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, Donglin Wang",
      "summary": "## VLA-Adapter：微型视觉-语言-动作模型的有效范式\n\n### 研究背景与问题\n\n传统的视觉-语言-动作（VLA）模型通常通过在机器人数据上预训练大型视觉-语言模型（VLM）来连接感知和动作空间。尽管这种方法显著提升了性能，但也带来了高昂的训练成本和对大规模VLM的严重依赖。\n\n### VLA-Adapter的提出\n\n本文引入了VLA-Adapter，一种旨在有效连接视觉-语言（VL）表示与动作（A）的新范式。其核心目标是减少VLA模型对大规模VLM和大量预训练的依赖。\n\n### 核心方法与创新点\n\n1.  **系统性分析VL条件**：研究人员首先系统地分析了各种VL条件的有效性，并揭示了哪些条件对于连接感知和动作空间至关重要。\n2.  **轻量级策略模块与桥接注意力（Bridge Attention）**：基于上述洞察，VLA-Adapter提出了一个轻量级的策略模块，该模块集成了桥接注意力机制。此机制能够自主地将最优条件注入到动作空间中。\n\n### 实验结果与性能优势\n\nVLA-Adapter在模拟和真实世界的机器人基准测试中进行了广泛实验，结果表明：\n\n*   **高性能与轻量化**：该方法仅使用一个0.5B参数的骨干网络，在没有任何机器人数据预训练的情况下，实现了最先进（SOTA）水平的性能。\n*   **快速推理速度**：VLA-Adapter报告了迄今为止最快的推理速度。\n*   **低训练成本**：得益于其先进的桥接范式，VLA-Adapter能够在单个消费级GPU上仅用8小时就训练出一个强大的VLA模型，极大地降低了VLA模型的部署门槛。",
      "shortSummary": "VLA-Adapter提出了一种新范式，旨在降低视觉-语言-动作（VLA）模型对大规模视觉-语言模型（VLM）和大量预训练的依赖。通过系统分析VL条件并引入带有桥接注意力的轻量级策略模块，VLA-Adapter在仅0.5B参数骨干网络下，无需机器人数据预训练即可达到SOTA性能。它还实现了最快的推理速度，并在单个消费级GPU上8小时内完成训练，显著降低了VLA模型的部署门槛。",
      "translated_title": "VLA-Adapter：一种用于微型视觉-语言-动作模型的有效范式",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/."
    },
    {
      "title": "OmniEVA：通过任务自适应3D接地和具身感知推理实现的具身多功能规划器 (原标题: OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning)",
      "link": "https://arxiv.org/abs/2509.09332",
      "pubDate": "Thu, 11 Sep 2025 06:32:22 GMT",
      "isoDate": "2025-09-11T06:32:22.000Z",
      "creator": "Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan",
      "summary": "# OmniEVA：具身多功能规划器\n\n## 引言\n多模态大型语言模型（MLLM）的最新进展为具身智能带来了新的机遇，使其能够实现多模态理解、推理、交互以及连续的空间决策。然而，当前的基于MLLM的具身系统面临两个关键限制。\n\n## 现有MLLM具身系统的局限性\n\n1.  **几何适应性差距（Geometric Adaptability Gap）**：\n    *   模型仅通过2D输入训练，或通过硬编码的3D几何注入，导致空间信息不足或2D泛化能力受限。\n    *   这使得模型在面对具有多样空间需求的任务时，适应性较差。\n\n2.  **具身约束差距（Embodiment Constraint Gap）**：\n    *   现有工作往往忽视真实机器人的物理约束和能力。\n    *   这导致生成的任务规划在理论上有效，但在实践中却无法执行。\n\n## OmniEVA的创新点与解决方案\n为了解决上述差距，研究人员引入了 **OmniEVA**——一个具身多功能规划器。OmniEVA通过两项关键创新，实现了先进的具身推理和任务规划：\n\n1.  **任务自适应3D接地机制（Task-Adaptive 3D Grounding mechanism）**：\n    *   引入了一个门控路由器（gated router），根据上下文需求对3D融合进行显式选择性调节。\n    *   这使得模型能够为多样化的具身任务实现上下文感知的3D接地。\n\n2.  **具身感知推理框架（Embodiment-Aware Reasoning framework）**：\n    *   将任务目标和具身约束共同整合到推理循环中。\n    *   从而产生既以目标为导向又可执行的规划决策。\n\n## 实验结果与性能\n广泛的实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，而且在广泛的下游场景中展现出强大的能力。通过对一系列提出的具身基准（包括原始任务和复合任务）的评估，证实了其鲁棒且多功能的规划能力。\n\n## 相关领域\n*   机器人学 (cs.RO)\n*   人工智能 (cs.AI)\n*   计算与语言 (cs.CL)\n*   计算机视觉与模式识别 (cs.CV)",
      "shortSummary": "OmniEVA是一个具身多功能规划器，旨在解决当前基于MLLM的具身系统在几何适应性和具身约束方面的局限。它通过引入任务自适应3D接地机制和具身感知推理框架，实现了上下文感知的3D接地和可执行的规划决策。实验证明，OmniEVA在具身推理方面达到了最先进的性能，并展现出强大的鲁棒性和多功能规划能力。",
      "translated_title": "OmniEVA：通过任务自适应3D接地和具身感知推理实现的具身多功能规划器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible.To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io"
    },
    {
      "title": "基于Transformer的漏洞检测在开源与工业数据上的跨域评估 (原标题: Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data)",
      "link": "https://arxiv.org/abs/2509.09313",
      "pubDate": "Thu, 11 Sep 2025 05:58:43 GMT",
      "isoDate": "2025-09-11T05:58:43.000Z",
      "creator": "Moritz Mock, Thomas Forrer, Barbara Russo",
      "summary": "# 基于Transformer的漏洞检测在开源与工业数据上的跨域评估\n\n本文旨在解决学术界提出的深度学习漏洞检测方案在实际开发中难以应用，以及其在工业环境中适用性鲜有探讨的问题。将此类技术从学术界转移到工业界面临多重挑战，包括可信度、遗留系统、有限的数字素养以及学术与工业专业知识之间的差距。对于深度学习而言，性能和与现有工作流的集成是额外的关注点。\n\n## 研究目标与方法\n\n本研究的主要目标是评估基于Transformer的深度学习模型（特别是CodeBERT）在漏洞检测方面的跨域泛化能力，并开发一个实用的工业集成解决方案。\n\n研究方法包括：\n*   **CodeBERT性能评估：** 评估CodeBERT在工业和开源软件中检测漏洞函数的能力。\n*   **跨域泛化分析：** 分析CodeBERT在开源数据上进行微调并在工业数据上测试，以及反之，其跨域泛化能力。\n*   **类别不平衡处理：** 探索处理类别不平衡的策略。\n\n## 主要研究发现\n\n研究结果揭示了跨域训练和测试的关键洞察：\n*   **领域内表现：** 在工业数据上训练的模型在其自身领域内能准确检测漏洞。\n*   **跨域性能下降：** 但这些模型在应用于开源代码时，性能会显著下降。\n*   **开放数据微调的优势：** 在开放数据上进行微调并结合适当欠采样技术的深度学习模型，能有效提高漏洞检测能力。\n\n## AI-DO系统：工业集成解决方案\n\n基于上述研究结果，本文开发了AI-DO（Automating vulnerability detection Integration for Developers' Operations）系统。\n\n*   **系统定位：** AI-DO是一个集成到持续集成-持续部署（CI/CD）流程中的推荐系统。\n*   **核心功能：** 它利用经过微调的CodeBERT在代码审查期间检测和定位漏洞。\n*   **设计理念：** 该系统旨在不中断现有开发工作流的前提下，提供高效的漏洞检测能力。\n\n## 工具评估\n\n*   **评估方式：** 通过对公司IT专业人员进行调查，评估了AI-DO工具的感知有用性。\n\n## 结论\n\n本研究深入探讨了基于Transformer的深度学习模型（CodeBERT）在开源和工业数据上进行漏洞检测的跨域泛化能力，并提出了一种实用的CI/CD集成推荐系统AI-DO，旨在弥合学术研究与工业应用之间的鸿沟，提升工业环境中漏洞检测的效率和可用性。",
      "shortSummary": "本文评估了基于Transformer的CodeBERT模型在开源和工业数据上进行漏洞检测的跨域泛化能力。研究发现，工业数据训练的模型在同领域表现良好，但跨域性能下降；而开放数据微调结合欠采样能提高检测能力。基于此，开发了AI-DO系统，一个CI/CD集成的推荐系统，利用CodeBERT在代码审查中检测和定位漏洞，且不中断工作流。该系统通过IT专业人员调查评估了其感知有用性，旨在弥合学术与工业漏洞检测的鸿沟。",
      "translated_title": "基于Transformer的漏洞检测在开源与工业数据上的跨域评估",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep learning solutions for vulnerability detection proposed in academic research are not always accessible to developers, and their applicability in industrial settings is rarely addressed. Transferring such technologies from academia to industry presents challenges related to trustworthiness, legacy systems, limited digital literacy, and the gap between academic and industrial expertise. For deep learning in particular, performance and integration into existing workflows are additional concerns. In this work, we first evaluate the performance of CodeBERT for detecting vulnerable functions in industrial and open-source software. We analyse its cross-domain generalisation when fine-tuned on open-source data and tested on industrial data, and vice versa, also exploring strategies for handling class imbalance. Based on these results, we develop AI-DO(Automating vulnerability detection Integration for Developers' Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated recommender system that uses fine-tuned CodeBERT to detect and localise vulnerabilities during code review without disrupting workflows. Finally, we assess the tool's perceived usefulness through a survey with the company's IT professionals. Our results show that models trained on industrial data detect vulnerabilities accurately within the same domain but lose performance on open-source code, while a deep learner fine-tuned on open data, with appropriate undersampling techniques, improves the detection of vulnerabilities."
    },
    {
      "title": "视觉可编程性：图表理解中“代码即思想”的指南 (原标题: Visual Programmability: A Guide for Code-as-Thought in Chart Understanding)",
      "link": "https://arxiv.org/abs/2509.09286",
      "pubDate": "Thu, 11 Sep 2025 05:22:16 GMT",
      "isoDate": "2025-09-11T05:22:16.000Z",
      "creator": "Bohao Tang, Yan Ma, Fei Zhang, Jiadi Su, Ethan Chern, Zhulin Hu, Zhixin Wang, Pengfei Liu, Ya Zhang",
      "summary": "## 视觉可编程性：图表理解中“代码即思想”的指南\n\n### 摘要\n\n本文提出了一种名为“视觉可编程性”的新方法，旨在解决视觉-语言模型（VLMs）在图表理解任务中面临的关键挑战。该方法通过引入“代码即思想”（Code-as-Thought, CaT）和自适应推理路径选择机制，显著提升了VLMs的推理能力和准确性。\n\n### 现有方法的局限性\n\n*   **依赖外部工具：** 现有方法常依赖外部工具，导致系统脆弱且受限于预定义的工具集。\n*   **单一推理策略：** 大多数方法采用单一推理策略，如基于文本的思维链（Chain-of-Thought, CoT），这限制了模型处理复杂图表的能力。\n*   **难以验证中间步骤：** 基于文本的推理中间步骤难以验证，这使得难以利用强化学习信号来奖励事实准确性，从而可能导致数值幻觉。\n\n### 提出“代码即思想”（CaT）方法\n\n*   **目标：** 将图表的视觉信息表示为可验证的、符号化的格式。\n*   **优势：** 符号化表示使得中间推理步骤更易于验证，为强化学习提供了更清晰的反馈信号。\n\n### 关键洞察与“视觉可编程性”\n\n*   **CaT的局限性：** 纯粹的、固定的代码实现（CaT）在面对符号表示不适用的复杂图表时会失效。\n*   **引入“视觉可编程性”：** 本文提出“视觉可编程性”作为一种可学习的属性。它使VLM能够判断特定图表-问题对更适合通过代码推理（CaT）解决，还是通过直接的视觉分析解决。\n\n### 自适应框架的实现\n\n*   **双路径选择：** 构建了一个自适应框架，其中VLM学习在以下两种推理路径之间进行选择：\n    1.  **CaT路径：** 基于代码的符号化推理。\n    2.  **直接视觉推理路径：** 直接进行视觉分析。\n\n### 创新的双重奖励系统\n\n*   **强化学习训练：** 模型的选择策略通过强化学习进行训练。\n*   **双重奖励机制：** 采用了一种新颖的双重奖励系统：\n    *   **数据准确性奖励：** 确保模型基于事实进行推理，防止数值幻觉。\n    *   **决策奖励：** 教导模型何时使用每种策略，防止其默认采用单一推理模式。\n\n### 实验结果与结论\n\n*   **强大而稳健的性能：** 实验证明，该方法在各种图表理解基准测试中展现出强大且稳健的性能。\n*   **核心发现：** 本研究表明，VLMs不仅可以被训练去推理，还可以被训练去学习“如何推理”，即动态地为每个任务选择最优的推理路径。",
      "shortSummary": "针对VLM在图表理解中现有方法的局限性，本文提出“代码即思想”（CaT）方法，以可验证的符号格式表示图表信息。为解决CaT在复杂图表上的不足，引入“视觉可编程性”，使VLM能自适应选择代码推理或直接视觉分析。该框架通过结合数据准确性和决策奖励的双重强化学习系统进行训练。实验证明，此方法在图表理解任务中表现出色，使VLM学会动态选择最佳推理路径。",
      "translated_title": "视觉可编程性：图表理解中“代码即思想”的指南",
      "images": [],
      "contentSource": "完整文章",
      "content": "Chart understanding presents a critical test to the reasoning capabilities of Vision-Language Models (VLMs). Prior approaches face critical limitations: some rely on external tools, making them brittle and constrained by a predefined toolkit, while others fine-tune specialist models that often adopt a single reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate steps of text-based reasoning are difficult to verify, which complicates the use of reinforcement-learning signals that reward factual accuracy. To address this, we propose a Code-as-Thought (CaT) approach to represent the visual information of a chart in a verifiable, symbolic format. Our key insight is that this strategy must be adaptive: a fixed, code-only implementation consistently fails on complex charts where symbolic representation is unsuitable. This finding leads us to introduce Visual Programmability: a learnable property that determines if a chart-question pair is better solved with code or direct visual analysis. We implement this concept in an adaptive framework where a VLM learns to choose between the CaT pathway and a direct visual reasoning pathway. The selection policy of the model is trained with reinforcement learning using a novel dual-reward system. This system combines a data-accuracy reward to ground the model in facts and prevent numerical hallucination, with a decision reward that teaches the model when to use each strategy, preventing it from defaulting to a single reasoning mode. Experiments demonstrate strong and robust performance across diverse chart-understanding benchmarks. Our work shows that VLMs can be taught not only to reason but also how to reason, dynamically selecting the optimal reasoning pathway for each task."
    },
    {
      "title": "驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度 (原标题: Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents)",
      "link": "https://arxiv.org/abs/2509.09265",
      "pubDate": "Thu, 11 Sep 2025 04:50:01 GMT",
      "isoDate": "2025-09-11T04:50:01.000Z",
      "creator": "Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, Ke Wang",
      "summary": "### 驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度\n\n本文提出了一种名为熵调制策略梯度（EMPG）的新框架，旨在解决大型语言模型（LLM）智能体在长周期任务中面临的挑战。\n\n**1. 背景与现有问题**\n*   **稀疏奖励问题：** 在长周期任务中，LLM智能体通常只能获得稀疏的、基于最终结果的奖励，这使得难以准确地将信用分配给中间步骤。\n*   **现有解决方案的局限性：** 之前的研究主要通过生成密集的奖励信号来指导学习，例如使用逆强化学习或过程奖励模型提供分步反馈。\n*   **本文发现的根本问题：** 作者识别出LLM学习动态中的一个基本问题——策略梯度的幅度与熵固有地耦合。这种耦合导致：\n    *   对于自信且正确的动作，更新幅度过小，学习效率低下。\n    *   对于不确定的动作，更新幅度可能过大，导致学习过程不稳定。\n\n**2. 提出的解决方案：熵调制策略梯度 (EMPG)**\n*   **核心思想：** EMPG通过根据分步不确定性和最终任务结果重新校准学习信号来解决上述问题。\n*   **EMPG的工作机制：**\n    *   **放大自信的正确动作：** 对于智能体自信且执行正确的动作，EMPG会放大其学习更新，从而加速学习。\n    *   **惩罚自信的错误：** 对于智能体自信但执行错误的动作，EMPG会施加惩罚，促使其纠正。\n    *   **衰减不确定步骤的更新：** 对于智能体不确定的步骤，EMPG会减弱其更新幅度，以稳定探索过程，避免因不确定性导致的大幅、不稳定更新。\n\n**3. 额外的奖励机制：未来清晰度奖励**\n*   为了进一步优化学习，EMPG还引入了一个“未来清晰度”奖励项。\n*   **目的：** 这个奖励项鼓励智能体寻找更可预测、更清晰的解决方案路径，从而提高任务完成的稳定性和效率。\n\n**4. 实验结果**\n*   **实验任务：** 研究人员在三个具有挑战性的智能体任务上对EMPG进行了全面实验，包括WebShop、ALFWorld和Deep Search。\n*   **性能提升：** 实验结果表明，EMPG取得了显著的性能提升，并显著优于现有的强大策略梯度基线方法。\n\n**5. 结论**\nEMPG通过创新性地解决策略梯度与熵的耦合问题，为长周期LLM智能体的学习提供了一个更稳定、高效的框架，并在多个复杂任务中展现出卓越的性能。",
      "shortSummary": "本文提出熵调制策略梯度（EMPG），以解决长周期LLM智能体中策略梯度幅度与熵耦合的问题。传统方法在稀疏奖励下信用分配困难，且自信正确动作更新小、不确定动作更新不稳定。EMPG根据分步不确定性和最终结果重新校准学习信号，放大自信正确更新，惩罚自信错误，并衰减不确定步骤更新以稳定探索。它还引入未来清晰度奖励。实验表明，EMPG在WebShop、ALFWorld和Deep Search任务上显著优于基线，提升了LLM智能体在复杂任务中的性能。",
      "translated_title": "驾驭不确定性：用于长周期LLM智能体的熵调制策略梯度",
      "images": [],
      "contentSource": "完整文章",
      "content": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/"
    },
    {
      "title": "迈向更好的牙科AI：全景X射线分析的多模态基准和指令数据集 (原标题: Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis)",
      "link": "https://arxiv.org/abs/2509.09254",
      "pubDate": "Thu, 11 Sep 2025 04:39:08 GMT",
      "isoDate": "2025-09-11T04:39:08.000Z",
      "creator": "Jing Hao, Yuxuan Fan, Yanpeng Sun, Kaixin Guo, Lizhuo Lin, Jinrong Yang, Qi Yong H. Ai, Lun M. Wong, Hao Tang, Kuo Feng Hung",
      "summary": "## 迈向更好的牙科AI：MMOral多模态基准与指令数据集\n\n### 引言\n\n尽管大型视觉-语言模型（LVLMs）在通用医疗任务上取得了显著进展，但它们在牙科等专业领域的有效性，特别是对全景X射线的解读能力，仍未得到充分探索。全景X射线因其密集的解剖结构和细微的病理线索，对解读提出了独特挑战，而现有医疗基准和指令数据集未能有效捕捉这些特点。\n\n### MMOral：首个大规模多模态指令数据集\n\n为解决上述挑战，研究者引入了MMOral，这是首个专为全景X射线解读设计的大规模多模态指令数据集和基准。\n\n*   **构成：** MMOral包含20,563张经过详细注释的图像，并配有130万个指令遵循实例。\n*   **任务类型：** 数据集涵盖了多种任务类型，包括：\n    *   属性提取\n    *   报告生成\n    *   视觉问答（VQA）\n    *   图像接地对话\n\n### MMOral-Bench：全面的评估套件\n\n除了数据集，研究者还提出了MMOral-Bench，一个综合性的评估套件，旨在全面衡量模型在牙科诊断中的表现。\n\n*   **评估维度：** MMOral-Bench涵盖了牙科诊断的五个关键维度。\n\n### 模型评估与发现\n\n研究团队在MMOral-Bench上评估了64个LVLMs，结果揭示了当前模型在该领域的显著局限性：\n\n*   即使是表现最佳的模型，如GPT-4o，其准确率也仅为41.45%。这表明现有通用LVLMs在处理牙科全景X射线这种高度专业化任务时，仍有巨大的提升空间。\n\n### OralGPT：提升牙科AI性能的新模型\n\n为促进该特定领域的进步，研究者提出了OralGPT模型。\n\n*   **基础模型：** OralGPT基于Qwen2.5-VL-7B模型。\n*   **训练方法：** 利用精心策划的MMOral指令数据集进行监督微调（SFT）。\n*   **性能提升：** 令人瞩目的是，仅通过一个SFT训练周期，OralGPT就实现了显著的性能增强，例如，性能提升了24.73%。\n\n### 重要意义与可用性\n\nMMOral数据集和OralGPT模型被视为智能牙科领域的关键基础，有望推动牙科领域更具临床影响力的多模态AI系统的发展。\n\n*   **资源公开：** 该数据集、模型、基准和评估套件均已公开提供。",
      "shortSummary": "大型视觉-语言模型（LVLMs）在牙科全景X射线解读上面临挑战。为解决此问题，研究者推出了MMOral，首个大规模多模态指令数据集（含20,563图像和130万实例）及MMOral-Bench评估基准。对64个LVLMs的评估显示，最佳模型GPT-4o准确率仅为41.45%，揭示了当前模型的局限性。为此，研究者提出了OralGPT模型，通过在MMOral上进行监督微调，实现了24.73%的显著性能提升。MMOral和OralGPT为智能牙科及更具临床影响力的多模态AI系统奠定了基础。",
      "translated_title": "迈向更好的牙科AI：全景X射线分析的多模态基准和指令数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in large vision-language models (LVLMs) have demonstrated strong performance on general-purpose medical tasks. However, their effectiveness in specialized domains such as dentistry remains underexplored. In particular, panoramic X-rays, a widely used imaging modality in oral radiology, pose interpretative challenges due to dense anatomical structures and subtle pathological cues, which are not captured by existing medical benchmarks or instruction datasets. To this end, we introduce MMOral, the first large-scale multimodal instruction dataset and benchmark tailored for panoramic X-ray interpretation. MMOral consists of 20,563 annotated images paired with 1.3 million instruction-following instances across diverse task types, including attribute extraction, report generation, visual question answering, and image-grounded dialogue. In addition, we present MMOral-Bench, a comprehensive evaluation suite covering five key diagnostic dimensions in dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing significant limitations of current models in this domain. To promote the progress of this specific domain, we also propose OralGPT, which conducts supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated MMOral instruction dataset. Remarkably, a single epoch of SFT yields substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a 24.73% improvement. Both MMOral and OralGPT hold significant potential as a critical foundation for intelligent dentistry and enable more clinically impactful multimodal AI systems in the dental field. The dataset, model, benchmark, and evaluation suite are available at https://github.com/isbrycee/OralGPT."
    },
    {
      "title": "EchoX：旨在通过回声训练缓解语音到语音大语言模型中的声学-语义鸿沟 (原标题: EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs)",
      "link": "https://arxiv.org/abs/2509.09174",
      "pubDate": "Thu, 11 Sep 2025 02:17:59 GMT",
      "isoDate": "2025-09-11T02:17:59.000Z",
      "creator": "Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li",
      "summary": "## EchoX：旨在通过回声训练缓解语音到语音大语言模型中的声学-语义鸿沟\n\n本文介绍了EchoX，一个旨在解决语音到语音大语言模型（SLLMs）在知识和推理能力方面退化问题的新方法。\n\n*   **问题背景：**\n    *   语音到语音大语言模型（SLLMs）正日益受到关注。\n    *   由于SLLMs通常源自基于文本的大语言模型（LLMs），它们在知识和推理能力上常表现出性能下降。\n    *   研究人员假设，这种局限性是由于当前SLLMs的训练范式未能有效弥合特征表示空间中的“声学-语义鸿沟”。\n\n*   **EchoX方法：**\n    *   为了解决上述问题，EchoX被提出，它利用语义表示并动态生成语音训练目标。\n    *   该方法将声学学习和语义学习紧密结合。\n    *   通过这种集成学习，EchoX旨在作为语音LLM保持其强大的推理能力。\n\n*   **实验结果：**\n    *   实验结果表明，EchoX在使用了大约六千小时的训练数据后，在多个基于知识的问答基准测试中取得了先进的性能。\n\n*   **项目可用性：**\n    *   该项目可在提供的URL获取。\n\n*   **相关主题：**\n    *   计算与语言 (cs.CL)\n    *   人工智能 (cs.AI)\n    *   声音 (cs.SD)",
      "shortSummary": "EchoX提出了一种新的训练范式，旨在解决语音到语音大语言模型（SLLMs）因声学-语义鸿沟导致的知识和推理能力退化问题。该方法通过整合声学和语义学习，利用语义表示并动态生成语音训练目标。实验证明，EchoX在约六千小时训练数据下，在多个知识问答基准测试中表现出色，有效保持了SLLMs的强大推理能力。",
      "translated_title": "EchoX：旨在通过回声训练缓解语音到语音大语言模型中的声学-语义鸿沟",
      "images": [],
      "contentSource": "完整文章",
      "content": "Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX."
    },
    {
      "title": "梯度注意力引导的双掩码协同框架，用于鲁棒的基于文本的人物检索 (原标题: Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval)",
      "link": "https://arxiv.org/abs/2509.09118",
      "pubDate": "Wed, 10 Sep 2025 23:06:22 GMT",
      "isoDate": "2025-09-10T23:06:22.000Z",
      "creator": "Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, Qichuan Ding",
      "summary": "### 背景与挑战\n\n尽管对比语言-图像预训练（CLIP）在各种视觉任务中表现出色，但其在人物表征学习中的应用面临两大关键挑战：\n\n*   **数据稀缺性：** 缺乏大规模、以人物为中心的标注视觉-语言数据。\n*   **全局对比学习的局限性：** 全局对比学习难以在保持对细粒度匹配至关重要的判别性局部特征的同时，抵御噪声文本标记的影响。\n\n### 本文贡献与方法\n\n本文通过数据整理和模型架构的协同改进，推动了CLIP在人物表征学习方面的应用，旨在解决上述挑战。\n\n#### 1. 数据整理：WebPerson数据集\n\n*   **噪声抵抗的数据构建流程：** 开发了一个抗噪声的数据构建流程，该流程利用多模态大语言模型（MLLMs）的上下文学习能力，自动过滤和标注网络来源的图像。\n*   **WebPerson数据集：** 通过此流程，构建了一个名为WebPerson的大规模数据集，其中包含500万高质量、以人物为中心的图像-文本对。\n\n#### 2. 模型架构：GA-DMS框架\n\n*   **GA-DMS（梯度注意力引导的双掩码协同）框架：** 引入了GA-DMS框架，通过以下方式改进了跨模态对齐：\n    *   **自适应掩码：** 基于梯度注意力相似性分数，自适应地掩盖噪声文本标记，从而减少噪声对模型学习的影响。\n    *   **掩码标记预测目标：** 结合了掩码标记预测目标，强制模型预测信息丰富的文本标记，以此增强细粒度语义表征学习能力。\n\n### 实验结果\n\n*   广泛的实验表明，GA-DMS框架在多个基准测试中取得了最先进（state-of-the-art）的性能。",
      "shortSummary": "本文针对CLIP在人物表征学习中面临的数据稀缺和全局对比学习局限性问题，提出了两项协同改进。首先，利用MLLMs构建了包含500万高质量图像-文本对的WebPerson数据集。其次，引入了GA-DMS（梯度注意力引导的双掩码协同）框架，通过自适应掩盖噪声文本标记和掩码标记预测目标，增强了跨模态对齐和细粒度语义表征学习。实验证明GA-DMS在多项基准测试中达到了最先进的性能，提升了基于文本的人物检索的鲁棒性。",
      "translated_title": "梯度注意力引导的双掩码协同框架，用于鲁棒的基于文本的人物检索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks."
    },
    {
      "title": "多模态推荐中基于多尺度双边注意力的模态对齐 (原标题: Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation)",
      "link": "https://arxiv.org/abs/2509.09114",
      "pubDate": "Wed, 10 Sep 2025 22:52:26 GMT",
      "isoDate": "2025-09-10T22:52:26.000Z",
      "creator": "Kelin Ren, Chan-Yang Ju, Dong-Ho Lee",
      "summary": "## MambaRec：多模态推荐中的模态对齐新框架\n\n### 背景与挑战\n多模态推荐系统在电商和内容平台中扮演着越来越重要的角色，通过联合建模用户历史行为和物品的多模态特征（如视觉和文本）来提供个性化服务。然而，现有方法普遍面临两大关键局限：\n\n1.  **细粒度跨模态关联不足：** 大多数方法依赖静态融合策略或基于图的局部交互建模，难以有效捕捉细粒度的跨模态关联，导致融合质量不佳。\n2.  **缺乏全局分布一致性：** 现有方法缺乏全局分布层面的模态一致性，容易造成表征偏差。\n\n### 提出的方法：MambaRec\n为解决上述挑战，本文提出了MambaRec，一个新颖的框架，它通过注意力引导学习，将局部特征对齐和全局分布正则化相结合。MambaRec旨在提升多模态推荐系统的融合质量、泛化能力和效率。\n\n### 核心组件：膨胀细化注意力模块（DREAM）\n\n*   **功能：** MambaRec的核心是**膨胀细化注意力模块（Dilated Refinement Attention Module, DREAM）**。\n*   **机制：** DREAM利用多尺度膨胀卷积，结合通道注意力和空间注意力机制，实现视觉和文本模态之间细粒度的语义模式对齐。\n*   **优势：** 该模块能够捕获层级关系和上下文感知关联，显著提升跨模态语义建模能力，从而优化跨模态特征的融合。\n\n### 全局模态对齐正则化\n\n*   **方法：** 除了局部特征对齐，MambaRec还引入了**最大均值差异（Maximum Mean Discrepancy, MMD）**和**对比损失函数**来约束全局模态对齐。\n*   **目的：** 这种双重正则化策略旨在增强模态间的语义一致性，减少特定模态的偏差，并提升模型的整体鲁棒性。\n\n### 可扩展性优化\n\n*   为提高MambaRec在处理高维多模态特征时的可扩展性，框架采用了**降维策略**，有效降低了计算成本。\n\n### 实验结果与代码可用性\n\n*   在真实世界的电商数据集上进行的广泛实验表明，MambaRec在融合质量、泛化能力和效率方面均优于现有方法。\n*   MambaRec的代码已公开提供。",
      "shortSummary": "本文提出了MambaRec框架，旨在解决多模态推荐系统中细粒度跨模态关联不足和全局分布一致性缺失的问题。MambaRec通过引入膨胀细化注意力模块（DREAM）实现视觉与文本模态的细粒度语义对齐，并结合最大均值差异（MMD）和对比损失函数进行全局模态正则化。该框架还通过降维策略提升可扩展性。实验证明，MambaRec在融合质量、泛化能力和效率上均优于现有方法。",
      "translated_title": "多模态推荐中基于多尺度双边注意力的模态对齐",
      "images": [],
      "contentSource": "完整文章",
      "content": "Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec."
    },
    {
      "title": "大型推理模型强化学习综述 (原标题: A Survey of Reinforcement Learning for Large Reasoning Models)",
      "link": "https://arxiv.org/abs/2509.08827",
      "pubDate": "Wed, 10 Sep 2025 13:59:43 GMT",
      "isoDate": "2025-09-10T13:59:43.000Z",
      "creator": "Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou",
      "summary": "本文对大型推理模型（LRMs）中强化学习（RL）的最新进展进行了综述。RL在推动大型语言模型（LLMs）能力发展方面取得了显著成功，尤其是在解决数学和编程等复杂逻辑任务方面。因此，RL已成为将LLMs转化为LRMs的基础方法。\n\n**主要观点与贡献：**\n\n*   **RL的关键作用**：RL在增强LLMs处理复杂推理任务（如数学和编码）的能力方面发挥了核心作用，使其能够演变为大型推理模型（LRMs）。\n*   **当前挑战**：随着该领域的快速发展，RL在LRMs中的进一步扩展面临着基础性挑战，不仅体现在计算资源上，还包括算法设计、训练数据和基础设施方面。\n*   **综述目的**：本文旨在及时回顾该领域的发展，重新评估其发展轨迹，并探索增强RL可扩展性以实现人工超智能（ASI）的策略。\n*   **研究范围**：特别关注自DeepSeek-R1发布以来，将RL应用于LLMs和LRMs以提升推理能力的研究。这包括：\n    *   基础组件\n    *   核心问题\n    *   训练资源\n    *   下游应用\n*   **未来展望**：通过对上述方面的考察，旨在识别这一快速发展领域的未来机遇和方向。\n*   **促进研究**：作者希望这篇综述能促进未来在更广泛推理模型中RL的研究。",
      "shortSummary": "本文综述了强化学习（RL）在大型推理模型（LRMs）中的最新进展。RL在提升大型语言模型（LLMs）处理复杂逻辑任务的能力方面取得了显著成功，是LLMs向LRMs转化的基础方法。然而，RL的进一步扩展面临计算资源、算法设计、训练数据和基础设施等方面的挑战。本综述旨在回顾该领域发展，评估其轨迹，并探索增强RL可扩展性的策略，以识别未来机遇和方向，促进相关研究。",
      "translated_title": "大型推理模型强化学习综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
    },
    {
      "title": "RewardDance：视觉生成中的奖励缩放 (原标题: RewardDance: Reward Scaling in Visual Generation)",
      "link": "https://arxiv.org/abs/2509.08826",
      "pubDate": "Wed, 10 Sep 2025 13:59:31 GMT",
      "isoDate": "2025-09-10T13:59:31.000Z",
      "creator": "Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, Weilin Huang",
      "summary": "## RewardDance：视觉生成中的奖励缩放\n\n本文介绍了RewardDance，一个创新的可扩展奖励建模框架，旨在解决视觉生成领域中现有奖励模型（RMs）面临的关键挑战。\n\n### 现有挑战与局限性\n\n在视觉生成中，奖励模型（RMs）的扩展范式尚未得到充分探索，主要原因在于现有方法的根本性局限：\n\n*   **CLIP-based RMs的限制**：受限于其固有的架构和输入模态。\n*   **Bradley-Terry 损失的错位**：与视觉-语言模型（VLMs）的下一词预测机制存在根本性不匹配，从而阻碍了有效的扩展。\n*   **奖励欺骗（Reward Hacking）问题**：强化学习人类反馈（RLHF）优化过程普遍存在奖励欺骗问题，即模型可能利用奖励信号的缺陷，而非真正提升生成内容的真实质量。\n\n### RewardDance 解决方案\n\nRewardDance 通过引入一种新颖的生成式奖励范式来克服这些障碍：\n\n*   **奖励分数重构**：将奖励分数重新定义为模型预测“是”（yes）标记的概率。这个“是”标记表示根据特定标准，生成的图像优于参考图像。\n*   **与 VLM 架构的内在对齐**：这种重构使得奖励目标与 VLM 架构实现了内在对齐，从而解锁了RMs的扩展潜力。\n\n### RewardDance 的扩展能力\n\nRewardDance 在两个关键维度上实现了扩展：\n\n1.  **模型扩展**：系统地将奖励模型扩展到高达260亿参数的规模。\n2.  **上下文扩展**：能够整合任务特定指令、参考示例以及思维链（CoT）推理。\n\n### 实验结果与优势\n\n*   **性能显著超越 SOTA**：广泛的实验证明，RewardDance 在文本到图像、文本到视频和图像到视频生成任务中显著超越了现有最先进的方法。\n*   **解决奖励欺骗问题**：RewardDance 成功解决了长期存在的“奖励欺骗”挑战。其大规模RMs在RL微调过程中表现并保持高奖励方差，证明了它们对欺骗的抵抗力，并能持续产生多样化、高质量的输出。\n*   **缓解模式崩溃**：这极大地缓解了困扰小型模型的模式崩溃问题。",
      "shortSummary": "RewardDance 是一种创新的可扩展奖励建模框架，旨在解决视觉生成中现有奖励模型（RMs）的局限性及奖励欺骗问题。它通过将奖励分数重构为模型预测“是”标记的概率，实现了与视觉-语言模型（VLMs）架构的内在对齐。RewardDance 支持模型（高达260亿参数）和上下文（指令、CoT）的扩展。实验表明，它在多种生成任务中显著超越现有SOTA，有效抵抗奖励欺骗，并能生成多样化、高质量的内容，缓解了模式崩溃问题。",
      "translated_title": "RewardDance：视觉生成中的奖励缩放",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reward Models (RMs) are critical for improving generation models via Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation remains largely unexplored. It primarily due to fundamental limitations in existing approaches: CLIP-based RMs suffer from architectural and input modality constraints, while prevalent Bradley-Terry losses are fundamentally misaligned with the next-token prediction mechanism of Vision-Language Models (VLMs), hindering effective scaling. More critically, the RLHF optimization process is plagued by Reward Hacking issue, where models exploit flaws in the reward signal without improving true quality. To address these challenges, we introduce RewardDance, a scalable reward modeling framework that overcomes these barriers through a novel generative reward paradigm. By reformulating the reward score as the model's probability of predicting a \"yes\" token, indicating that the generated image outperforms a reference image according to specific criteria, RewardDance intrinsically aligns reward objectives with VLM architectures. This alignment unlocks scaling across two dimensions: (1) Model Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context Scaling: Integration of task-specific instructions, reference examples, and chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that RewardDance significantly surpasses state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Crucially, we resolve the persistent challenge of \"reward hacking\": Our large-scale RMs exhibit and maintain high reward variance during RL fine-tuning, proving their resistance to hacking and ability to produce diverse, high-quality outputs. It greatly relieves the mode collapse problem that plagues smaller models."
    },
    {
      "title": "AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策 (原标题: AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2509.08755",
      "pubDate": "Wed, 10 Sep 2025 12:46:11 GMT",
      "isoDate": "2025-09-10T12:46:11.000Z",
      "creator": "Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang",
      "summary": "# AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策\n\n## 1. 引言与背景\n开发能够通过一系列智能决策解决复杂现实世界任务的自主大型语言模型（LLM）智能体是当前研究的热点。智能体需要通过与环境的探索和交互来获取知识和技能。然而，目前社区缺乏一个统一的、交互式的强化学习（RL）框架，能够有效地从零开始（不依赖于监督微调SFT）在多样化和真实的环​​境中训练此类智能体。\n\n## 2. AgentGym-RL 框架介绍\n为了解决上述问题，研究人员引入了 **AgentGym-RL**，这是一个用于通过强化学习训练LLM智能体进行多轮交互式决策的新框架。\n\n*   **核心目标**：通过强化学习训练LLM智能体，使其能够进行多轮交互式决策，以解决长周期任务。\n*   **架构特点**：该框架采用模块化和解耦的架构设计，确保了高度的灵活性和可扩展性。\n*   **应用范围**：AgentGym-RL 涵盖了广泛的现实世界场景，并支持主流的强化学习算法。\n\n## 3. ScalingInter-RL 训练方法\n为了进一步优化训练过程，研究人员提出了一种名为 **ScalingInter-RL** 的训练方法，旨在实现探索与利用的平衡以及稳定的强化学习优化。\n\n*   **早期阶段**：该方法通过限制交互次数来强调利用（exploitation），使智能体在初期阶段能够更有效地利用已知信息。\n*   **后期阶段**：随着训练的进行，它逐渐转向探索（exploration），通过更大的决策周期（horizons）来鼓励智能体发展多样化的解决问题策略。\n*   **优势**：通过这种方式，智能体能够发展出更多样化的行为，并且在长周期任务中更不容易出现性能崩溃。\n\n## 4. 实验验证与成果\n研究人员进行了广泛的实验，以验证 AgentGym-RL 框架和 ScalingInter-RL 方法的稳定性和有效性。\n\n*   **实验结果**：实验表明，所训练的智能体在多样化环境中的27项任务上，其性能与商业模型相当或超越商业模型。\n*   **未来贡献**：研究将提供关键的见解，并计划开源完整的 AgentGym-RL 框架，包括代码和数据集，以赋能研究社区开发下一代智能智能体。",
      "shortSummary": "AgentGym-RL是一个新颖的强化学习框架，旨在通过多轮交互式决策训练LLM智能体进行长周期任务。它解决了缺乏统一RL框架从零开始训练LLM智能体的问题。该框架采用模块化设计，并引入了ScalingInter-RL训练方法，以平衡探索与利用，确保稳定优化。实验证明，AgentGym-RL智能体在27项任务上性能匹敌或超越商业模型。该框架及其代码和数据集将开源，以推动智能体研究。",
      "translated_title": "AgentGym-RL：通过多轮强化学习训练LLM智能体进行长周期决策",
      "images": [],
      "contentSource": "完整文章",
      "content": "Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents."
    },
    {
      "title": "HuMo：通过协同多模态条件控制实现以人为中心的视频生成 (原标题: HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning)",
      "link": "https://arxiv.org/abs/2509.08519",
      "pubDate": "Wed, 10 Sep 2025 07:54:29 GMT",
      "isoDate": "2025-09-10T07:54:29.000Z",
      "creator": "Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu",
      "summary": "# HuMo：通过协同多模态条件控制实现以人为中心的视频生成\n\n## 1. 引言与背景\n以人为中心的视频生成（Human-Centric Video Generation, HCVG）旨在从多模态输入（包括文本、图像和音频）中合成人类视频。现有方法在有效协调这些异构模态时面临两大挑战：\n*   **数据稀缺性**：缺乏配对的三元组条件（文本、参考图像、音频）训练数据。\n*   **任务协作困难**：难以协同处理主体保留（subject preservation）和音视频同步（audio-visual sync）这两个子任务，尤其是在多模态输入下。\n\n## 2. HuMo 框架介绍\n本文提出了 HuMo，一个统一的 HCVG 框架，用于实现协同多模态控制。\n\n## 3. 核心贡献与策略\n\n### 3.1 解决数据稀缺性\n为了应对数据稀缺问题，HuMo 构建了一个高质量的数据集，其中包含多样化且配对的文本、参考图像和音频。\n\n### 3.2 解决任务协作困难：两阶段渐进式多模态训练范式\nHuMo 提出了一种两阶段渐进式多模态训练范式，并为每个子任务设计了特定的策略：\n\n#### a. 主体保留任务 (Subject Preservation)\n*   **目标**：在保持基础模型的提示遵循能力和视觉生成能力的同时，有效保留视频主体。\n*   **策略**：采用“最小侵入式图像注入策略”（minimal-invasive image injection strategy）。\n\n#### b. 音视频同步任务 (Audio-Visual Sync)\n*   **目标**：确保生成的视频与音频内容高度同步。\n*   **策略**：\n    *   除了常用的音频交叉注意力层（audio cross-attention layer）外，HuMo 还提出了一种“通过预测聚焦策略”（focus-by-predicting strategy）。\n    *   该策略隐式地引导模型将音频与面部区域关联起来，从而实现更精确的同步。\n\n#### c. 多模态输入下的联合学习\n*   在获得上述能力的基础上，HuMo 逐步整合音视频同步任务，实现多模态输入下可控性的联合学习。\n\n### 3.3 推理阶段的灵活控制\n在推理阶段，为了实现灵活且细粒度的多模态控制，HuMo 设计了一种“时间自适应无分类器引导策略”（time-adaptive Classifier-Free Guidance strategy）。该策略能够动态调整去噪步骤中的引导权重。\n\n## 4. 实验结果\n广泛的实验结果表明，HuMo 在各项子任务中均超越了专门的现有最先进方法，成功建立了一个用于协同多模态条件控制的统一 HCVG 框架。",
      "shortSummary": "HuMo是一个统一的以人为中心的视频生成（HCVG）框架，旨在解决多模态输入下的数据稀缺和任务协作挑战。它通过构建高质量的配对数据集来解决数据问题，并采用两阶段渐进式训练范式。该范式包含“最小侵入式图像注入”以保留主体，以及“通过预测聚焦”策略以增强音视频同步。HuMo还引入了时间自适应无分类器引导策略进行灵活推理控制。实验证明，HuMo在各项子任务中超越了现有先进方法，实现了协同多模态条件控制。",
      "translated_title": "HuMo：通过协同多模态条件控制实现以人为中心的视频生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos from multimodal inputs, including text, image, and audio. Existing methods struggle to effectively coordinate these heterogeneous modalities due to two challenges: the scarcity of training data with paired triplet conditions and the difficulty of collaborating the sub-tasks of subject preservation and audio-visual sync with multimodal inputs. In this work, we present HuMo, a unified HCVG framework for collaborative multimodal control. For the first challenge, we construct a high-quality dataset with diverse and paired text, reference images, and audio. For the second challenge, we propose a two-stage progressive multimodal training paradigm with task-specific strategies. For the subject preservation task, to maintain the prompt following and visual generation abilities of the foundation model, we adopt the minimal-invasive image injection strategy. For the audio-visual sync task, besides the commonly adopted audio cross-attention layer, we propose a focus-by-predicting strategy that implicitly guides the model to associate audio with facial regions. For joint learning of controllabilities across multimodal inputs, building on previously acquired capabilities, we progressively incorporate the audio-visual sync task. During inference, for flexible and fine-grained multimodal control, we design a time-adaptive Classifier-Free Guidance strategy that dynamically adjusts guidance weights across denoising steps. Extensive experimental results demonstrate that HuMo surpasses specialized state-of-the-art methods in sub-tasks, establishing a unified framework for collaborative multimodal-conditioned HCVG. Project Page: https://phantom-video.github.io/HuMo."
    }
  ],
  "lastUpdated": "2025-09-13T09:27:36.003Z"
}