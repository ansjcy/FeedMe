{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "从像素到文字——迈向大规模原生视觉-语言基元 (原标题: From Pixels to Words -- Towards Native Vision-Language Primitives at Scale)",
      "link": "https://arxiv.org/abs/2510.14979",
      "pubDate": "Thu, 16 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-16T13:59:58.000Z",
      "creator": "Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu",
      "summary": "### 背景与挑战\n\n原生视觉-语言模型（VLMs）作为模块化VLMs的有力竞争者正在兴起，其发展受到模型架构和训练范式演变的影响。然而，其广泛探索和推广面临两大核心挑战：\n\n*   **根本限制**：原生VLMs与典型的模块化VLMs之间存在哪些根本性限制？这些障碍能在多大程度上被克服？\n*   **研究可及性**：如何使原生VLMs的研究更易于获取和民主化，从而加速该领域的进展？\n\n### 构建原生VLM的指导原则\n\n本文旨在阐明这些挑战，并概述构建原生VLMs的指导原则。一个有效的原生VLM基元应具备以下特性：\n\n*   **像素与文字对齐**：能够在共享语义空间内有效地对齐像素和文字表示。\n*   **模块优势整合**：无缝整合先前独立的视觉和语言模块的优势。\n*   **跨模态特性**：内在体现支持统一视觉-语言编码、对齐和推理的各种跨模态特性。\n\n### NEO：一种新型原生VLM家族\n\n基于上述指导原则，研究人员推出了NEO，一个从第一性原理构建的新型原生VLM家族。NEO在多个方面展现出卓越的能力：\n\n*   **性能媲美**：NEO能够与顶级模块化VLM在各种真实场景中竞争，展现出强大的性能。\n*   **高效视觉感知**：仅使用3.9亿个图像-文本示例，NEO就能有效地从头开始发展视觉感知能力。\n*   **冲突缓解**：它能够在一个根据精心设计的基元构建的密集、单一模型内部，有效缓解视觉-语言冲突。\n*   **生态系统构建**：NEO被定位为可扩展且强大的原生VLMs的基石，并配备了一套丰富的可重用组件，以促进一个经济高效且可扩展的生态系统。\n\n### 可用性\n\nNEO的代码和模型已公开发布，旨在促进该领域的研究和应用。",
      "shortSummary": "本文介绍了NEO，一个新型原生视觉-语言模型（VLM）家族，旨在解决原生VLM与模块化VLM之间的限制以及研究可及性问题。NEO从第一性原理构建，通过有效对齐像素与文字表示、整合视觉与语言模块优势，并支持统一跨模态推理，实现了与顶级模块化VLM相媲美的性能。它仅用3.9亿图像-文本示例便能高效发展视觉感知，并缓解模型内部的视觉-语言冲突。NEO及其可重用组件为大规模原生VLM提供了一个可扩展、经济高效的生态系统。",
      "translated_title": "从像素到文字——迈向大规模原生视觉-语言基元",
      "images": [],
      "contentSource": "完整文章",
      "content": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO."
    },
    {
      "title": "组合机器的智能体设计 (原标题: Agentic Design of Compositional Machines)",
      "link": "https://arxiv.org/abs/2510.14980",
      "pubDate": "Thu, 16 Oct 2025 13:59:58 GMT",
      "isoDate": "2025-10-16T13:59:58.000Z",
      "creator": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
      "summary": "# 组合机器的智能体设计\n\n本文探讨了大型语言模型（LLMs）在复杂机器设计领域的应用潜力，特别是通过“组合机器设计”这一任务。\n\n## 研究背景与目标\n\n*   **人类智能的体现与工程实践的基础：** 复杂机器的设计是人类智能的标志，也是工程实践的核心。\n*   **LLMs的创造能力：** 鉴于LLMs的最新进展，研究者们提出疑问：LLMs是否也能学习创造？\n*   **研究切入点——组合机器设计：** 本文通过组合机器设计来回答这一问题。该任务要求从标准化组件中组装机器，以满足在模拟物理环境中（如运动或操纵）的功能需求。\n\n## BesiegeField测试平台\n\n*   **引入：** 为支持这项研究，本文引入了BesiegeField，这是一个基于机器建造游戏Besiege构建的测试平台。\n*   **核心功能：**\n    *   **基于部件的建造：** 允许用户或智能体使用标准化部件进行机器组装。\n    *   **物理模拟：** 提供真实的物理环境模拟，以测试机器的功能性。\n    *   **奖励驱动评估：** 能够根据机器的表现（如能否完成特定任务）进行奖励评估。\n\n## LLMs的基准测试与关键能力\n\n*   **方法：** 研究者使用BesiegeField对最先进的LLMs进行了基准测试，并采用了智能体工作流（agentic workflows）。\n*   **成功所需关键能力：** 通过测试，研究者识别出LLMs在机器设计任务中取得成功所需的几项关键能力：\n    *   **空间推理：** 理解部件在三维空间中的位置和相互关系。\n    *   **策略性组装：** 规划并执行有效的组装策略以实现目标功能。\n    *   **指令遵循：** 准确理解并执行设计任务的详细指令。\n\n## 挑战与未来方向\n\n*   **当前模型的局限性：** 现有的开源LLMs在这些关键能力上表现不足，未能完全胜任组合机器设计任务。\n*   **强化学习（RL）的探索：** 作为改进途径，研究者探索了强化学习（RL）的应用。\n    *   **数据集构建：** 整理了一个“冷启动”数据集（cold-start dataset）。\n    *   **RL微调实验：** 进行了RL微调实验。\n*   **开放性挑战：** 本文强调了语言、机器设计和物理推理交叉领域中存在的开放性挑战，为未来的研究指明了方向。",
      "shortSummary": "本文探讨大型语言模型（LLMs）在组合机器设计中的应用，即从标准化组件组装机器以满足模拟物理环境中的功能需求。研究引入了基于游戏Besiege的测试平台BesiegeField，用于部件建造、物理模拟和奖励评估。基准测试发现，LLMs需具备空间推理、策略性组装和指令遵循能力，但当前开源模型表现不足。为改进，研究探索了强化学习微调，并指出了语言、机器设计与物理推理交叉领域的开放挑战。",
      "translated_title": "组合机器的智能体设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning."
    },
    {
      "title": "无需图像编辑对即可学习图像编辑模型 (原标题: Learning an Image Editing Model without Image Editing Pairs)",
      "link": "https://arxiv.org/abs/2510.14978",
      "pubDate": "Thu, 16 Oct 2025 13:59:57 GMT",
      "isoDate": "2025-10-16T13:59:57.000Z",
      "creator": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
      "summary": "## 无需图像编辑对即可学习图像编辑模型\n\n### 摘要\n\n本文提出了一种创新的训练范式，旨在解决当前图像编辑模型对大量输入-目标对数据集的依赖问题。传统方法需要监督式微调，而这些配对数据难以大规模获取，导致了训练瓶颈。现有的一些替代方案使用合成训练对，但这可能将预训练模型的伪影传播并放大到最终模型中。\n\n### 核心方法\n\n该研究的核心在于完全消除了对配对数据的需求，其方法包括：\n\n*   **直接优化扩散模型**：在训练过程中展开（unrolling）一个少步扩散模型进行直接优化。\n*   **利用视觉-语言模型（VLM）反馈**：VLM在训练中扮演关键角色，它针对每个输入和编辑指令，评估编辑是否遵循指令并保留了未更改的内容。\n*   **提供直接梯度**：VLM的评估结果为端到端优化提供了直接梯度。\n*   **引入分布匹配损失（DMD）**：为确保生成图像的视觉保真度，模型融入了DMD。这限制了生成的图像保持在预训练模型学习到的图像流形（image manifold）内。\n\n### 实验与结果\n\n研究人员在标准基准上对该方法进行了评估，并进行了广泛的消融研究。结果表明：\n\n*   在少步设置下，该方法在没有任何配对数据的情况下，其性能与那些在大量监督配对数据上训练的各种图像编辑扩散模型相当。\n*   在将相同的VLM用作奖励模型时，该方法还优于基于强化学习（RL）的技术，例如Flow-GRPO。",
      "shortSummary": "本文提出一种无需图像编辑对即可训练图像编辑模型的新范式。该方法通过在训练中展开少步扩散模型，并利用视觉-语言模型（VLM）的反馈提供直接梯度进行端到端优化。同时，引入分布匹配损失（DMD）以确保视觉保真度。实验表明，在没有配对数据的情况下，该方法在性能上与使用大量监督配对数据训练的模型相当，并优于某些基于强化学习的技术。",
      "translated_title": "无需图像编辑对即可学习图像编辑模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO."
    },
    {
      "title": "Ponimator：展开交互姿态以实现多功能人机交互动画 (原标题: Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation)",
      "link": "https://arxiv.org/abs/2510.14976",
      "pubDate": "Thu, 16 Oct 2025 13:59:56 GMT",
      "isoDate": "2025-10-16T13:59:56.000Z",
      "creator": "Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang",
      "summary": "本文介绍了一个名为 Ponimator 的框架，旨在通过利用近距离人机交互姿态来生成多功能交互动画。\n\n**核心思想与灵感**\n*   **背景洞察**：近距离的人机交互姿态蕴含丰富的上下文信息，人类可以凭直觉推断其语境，并预测过去和未来的动态，这得益于对人类行为的强大先验知识。\n*   **灵感来源**：受此观察启发，Ponimator 框架以近距离交互姿态为核心，实现多功能交互动画。\n\n**Ponimator 框架构成**\n*   **训练数据**：框架的训练数据来源于运动捕捉（mocap）交互数据集，包含紧密接触的两人姿态及其周围的时间上下文信息。\n*   **核心模型**：Ponimator 利用交互姿态先验知识，采用了两个条件扩散模型：\n    1.  **姿态动画器 (Pose Animator)**：该模型利用时间先验，从给定的交互姿态生成动态运动序列。\n    2.  **姿态生成器 (Pose Generator)**：该模型应用空间先验，在交互姿态不可用时，可以从单个姿态、文本或两者结合来合成交互姿态。\n\n**支持的任务与应用**\nPonimator 框架支持多种多样的任务，包括：\n*   基于图像的交互动画\n*   反应动画\n*   文本到交互的合成\n\n**目标与成果**\n*   **目标**：该框架旨在促进将高质量运动捕捉数据中的交互知识转移到开放世界场景中。\n*   **实验验证**：在各种数据集和应用上的实证实验表明，姿态先验的普适性以及 Ponimator 框架的有效性和鲁棒性。\n\n**其他信息**\n*   **接受情况**：该研究已被 ICCV 2025 接受。\n*   **项目页面**：[this https URL](this https URL)\n*   **研究领域**：计算机视觉与模式识别 (cs.CV)、图形学 (cs.GR)、机器人学 (cs.RO)。\n*   **引用方式**：arXiv:2510.14976 [cs.CV]。",
      "shortSummary": "Ponimator 是一个基于近距离交互姿态的框架，用于生成多功能人机交互动画。它利用运动捕捉数据训练的两个条件扩散模型：姿态动画器从交互姿态生成运动序列，姿态生成器则从单个姿态或文本合成交互姿态。该框架支持图像动画、反应动画和文本到交互的合成，旨在将高质量交互知识应用于开放世界场景，并已在ICCV 2025上被接受。",
      "translated_title": "Ponimator：展开交互姿态以实现多功能人机交互动画",
      "images": [],
      "contentSource": "完整文章",
      "content": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework."
    },
    {
      "title": "WithAnyone: 面向可控和身份一致的图像生成 (原标题: WithAnyone: Towards Controllable and ID Consistent Image Generation)",
      "link": "https://arxiv.org/abs/2510.14975",
      "pubDate": "Thu, 16 Oct 2025 13:59:54 GMT",
      "isoDate": "2025-10-16T13:59:54.000Z",
      "creator": "Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang",
      "summary": "# WithAnyone: 面向可控和身份一致的图像生成\n\n本文介绍了一种名为 WithAnyone 的新型扩散模型，旨在解决文本到图像生成领域中身份一致性生成面临的挑战。\n\n## 引言\n\n*   **背景与问题**\n    *   身份一致性生成是文本到图像研究的重要方向，旨在生成与参考身份高度匹配的图像。\n    *   现有模型在生成与参考身份一致的图像方面取得了一定成功，但普遍存在一个被称为“复制粘贴”（copy-paste）的失败模式。\n    *   “复制粘贴”问题源于缺乏包含同一人物多张图像的大规模配对数据集，导致大多数方法依赖于基于重建的训练。\n    *   这种依赖性使得模型倾向于直接复制参考人脸，而非在姿态、表情或光照等自然变化中保持身份一致性。\n    *   过度相似性损害了生成的可控性，并限制了生成的表达能力。\n\n## 主要贡献\n\n为解决上述限制，作者团队做出了以下三项主要贡献：\n\n1.  **构建大规模配对数据集 MultiID-2M**\n    *   专门为多人物场景设计，为每个身份提供多样化的参考图像。\n2.  **引入新基准**\n    *   该基准用于量化“复制粘贴”伪影，并衡量身份保真度与多样性之间的权衡。\n3.  **提出新颖的训练范式**\n    *   引入了一种带有对比身份损失（contrastive identity loss）的训练范式。\n    *   该范式利用配对数据来平衡身份保真度与生成多样性。\n\n## WithAnyone 模型\n\n*   上述贡献最终促成了 WithAnyone 模型的诞生。\n*   WithAnyone 是一个基于扩散的模型，能够有效缓解“复制粘贴”问题，同时保持高身份相似性。\n\n## 实验结果与验证\n\n*   **广泛的定性与定量实验**\n    *   实验结果表明，WithAnyone 显著减少了“复制粘贴”伪影。\n    *   提高了对姿态和表情的可控性。\n    *   保持了强大的感知质量。\n*   **用户研究**\n    *   用户研究进一步验证了 WithAnyone 方法在实现高身份保真度的同时，能够进行富有表现力的可控生成。",
      "shortSummary": "现有身份一致性图像生成模型因数据稀缺常出现“复制粘贴”问题，限制了可控性。WithAnyone 模型通过构建大规模配对数据集MultiID-2M、引入新基准以及提出带有对比身份损失的训练范式来解决此问题。作为一个扩散模型，WithAnyone 有效缓解了“复制粘贴”伪影，显著提高了姿态和表情的可控性，并保持了高身份保真度和感知质量，经广泛实验和用户研究验证。",
      "translated_title": "WithAnyone: 面向可控和身份一致的图像生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation."
    },
    {
      "title": "π-Flow：基于策略的模仿蒸馏少步生成 (原标题: pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation)",
      "link": "https://arxiv.org/abs/2510.14974",
      "pubDate": "Thu, 16 Oct 2025 13:59:51 GMT",
      "isoDate": "2025-10-16T13:59:51.000Z",
      "creator": "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi",
      "summary": "### π-Flow：基于策略的模仿蒸馏少步生成\n\n本文提出了一种名为 **π-Flow**（基于策略的流模型）的新方法，旨在解决少步扩散或基于流的生成模型在蒸馏过程中面临的挑战。\n\n#### 现有问题\n\n*   **格式不匹配**：传统的少步扩散或流模型通常将预测速度的教师模型蒸馏成预测去噪数据快捷方式的学生模型。\n*   **复杂蒸馏过程**：这种格式不匹配导致蒸馏过程复杂。\n*   **质量-多样性权衡**：现有方法常在生成质量和多样性之间做出权衡。\n\n#### π-Flow 方法\n\nπ-Flow 通过以下方式解决上述问题：\n\n1.  **修改学生模型输出层**：它修改了学生流模型的输出层，使其在一个时间步预测一个**无网络策略**（network-free policy）。\n2.  **动态流速度生成**：该策略随后在未来的子步中产生动态流速度，且开销可忽略不计。\n3.  **快速准确的ODE积分**：这使得在这些子步上能够进行快速准确的常微分方程（ODE）积分，而无需额外的网络评估。\n\n#### 模仿蒸馏方法\n\n为使策略的ODE轨迹与教师模型的轨迹匹配，π-Flow 引入了一种新颖的**模仿蒸馏**方法：\n\n*   它沿着策略的轨迹，使用标准的 $\\ell_2$ 流匹配损失，将策略的速度与教师模型的速度进行匹配。\n*   通过简单地模仿教师模型的行为，π-Flow 实现了稳定且可扩展的训练，并有效避免了质量-多样性权衡。\n\n#### 实验结果\n\nπ-Flow 在多个基准测试中展现出卓越性能：\n\n*   **ImageNet 256$^2$**：在1个网络函数评估（1-NFE）下，实现了2.85的FID分数，优于采用相同DiT架构的MeanFlow。\n*   **FLUX.1-12B 和 Qwen-Image-20B**：在4个NFE下，π-Flow 相比最先进的少步方法，显著提高了多样性，同时保持了教师模型级别的生成质量。",
      "shortSummary": "π-Flow 提出一种基于策略的模仿蒸馏方法，旨在解决少步生成模型中蒸馏复杂性和质量-多样性权衡问题。它通过修改学生模型输出层预测无网络策略，实现未来子步的动态流速度和快速ODE积分。通过新颖的模仿蒸馏，π-Flow 匹配策略与教师模型的速度，实现稳定训练。实验证明，π-Flow 在ImageNet 256$^2$上以1-NFE达到2.85 FID，并在FLUX.1-12B和Qwen-Image-20B上显著提升多样性，同时保持教师级质量。",
      "translated_title": "π-Flow：基于策略的模仿蒸馏少步生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models (pi-Flow). pi-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard ell_2 flow matching loss. By simply mimicking the teacher's behavior, pi-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256^2, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality."
    },
    {
      "title": "扩散大语言模型中KV缓存的注意力机制 (原标题: Attention Is All You Need for KV Cache in Diffusion LLMs)",
      "link": "https://arxiv.org/abs/2510.14973",
      "pubDate": "Thu, 16 Oct 2025 13:59:48 GMT",
      "isoDate": "2025-10-16T13:59:48.000Z",
      "creator": "Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen",
      "summary": "# 扩散大语言模型中KV缓存的注意力机制：Elastic-Cache\n\n本文研究了如何自适应地重新计算扩散大语言模型（DLMs）中的键值（KV）缓存，以在最大化预测准确性的同时最小化解码延迟。\n\n## 背景与问题\n\n传统的DLM解码器在每个去噪步骤和每个层都会重新计算所有token的QKV（查询、键、值），尽管KV状态在大多数步骤中，尤其是在浅层中，变化很小。这种做法导致了大量的冗余计算和高延迟。\n\n## 核心观察\n\n作者基于以下三点观察提出了解决方案：\n\n1.  **远距离`MASK` token的作用**：远距离的`MASK` token主要作为长度偏差，可以在活动预测窗口之外进行块级缓存，减少不必要的重计算。\n2.  **KV动态性与深度关系**：KV状态的动态性随网络深度的增加而增强，这表明从深层开始进行选择性刷新就足够了，而浅层可以更多地重用缓存。\n3.  **最受关注token的KV漂移**：最受关注的token表现出最小的KV漂移，这为其他token的缓存变化提供了一个保守的下限，可用于判断何时需要刷新。\n\n## Elastic-Cache 策略\n\n基于上述观察，本文提出了**Elastic-Cache**，这是一种无需训练、与架构无关的策略，它联合决定：\n\n*   **何时刷新**：通过对最受关注token进行注意力感知的漂移测试来决定。当漂移超过一定阈值时，触发缓存刷新。\n*   **何处刷新**：通过深度感知的调度来决定，该调度从选定的层开始重新计算，同时重用浅层缓存和窗口外`MASK`缓存，避免了全局刷新。\n\n## Elastic-Cache 的优势\n\n*   **自适应与层感知**：与固定周期方案不同，Elastic-Cache为扩散LLM执行自适应、层感知的缓存更新，更加高效。\n*   **减少冗余计算**：显著减少了不必要的计算，从而降低了计算成本。\n*   **加速解码**：在生成质量损失可忽略不计的情况下，大大加快了解码速度。\n*   **无需训练**：无需额外的训练即可应用于现有模型。\n*   **架构无关**：适用于多种DLM架构，具有良好的通用性。\n\n## 实验结果\n\n实验在LLaDA-Instruct、LLaDA-1.5和LLaDA-V模型上进行，涵盖了数学推理（GSM8K）和代码生成（HumanEval）任务，结果显示：\n\n*   **显著加速**：\n    *   在GSM8K（256个token）上加速 **8.7倍**。\n    *   在更长序列上加速 **45.1倍**。\n    *   在HumanEval上加速 **4.8倍**。\n*   **保持甚至提高准确性**：始终保持比基线更高的准确性。\n*   **更高吞吐量**：在GSM8K上比现有基于置信度的方法实现了 **6.8倍** 的吞吐量提升，同时保持了生成质量。\n\n## 结论\n\nElastic-Cache方法通过显著提高效率和吞吐量，同时保持生成质量，使得扩散大语言模型的实际部署成为可能。",
      "shortSummary": "本文提出Elastic-Cache，一种无需训练、与架构无关的策略，旨在优化扩散大语言模型（DLMs）中的KV缓存重计算。针对现有方法冗余计算问题，Elastic-Cache基于KV动态性、`MASK` token处理和最受关注token的漂移，自适应地决定何时何处刷新缓存。实验表明，该方法在数学推理和代码生成任务上实现了显著的解码加速（例如GSM8K上8.7倍，长序列上45.1倍），同时保持或提高了生成准确性，并大幅提升了吞吐量，从而促进了DLMs的实际部署。",
      "translated_title": "扩散大语言模型中KV缓存的注意力机制",
      "images": [],
      "contentSource": "完整文章",
      "content": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant {bf MASK} tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose {bf Elastic-Cache}, a training-free, architecture-agnostic strategy that jointly decides {when} to refresh (via an attention-aware drift test on the most-attended token) and {where} to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: 8.7times on GSM8K (256 tokens), 45.1times on longer sequences, and 4.8times on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput (6.8times on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs."
    },
    {
      "title": "TokDrift：当LLM以子词说话而代码以语法说话时 (原标题: TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar)",
      "link": "https://arxiv.org/abs/2510.14972",
      "pubDate": "Thu, 16 Oct 2025 13:59:45 GMT",
      "isoDate": "2025-10-16T13:59:45.000Z",
      "creator": "Yinxi Li, Yuntian Deng, Pengyu Nie",
      "summary": "# TokDrift：LLM代码子词分词与语法不匹配问题\n\n## 核心问题\n\n*   **分词器与语法的脱节**：用于代码的大型语言模型（LLMs）依赖于子词分词器（如字节对编码BPE）。这些分词器是从混合的自然语言文本和编程语言代码中学习的，其运作机制是基于统计而非编程语言的语法规则。\n*   **语义相同代码的不同分词**：由于这种统计驱动的特性，即使是语义完全相同的代码片段，也可能因为表面因素（例如空格、注释或标识符命名方式的微小变化）而被分词器处理成不同的子词序列。\n\n## TokDrift框架\n\n*   **目的**：为量化这种分词与语法不匹配所带来的影响，研究引入了名为“TokDrift”的框架。\n*   **方法**：TokDrift通过应用一系列“语义保留的重写规则”来创建代码变体。这些变体在功能和语义上与原始代码完全一致，但其分词方式却因重写规则而产生差异。\n\n## 主要发现\n\n*   **模型行为的显著转变**：研究在九个不同的代码LLMs上进行了实验，其中包括参数量超过300亿的大型模型。结果显示，即使是代码中微不足道的格式更改，也可能导致模型行为发生实质性的、不可预测的转变。\n*   **问题根源**：通过对模型进行层级分析，研究发现这一问题主要源于模型的早期嵌入层。在这些层中，子词分割过程未能有效地捕获编程语言的语法标记边界，从而导致了初始表示的偏差。\n\n## 结论与展望\n\n*   **隐性障碍**：研究明确指出，这种分词与语法的不匹配是阻碍代码LLMs实现可靠代码理解和生成的一个“隐性障碍”。\n*   **未来方向**：这些发现强调了未来代码LLMs在开发过程中，迫切需要采用“语法感知的分词方法”，以确保模型能够更准确、更稳定地处理和理解代码。",
      "shortSummary": "TokDrift研究发现，用于代码的LLM采用的子词分词器是统计驱动而非语法感知。这导致语义相同的代码片段因表面格式差异而分词不同，进而显著改变模型行为。该问题源于早期嵌入层，子词分割未能捕获语法边界。研究强调，这种分词不匹配是代码LLM可靠性的隐性障碍，亟需开发语法感知的分词方法。",
      "translated_title": "TokDrift：当LLM以子词说话而代码以语法说话时",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs."
    },
    {
      "title": "LLM作为可扩展的通用模拟器，用于演进数字智能体训练 (原标题: LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training)",
      "link": "https://arxiv.org/abs/2510.14969",
      "pubDate": "Thu, 16 Oct 2025 13:59:38 GMT",
      "isoDate": "2025-10-16T13:59:38.000Z",
      "creator": "Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang",
      "summary": "### LLM作为可扩展的通用模拟器，用于演进数字智能体训练\n\n**核心问题**\n\n*   数字智能体需要大量多样化的UI轨迹数据才能在真实世界任务中实现泛化。\n*   然而，从人工标注、基础设施到工程角度来看，收集此类数据成本极高。\n\n**解决方案：UI-Simulator**\n\n本文引入了**UI-Simulator**，一个可扩展的范式，旨在解决上述数据收集难题。它通过生成结构化的UI状态和转换，大规模合成训练轨迹。\n\nUI-Simulator的核心组件包括：\n\n*   **数字世界模拟器**：用于生成多样化的UI状态。\n*   **引导式展开过程**：确保连贯的探索。\n*   **轨迹封装器**：生成高质量、多样化的轨迹，用于智能体训练。\n\n**增强策略：UI-Simulator-Grow**\n\n为了进一步提升效率和可扩展性，作者提出了**UI-Simulator-Grow**。这是一种有针对性的扩展策略，它通过以下方式实现更快速、数据效率更高的扩展：\n\n*   优先处理高影响力任务。\n*   合成信息丰富的轨迹变体。\n\n**实验结果与性能**\n\n在WebArena和AndroidWorld上的实验验证了UI-Simulator及其扩展策略的有效性：\n\n*   **UI-Simulator**：\n    *   性能与使用真实UI训练的开源智能体相当或超越。\n    *   即使使用较弱的教师模型，也展现出显著更好的鲁棒性。\n*   **UI-Simulator-Grow**：\n    *   仅使用Llama-3-8B-Instruct作为基础模型，就能达到Llama-3-70B-Instruct的性能。\n\n**结论与潜力**\n\n这些结果凸显了有针对性的合成扩展范式在持续高效增强数字智能体方面的巨大潜力。",
      "shortSummary": "数字智能体训练面临UI轨迹数据收集成本高昂的挑战。本文提出**UI-Simulator**，一个可扩展的范式，通过模拟UI状态和转换大规模合成训练轨迹。在此基础上，**UI-Simulator-Grow**通过优先处理高影响力任务，实现更快速、数据效率更高的扩展。实验表明，UI-Simulator在鲁棒性上超越现有智能体，而UI-Simulator-Grow能以更小的模型达到大型模型的性能，展现了其在持续提升数字智能体方面的巨大潜力。",
      "translated_title": "LLM作为可扩展的通用模拟器，用于演进数字智能体训练",
      "images": [],
      "contentSource": "完整文章",
      "content": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce UI-Simulator, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose UI-Simulator-Grow, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents."
    },
    {
      "title": "基于信息增益的策略优化：一种用于多轮LLM智能体的简单有效方法 (原标题: Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents)",
      "link": "https://arxiv.org/abs/2510.14967",
      "pubDate": "Thu, 16 Oct 2025 13:59:32 GMT",
      "isoDate": "2025-10-16T13:59:32.000Z",
      "creator": "Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying",
      "summary": "### 基于信息增益的策略优化 (Information Gain-based Policy Optimization)\n\n**引言**\n\n大型语言模型（LLM）智能体正越来越多地通过强化学习（RL）进行训练，以增强其通过工具使用与外部环境交互的能力。这在需要多轮推理和知识获取的搜索场景中尤为重要。\n\n**现有方法的问题**\n\n然而，现有方法通常依赖于仅在最终答案时提供的结果导向奖励，这导致了奖励稀疏性。在多轮交互设置中，长轨迹会加剧两个关键问题：\n\n*   **优势崩溃 (Advantage Collapse)**：所有rollout都收到相同的奖励，无法提供有用的学习信号。\n*   **缺乏细粒度信用分配 (Lack of Fine-grained Credit Assignment)**：轮次之间的依赖关系变得模糊，尤其是在长周期任务中。\n\n**提出的方法：基于信息增益的策略优化 (IGPO)**\n\n本文提出了基于信息增益的策略优化（IGPO），这是一个简单而有效的RL框架，旨在为多轮智能体训练提供密集且内在的监督。\n\n*   **核心机制**：\n    *   IGPO将每次交互轮次建模为获取关于真实信息的增量过程。\n    *   它将轮次级别的奖励定义为策略生成正确答案的概率的边际增加。\n*   **关键优势**：\n    *   与依赖外部奖励模型或昂贵的蒙特卡洛估计的现有过程级奖励方法不同，IGPO直接从模型自身的信念更新中推导出内在奖励。\n    *   这些内在的轮次级别奖励与结果级别监督相结合，形成密集的奖励轨迹，有效解决了奖励稀疏性和信用分配问题。\n\n**实验结果**\n\n通过在域内和域外基准上进行广泛实验，结果表明IGPO在多轮场景中持续优于强大的基线方法，实现了更高的准确性和改进的样本效率。\n\n**研究领域**\n\n该研究属于计算与语言（cs.CL）、人工智能（cs.AI）和机器学习（cs.LG）领域。",
      "shortSummary": "本文提出了基于信息增益的策略优化（IGPO），旨在解决多轮LLM智能体在强化学习中面临的奖励稀疏性和信用分配问题。IGPO将每次交互轮次建模为信息获取的增量过程，并直接从模型信念更新中推导轮次级别的内在奖励，提供密集监督。实验证明，IGPO在多轮场景中显著优于现有基线，提高了准确性和样本效率。",
      "translated_title": "基于信息增益的策略优化：一种用于多轮LLM智能体的简单有效方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency."
    },
    {
      "title": "循环深度模型的高效并行采样器及其与扩散语言模型的联系 (原标题: Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models)",
      "link": "https://arxiv.org/abs/2510.14961",
      "pubDate": "Thu, 16 Oct 2025 13:59:07 GMT",
      "isoDate": "2025-10-16T13:59:07.000Z",
      "creator": "Jonas Geiping, Xinyu Yang, Guinan Su",
      "summary": "## 循环深度模型与扩散语言模型的新进展\n\n### 引言\n\n循环深度模型（也称为通用或循环Transformer）的特点在于能够通过重复层来增加计算量。最近的预训练研究表明，这些架构能够适应现代语言建模任务，并在推理任务中展现出优势。\n\n### 核心研究与创新\n\n本研究深入探讨了循环深度模型与扩散语言模型之间的关系。基于它们之间的相似性，研究人员开发了一种新的**扩散强制采样器（diffusion forcing sampler）**，旨在加速这些模型的生成过程。\n\n### 扩散强制采样器的工作原理与优势\n\n*   **并行解码与精炼**：该采样器在模型的每次前向传播中解码新的token，同时这些token的潜在状态可以通过循环机制并行地进行进一步精炼。\n*   **理论表达力**：从理论上讲，在现代硬件上使用相同的时间预算，该采样器实现的生成能力比基线自回归生成更具表达力。\n*   **实际应用与性能提升**：\n    *   该采样器基于扩散文献中的原理，可以直接应用于现有的3.5B循环深度Transformer，**无需任何调优**。\n    *   应用后，生成速度可实现**高达5倍的加速**。\n\n### 主要发现与意义\n\n本研究的发现具有双重意义：\n\n1.  **高效并行推理机制**：它为循环深度模型在推理时并行化额外计算提供了一种高效的机制。\n2.  **模型新视角**：它表明循环深度模型可以被自然地视为强大的连续（尽管是因果的）扩散语言模型。",
      "shortSummary": "本研究探讨了循环深度模型与扩散语言模型的关系，并提出了一种新的扩散强制采样器。该采样器通过在每次前向传播中并行解码新token并循环精炼其潜在状态，显著加速了生成过程。它无需调优即可应用于现有3.5B循环深度Transformer，实现高达5倍的加速。这不仅为循环深度模型提供了高效的并行推理机制，也表明它们可被视为强大的因果扩散语言模型。",
      "translated_title": "循环深度模型的高效并行采样器及其与扩散语言模型的联系",
      "images": [],
      "contentSource": "完整文章",
      "content": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models."
    },
    {
      "title": "MathCanvas：用于多模态数学推理的内在视觉思维链 (原标题: MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning)",
      "link": "https://arxiv.org/abs/2510.14958",
      "pubDate": "Thu, 16 Oct 2025 13:58:58 GMT",
      "isoDate": "2025-10-16T13:58:58.000Z",
      "creator": "Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li",
      "summary": "## MathCanvas：用于多模态数学推理的内在视觉思维链\n\n### 摘要\n\n大型语言模型（LLMs）在文本推理方面表现出色，但在几何等本质上依赖视觉辅助的数学领域中却面临挑战。现有的视觉思维链（VCoT）方法通常受限于僵化的外部工具，或无法生成复杂问题解决所需的高保真、策略性图表。\n\n### MathCanvas 框架\n\n为解决这一问题，研究人员引入了 MathCanvas，这是一个全面的框架，旨在赋予统一的大型多模态模型（LMMs）内在的数学 VCoT 能力。该方法分为两个阶段：\n\n1.  **视觉操作阶段（预训练）**：\n    *   模型在一个包含 1520 万对数据的新型语料库上进行预训练。\n    *   该语料库包括：\n        *   **MathCanvas-Imagen (1000 万对)**：包含标题到图表的配对，用于掌握图表生成。\n        *   **MathCanvas-Edit (520 万对)**：包含分步编辑轨迹，用于掌握图表编辑。\n\n2.  **战略性视觉辅助推理阶段（微调）**：\n    *   模型在一个新的包含 21.9 万个示例的 MathCanvas-Instruct 数据集上进行微调。\n    *   该数据集包含交错的视觉-文本推理路径，旨在教授模型何时以及如何利用视觉辅助。\n\n### 评估与成果\n\n*   **MathCanvas-Bench**：为了促进严格的评估，研究人员引入了一个具有挑战性的基准测试，包含 3000 个问题，要求模型生成交错的视觉-文本解决方案。\n*   **BAGEL-Canvas 模型**：在该框架下训练的模型 BAGEL-Canvas，在 MathCanvas-Bench 上比强大的 LMM 基线实现了 86% 的相对改进。\n*   **泛化能力**：该模型还展示了对其他公共数学基准测试的优秀泛化能力。\n\n### 结论\n\n这项工作提供了一个完整的工具包——包括框架、数据集和基准测试——旨在解锁 LMMs 中复杂、类人化的视觉辅助推理能力。",
      "shortSummary": "MathCanvas 框架旨在通过赋予大型多模态模型（LMMs）内在的视觉思维链（VCoT）能力，解决其在视觉依赖型数学（如几何）上的不足。该框架包含两个阶段：视觉操作（预训练图表生成和编辑）和战略性视觉辅助推理（微调何时使用视觉辅助）。通过 MathCanvas-Bench 基准测试，其模型 BAGEL-Canvas 比现有 LMM 基线实现了 86% 的相对改进，并展现出良好的泛化能力，为 LMMs 的类人视觉辅助推理提供了完整工具包。",
      "translated_title": "MathCanvas：用于多模态数学推理的内在视觉思维链",
      "images": [],
      "contentSource": "完整文章",
      "content": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/"
    },
    {
      "title": "RealDPO：真假难辨，偏好定夺 (原标题: RealDPO: Real or Not Real, that is the Preference)",
      "link": "https://arxiv.org/abs/2510.14955",
      "pubDate": "Thu, 16 Oct 2025 13:58:25 GMT",
      "isoDate": "2025-10-16T13:58:25.000Z",
      "creator": "Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu",
      "summary": "# RealDPO：通过偏好学习提升视频生成模型的运动真实感\n\n## 引言\n视频生成模型在合成质量方面取得了显著进步，但生成复杂、自然、流畅且上下文一致的运动仍是关键挑战。生成运动与真实世界运动之间的差距限制了这些模型的实际应用。\n\n## RealDPO 方法概述\n为解决现有模型在复杂运动合成方面的不足，研究引入了 **RealDPO**，这是一种新颖的对齐范式。\n*   **核心理念：** RealDPO 利用真实世界数据作为偏好学习的正面样本，旨在实现更准确的运动合成。\n*   **与传统方法的区别：** 与提供有限纠正反馈的传统监督微调（SFT）不同，RealDPO 采用直接偏好优化（DPO）方法。\n*   **机制：** RealDPO 结合了定制的损失函数来增强运动的真实感。它通过对比真实世界视频与模型生成的错误输出，实现迭代式的自我修正，从而逐步提升运动质量。\n\n## RealAction-5K 数据集\n为了支持复杂运动合成的训练后阶段，研究提出了 **RealAction-5K** 数据集。\n*   **特点：** 这是一个精心策划的高质量视频数据集，专门用于捕捉人类日常活动中丰富而精确的运动细节。\n\n## 实验结果\n广泛的实验结果表明，与最先进的模型和现有偏好优化技术相比，RealDPO 在以下方面取得了显著改进：\n*   视频质量\n*   文本对齐度\n*   运动真实感\n\n## 相关领域\n该研究主要涉及计算机视觉与模式识别（cs.CV）以及人工智能（cs.AI）领域。",
      "shortSummary": "RealDPO是一种新颖的视频生成模型对齐范式，旨在解决复杂运动合成的挑战。它利用真实世界数据作为DPO的正面样本，通过迭代修正提升运动真实感。研究还提出了RealAction-5K数据集。实验证明，RealDPO显著提高了视频质量、文本对齐和运动真实感。",
      "translated_title": "RealDPO：真假难辨，偏好定夺",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques."
    },
    {
      "title": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进 (原标题: DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation)",
      "link": "https://arxiv.org/abs/2510.14949",
      "pubDate": "Thu, 16 Oct 2025 13:56:55 GMT",
      "isoDate": "2025-10-16T13:56:55.000Z",
      "creator": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng",
      "summary": "### DialectGen：多模态生成中方言鲁棒性的基准测试与改进\n\n本研究深入探讨了多模态生成模型在处理方言文本输入时的有效性，并提出了一种创新的改进策略。\n\n**研究背景与问题：**\n*   英语等接触语言展现出丰富的区域方言变体，方言使用者在与生成模型交互时经常使用这些方言。\n*   然而，当前的多模态生成模型是否能有效处理方言文本输入，是一个尚未充分解决的关键问题。\n\n**研究方法与基准：**\n*   为了系统地研究这一问题，研究团队构建了一个名为 **DialectGen** 的新型大规模基准数据集。\n*   该基准涵盖了六种常见的英语方言，旨在全面评估模型的方言处理能力。\n*   研究人员与方言使用者合作，收集并验证了超过4200个独特的提示词，确保了数据的真实性和多样性。\n*   随后，对17个主流的图像和视频生成模型进行了广泛评估。\n\n**主要发现：**\n*   评估结果显示，当前最先进的多模态生成模型在提示词中仅使用一个方言词时，其性能会显著下降，降幅在 **32.26% 到 48.17%** 之间。\n*   常见的缓解方法，如模型微调（fine-tuning）和提示词重写（prompt rewriting），只能将方言性能提高很小的幅度（< 7%）。\n*   更重要的是，这些传统方法在提升方言性能的同时，可能会导致标准美式英语（SAE）性能的显著下降，从而牺牲了模型的通用性。\n\n**提出的解决方案：**\n*   为了克服上述挑战，本研究设计了一种通用的 **基于编码器的缓解策略**，专门用于多模态生成模型。\n*   该方法的核心思想是教会模型识别和理解新的方言特征，同时精心设计以确保能够完整地保留标准美式英语的性能，避免了传统方法的弊端。\n\n**实验结果：**\n*   在Stable Diffusion 1.5等代表性模型上的实验结果令人鼓舞。\n*   所提出的方法能够同时将五种方言的性能提升到与标准美式英语（SAE）相当的水平，实现了显著的 **+34.4%** 的性能提升。\n*   尤为重要的是，在提升方言性能的同时，该方法对标准美式英语性能的损害几乎为零，展现了其高效性和鲁棒性。\n\n**结论：**\n本研究不仅揭示了当前多模态生成模型在方言鲁棒性方面的显著不足，而且成功提出了一种创新的、基于编码器的缓解策略。该策略能够显著提升模型处理方言输入的能力，同时有效保持了标准语言的性能，为未来多模态生成模型的方言适应性研究奠定了基础。",
      "shortSummary": "本研究构建了DialectGen基准，评估了多模态生成模型在处理六种英语方言时的鲁棒性。结果显示，当前模型在方言输入下性能显著下降（32.26%-48.17%），且现有缓解方法效果有限并可能损害标准英语性能。为此，研究提出了一种通用的基于编码器的缓解策略，该策略能将五种方言的性能提升至与标准美式英语相当（+34.4%），且对标准美式英语性能几乎无损，显著提升了模型的方言适应性。",
      "translated_title": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进",
      "images": [],
      "contentSource": "完整文章",
      "content": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (&lt; 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance."
    },
    {
      "title": "LaSeR：基于末尾词元自奖励的强化学习 (原标题: LaSeR: Reinforcement Learning with Last-Token Self-Rewarding)",
      "link": "https://arxiv.org/abs/2510.14943",
      "pubDate": "Thu, 16 Oct 2025 13:55:11 GMT",
      "isoDate": "2025-10-16T13:55:11.000Z",
      "creator": "Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin",
      "summary": "# LaSeR：基于末尾词元自奖励的强化学习\n\n## 摘要\n\n本文介绍了一种名为 LaSeR（Reinforcement Learning with Last-Token Self-Rewarding）的新型强化学习算法，旨在解决大型语言模型（LLMs）在推理和自我验证方面的效率问题。\n\n## 背景与问题\n\n*   **可验证奖励强化学习 (RLVR)**：RLVR 已成为增强 LLM 推理能力的核心范式。\n*   **测试时验证信号缺失**：为了解决测试时缺乏验证信号的问题，现有研究将模型自我验证能力的训练整合到标准的 RLVR 过程中，从而在单个 LLM 中统一推理和验证能力。\n*   **效率低下**：然而，以往的方法要求 LLM 使用两个独立的提示模板顺序生成解决方案和自我验证，这显著降低了效率。\n\n## 核心洞察与理论基础\n\n*   **理论揭示**：本文从理论上揭示，自我验证的 RL 目标闭式解可以简化为一个非常简单的形式。\n*   **末尾词元自奖励分数**：解决方案的真实推理奖励等于其“末尾词元自奖励分数”。\n*   **计算方式**：该分数通过计算策略模型在解决方案的最后一个词元处，对任何预设词元分配的下一个词元对数概率与一个预先计算的常数之间的差异来计算，并按 KL 系数进行缩放。\n\n## LaSeR 算法\n\n*   **算法名称**：LaSeR (Reinforcement Learning with Last-Token Self-Rewarding)。\n*   **核心机制**：LaSeR 算法通过一个均方误差 (MSE) 损失来增强原始的 RLVR 损失。\n*   **目标**：该 MSE 损失旨在使末尾词元自奖励分数与基于验证器的推理奖励对齐，从而联合优化 LLM 的推理和自奖励能力。\n*   **应用**：优化后的自奖励分数可以在训练和测试阶段使用，以提升模型性能。\n*   **效率优势**：LaSeR 仅在生成后立即从最后一个词元的预测下一个词元概率分布中推导出这些分数，仅产生一次额外词元推理的最小额外成本。\n\n## 实验结果\n\n*   **性能提升**：实验表明，LaSeR 方法不仅提高了模型的推理性能。\n*   **自我奖励能力**：还赋予模型显著的自我奖励能力。\n*   **推理时扩展性能**：从而提升了其推理时的扩展性能。\n\n## 总结\n\nLaSeR 提供了一种高效且有效的方法，通过引入末尾词元自奖励机制，统一并优化了 LLM 的推理和自我验证过程，显著提高了模型的整体性能和效率。",
      "shortSummary": "LaSeR 是一种新型强化学习算法，旨在提高大型语言模型（LLMs）的推理和自我验证效率。它理论上揭示了解决方案的真实推理奖励可简化为“末尾词元自奖励分数”，该分数通过最后一个词元的下一个词元概率分布计算。LaSeR 通过一个均方误差（MSE）损失将此分数与验证器奖励对齐，联合优化推理和自奖励能力。实验证明，LaSeR 显著提升了 LLM 的推理性能、自我奖励能力及推理效率，且仅需极小的额外计算成本。",
      "translated_title": "LaSeR：基于末尾词元自奖励的强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance."
    },
    {
      "title": "GroundedPRM：树引导和保真度感知的步骤级推理过程奖励建模 (原标题: GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning)",
      "link": "https://arxiv.org/abs/2510.14942",
      "pubDate": "Thu, 16 Oct 2025 13:54:07 GMT",
      "isoDate": "2025-10-16T13:54:07.000Z",
      "creator": "Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp",
      "summary": "# GroundedPRM：树引导和保真度感知的步骤级推理过程奖励建模\n\n## 1. 引言与现有挑战\n\n过程奖励模型（PRMs）旨在通过监督中间步骤和识别错误来改进大型语言模型（LLMs）的多步骤推理能力。然而，构建有效的PRMs面临显著挑战，主要原因在于缺乏可扩展、高质量的标注数据。\n\n**现有方法的局限性：**\n*   **高成本的人工标注：** 依赖昂贵的人力资源。\n*   **LLM自评估：** 容易产生“幻觉”（hallucination），导致不准确的监督信号。\n*   **蒙特卡洛（MC）估计：** 仅根据最终结果推断步骤质量，常因信用归因不当而引入噪声和错位的监督。\n\n**核心限制：** 这些问题导致了PRMs的三个核心限制：奖励噪声大、事实保真度低、与步骤级推理目标不一致。\n\n## 2. GroundedPRM 框架\n\nGroundedPRM 是一种树引导和保真度感知的框架，用于自动过程监督，旨在解决上述挑战。其核心机制包括：\n\n### 2.1 减少奖励噪声与精细信用分配：蒙特卡洛树搜索 (MCTS)\n*   **方法：** 通过构建结构化的推理路径，GroundedPRM 利用蒙特卡洛树搜索（MCTS）来减少奖励噪声。\n*   **优势：** MCTS 能够实现更精细的信用分配，确保奖励信号更准确地反映每个步骤的贡献。\n\n### 2.2 消除幻觉监督：外部工具验证\n*   **方法：** 为了消除LLM自评估中可能产生的幻觉监督，GroundedPRM 使用一个外部工具来验证每个中间步骤。\n*   **优势：** 这提供了基于执行的正确性信号（execution-grounded correctness signals），确保了事实的准确性和保真度。\n\n### 2.3 结合步骤级验证与全局结果评估：混合奖励聚合机制\n*   **方法：** GroundedPRM 设计了一种混合奖励聚合机制，将基于工具的验证结果与MCTS导出的反馈相结合。\n*   **优势：** 这种机制能够同时考虑步骤级的正确性（通过工具验证）和全局结果的评估（通过MCTS反馈），提供更全面的奖励信号。\n\n### 2.4 提升可解释性与兼容性：生成式奖励信号格式\n*   **方法：** 最终，GroundedPRM 将奖励信号格式化为一种增强了理由（rationale-enhanced）的生成式结构。\n*   **优势：** 这有助于提高奖励信号的可解释性，并使其与指令微调的LLMs兼容，便于模型理解和利用。\n\n## 3. 实验结果与性能\n\nGroundedPRM 在效率和性能方面均展现出卓越表现：\n*   **数据效率：** 仅使用 40K 个自动标注样本进行训练，这仅是表现最佳的自动标注PRM所用数据量的10%。\n*   **性能提升：** 尽管数据量大幅减少，GroundedPRM 在 ProcessBench 上的平均性能仍实现了高达 26% 的相对提升。\n*   **超越人工标注PRMs：** 当用于奖励引导的贪婪搜索时，GroundedPRM 甚至超越了使用人工标注监督训练的PRMs。\n\n## 4. 结论\n\nGroundedPRM 提供了一条可扩展且可验证的路径，以实现高质量的步骤级推理。它通过创新的树引导和保真度感知框架，有效解决了现有PRMs在奖励噪声、事实保真度和对齐方面的局限性，为LLMs的多步推理能力发展开辟了新方向。",
      "shortSummary": "GroundedPRM 提出了一种树引导和保真度感知的框架，旨在解决现有过程奖励模型（PRMs）在LLMs多步推理中面临的奖励噪声、低事实保真度和错位问题。它通过蒙特卡洛树搜索（MCTS）构建结构化推理路径，并利用外部工具验证中间步骤，结合混合奖励聚合机制。GroundedPRM 仅用少量自动标注数据，在ProcessBench上实现了高达26%的性能提升，甚至超越了人工标注的PRMs，为高质量步骤级推理提供了可扩展且可验证的方案。",
      "translated_title": "GroundedPRM：树引导和保真度感知的步骤级推理过程奖励建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning."
    },
    {
      "title": "利用上下文感知缩放定律预测任务性能 (原标题: Predicting Task Performance with Context-aware Scaling Laws)",
      "link": "https://arxiv.org/abs/2510.14919",
      "pubDate": "Thu, 16 Oct 2025 13:35:18 GMT",
      "isoDate": "2025-10-16T13:35:18.000Z",
      "creator": "Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang",
      "summary": "### 利用上下文感知缩放定律预测任务性能\n\n**引言**\n传统缩放定律在理解大型语言模型（LLMs）方面发挥了重要作用，它们将交叉熵损失等上游指标与模型大小、训练数据和计算量等设计因素联系起来。然而，这些传统定律未能有效捕捉下游任务的性能，而上下文在下游任务中扮演着至关重要的角色。\n\n**研究目标与方法**\n本文提出了一种直接且可解释的框架，旨在联合建模下游任务性能，将其视为训练计算量和所提供上下文的函数。该框架旨在弥补传统缩放定律在下游任务预测方面的不足。\n\n**实验验证**\n为了验证所提出的框架，研究人员进行了广泛的实证验证：\n*   **模型选择：** 使用了Llama-2-7B和Llama-2-13B的扩展上下文变体。\n*   **数据集：** 在涵盖三种不同任务的65,500个独特实例上进行了拟合。\n*   **任务类型：**\n    *   算术推理\n    *   常识推理\n    *   机器翻译\n\n**主要发现**\n实验结果有力地证明了该框架的有效性：\n*   **准确性：** 框架能够准确地建模分布内的下游任务性能。\n*   **泛化能力：** 它在三个数量级的训练计算量上表现出良好的泛化能力。\n*   **外推能力：** 随着上下文量的增加，该框架能够可靠地外推模型性能。\n\n**研究意义**\n这些发现为训练计算量和上下文利用之间的相互作用提供了宝贵的见解。它们为设计更高效、更适用于各种下游任务的长上下文大型语言模型提供了重要的指导。\n\n**代码可用性**\n本研究的代码已公开。",
      "shortSummary": "本文提出一个上下文感知的缩放定律框架，旨在解决传统缩放定律无法预测大型语言模型下游任务性能的问题。该框架将下游性能建模为训练计算量和上下文的函数。通过在Llama-2模型上对算术推理、常识推理和机器翻译等任务进行验证，结果表明该框架能准确建模、泛化并可靠外推性能。这为设计高效的长上下文LLM提供了关键指导。",
      "translated_title": "利用上下文感知缩放定律预测任务性能",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling."
    },
    {
      "title": "通过判别式验证实现预算感知测试时扩展 (原标题: Budget-aware Test-time Scaling via Discriminative Verification)",
      "link": "https://arxiv.org/abs/2510.14913",
      "pubDate": "Thu, 16 Oct 2025 13:30:02 GMT",
      "isoDate": "2025-10-16T13:30:02.000Z",
      "creator": "Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang",
      "summary": "### 预算感知测试时扩展：判别式验证的混合方法\n\n**背景与问题：**\n*   **测试时扩展（Test-time scaling）** 是提升大型语言模型（LLMs）在复杂推理任务上性能的有效策略。\n*   然而，当前最先进的方法通常依赖于**生成式验证器（generative verifiers）** 从候选解决方案池中选择最佳方案，这导致了**过高的计算成本**，严重限制了其实用性。\n\n**核心贡献与方法：**\n*   本文将研究重点转向一种更具**预算感知（budget-aware）** 的范式：**判别式验证（discriminative verification）**。\n*   通过深入的实证分析，研究发现：\n    *   单独使用判别式验证器可能表现不佳。\n    *   但当将其与**自洽性（self-consistency）** 结合，形成一种**混合方法**时，能够创建一个强大且高效的测试时扩展机制。\n\n**主要发现与优势：**\n*   **显著的性能提升：** 在固定计算预算下，这种混合方法显著超越了最先进的生成式验证方法。\n    *   例如，在AIME2025数据集上，其准确率提升高达**15.3%**。\n*   **实用性和效率：** 研究结果表明，对于实际的、真实世界的应用，采用判别式验证的预算感知扩展不仅是自洽性方法的一个“免费”升级，而且是比昂贵的生成式技术更有效、更高效的替代方案。\n\n**代码可用性：**\n*   相关代码已在指定URL提供。\n\n**注意：** 文章内容中未包含有效的实际图片链接，因此详细摘要中不包含任何图片。",
      "shortSummary": "针对大型语言模型（LLMs）测试时扩展中生成式验证器计算成本高昂的问题，本文提出了一种预算感知的判别式验证方法。研究发现，将判别式验证与自洽性结合的混合方法，在固定计算预算下，显著优于现有生成式验证技术，例如在AIME2025上准确率提升高达15.3%。这为实际应用提供了一种更高效、更经济的性能提升方案，是自洽性方法的“免费”升级。",
      "translated_title": "通过判别式验证实现预算感知测试时扩展",
      "images": [],
      "contentSource": "完整文章",
      "content": "Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification."
    },
    {
      "title": "VLA^2：通过代理框架赋能视觉-语言-动作模型以处理未见概念操作 (原标题: VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation)",
      "link": "https://arxiv.org/abs/2510.14902",
      "pubDate": "Thu, 16 Oct 2025 13:18:34 GMT",
      "isoDate": "2025-10-16T13:18:34.000Z",
      "creator": "Han Zhao, Jiaxuan Zhang, Wenxuan Song, Pengxiang Ding, Donglin Wang",
      "summary": "### 摘要\n\n当前预训练在大量机器人数据上的视觉-语言-动作（VLA）模型在多任务处理和对视觉与语言指令变化的泛化方面表现出色。然而，当它们面临训练数据之外的物体概念时，例如数据集中未见的物体描述和纹理，其成功率会显著下降。\n\n### 提出的解决方案：VLA^2 代理框架\n\n为了解决这一泛化失败问题，本文提出了一种新颖的代理框架 VLA^2。\n\n*   **核心思想**：VLA^2 利用 OpenVLA 作为其执行骨干。\n*   **关键机制**：该框架有效利用外部模块，如网络检索和物体检测，为 VLA 模型提供关于目标物体的视觉和文本知识。\n*   **目的**：通过这种方式，VLA^2 旨在缓解处理分布外（out-of-distribution, OOD）物体时的泛化失败。\n\n### 评估与基准\n\n*   **环境**：研究人员基于 LIBERO 模拟环境构建了一个新的评估基准。\n*   **新颖性**：该基准引入了新的物体和物体描述，以测试方法的有效性。\n*   **难度级别**：基准被设计为包含三个难度级别。\n\n### 实验结果\n\n*   **性能超越**：VLA^2 框架在所设计的硬级别泛化基准上成功超越了当前的最新（state-of-the-art, SOTA）模型。\n*   **显著提升**：\n    *   与独立的 OpenVLA 基线相比，VLA^2 在硬级别基准上的成功率提高了 44.2%。\n    *   在所有定制环境中，平均成功率提高了 20.2%。\n*   **无性能下降**：在领域内（in-domain）任务上，VLA^2 没有出现任何性能下降。\n\n### 结论\n\nVLA^2 代理框架通过集成外部知识模块，显著提升了 VLA 模型在处理未见物体概念时的泛化能力，为解决机器人操作中的 OOD 问题提供了有效途径。",
      "shortSummary": "VLA^2 提出了一种代理框架，旨在解决当前视觉-语言-动作（VLA）模型在处理训练数据外未见物体概念时泛化能力不足的问题。该框架以 OpenVLA 为执行骨干，并整合了网络检索和物体检测等外部模块，为 VLA 提供目标物体的视觉和文本知识。在 LIBERO 模拟环境的硬级别泛化基准上，VLA^2 成功超越了现有SOTA模型，相较于OpenVLA基线，成功率提升了44.2%，平均提升20.2%，且未影响领域内任务性能。",
      "translated_title": "VLA^2：通过代理框架赋能视觉-语言-动作模型以处理未见概念操作",
      "images": [],
      "contentSource": "完整文章",
      "content": "Current vision-language-action (VLA) models, pre-trained on large-scale robotic data, exhibit strong multi-task capabilities and generalize well to variations in visual and language instructions for manipulation. However, their success rate drops significantly when faced with object concepts outside the training data, such as unseen object descriptions and textures in the dataset. To address this, we propose a novel agentic framework, VLA^2, which leverages OpenVLA as the execution backbone and effectively leverages external modules such as web retrieval and object detection to provide visual and textual knowledge about target objects to the VLA. This approach mitigates generalization failure when handling out-of-distribution objects. Based on the LIBERO simulation environment, we introduced novel objects and object descriptions to construct a new evaluation benchmark with three difficulty levels to test the effectiveness of our method. Our framework successfully outperformed the current state-of-the-art models on our designed hard-level generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2 achieves a 44.2% improvement in the success rate in the hard-level benchmark and an average improvement of 20.2% in all customized environments without any performance degradation on in-domain tasks. Project website: https://vla-2.github.io."
    },
    {
      "title": "出色的（小型）检索器及其训练方法：mxbai-edge-colbert-v0 技术报告 (原标题: Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report)",
      "link": "https://arxiv.org/abs/2510.14880",
      "pubDate": "Thu, 16 Oct 2025 13:00:35 GMT",
      "isoDate": "2025-10-16T13:00:35.000Z",
      "creator": "Rikiya Takehi, Benjamin Clavié, Sean Lee, Aamir Shakir",
      "summary": "# mxbai-edge-colbert-v0 技术报告：出色的（小型）检索器及其训练方法\n\n本技术报告介绍了 **mxbai-edge-colbert-v0** 模型，该模型旨在作为未来一系列小型概念验证模型的基础骨干，以支持从云端到本地设备的各种规模检索。\n\n## 模型概述\n\n*   **参数规模**：mxbai-edge-colbert-v0 推出了两种不同参数计数模型：17M 和 32M。\n*   **研究目标**：\n    *   通过大量实验改进检索和晚期交互模型。\n    *   将研究成果精炼成更小的模型，作为概念验证。\n    *   最终目标是实现从云端大规模检索到可在任何设备上本地运行的模型，支持所有规模的检索。\n*   **基础作用**：mxbai-edge-colbert-v0 被定位为未来所有实验的坚实基础骨干，是这一系列小型概念验证模型的首个版本。\n\n## 开发与方法\n\n*   在开发 mxbai-edge-colbert-v0 的过程中，研究人员进行了多项消融研究，并报告了相关结果。\n\n## 性能表现\n\n*   **下游任务性能**：mxbai-edge-colbert-v0 是一款性能卓越的小型模型。\n*   **短文本基准测试**：在常见的短文本基准测试（BEIR）上，其性能超越了 ColBERTv2。\n*   **长上下文任务**：在长上下文任务中取得了巨大进展，并展现出前所未有的效率。\n\n## 领域\n\n*   该研究属于信息检索（cs.IR）领域。",
      "shortSummary": "本技术报告介绍了mxbai-edge-colbert-v0模型，包括17M和32M两种参数规模。该模型旨在作为未来小型概念验证模型的基础，支持从云端到本地设备的各种规模检索。mxbai-edge-colbert-v0在短文本基准测试（BEIR）上超越了ColBERTv2，并在长上下文任务中展现出前所未有的效率，是信息检索领域的一大进步。",
      "translated_title": "出色的（小型）检索器及其训练方法：mxbai-edge-colbert-v0 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency."
    }
  ],
  "lastUpdated": "2025-10-18T09:31:56.579Z"
}