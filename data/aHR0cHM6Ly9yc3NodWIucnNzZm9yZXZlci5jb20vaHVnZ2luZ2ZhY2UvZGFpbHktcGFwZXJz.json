{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "时间是一种特征：利用扩散语言模型中的时间动态 (原标题: Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models)",
      "link": "https://arxiv.org/abs/2508.09138",
      "pubDate": "Tue, 12 Aug 2025 13:59:57 GMT",
      "isoDate": "2025-08-12T13:59:57.000Z",
      "creator": "Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen",
      "summary": "### 核心问题：扩散语言模型中的时间振荡\n\n*   **背景：** 扩散大型语言模型（dLLMs）通过迭代去噪生成文本。然而，当前的解码策略通常会丢弃丰富的中间预测，只保留最终输出。\n*   **发现：** 本研究揭示了一个关键现象——“时间振荡”（temporal oscillation）。这意味着正确的答案经常在去噪过程的中间阶段出现，但在后续的去噪步骤中被覆盖或改写。\n\n### 解决方案：利用时间动态\n\n为了解决时间振荡问题，本研究引入了两种互补的方法，旨在利用时间一致性：\n\n1.  **时间自洽投票（Temporal Self-Consistency Voting, TSCV）：**\n    *   这是一种无需训练、在测试时使用的解码策略。\n    *   它通过聚合去噪步骤中的预测结果，选择最一致的输出。\n\n2.  **时间一致性强化（Temporal Consistency Reinforcement, TCR）：**\n    *   这是一种后训练方法。\n    *   它使用“时间语义熵”（Temporal Semantic Entropy, TSE）作为奖励信号。TSE衡量的是中间预测结果的语义稳定性。\n    *   通过TSE奖励，该方法鼓励模型生成更稳定的结果。\n\n### 实验结果\n\n在多个基准测试中，本方法展示了其有效性：\n\n*   **仅使用负TSE奖励：** 在Countdown数据集上，相对于现有dLLM，实现了显著的24.7%的平均改进。\n*   **结合准确性奖励：**\n    *   在GSM8K数据集上，绝对增益为2.0%。\n    *   在MATH500数据集上，绝对增益为4.3%。\n    *   在SVAMP数据集上，绝对增益为6.6%。\n    *   在Countdown数据集上，绝对增益为25.3%。\n\n### 结论\n\n本研究强调了dLLMs中未被充分利用的时间动态的潜力，并提供了两种简单而有效的工具来利用这些动态。",
      "shortSummary": "本研究发现扩散语言模型（dLLMs）存在“时间振荡”现象，即正确答案在中间去噪步骤中出现后被覆盖。为解决此问题，论文提出了两种方法：1) 时间自洽投票（TSCV），通过聚合中间预测选择最一致输出；2) 时间一致性强化（TCR），利用时间语义熵（TSE）作为奖励信号鼓励稳定生成。实验结果显示，这些方法显著提升了dLLMs在多个基准测试上的性能，例如在Countdown数据集上最高提升25.3%。这揭示了dLLMs中时间动态的巨大潜力。",
      "translated_title": "时间是一种特征：利用扩散语言模型中的时间动态",
      "images": [],
      "contentSource": "完整文章",
      "content": "Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them."
    },
    {
      "title": "OpenCUA：计算机使用代理的开放基础 (原标题: OpenCUA: Open Foundations for Computer-Use Agents)",
      "link": "https://arxiv.org/abs/2508.09123",
      "pubDate": "Tue, 12 Aug 2025 13:52:32 GMT",
      "isoDate": "2025-08-12T13:52:32.000Z",
      "creator": "Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, Tao Yu",
      "summary": "## OpenCUA：计算机使用代理的开放基础框架\n\n### 引言：计算机使用代理（CUA）的背景与挑战\n\n视觉-语言模型在作为计算机使用代理（CUA）方面展现出令人印象深刻的能力，能够自动化各种计算机任务。然而，随着其商业潜力的增长，大多数功能强大的CUA系统的关键细节仍然是封闭的。鉴于这些代理将越来越多地介导数字交互并代表我们执行重要的决策，研究社区迫切需要访问开放的CUA框架，以研究其能力、局限性和风险。\n\n### OpenCUA框架构成\n\n为了弥合这一差距，研究团队提出了OpenCUA，一个全面的开源框架，旨在扩展CUA数据和基础模型。该框架由以下核心组件构成：\n\n1.  **标注基础设施**：\n    *   提供一个能够无缝捕获人类计算机使用演示的标注基础设施。\n\n2.  **AgentNet数据集**：\n    *   首次大规模的计算机使用任务数据集。\n    *   涵盖3个操作系统和200多个应用程序及网站。\n\n3.  **可扩展的管道**：\n    *   一个可扩展的管道，能够将演示转化为状态-动作对。\n    *   结合了反思性的长链式思考（Chain-of-Thought）推理，确保数据规模扩大时仍能保持稳健的性能提升。\n\n### 性能表现\n\nOpenCUA的端到端代理模型在CUA基准测试中展现出强大的性能：\n\n*   **整体性能**：在CUA基准测试中表现出色。\n*   **OpenCUA-32B的具体成就**：\n    *   在OSWorld-Verified数据集上取得了34.8%的平均成功率。\n    *   在开源模型中树立了新的最先进（SOTA）水平。\n    *   性能超越了OpenAI CUA（GPT-4o）。\n*   **泛化能力与计算效益**：\n    *   进一步分析证实，该方法在不同领域具有良好的泛化能力。\n    *   显著受益于增加测试时的计算资源。\n\n### 开放资源\n\n为了为进一步的CUA研究构建开放的基础，研究团队已发布了其标注工具、数据集、代码和模型。\n\n### 研究领域与作者\n\n*   **研究领域**：人工智能（cs.AI）、计算机视觉与模式识别（cs.CV）。\n*   **作者**：本文由Xinyuan Wang、Bowen Wang、Dunjie Lu等多位研究人员共同撰写。",
      "shortSummary": "OpenCUA是一个开源框架，旨在为计算机使用代理（CUA）提供开放基础。它通过提供标注基础设施、首个大规模多操作系统数据集AgentNet，以及可扩展的推理管道，解决了现有CUA系统封闭的挑战。OpenCUA-32B在OSWorld-Verified基准测试中取得了34.8%的成功率，超越了GPT-4o，成为开源模型的最新SOTA。该项目已发布其工具、数据集、代码和模型，以促进CUA领域的开放研究。",
      "translated_title": "OpenCUA：计算机使用代理的开放基础",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research."
    },
    {
      "title": "AutoCodeBench：大型语言模型是自动代码基准生成器 (原标题: AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators)",
      "link": "https://arxiv.org/abs/2508.09101",
      "pubDate": "Tue, 12 Aug 2025 13:29:20 GMT",
      "isoDate": "2025-08-12T13:29:20.000Z",
      "creator": "Jason Chou, Ao Liu, Yuchi Deng, Zhiying Zeng, Tao Zhang, Haotian Zhu, Jianwei Cai, Yue Mao, Chenchen Zhang, Lingyun Tan, Ziyan Xu, Bohui Zhai, Hengyi Liu, Speed Zhu, Wiggin Zhou, Fengzong Lian",
      "summary": "# AutoCodeBench：大型语言模型是自动代码基准生成器\n\n## 摘要\n\n大型语言模型（LLMs）在代码生成领域展现出卓越能力，但现有评估其代码生成能力的基准面临多项关键限制。\n\n## 现有基准的局限性\n\n1.  **依赖手动标注**：耗时且难以扩展到不同编程语言和问题复杂度。\n2.  **语言覆盖不均**：多数基准主要关注Python；少数多语言基准则难度有限，且语言分布不均。\n\n## AutoCodeGen：自动化数据集生成方法\n\n为解决上述挑战，研究人员提出了 **AutoCodeGen**，这是一种无需手动标注即可生成高难度多语言代码生成数据集的自动化方法。\n\n*   **确保测试用例的正确性和完整性**：\n    *   利用LLMs生成测试输入。\n    *   通过多语言沙盒获取测试输出。\n*   **实现高数据质量**：\n    *   采用逆序问题生成。\n    *   经过多重过滤步骤。\n\n## AutoCodeBench：大规模代码生成基准\n\n基于AutoCodeGen方法，研究人员引入了 **AutoCodeBench**，这是一个大规模的代码生成基准。\n\n*   **规模与分布**：包含3,920个问题，均匀分布在20种编程语言中。\n*   **设计目标**：专门用于评估LLMs在具有挑战性、多样化和实用的多语言任务上的表现。\n\n## LLM评估与结果\n\n研究人员在AutoCodeBench及其简化版本 **AutoCodeBench-Lite** 上评估了超过30个领先的开源和专有LLMs。\n\n*   **评估结果**：即使是最先进的LLMs，在面对这些任务的复杂性、多样性和多语言特性时也表现出困难。\n\n## AutoCodeBench-Complete：针对基础模型的补充\n\n此外，研究人员还推出了 **AutoCodeBench-Complete**，它专门设计用于评估基础模型的少样本（few-shot）代码生成能力。\n\n## 展望\n\n研究人员希望AutoCodeBench系列能成为宝贵的资源，并激励社区关注更具挑战性和实用性的多语言代码生成场景。",
      "shortSummary": "AutoCodeBench旨在解决现有LLM代码生成基准手动标注、语言偏向和难度不足的问题。通过自动化方法AutoCodeGen，该研究生成了AutoCodeBench，一个包含3,920个问题、覆盖20种语言的大规模、高难度多语言代码生成基准。评估显示，即使是先进的LLMs也难以应对其复杂性。AutoCodeBench-Complete进一步评估基础模型的少样本能力。该系列旨在推动更具挑战性的多语言代码生成研究。",
      "translated_title": "AutoCodeBench：大型语言模型是自动代码基准生成器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios."
    },
    {
      "title": "VertexRegen：连续细节层次的网格生成 (原标题: VertexRegen: Mesh Generation with Continuous Level of Detail)",
      "link": "https://arxiv.org/abs/2508.09062",
      "pubDate": "Tue, 12 Aug 2025 12:25:46 GMT",
      "isoDate": "2025-08-12T12:25:46.000Z",
      "creator": "Xiang Zhang, Yawar Siddiqui, Armen Avetisyan, Chris Xie, Jakob Engel, Henry Howard-Jenkins",
      "summary": "## VertexRegen：连续细节层次的网格生成框架\n\n### 核心贡献\n\n*   **引入新型框架**：本文提出了一种名为 VertexRegen 的全新网格生成框架。\n*   **实现连续细节层次（LOD）**：VertexRegen 的独特之处在于它能够以连续的细节层次生成网格，这意味着在生成过程中可以灵活地控制网格的精细程度。\n\n### 与现有方法的对比\n\n*   **现有自回归方法**：传统的自回归网格生成方法通常采用“从部分到完整”的方式进行，导致在生成过程的中间步骤中，所产生的结构是不完整的。\n\n### VertexRegen 的方法论\n\n*   **灵感来源**：VertexRegen 的设计灵感来源于渐进式网格（progressive meshes）的概念。\n*   **过程重构**：它将网格生成过程重新定义为边坍缩（edge collapse）的逆向操作，即顶点分裂（vertex split）。\n*   **学习机制**：这一逆向过程是通过一个生成模型进行学习的。\n\n### 实验结果与独特优势\n\n*   **生成质量**：实验结果表明，VertexRegen 生成的网格质量与当前最先进的方法相当。\n*   **“随时生成”（Anytime Generation）**：VertexRegen 的一个独特优势是它提供了“随时生成”的能力。这意味着用户可以在生成过程的任何步骤停止，并获得一个有效的、具有不同细节层次的网格。\n*   **灵活性**：这种灵活性使得用户可以根据需求，在计算资源或时间有限的情况下，获得满足特定细节要求的网格。\n\n### 相关信息\n\n*   **发表会议**：该研究成果已被 ICCV 2025 接收。\n*   **研究领域**：主要涉及图形学（cs.GR）、计算机视觉与模式识别（cs.CV）和机器学习（cs.LG）。",
      "shortSummary": "VertexRegen 是一种新型网格生成框架，能够实现连续细节层次（LOD）的网格生成。与现有方法不同，它将生成过程重构为顶点分裂（边坍缩的逆向），并通过生成模型学习。实验证明，VertexRegen 生成的网格质量与最先进方法相当，并提供独特的“随时生成”能力，允许在任何步骤获得具有不同LOD的有效网格，展现出高度灵活性。",
      "translated_title": "VertexRegen：连续细节层次的网格生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail."
    },
    {
      "title": "弥合量子博弈论的理论与实践：在NISQ硬件上优化实现性别之战并进行错误缓解 (原标题: Bridging Theory and Practice in Quantum Game Theory: Optimized Implementation of the Battle of the Sexes with Error Mitigation on NISQ Hardware)",
      "link": "https://arxiv.org/abs/2508.09050",
      "pubDate": "Tue, 12 Aug 2025 12:10:05 GMT",
      "isoDate": "2025-08-12T12:10:05.000Z",
      "creator": "Germán Díaz Agreda, Carlos Andres Duran Paredes, Mateo Buenaventura Samboni, Jhon Alejandro Andrade, Sebastián Andrés Cajas Ordoñez",
      "summary": "## 量子博弈论在NISQ硬件上的实验实现：以“性别之战”为例\n\n### 引言\n在真实硬件上实现量子博弈论面临诸多挑战，包括噪声、退相干和有限的量子比特连接性。然而，此类实验演示对于验证理论预测至关重要。本研究旨在通过在噪声中等规模量子（NISQ）硬件上实现一个完整的量子博弈论实验，来弥合理论与实践之间的鸿沟。\n\n### 实验设置\n*   **博弈类型**：本研究首次完整实验实现了Eisert-Wilkens-Lewenstein (EWL) 框架下的“性别之战”（Battle of the Sexes）博弈。\n*   **硬件平台**：实验在IBM Quantum的ibm sherbrooke超导处理器上进行。\n*   **评估策略**：评估了四种量子策略：I（恒等）、H（哈达玛）、R(π/4) 和 R(π)。\n*   **参数配置**：在31个纠缠值 γ ∈ [0, π] 的范围内进行了评估，每个配置执行2048次测量，以便直接比较分析预测与硬件执行结果。\n\n### 错误缓解方法\n为了减轻硬件噪声和变异性的影响，研究引入了一种名为**引导电路映射（Guided Circuit Mapping, GCM）**的新方法。GCM方法能够根据实时的拓扑结构和校准数据，动态地选择最佳的量子比特对并优化电路路由。\n\n### 结果与讨论\n*   **理论预测**：分析模型预测，相对于经典均衡，量子策略的收益可以提高高达108%。\n*   **实验表现**：尽管存在硬件引起的偏差，但通过GCM方法，实验结果成功地保持了预期的收益趋势，相对误差控制在3.5%至12%之间。\n*   **关键发现**：这些发现表明，在现实的NISQ条件下，战略协调中的量子优势依然可以持续存在。\n\n### 结论与意义\n本研究为在多智能体、经济和分布式决策系统等领域中实现量子博弈论的实际应用提供了可行的途径。它证明了即使在当前噪声量子硬件的限制下，通过有效的错误缓解技术，量子博弈论的理论优势仍能得到实验验证和保持。",
      "shortSummary": "该研究在IBM Quantum的NISQ硬件上，首次完整实验实现了EWL框架下的“性别之战”量子博弈。为应对噪声，引入了引导电路映射（GCM）方法。实验结果显示，尽管硬件存在偏差，GCM使量子策略的收益趋势与理论预测保持一致，相对误差在3.5%-12%之间。这证明在现实NISQ条件下，战略协调中的量子优势可以持续存在，为量子博弈论在多智能体和经济决策系统中的实际应用开辟了道路。",
      "translated_title": "弥合量子博弈论的理论与实践：在NISQ硬件上优化实现性别之战并进行错误缓解",
      "images": [],
      "contentSource": "完整文章",
      "content": "Implementing quantum game theory on real hardware is challenging due to noise, decoherence, and limited qubit connectivity, yet such demonstrations are essential to validate theoretical predictions. We present one of the first full experimental realizations of the Battle of the Sexes game under the Eisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke superconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi)) were evaluated across 31 entanglement values gamma in [0, pi] using 2048 shots per configuration, enabling a direct comparison between analytical predictions and hardware execution. To mitigate noise and variability, we introduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit pairs and optimizes routing based on real-time topology and calibration data. The analytical model forecasts up to 108% payoff improvement over the classical equilibrium, and despite hardware-induced deviations, experimental results with GCM preserve the expected payoff trends within 3.5%-12% relative error. These findings show that quantum advantages in strategic coordination can persist under realistic NISQ conditions, providing a pathway toward practical applications of quantum game theory in multi-agent, economic, and distributed decision-making systems."
    },
    {
      "title": "长训练，短思考：面向高效推理的课程学习 (原标题: Train Long, Think Short: Curriculum Learning for Efficient Reasoning)",
      "link": "https://arxiv.org/abs/2508.08940",
      "pubDate": "Tue, 12 Aug 2025 09:48:03 GMT",
      "isoDate": "2025-08-12T09:48:03.000Z",
      "creator": "Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem",
      "summary": "## 长训练，短思考：面向高效推理的课程学习\n\n### 摘要\n\n本文提出了一种新的课程学习策略，旨在提高大型语言模型（LLMs）的推理效率，同时保持准确性。现有方法在控制计算成本时，依赖于固定的训练预算，未能利用学习过程中从探索到压缩的自然进展。\n\n### 核心方法：课程学习与GRPO\n\n*   **问题背景**：LLMs的推理能力提升通常伴随计算成本增加。现有通过显式长度控制的方法，其固定预算训练模式未能有效利用学习的渐进性。\n*   **解决方案**：引入一种基于课程学习的策略，用于长度受控的推理。该策略采用**群组相对策略优化（Group Relative Policy Optimization, GRPO）**。\n*   **训练过程**：\n    *   训练初期：模型被赋予慷慨的token预算，鼓励其充分探索并发现有效的解决方案策略。\n    *   训练后期：token预算逐渐收紧，促使模型将已发现的策略提炼并压缩成更简洁的推理轨迹。\n\n### 奖励函数设计\n\n为了平衡训练目标，GRPO被增强了一个综合奖励函数，该函数结合了三个关键信号：\n\n1.  **任务正确性**：通过验证器反馈评估模型的任务完成准确性。\n2.  **长度效率**：奖励模型生成更简洁、更高效的推理过程。\n3.  **格式依从性**：通过结构化标签（structural tags）确保模型输出符合预设的格式要求。\n\n### 实验结果与贡献\n\n*   **基准测试**：在多个数学和推理数据集上进行了实验，包括GSM8K、MATH500、SVAMP、College Math和GSM+。\n*   **性能提升**：基于课程的训练方法在相同的最终预算下，始终优于固定预算的基线方法。\n*   **关键优势**：\n    *   实现了更高的准确性。\n    *   显著提高了token效率。\n*   **消融研究**：进一步分析了奖励权重和衰减调度设计的影响。研究表明，渐进式约束（progressive constraint）作为一种强大的归纳偏置，对于训练高效的推理模型至关重要。\n\n### 结论\n\n本文提出的课程学习策略，通过动态调整token预算和综合奖励机制，有效提升了LLMs的推理效率和准确性，为开发更高效的推理模型提供了新的方向。",
      "shortSummary": "本文提出一种名为“长训练，短思考”的课程学习策略，用于提高大型语言模型（LLMs）的推理效率。该方法利用群组相对策略优化（GRPO），通过从宽松到严格的token预算渐进式训练，鼓励模型先探索解决方案，再将其精炼为简洁的推理轨迹。结合任务正确性、长度效率和格式依从性的奖励函数，实验证明此方法在多个基准测试中，相较于固定预算基线，能显著提升准确性和token效率。",
      "translated_title": "长训练，短思考：面向高效推理的课程学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo."
    },
    {
      "title": "DeCRED：基于编码器-解码器语音识别的解码器中心正则化 (原标题: DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech Recognition)",
      "link": "https://arxiv.org/abs/2508.08938",
      "pubDate": "Tue, 12 Aug 2025 09:44:50 GMT",
      "isoDate": "2025-08-12T09:44:50.000Z",
      "creator": "Alexander Polok, Santosh Kesiraju, Karel Beneš, Bolaji Yusuf, Lukáš Burget, Jan Černocký",
      "summary": "### DeCRED：基于编码器-解码器语音识别的解码器中心正则化\n\n本文介绍了一种名为“DeCRED”（Decoder-Centric Regularization in Encoder-Decoder）的简单而有效的正则化方法，旨在改进编码器-解码器自动语音识别（ASR）模型中由解码器诱导的内部语言模型。该方法通过增强模型的鲁棒性和泛化能力，使其在域内和域外设置中均表现更佳。\n\n**核心方法：**\nDeCRED通过向解码器添加辅助分类器来实现。这些分类器使得模型能够通过中间逻辑（logits）预测下一个词元（token），从而对解码器的行为进行正则化。\n\n**主要研究发现与成果：**\n\n*   **内部语言模型困惑度降低：**\n    *   DeCRED相对于11个测试集，将平均内部语言模型BPE（Byte Pair Encoding）困惑度相对降低了36.6%。\n*   **词错误率（WER）改进：**\n    *   **域内（In-domain）设置：** 在7个域内测试集中的5个上，DeCRED实现了比基线模型更好的WER表现，将宏观WER从6.4%降低到6.3%。\n    *   **域外（Out-of-domain）设置：** 在4个域外测试集中的3个上，DeCRED也展现了WER的改进，将宏观WER从18.2%降低到16.2%。\n*   **在TEDLIUM3数据集上的表现：**\n    *   DeCRED在TEDLIUM3数据集上实现了7.0%的WER，这比基线模型低0.6%，比编码器中心的InterCTC正则化方法低0.5%。\n*   **与现有模型的比较：**\n    *   尽管DeCRED在训练数据量和模型参数方面远少于OWSM v3.1和Whisper-medium等模型，但它仍能展现出具有竞争力的WER表现。\n\n**出版信息：**\n该研究已被IEEE ASRU 2025接受。",
      "shortSummary": "DeCRED是一种针对编码器-解码器ASR模型的解码器中心正则化方法。它通过在解码器中添加辅助分类器，利用中间逻辑预测下一个词元，从而增强内部语言模型。实验表明，DeCRED显著降低了内部语言模型困惑度（36.6%），并在域内和域外设置中有效降低了词错误率（宏观WER分别从6.4%降至6.3%和18.2%降至16.2%）。尽管训练数据和参数较少，DeCRED仍能与OWSM v3.1和Whisper-medium等模型保持竞争力。",
      "translated_title": "DeCRED：基于编码器-解码器语音识别的解码器中心正则化",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents a simple yet effective regularization for the internal language model induced by the decoder in encoder-decoder ASR models, thereby improving robustness and generalization in both in- and out-of-domain settings. The proposed method, Decoder-Centric Regularization in Encoder-Decoder (DeCRED), adds auxiliary classifiers to the decoder, enabling next token prediction via intermediate logits. Empirically, DeCRED reduces the mean internal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this translates into actual WER improvements over the baseline in 5 of 7 in-domain and 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and 18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing the baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%, respectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium, showing competitive WERs despite training on much less data with fewer parameters."
    },
    {
      "title": "迈向具有类人先验的机器人灵巧抓取 (原标题: Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors)",
      "link": "https://arxiv.org/abs/2508.08896",
      "pubDate": "Tue, 12 Aug 2025 08:36:01 GMT",
      "isoDate": "2025-08-12T08:36:01.000Z",
      "creator": "Haoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou",
      "summary": "## AffordDex：迈向可供性感知机器人灵巧抓取\n\n### 引言\n\n通用型具身人工智能的发展，其基础在于灵巧机械手能够普遍性地抓取物体。然而，现有方法主要关注低层次的抓取稳定性指标，却忽视了对下游操作至关重要的可供性感知定位和类人姿态。\n\n### AffordDex 框架\n\n为了解决上述局限性，研究人员提出了 AffordDex，这是一个新颖的框架，采用两阶段训练，旨在学习一种通用的抓取策略，该策略内在地理解运动先验和物体可供性。\n\n#### 两阶段训练过程：\n\n1.  **第一阶段：轨迹模仿器预训练**\n    *   在一个大型人类手部运动语料库上进行预训练，以灌输对自然运动的强大先验知识。\n\n2.  **第二阶段：残差模块训练**\n    *   训练一个残差模块，将这些通用的类人运动适应到特定的物体实例上。\n    *   这一精炼过程关键地由以下两个组件引导：\n        *   **负可供性感知分割 (NAA) 模块**：用于识别功能上不合适的接触区域。\n        *   **特权教师-学生蒸馏过程**：确保最终基于视觉的策略具有高度的成功率。\n\n### 主要特点与优势\n\nAffordDex 不仅实现了通用的灵巧抓取，而且在姿态上保持了显著的类人特性，并在接触位置上实现了功能上的适当性。\n\n### 实验结果\n\n广泛的实验表明，AffordDex 在已见物体、未见实例甚至全新类别上，均显著优于最先进的基线方法。\n\n### 作者\n\nHaoyu Zhao, Linghao Zhuang, Xingyue Zhao, Cheng Zeng, Haoran Xu, Yuming Jiang, Jun Cen, Kexiang Wang, Jiayan Guo, Siteng Huang, Xin Li, Deli Zhao, Hua Zou。",
      "shortSummary": "AffordDex 是一种新型两阶段训练框架，旨在解决现有机器人灵巧抓取方法忽视可供性感知和类人姿态的问题。它通过模仿人类运动学习通用抓取策略，并利用负可供性感知分割和教师-学生蒸馏进行精炼。AffordDex 实现了通用、类人且功能适当的抓取，在各种物体上均显著超越现有技术。",
      "translated_title": "迈向具有类人先验的机器人灵巧抓取",
      "images": [],
      "contentSource": "完整文章",
      "content": "A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories."
    },
    {
      "title": "通过自动化构建环境实现大型语言模型工具使用能力的反馈驱动改进 (原标题: Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments)",
      "link": "https://arxiv.org/abs/2508.08791",
      "pubDate": "Tue, 12 Aug 2025 05:45:19 GMT",
      "isoDate": "2025-08-12T05:45:19.000Z",
      "creator": "Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Xuanjing Huang, Jiecao Chen",
      "summary": "## 大型语言模型（LLMs）工具使用能力的反馈驱动改进：通过自动化构建环境实现\n\n### 背景与挑战\n\n*   **重要性：** 有效的工具使用对大型语言模型（LLMs）与环境进行有意义的交互至关重要。\n*   **当前限制：** 进展受限于缺乏专门为工具使用设计的、高效的强化学习（RL）框架。这主要源于构建稳定训练环境和设计可验证奖励机制的挑战。\n\n### 提出的解决方案\n\n为了解决上述挑战，本文提出了两项核心创新：\n\n1.  **自动化环境构建流程：**\n    *   **目标：** 创建高质量的训练环境，提供详细且可衡量的反馈，且不依赖外部工具。\n    *   **组成部分：**\n        *   **场景分解：** 将复杂任务分解为可管理的场景。\n        *   **文档生成：** 为工具和任务生成相关文档。\n        *   **函数集成：** 将工具功能集成到环境中。\n        *   **复杂度扩展：** 逐步增加环境的复杂性。\n        *   **本地化部署：** 实现环境的本地化部署以提高稳定性。\n\n2.  **可验证的奖励机制：**\n    *   **评估维度：**\n        *   工具使用的精确性。\n        *   任务执行的完整性。\n    *   **集成方式：** 该机制与从构建环境中收集的轨迹数据相结合，能够无缝集成到标准的强化学习算法中，从而促进反馈驱动的模型训练。\n\n### 实验结果与分析\n\n*   **性能提升：** 在不同规模的LLMs上进行的实验表明，该方法显著提升了模型的工具使用性能。\n*   **通用能力保持：** 这种提升并未损害模型的通用能力，无论采用何种推理模式或训练算法。\n*   **内在机制：** 分析表明，性能提升源于模型底层MLP参数的更新，从而改善了上下文理解和推理能力。",
      "shortSummary": "为提升大型语言模型（LLMs）工具使用能力，本文提出自动化构建环境和可验证奖励机制。该方法解决了强化学习在工具使用中面临的环境与奖励挑战。实验证明，此方案显著增强了LLMs的工具使用性能，且不损害其通用能力，主要得益于模型上下文理解和推理能力的改进。",
      "translated_title": "通过自动化构建环境实现大型语言模型工具使用能力的反馈驱动改进",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models."
    },
    {
      "title": "Aryabhata：一个专注于JEE数学考试的语言模型 (原标题: Aryabhata: An exam-focused language model for JEE Math)",
      "link": "https://arxiv.org/abs/2508.08665",
      "pubDate": "Tue, 12 Aug 2025 02:20:07 GMT",
      "isoDate": "2025-08-12T02:20:07.000Z",
      "creator": "Ritvik Rastogi, Sachin Dharashivkar, Sandeep Varma",
      "summary": "### 模型介绍\n*   **名称：** Aryabhata 1.0\n*   **类型：** 一个紧凑的70亿参数数学推理模型。\n*   **优化目标：** 专为印度学术考试——联合入学考试（JEE）优化。\n*   **背景：** 尽管大型语言模型（LLMs）发展迅速，但现有模型通常不适用于教育用途。\n\n### 构建方法\n*   **基础模型合并：** 通过合并强大的开源推理模型构建。\n*   **监督微调（SFT）：** 采用课程学习（curriculum learning）对经过验证的思维链（CoT）轨迹进行SFT。这些CoT轨迹通过“最佳n拒绝采样”（best-of-n rejection sampling）精心策划。\n*   **强化学习（RLVR）：** 应用可验证奖励的强化学习，使用A2C目标和组相对优势估计（group-relative advantage estimation）。\n*   **探索策略：** 引入了新颖的探索策略，如“自适应组大小调整”（Adaptive Group Resizing）和“温度缩放”（Temperature Scaling）。\n\n### 性能评估\n*   **评估基准：**\n    *   **同分布（in-distribution）：** JEE Main 2025。\n    *   **异分布（out-of-distribution）：** MATH、GSM8K。\n*   **评估结果：** Aryabhata在准确性和效率方面均优于现有模型。\n*   **教育价值：** 此外，它还提供了具有教学意义的逐步推理过程。\n\n### 发布与未来展望\n*   **发布形式：** Aryabhata作为基础模型发布，旨在推动以考试为中心、开源的小型语言模型的发展。\n*   **社区反馈：** 这是首次公开发布，以收集社区反馈。\n*   **未来计划：** PW（开发团队）正在积极训练未来的模型，以进一步提高学生的学习成果。",
      "shortSummary": "Aryabhata 1.0是一个紧凑的70亿参数数学推理模型，专为印度JEE考试优化。该模型通过合并开源推理模型、监督微调和可验证奖励的强化学习构建，并引入了新颖的探索策略。在JEE Main 2025、MATH和GSM8K等基准测试中，Aryabhata在准确性和效率上均超越现有模型，并能提供有教学价值的逐步推理。它作为基础模型发布，旨在推动考试导向的开源小型语言模型发展。",
      "translated_title": "Aryabhata：一个专注于JEE数学考试的语言模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-n rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0 on Hugging Face}); PW is actively training future models to further improve learning outcomes for students."
    },
    {
      "title": "Cut2Next：通过上下文调优生成下一镜头 (原标题: Cut2Next: Generating Next Shot via In-Context Tuning)",
      "link": "https://arxiv.org/abs/2508.08244",
      "pubDate": "Mon, 11 Aug 2025 13:56:59 GMT",
      "isoDate": "2025-08-11T13:56:59.000Z",
      "creator": "Jingwen He, Hongbo Liu, Jiajun Li, Ziqi Huang, Yu Qiao, Wanli Ouyang, Ziwei Liu",
      "summary": "### Cut2Next：通过上下文调优生成下一镜头\n\n本文介绍了一个名为 **Cut2Next** 的创新框架，旨在解决当前多镜头生成方法在电影叙事连贯性和专业编辑模式方面的不足，从而实现 **下一镜头生成 (Next Shot Generation, NSG)**。\n\n#### 现有挑战与NSG目标\n*   **现有问题：** 当前的多镜头生成方法通常优先考虑基本的视觉一致性，但忽视了驱动叙事流程的关键编辑模式（例如，正反打、插入镜头）和严格的电影连续性。这导致生成的输出可能在视觉上连贯，但缺乏叙事复杂性和真正的电影完整性。\n*   **NSG目标：** 旨在合成一个高质量的后续镜头，该镜头必须严格符合专业的编辑模式，并保持严谨的电影连续性。\n\n#### Cut2Next 框架概述\nCut2Next 框架利用 **Diffusion Transformer (DiT)** 作为其核心生成模型，并通过一种新颖的 **分层多提示 (Hierarchical Multi-Prompting)** 策略进行上下文调优。\n\n*   **分层多提示策略：**\n    *   **关系提示 (Relational Prompts)：** 用于定义整体上下文和镜头间的编辑风格。\n    *   **个体提示 (Individual Prompts)：** 用于指定每个镜头的具体内容和电影摄影属性。\n    *   这些提示共同指导 Cut2Next 生成符合电影规范的下一镜头。\n\n*   **架构创新：** 为了进一步整合这些多样化的信号，Cut2Next 引入了两项架构创新，且不引入新的参数：\n    *   **上下文感知条件注入 (Context-Aware Condition Injection, CACI)**\n    *   **分层注意力掩码 (Hierarchical Attention Mask, HAM)**\n\n#### 数据集与评估\n为了支持 Cut2Next 的开发和评估，研究团队构建了两个数据集并引入了一个新的评估基准：\n\n*   **数据集：**\n    *   **RawCuts：** 大规模数据集。\n    *   **CuratedCuts：** 精炼数据集。\n    *   这两个数据集都包含分层提示。\n*   **评估基准：** 引入了 **CutBench** 用于评估。\n*   **实验结果：**\n    *   实验表明 Cut2Next 在视觉一致性和文本忠实度方面表现出色。\n    *   用户研究显示，用户对 Cut2Next 有强烈的偏好，尤其是在其对预期编辑模式的遵循和整体电影连续性方面，这验证了其生成高质量、叙事表达丰富且电影连贯的后续镜头的能力。",
      "shortSummary": "Cut2Next是一个创新框架，旨在通过上下文调优生成符合专业编辑模式和电影连续性的高质量后续镜头。它利用Diffusion Transformer，结合分层多提示策略（关系提示和个体提示）以及上下文感知条件注入（CACI）和分层注意力掩码（HAM）等架构创新。实验和用户研究表明，Cut2Next在视觉一致性、文本忠实度和电影叙事表达方面表现出色，有效解决了现有方法在叙事连贯性上的不足。",
      "translated_title": "Cut2Next：通过上下文调优生成下一镜头",
      "images": [],
      "contentSource": "完整文章",
      "content": "Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots."
    },
    {
      "title": "第一部分：技巧还是陷阱？深入探讨强化学习在大型语言模型推理中的应用 (原标题: Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2508.08221",
      "pubDate": "Mon, 11 Aug 2025 13:39:45 GMT",
      "isoDate": "2025-08-11T13:39:45.000Z",
      "creator": "Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, Bo Zheng",
      "summary": "## 强化学习在大型语言模型推理中的应用：挑战与解决方案\n\n### 研究背景与挑战\n\n强化学习（RL）在大型语言模型（LLM）推理领域的应用正迅速成为一个重要的研究方向，相关算法创新和实际应用研究显著增加。然而，该领域面临多项关键挑战：\n\n*   **缺乏标准化指导方针**：RL技术的使用缺乏统一标准。\n*   **机制理解碎片化**：对RL底层机制的理解不完整。\n*   **实验设置不一致**：训练数据、模型初始化和实验环境的差异导致结论相互矛盾，模糊了技术的关键特性。\n*   **实践者困惑**：上述问题使得实践者在选择合适的RL技术时感到困惑。\n\n### 本文方法与贡献\n\n为解决这些问题，本文系统地回顾了广泛采用的RL技术，并采取了以下严谨的方法：\n\n*   **严格复现与独立评估**：在一个统一的开源框架内，对RL技术进行严格复现和独立评估。\n*   **细粒度实验分析**：通过在不同难度数据集、模型大小和架构上进行细粒度实验，深入分析每种技术的内部机制、适用场景和核心原理。\n\n### 核心发现与实践指导\n\n基于深入的分析，本文提供了以下关键成果：\n\n*   **明确的RL技术选择指南**：为实践者提供了根据特定设置选择RL技术的清晰指南。\n*   **可靠的实践路线图**：为在LLM领域应用RL的实践者提供了可靠的路线图。\n*   **极简组合的有效性**：研究发现，通过结合两种极简技术，可以使用普通的PPO损失来解锁无评论家策略的学习能力。实验结果表明，这种简单的组合能够持续提升性能，甚至超越了GRPO和DAPO等策略。\n\n本文旨在为RL在LLM推理领域的应用提供更清晰的理解和更实用的指导。",
      "shortSummary": "本文深入探讨了强化学习（RL）在大型语言模型（LLM）推理中的应用，指出该领域面临缺乏标准化指南、机制理解碎片化及实验结果不一致等挑战。作者通过系统复现和细致实验，分析了RL技术的内部机制和适用场景，并提供了选择指南和实践路线图。研究发现，一种极简的RL技术组合能有效提升无评论家策略的性能，超越现有方法，为该领域提供了清晰的理解和实用指导。",
      "translated_title": "第一部分：技巧还是陷阱？深入探讨强化学习在大型语言模型推理中的应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO."
    },
    {
      "title": "视觉中的强化学习：一项综述 (原标题: Reinforcement Learning in Vision: A Survey)",
      "link": "https://arxiv.org/abs/2508.08189",
      "pubDate": "Mon, 11 Aug 2025 13:08:55 GMT",
      "isoDate": "2025-08-11T13:08:55.000Z",
      "creator": "Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou",
      "summary": "本综述对强化学习（RL）与视觉智能交叉领域的新进展进行了批判性且最新的综合分析。该领域已使智能体能够感知复杂的视觉场景，并在其中进行推理、生成和行动。\n\n**主要内容和结构：**\n\n*   **问题形式化与策略优化：**\n    *   首先对视觉强化学习问题进行了形式化定义。\n    *   追溯了策略优化策略的演变，从人类反馈强化学习（RLHF）到可验证奖励范式，以及从近端策略优化（PPO）到群相对策略优化（Group Relative Policy Optimization）。\n\n*   **四大主题支柱：**\n    *   综述将200多项代表性工作组织成四个主要主题：\n        1.  **多模态大型语言模型（Multi-modal Large Language Models）：** 探讨其在视觉RL中的应用。\n        2.  **视觉生成（Visual Generation）：** 关注如何利用RL进行图像和视频生成。\n        3.  **统一模型框架（Unified Model Frameworks）：** 讨论旨在整合不同视觉RL任务的统一架构。\n        4.  **视觉-语言-动作模型（Vision-Language-Action Models）：** 探索结合视觉、语言理解和物理行动的智能体。\n    *   对于每个支柱，综述深入探讨了：\n        *   **算法设计：** 核心算法原理和创新。\n        *   **奖励工程：** 如何设计有效的奖励机制以引导学习。\n        *   **基准进展：** 现有评估标准和数据集的进步。\n        *   **趋势：** 识别了课程驱动训练、偏好对齐扩散模型和统一奖励建模等新兴趋势。\n\n*   **评估协议：**\n    *   审查了当前视觉RL的评估协议，包括：\n        *   **集合级保真度（Set-level fidelity）：** 评估生成结果与真实数据分布的匹配程度。\n        *   **样本级偏好（Sample-level preference）：** 评估模型输出在用户或专家偏好方面的表现。\n        *   **状态级稳定性（State-level stability）：** 评估智能体在不同状态下的行为一致性和鲁棒性。\n\n*   **开放挑战与未来方向：**\n    *   识别了该领域面临的开放挑战，包括：\n        *   **样本效率（Sample efficiency）：** 减少训练所需的交互数据量。\n        *   **泛化能力（Generalization）：** 提高模型在未见过环境或任务中的表现。\n        *   **安全部署（Safe deployment）：** 确保RL智能体在现实世界应用中的安全性和可靠性。\n\n**目标：**\n本综述旨在为研究人员和实践者提供一张关于快速扩展的视觉强化学习领域的连贯地图，并指出未来研究的有前景方向。",
      "shortSummary": "本综述全面审视了视觉强化学习（Visual RL）领域的最新进展。文章首先形式化了视觉RL问题，并追溯了策略优化策略的演变。随后，将200多项代表性工作归纳为多模态大型语言模型、视觉生成、统一模型框架和视觉-语言-动作模型四大主题，并探讨了各主题的算法设计、奖励工程、基准进展及趋势。最后，综述了评估协议，并指出了样本效率、泛化能力和安全部署等开放挑战，旨在为研究人员提供该领域的清晰图景和未来方向。",
      "translated_title": "视觉中的强化学习：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."
    },
    {
      "title": "随形而动：通过轨迹引导区域控制实现形状感知图像编辑 (原标题: Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control)",
      "link": "https://arxiv.org/abs/2508.08134",
      "pubDate": "Mon, 11 Aug 2025 12:10:00 GMT",
      "isoDate": "2025-08-11T12:10:00.000Z",
      "creator": "Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma",
      "summary": "### 随形而动：通过轨迹引导区域控制实现形状感知图像编辑\n\n**1. 背景与问题**\n\n*   **现有挑战**：尽管近期基于流的图像编辑模型在各种任务中展现出通用能力，但在处理复杂场景，特别是涉及大规模形状变换时，它们往往难以达到专业水平。\n*   **具体问题**：在执行此类结构性编辑时，这些方法要么无法实现预期的形状改变，要么会无意中修改非目标区域，导致背景质量下降。\n\n**2. 提出的解决方案：Follow-Your-Shape 框架**\n\n*   **核心理念**：本文提出 Follow-Your-Shape，一个无需训练且无需掩码的框架，旨在支持对物体形状进行精确和可控的编辑，同时严格保留非目标内容。\n*   **创新机制**：\n    *   **动机**：受反演（inversion）和编辑（editing）轨迹之间差异的启发。\n    *   **轨迹散度图（Trajectory Divergence Map, TDM）**：通过比较反演路径和去噪路径之间逐令牌（token-wise）的速度差异来计算 TDM。\n    *   **TDM 的作用**：TDM 能够精确地定位可编辑区域。\n    *   **计划性 KV 注入（Scheduled KV Injection）**：TDM 进一步引导一个计划性 KV 注入机制，以确保编辑过程的稳定性和忠实性。\n\n**3. 评估与基准**\n\n*   **ReShapeBench**：为了促进严格的评估，研究人员引入了一个名为 ReShapeBench 的新基准。该基准包含120张新图像和专门为形状感知编辑精心策划的丰富提示对。\n\n**4. 实验结果**\n\n*   实验证明，Follow-Your-Shape 方法在可编辑性和视觉保真度方面均表现出卓越的性能。\n*   尤其在需要大规模形状替换的任务中，其效果尤为显著。",
      "shortSummary": "Follow-Your-Shape 是一种无需训练、无需掩码的图像编辑框架，旨在解决现有模型在处理大规模形状变换时的问题。它通过计算轨迹散度图（TDM）来精确识别可编辑区域，并引导计划性 KV 注入机制，实现对物体形状的精确、可控编辑，同时严格保留非目标内容。该方法在形状感知编辑任务中，尤其在需要大规模形状替换时，展现出卓越的编辑能力和视觉保真度。",
      "translated_title": "随形而动：通过轨迹引导区域控制实现形状感知图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement."
    },
    {
      "title": "HierSearch：一个集成本地和网络搜索的分层企业深度搜索框架 (原标题: HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches)",
      "link": "https://arxiv.org/abs/2508.08088",
      "pubDate": "Mon, 11 Aug 2025 11:31:47 GMT",
      "isoDate": "2025-08-11T11:31:47.000Z",
      "creator": "Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen",
      "summary": "# HierSearch：一个集成本地和网络搜索的分层企业深度搜索框架\n\n## 摘要\n\n当前深度搜索系统通常局限于单一知识源（本地或网络），而企业级应用常需要同时利用本地和网络语料库进行私有深度搜索。简单地使用扁平强化学习（RL）训练一个配备多种搜索工具的智能体存在数据训练效率低和复杂工具掌握能力差的问题。为解决这些挑战，本文提出了一个名为 HierSearch 的分层智能体深度搜索框架，该框架通过分层强化学习进行训练。\n\n## 框架结构与机制\n\nHierSearch 采用分层架构设计：\n\n*   **低层智能体**：\n    *   一个**本地深度搜索智能体**，负责从本地领域检索证据。\n    *   一个**网络深度搜索智能体**，负责从网络领域检索证据。\n    *   这两个智能体分别针对其对应的数据源进行训练。\n\n*   **高层智能体**：\n    *   一个**规划器智能体**，负责协调低层智能体的操作，并最终提供答案。\n\n*   **知识精炼器**：\n    *   为了防止直接复制答案和错误传播，HierSearch 设计了一个**知识精炼器**。\n    *   该精炼器能够过滤掉低层智能体返回的幻觉信息和不相关证据。\n\n## 实验结果\n\n实验结果表明，HierSearch 相比扁平强化学习取得了更好的性能。它在通用、金融和医疗领域的六个基准测试中，均优于各种深度搜索和多源检索增强生成（RAG）基线方法。\n\n## 代码与数据集\n\n该研究的代码和数据集已公开提供。",
      "shortSummary": "HierSearch 是一个为企业设计的、集成本地和网络搜索的分层深度搜索框架。它通过分层强化学习训练，包含低层本地/网络搜索智能体和高层规划器，并引入知识精炼器过滤无效信息。该框架解决了现有深度搜索单一来源和扁平强化学习效率低的问题，实验证明其性能优于多种基线方法。代码和数据集已公开。",
      "translated_title": "HierSearch：一个集成本地和网络搜索的分层企业深度搜索框架",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains."
    },
    {
      "title": "Matrix-3D：全向可探索三维世界生成 (原标题: Matrix-3D: Omnidirectional Explorable 3D World Generation)",
      "link": "https://arxiv.org/abs/2508.08086",
      "pubDate": "Mon, 11 Aug 2025 11:29:57 GMT",
      "isoDate": "2025-08-11T11:29:57.000Z",
      "creator": "Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, Eric Li, Yang Liu, Yikai Wang, Hao-Xiang Guo, Yahui Zhou",
      "summary": "## Matrix-3D：全向可探索三维世界生成\n\n### 研究背景与挑战\n\n*   从单一图像或文本提示生成可探索的三维世界是空间智能领域的核心任务。\n*   尽管近期工作（常利用视频模型）在实现大范围和通用性三维世界生成方面取得了进展，但现有方法通常在生成场景的范围上存在局限性。\n\n### Matrix-3D 框架介绍\n\n*   本文提出了 **Matrix-3D**，一个旨在实现宽覆盖、全向可探索三维世界生成的框架。\n*   Matrix-3D 结合了条件视频生成和全景三维重建技术。\n*   其核心思想是利用**全景表示**来克服现有方法在场景范围上的限制。\n\n### 关键技术组件\n\n1.  **轨迹引导的全景视频扩散模型**：\n    *   该模型以场景网格渲染作为条件进行训练。\n    *   目标是生成高质量、几何一致的场景视频，为后续的三维重建提供基础。\n\n2.  **全景场景视频到三维世界的转换方法**：\n    *   为了将生成的全景场景视频提升为三维世界，Matrix-3D 提出了两种独立的方法：\n        *   **方法一：前馈式大型全景重建模型**：用于实现快速的三维场景重建，适用于需要高效率的场景。\n        *   **方法二：基于优化的管道**：用于实现更准确且细节丰富的三维场景重建，适用于对精度要求更高的场景。\n\n### Matrix-Pano 数据集\n\n*   为了促进有效的模型训练，研究团队引入了 **Matrix-Pano 数据集**。\n*   这是首个大规模合成数据集，包含 11.6 万个高质量静态全景视频序列，并附带了详细的深度和轨迹标注。\n*   该数据集对于训练能够处理全景数据的三维生成模型至关重要。\n\n### 实验结果与性能\n\n*   广泛的实验结果表明，Matrix-3D 框架在全景视频生成和三维世界生成方面均达到了**最先进的性能**。\n*   这验证了其在解决现有方法局限性方面的有效性。\n\n### 其他信息\n\n*   本工作以技术报告形式发布。\n*   研究领域主要包括计算机视觉与模式识别 (cs.CV) 和图形学 (cs.GR)。\n*   更多详情可参考 arXiv 论文：arXiv:2508.08086。",
      "shortSummary": "Matrix-3D 提出一个新框架，旨在通过结合条件视频生成和全景三维重建，实现宽覆盖、全向可探索的三维世界生成。该框架训练轨迹引导的全景视频扩散模型，并提供两种三维重建方法（快速前馈式和精确优化式）。为支持训练，还引入了大规模 Matrix-Pano 数据集。实验表明，Matrix-3D 在全景视频和三维世界生成方面均达到最先进水平，解决了现有方法场景范围有限的问题。",
      "translated_title": "Matrix-3D：全向可探索三维世界生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io."
    },
    {
      "title": "WideSearch：代理式广域信息搜寻的基准测试 (原标题: WideSearch: Benchmarking Agentic Broad Info-Seeking)",
      "link": "https://arxiv.org/abs/2508.07999",
      "pubDate": "Mon, 11 Aug 2025 10:03:09 GMT",
      "isoDate": "2025-08-11T10:03:09.000Z",
      "creator": "Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang",
      "summary": "## WideSearch：代理式广域信息搜寻的基准测试\n\n### 引言\n\n*   在专业研究和日常规划等诸多任务中，广域信息搜寻是主要的瓶颈，其特点是重复性高而非认知复杂。\n*   随着大型语言模型（LLMs）的快速发展，由LLMs驱动的自动化搜索代理有望将人类从这项繁琐的工作中解放出来。\n*   然而，由于缺乏合适的基准，这些代理执行“广域上下文”信息收集的可靠性和完整性尚未得到充分评估。\n\n### WideSearch 基准的引入\n\n*   为弥补这一空白，研究人员推出了 WideSearch，这是一个专门用于评估代理在大规模信息收集任务中可靠性的新基准。\n\n### WideSearch 的特点\n\n*   **问题数量与来源：** 包含200个手工整理的问题（100个英文，100个中文），来源于超过15个不同领域，并基于真实用户查询。\n*   **任务要求：** 每个任务要求代理收集大规模的原子信息，这些信息可以逐一客观验证，并需要将其整理成结构良好的输出。\n*   **质量控制：** 采用严格的五阶段质量控制流程，确保数据集的难度、完整性和可验证性。\n\n### 基准测试结果\n\n*   **测试对象：** 对超过10个最先进的代理式搜索系统进行了基准测试，包括单代理、多代理框架以及端到端商业系统。\n*   **代理表现：** 大多数系统的总体成功率接近0%，表现最好的系统也仅达到5%。\n*   **人类表现：** 然而，在充足的时间下，通过多个人类测试员的交叉验证，可以实现接近100%的成功率。\n*   **结论：** 这些结果表明，当前的搜索代理在处理大规模信息搜寻方面存在严重缺陷，这突显了未来代理式搜索研究和开发的紧迫领域。\n\n### 数据可用性\n\n*   WideSearch 数据集、评估流程和基准测试结果已公开发布。",
      "shortSummary": "研究人员推出了WideSearch，一个评估LLM驱动的代理在广域信息搜寻任务中可靠性的新基准。该基准包含200个手工整理的真实用户查询，要求代理收集并组织大规模可验证的原子信息。对10多个最先进代理系统的测试显示，它们的成功率普遍接近0%，最佳仅5%，而人类可达近100%。这表明当前搜索代理在处理大规模信息搜寻方面存在严重缺陷，亟需进一步研究与开发。数据集已公开。",
      "translated_title": "WideSearch：代理式广域信息搜寻的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/"
    },
    {
      "title": "Omni-Effects：统一且空间可控的视觉效果生成 (原标题: Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation)",
      "link": "https://arxiv.org/abs/2508.07981",
      "pubDate": "Mon, 11 Aug 2025 09:41:24 GMT",
      "isoDate": "2025-08-11T09:41:24.000Z",
      "creator": "Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, Xiangxiang Chu",
      "summary": "### Omni-Effects：统一且空间可控的视觉效果生成\n\n**1. 背景与挑战**\n\n*   **重要性：** 视觉效果（VFX）是现代电影制作中不可或缺的视觉增强技术。\n*   **现有问题：** 尽管视频生成模型为VFX制作提供了经济高效的解决方案，但当前方法受限于“每效果LoRA训练”，这导致它们只能生成单一效果。\n*   **局限性：** 这种根本性限制阻碍了需要空间可控复合效果（即在指定位置同时生成多种效果）的应用。\n*   **主要挑战：** 将多样化效果整合到统一框架中面临两大挑战：\n    *   效果差异带来的干扰。\n    *   多VFX联合训练期间的空间不可控性。\n\n**2. 提出的解决方案：Omni-Effects 框架**\n\n*   **目标：** 提出Omni-Effects，这是首个能够生成提示引导效果和空间可控复合效果的统一框架。\n*   **核心创新：**\n    *   **LoRA-based Mixture of Experts (LoRA-MoE)：**\n        *   采用一组专家LoRA，将多样化效果整合到统一模型中。\n        *   有效缓解了跨任务干扰。\n    *   **Spatial-Aware Prompt (SAP) 空间感知提示：**\n        *   将空间掩码信息整合到文本标记中。\n        *   实现精确的空间控制。\n    *   **Independent-Information Flow (IIF) 独立信息流模块：**\n        *   集成在SAP内部。\n        *   隔离对应于单个效果的控制信号，防止不必要的混合。\n\n**3. 数据集与评估**\n\n*   **Omni-VFX 数据集：**\n    *   为促进研究，构建了一个全面的VFX数据集。\n    *   通过结合图像编辑和First-Last Frame-to-Video (FLF2V) 合成的新颖数据收集流程构建。\n*   **VFX 评估框架：**\n    *   引入了一个专用的VFX评估框架，用于验证模型性能。\n\n**4. 实验结果**\n\n*   广泛的实验证明，Omni-Effects 实现了精确的空间控制和多样化的效果生成。\n*   用户能够指定所需效果的类别和位置。",
      "shortSummary": "Omni-Effects 提出一个统一框架，旨在解决现有视频生成模型在视觉效果（VFX）生成中单一效果和空间不可控的限制。通过引入 LoRA-based Mixture of Experts (LoRA-MoE) 来整合多样化效果并减少干扰，以及 Spatial-Aware Prompt (SAP) 结合 Independent-Information Flow (IIF) 实现精确的空间控制，Omni-Effects 能够生成提示引导且空间可控的复合VFX。该框架还构建了 Omni-VFX 数据集并引入了专用评估框架，实验证明其在多样化效果生成和空间控制方面表现出色。",
      "translated_title": "Omni-Effects：统一且空间可控的视觉效果生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects."
    },
    {
      "title": "超越十轮：通过大规模异步强化学习解锁长周期智能体搜索 (原标题: Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL)",
      "link": "https://arxiv.org/abs/2508.07976",
      "pubDate": "Mon, 11 Aug 2025 09:36:57 GMT",
      "isoDate": "2025-08-11T09:36:57.000Z",
      "creator": "Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu",
      "summary": "### ASearcher：通过大规模异步强化学习解锁长周期智能体搜索\n\n**背景与问题**\n\n*   近期基于大型语言模型（LLM）的智能体在处理复杂、知识密集型任务方面展现出卓越能力，尤其是在集成外部工具时。\n*   搜索工具在获取外部知识方面发挥着关键作用。\n*   然而，现有开源智能体在实现“搜索智能”（Search Intelligence）方面仍显不足，这包括解决模糊查询、生成精确搜索、分析结果和进行彻底探索的能力。\n*   现有方法存在可扩展性、效率和数据质量方面的缺陷。例如，现有在线强化学习（RL）方法中有限的轮次（如≤10）限制了复杂策略的学习。\n\n**ASearcher 项目介绍**\n\n*   本文引入了 ASearcher，一个用于大规模强化学习训练搜索智能体的开源项目。\n*   **核心贡献包括：**\n    1.  **可扩展的完全异步强化学习训练：** 实现了长周期搜索，同时保持了高训练效率。\n    2.  **基于提示的LLM智能体：** 能够自主合成高质量和具有挑战性的问答（QA），从而创建了大规模的QA数据集。\n\n**主要成果与性能**\n\n*   通过强化学习训练，基于提示的 QwQ-32B 智能体取得了显著的性能提升：\n    *   在 xBench 上，Avg@4 增益达到 46.7%。\n    *   在 GAIA 上，Avg@4 增益达到 20.8%。\n*   该智能体展现出极端的长周期搜索能力，在训练期间工具调用次数超过 40 轮，输出令牌超过 150k。\n*   ASearcher-Web-QwQ 采用简单的智能体设计，且不依赖外部 LLM，其性能超越了现有开源的 32B 智能体：\n    *   在 xBench 上的 Avg@4 分数为 42.1。\n    *   在 GAIA 上的 Avg@4 分数为 52.8。\n\n**开源信息**\n\n*   该项目已开源其模型、训练数据和代码。",
      "shortSummary": "ASearcher是一个开源项目，旨在通过大规模异步强化学习（RL）提升LLM智能体的搜索智能，解决现有方法短周期搜索的局限。它通过可扩展的异步RL训练和自主生成高质量QA数据集，实现了长周期搜索。其QwQ-32B智能体在xBench和GAIA上分别取得了46.7%和20.8%的Avg@4显著提升，并展现了超过40轮的工具调用能力。ASearcher-Web-QwQ在不依赖外部LLM的情况下，性能超越了现有开源32B智能体。",
      "translated_title": "超越十轮：通过大规模异步强化学习解锁长周期智能体搜索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. &lt;=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher."
    },
    {
      "title": "MolmoAct：能够在空间中进行推理的动作推理模型 (原标题: MolmoAct: Action Reasoning Models that can Reason in Space)",
      "link": "https://arxiv.org/abs/2508.07917",
      "pubDate": "Mon, 11 Aug 2025 08:32:45 GMT",
      "isoDate": "2025-08-11T08:32:45.000Z",
      "creator": "Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna",
      "summary": "### MolmoAct：一种新型动作推理模型\n\n**引言与背景**\n大多数现有的机器人基础模型将感知和指令直接映射到控制，这限制了它们的适应性、泛化能力和语义基础。然而，推理对于有目的的动作至关重要。\n\n**MolmoAct：动作推理模型 (ARMs)**\n*   **定义：** MolmoAct 引入了动作推理模型（ARMs），这是一类集成了感知、规划和控制的视觉-语言-动作模型。\n*   **三阶段结构化管道：** MolmoAct 通过一个结构化的三阶段管道实现其功能：\n    1.  **感知编码：** 将观察结果和指令编码为深度感知的感知令牌。\n    2.  **空间规划：** 生成中级空间计划，表现为可编辑的轨迹痕迹。\n    3.  **低级动作预测：** 预测精确的低级动作。\n*   **优势：** 这种设计使得MolmoAct能够实现可解释和可控的行为。\n\n**MolmoAct-7B-D 的性能表现**\nMolmoAct-7B-D 在模拟和真实世界环境中均展现出强大的性能：\n\n*   **SimplerEnv 视觉匹配任务：**\n    *   实现了 70.5% 的零样本准确率。\n    *   超越了闭源模型 Pi-0 和 GR00T N1。\n*   **LIBERO 任务：**\n    *   平均成功率达到 86.6%。\n    *   在长时程任务上，比 ThinkAct 额外提升了 6.3%。\n*   **真实世界微调（与 Pi-0-FAST 相比）：**\n    *   单臂任务进展额外提升 10%。\n    *   双臂任务进展额外提升 22.7%。\n*   **域外泛化能力：**\n    *   在域外泛化方面，比基线模型额外提升 23.3%。\n*   **人类偏好评分：**\n    *   在开放式指令遵循和轨迹引导方面，获得了最高的人类偏好评分。\n\n**MolmoAct 数据集**\n*   **首次发布：** 该研究首次发布了 MolmoAct 数据集。\n*   **内容：** 这是一个中训练阶段的机器人数据集，包含超过 10,000 条高质量的机器人轨迹，涵盖了多样化的场景和任务。\n*   **价值：** 使用该数据集进行训练，使模型在整体性能上比基础模型平均提升 5.5%。\n\n**开放资源与未来展望**\n研究团队开源了所有模型权重、训练代码、收集的数据集以及动作推理数据集。这使得 MolmoAct 不仅成为一个最先进的机器人基础模型，也为构建通过结构化推理将感知转化为有目的动作的 ARMs 提供了一个开放的蓝图。",
      "shortSummary": "MolmoAct 是一种新型动作推理模型（ARM），通过三阶段管道将感知、规划和控制整合，以实现可解释和可控的机器人行为。它在模拟和真实世界任务中均表现出色，例如在SimplerEnv上零样本准确率达70.5%，在LIBERO上平均成功率达86.6%，并显著提升了域外泛化能力。该研究还首次发布了包含10,000多条高质量机器人轨迹的MolmoAct数据集，并开源了所有模型和数据，为机器人基础模型的发展提供了重要贡献和开放蓝图。",
      "translated_title": "MolmoAct：能够在空间中进行推理的动作推理模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact"
    }
  ],
  "lastUpdated": "2025-08-13T09:35:31.953Z"
}