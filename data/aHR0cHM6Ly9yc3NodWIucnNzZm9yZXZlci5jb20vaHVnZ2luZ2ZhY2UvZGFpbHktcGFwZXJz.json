{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "第一部分：技巧还是陷阱？深入探讨强化学习在大型语言模型推理中的应用 (原标题: Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning)",
      "link": "https://arxiv.org/abs/2508.08221",
      "pubDate": "Mon, 11 Aug 2025 13:39:45 GMT",
      "isoDate": "2025-08-11T13:39:45.000Z",
      "creator": "Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, Bo Zheng",
      "summary": "## 强化学习在大型语言模型推理中的应用：挑战与解决方案\n\n### 研究背景与挑战\n\n强化学习（RL）在大型语言模型（LLM）推理领域的应用正迅速成为一个重要的研究方向，相关算法创新和实际应用研究显著增加。然而，该领域面临多项关键挑战：\n\n*   **缺乏标准化指导方针**：RL技术的使用缺乏统一标准。\n*   **机制理解碎片化**：对RL底层机制的理解不完整。\n*   **实验设置不一致**：训练数据、模型初始化和实验环境的差异导致结论相互矛盾，模糊了技术的关键特性。\n*   **实践者困惑**：上述问题使得实践者在选择合适的RL技术时感到困惑。\n\n### 本文方法与贡献\n\n为解决这些问题，本文系统地回顾了广泛采用的RL技术，并采取了以下严谨的方法：\n\n*   **严格复现与独立评估**：在一个统一的开源框架内，对RL技术进行严格复现和独立评估。\n*   **细粒度实验分析**：通过在不同难度数据集、模型大小和架构上进行细粒度实验，深入分析每种技术的内部机制、适用场景和核心原理。\n\n### 核心发现与实践指导\n\n基于深入的分析，本文提供了以下关键成果：\n\n*   **明确的RL技术选择指南**：为实践者提供了根据特定设置选择RL技术的清晰指南。\n*   **可靠的实践路线图**：为在LLM领域应用RL的实践者提供了可靠的路线图。\n*   **极简组合的有效性**：研究发现，通过结合两种极简技术，可以使用普通的PPO损失来解锁无评论家策略的学习能力。实验结果表明，这种简单的组合能够持续提升性能，甚至超越了GRPO和DAPO等策略。\n\n本文旨在为RL在LLM推理领域的应用提供更清晰的理解和更实用的指导。",
      "shortSummary": "本文深入探讨了强化学习（RL）在大型语言模型（LLM）推理中的应用，指出该领域面临缺乏标准化指南、机制理解碎片化及实验结果不一致等挑战。作者通过系统复现和细致实验，分析了RL技术的内部机制和适用场景，并提供了选择指南和实践路线图。研究发现，一种极简的RL技术组合能有效提升无评论家策略的性能，超越现有方法，为该领域提供了清晰的理解和实用指导。",
      "translated_title": "第一部分：技巧还是陷阱？深入探讨强化学习在大型语言模型推理中的应用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO."
    },
    {
      "title": "视觉中的强化学习：一项综述 (原标题: Reinforcement Learning in Vision: A Survey)",
      "link": "https://arxiv.org/abs/2508.08189",
      "pubDate": "Mon, 11 Aug 2025 13:08:55 GMT",
      "isoDate": "2025-08-11T13:08:55.000Z",
      "creator": "Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou",
      "summary": "本综述对强化学习（RL）与视觉智能交叉领域的新进展进行了批判性且最新的综合分析。该领域已使智能体能够感知复杂的视觉场景，并在其中进行推理、生成和行动。\n\n**主要内容和结构：**\n\n*   **问题形式化与策略优化：**\n    *   首先对视觉强化学习问题进行了形式化定义。\n    *   追溯了策略优化策略的演变，从人类反馈强化学习（RLHF）到可验证奖励范式，以及从近端策略优化（PPO）到群相对策略优化（Group Relative Policy Optimization）。\n\n*   **四大主题支柱：**\n    *   综述将200多项代表性工作组织成四个主要主题：\n        1.  **多模态大型语言模型（Multi-modal Large Language Models）：** 探讨其在视觉RL中的应用。\n        2.  **视觉生成（Visual Generation）：** 关注如何利用RL进行图像和视频生成。\n        3.  **统一模型框架（Unified Model Frameworks）：** 讨论旨在整合不同视觉RL任务的统一架构。\n        4.  **视觉-语言-动作模型（Vision-Language-Action Models）：** 探索结合视觉、语言理解和物理行动的智能体。\n    *   对于每个支柱，综述深入探讨了：\n        *   **算法设计：** 核心算法原理和创新。\n        *   **奖励工程：** 如何设计有效的奖励机制以引导学习。\n        *   **基准进展：** 现有评估标准和数据集的进步。\n        *   **趋势：** 识别了课程驱动训练、偏好对齐扩散模型和统一奖励建模等新兴趋势。\n\n*   **评估协议：**\n    *   审查了当前视觉RL的评估协议，包括：\n        *   **集合级保真度（Set-level fidelity）：** 评估生成结果与真实数据分布的匹配程度。\n        *   **样本级偏好（Sample-level preference）：** 评估模型输出在用户或专家偏好方面的表现。\n        *   **状态级稳定性（State-level stability）：** 评估智能体在不同状态下的行为一致性和鲁棒性。\n\n*   **开放挑战与未来方向：**\n    *   识别了该领域面临的开放挑战，包括：\n        *   **样本效率（Sample efficiency）：** 减少训练所需的交互数据量。\n        *   **泛化能力（Generalization）：** 提高模型在未见过环境或任务中的表现。\n        *   **安全部署（Safe deployment）：** 确保RL智能体在现实世界应用中的安全性和可靠性。\n\n**目标：**\n本综述旨在为研究人员和实践者提供一张关于快速扩展的视觉强化学习领域的连贯地图，并指出未来研究的有前景方向。",
      "shortSummary": "本综述全面审视了视觉强化学习（Visual RL）领域的最新进展。文章首先形式化了视觉RL问题，并追溯了策略优化策略的演变。随后，将200多项代表性工作归纳为多模态大型语言模型、视觉生成、统一模型框架和视觉-语言-动作模型四大主题，并探讨了各主题的算法设计、奖励工程、基准进展及趋势。最后，综述了评估协议，并指出了样本效率、泛化能力和安全部署等开放挑战，旨在为研究人员提供该领域的清晰图景和未来方向。",
      "translated_title": "视觉中的强化学习：一项综述",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."
    },
    {
      "title": "随形而动：通过轨迹引导区域控制实现形状感知图像编辑 (原标题: Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control)",
      "link": "https://arxiv.org/abs/2508.08134",
      "pubDate": "Mon, 11 Aug 2025 12:10:00 GMT",
      "isoDate": "2025-08-11T12:10:00.000Z",
      "creator": "Zeqian Long, Mingzhe Zheng, Kunyu Feng, Xinhua Zhang, Hongyu Liu, Harry Yang, Linfeng Zhang, Qifeng Chen, Yue Ma",
      "summary": "### 随形而动：通过轨迹引导区域控制实现形状感知图像编辑\n\n**1. 背景与问题**\n\n*   **现有挑战**：尽管近期基于流的图像编辑模型在各种任务中展现出通用能力，但在处理复杂场景，特别是涉及大规模形状变换时，它们往往难以达到专业水平。\n*   **具体问题**：在执行此类结构性编辑时，这些方法要么无法实现预期的形状改变，要么会无意中修改非目标区域，导致背景质量下降。\n\n**2. 提出的解决方案：Follow-Your-Shape 框架**\n\n*   **核心理念**：本文提出 Follow-Your-Shape，一个无需训练且无需掩码的框架，旨在支持对物体形状进行精确和可控的编辑，同时严格保留非目标内容。\n*   **创新机制**：\n    *   **动机**：受反演（inversion）和编辑（editing）轨迹之间差异的启发。\n    *   **轨迹散度图（Trajectory Divergence Map, TDM）**：通过比较反演路径和去噪路径之间逐令牌（token-wise）的速度差异来计算 TDM。\n    *   **TDM 的作用**：TDM 能够精确地定位可编辑区域。\n    *   **计划性 KV 注入（Scheduled KV Injection）**：TDM 进一步引导一个计划性 KV 注入机制，以确保编辑过程的稳定性和忠实性。\n\n**3. 评估与基准**\n\n*   **ReShapeBench**：为了促进严格的评估，研究人员引入了一个名为 ReShapeBench 的新基准。该基准包含120张新图像和专门为形状感知编辑精心策划的丰富提示对。\n\n**4. 实验结果**\n\n*   实验证明，Follow-Your-Shape 方法在可编辑性和视觉保真度方面均表现出卓越的性能。\n*   尤其在需要大规模形状替换的任务中，其效果尤为显著。",
      "shortSummary": "Follow-Your-Shape 是一种无需训练、无需掩码的图像编辑框架，旨在解决现有模型在处理大规模形状变换时的问题。它通过计算轨迹散度图（TDM）来精确识别可编辑区域，并引导计划性 KV 注入机制，实现对物体形状的精确、可控编辑，同时严格保留非目标内容。该方法在形状感知编辑任务中，尤其在需要大规模形状替换时，展现出卓越的编辑能力和视觉保真度。",
      "translated_title": "随形而动：通过轨迹引导区域控制实现形状感知图像编辑",
      "images": [],
      "contentSource": "完整文章",
      "content": "While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement."
    },
    {
      "title": "WideSearch：代理式广域信息搜寻的基准测试 (原标题: WideSearch: Benchmarking Agentic Broad Info-Seeking)",
      "link": "https://arxiv.org/abs/2508.07999",
      "pubDate": "Mon, 11 Aug 2025 10:03:09 GMT",
      "isoDate": "2025-08-11T10:03:09.000Z",
      "creator": "Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang",
      "summary": "## WideSearch：代理式广域信息搜寻的基准测试\n\n### 引言\n\n*   在专业研究和日常规划等诸多任务中，广域信息搜寻是主要的瓶颈，其特点是重复性高而非认知复杂。\n*   随着大型语言模型（LLMs）的快速发展，由LLMs驱动的自动化搜索代理有望将人类从这项繁琐的工作中解放出来。\n*   然而，由于缺乏合适的基准，这些代理执行“广域上下文”信息收集的可靠性和完整性尚未得到充分评估。\n\n### WideSearch 基准的引入\n\n*   为弥补这一空白，研究人员推出了 WideSearch，这是一个专门用于评估代理在大规模信息收集任务中可靠性的新基准。\n\n### WideSearch 的特点\n\n*   **问题数量与来源：** 包含200个手工整理的问题（100个英文，100个中文），来源于超过15个不同领域，并基于真实用户查询。\n*   **任务要求：** 每个任务要求代理收集大规模的原子信息，这些信息可以逐一客观验证，并需要将其整理成结构良好的输出。\n*   **质量控制：** 采用严格的五阶段质量控制流程，确保数据集的难度、完整性和可验证性。\n\n### 基准测试结果\n\n*   **测试对象：** 对超过10个最先进的代理式搜索系统进行了基准测试，包括单代理、多代理框架以及端到端商业系统。\n*   **代理表现：** 大多数系统的总体成功率接近0%，表现最好的系统也仅达到5%。\n*   **人类表现：** 然而，在充足的时间下，通过多个人类测试员的交叉验证，可以实现接近100%的成功率。\n*   **结论：** 这些结果表明，当前的搜索代理在处理大规模信息搜寻方面存在严重缺陷，这突显了未来代理式搜索研究和开发的紧迫领域。\n\n### 数据可用性\n\n*   WideSearch 数据集、评估流程和基准测试结果已公开发布。",
      "shortSummary": "研究人员推出了WideSearch，一个评估LLM驱动的代理在广域信息搜寻任务中可靠性的新基准。该基准包含200个手工整理的真实用户查询，要求代理收集并组织大规模可验证的原子信息。对10多个最先进代理系统的测试显示，它们的成功率普遍接近0%，最佳仅5%，而人类可达近100%。这表明当前搜索代理在处理大规模信息搜寻方面存在严重缺陷，亟需进一步研究与开发。数据集已公开。",
      "translated_title": "WideSearch：代理式广域信息搜寻的基准测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such \"wide-context\" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/"
    },
    {
      "title": "Omni-Effects：统一且空间可控的视觉效果生成 (原标题: Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation)",
      "link": "https://arxiv.org/abs/2508.07981",
      "pubDate": "Mon, 11 Aug 2025 09:41:24 GMT",
      "isoDate": "2025-08-11T09:41:24.000Z",
      "creator": "Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, Xiangxiang Chu",
      "summary": "### Omni-Effects：统一且空间可控的视觉效果生成\n\n**1. 背景与挑战**\n\n*   **重要性：** 视觉效果（VFX）是现代电影制作中不可或缺的视觉增强技术。\n*   **现有问题：** 尽管视频生成模型为VFX制作提供了经济高效的解决方案，但当前方法受限于“每效果LoRA训练”，这导致它们只能生成单一效果。\n*   **局限性：** 这种根本性限制阻碍了需要空间可控复合效果（即在指定位置同时生成多种效果）的应用。\n*   **主要挑战：** 将多样化效果整合到统一框架中面临两大挑战：\n    *   效果差异带来的干扰。\n    *   多VFX联合训练期间的空间不可控性。\n\n**2. 提出的解决方案：Omni-Effects 框架**\n\n*   **目标：** 提出Omni-Effects，这是首个能够生成提示引导效果和空间可控复合效果的统一框架。\n*   **核心创新：**\n    *   **LoRA-based Mixture of Experts (LoRA-MoE)：**\n        *   采用一组专家LoRA，将多样化效果整合到统一模型中。\n        *   有效缓解了跨任务干扰。\n    *   **Spatial-Aware Prompt (SAP) 空间感知提示：**\n        *   将空间掩码信息整合到文本标记中。\n        *   实现精确的空间控制。\n    *   **Independent-Information Flow (IIF) 独立信息流模块：**\n        *   集成在SAP内部。\n        *   隔离对应于单个效果的控制信号，防止不必要的混合。\n\n**3. 数据集与评估**\n\n*   **Omni-VFX 数据集：**\n    *   为促进研究，构建了一个全面的VFX数据集。\n    *   通过结合图像编辑和First-Last Frame-to-Video (FLF2V) 合成的新颖数据收集流程构建。\n*   **VFX 评估框架：**\n    *   引入了一个专用的VFX评估框架，用于验证模型性能。\n\n**4. 实验结果**\n\n*   广泛的实验证明，Omni-Effects 实现了精确的空间控制和多样化的效果生成。\n*   用户能够指定所需效果的类别和位置。",
      "shortSummary": "Omni-Effects 提出一个统一框架，旨在解决现有视频生成模型在视觉效果（VFX）生成中单一效果和空间不可控的限制。通过引入 LoRA-based Mixture of Experts (LoRA-MoE) 来整合多样化效果并减少干扰，以及 Spatial-Aware Prompt (SAP) 结合 Independent-Information Flow (IIF) 实现精确的空间控制，Omni-Effects 能够生成提示引导且空间可控的复合VFX。该框架还构建了 Omni-VFX 数据集并引入了专用评估框架，实验证明其在多样化效果生成和空间控制方面表现出色。",
      "translated_title": "Omni-Effects：统一且空间可控的视觉效果生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects."
    },
    {
      "title": "MolmoAct：能够在空间中进行推理的动作推理模型 (原标题: MolmoAct: Action Reasoning Models that can Reason in Space)",
      "link": "https://arxiv.org/abs/2508.07917",
      "pubDate": "Mon, 11 Aug 2025 08:32:45 GMT",
      "isoDate": "2025-08-11T08:32:45.000Z",
      "creator": "Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna",
      "summary": "### MolmoAct：一种新型动作推理模型\n\n**引言与背景**\n大多数现有的机器人基础模型将感知和指令直接映射到控制，这限制了它们的适应性、泛化能力和语义基础。然而，推理对于有目的的动作至关重要。\n\n**MolmoAct：动作推理模型 (ARMs)**\n*   **定义：** MolmoAct 引入了动作推理模型（ARMs），这是一类集成了感知、规划和控制的视觉-语言-动作模型。\n*   **三阶段结构化管道：** MolmoAct 通过一个结构化的三阶段管道实现其功能：\n    1.  **感知编码：** 将观察结果和指令编码为深度感知的感知令牌。\n    2.  **空间规划：** 生成中级空间计划，表现为可编辑的轨迹痕迹。\n    3.  **低级动作预测：** 预测精确的低级动作。\n*   **优势：** 这种设计使得MolmoAct能够实现可解释和可控的行为。\n\n**MolmoAct-7B-D 的性能表现**\nMolmoAct-7B-D 在模拟和真实世界环境中均展现出强大的性能：\n\n*   **SimplerEnv 视觉匹配任务：**\n    *   实现了 70.5% 的零样本准确率。\n    *   超越了闭源模型 Pi-0 和 GR00T N1。\n*   **LIBERO 任务：**\n    *   平均成功率达到 86.6%。\n    *   在长时程任务上，比 ThinkAct 额外提升了 6.3%。\n*   **真实世界微调（与 Pi-0-FAST 相比）：**\n    *   单臂任务进展额外提升 10%。\n    *   双臂任务进展额外提升 22.7%。\n*   **域外泛化能力：**\n    *   在域外泛化方面，比基线模型额外提升 23.3%。\n*   **人类偏好评分：**\n    *   在开放式指令遵循和轨迹引导方面，获得了最高的人类偏好评分。\n\n**MolmoAct 数据集**\n*   **首次发布：** 该研究首次发布了 MolmoAct 数据集。\n*   **内容：** 这是一个中训练阶段的机器人数据集，包含超过 10,000 条高质量的机器人轨迹，涵盖了多样化的场景和任务。\n*   **价值：** 使用该数据集进行训练，使模型在整体性能上比基础模型平均提升 5.5%。\n\n**开放资源与未来展望**\n研究团队开源了所有模型权重、训练代码、收集的数据集以及动作推理数据集。这使得 MolmoAct 不仅成为一个最先进的机器人基础模型，也为构建通过结构化推理将感知转化为有目的动作的 ARMs 提供了一个开放的蓝图。",
      "shortSummary": "MolmoAct 是一种新型动作推理模型（ARM），通过三阶段管道将感知、规划和控制整合，以实现可解释和可控的机器人行为。它在模拟和真实世界任务中均表现出色，例如在SimplerEnv上零样本准确率达70.5%，在LIBERO上平均成功率达86.6%，并显著提升了域外泛化能力。该研究还首次发布了包含10,000多条高质量机器人轨迹的MolmoAct数据集，并开源了所有模型和数据，为机器人基础模型的发展提供了重要贡献和开放蓝图。",
      "translated_title": "MolmoAct：能够在空间中进行推理的动作推理模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact"
    },
    {
      "title": "Grove MoE：迈向高效卓越的MoE LLM，采用伴随专家 (原标题: Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts)",
      "link": "https://arxiv.org/abs/2508.07785",
      "pubDate": "Mon, 11 Aug 2025 05:15:36 GMT",
      "isoDate": "2025-08-11T05:15:36.000Z",
      "creator": "Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li",
      "summary": "## Grove MoE：迈向高效卓越的MoE LLM，采用伴随专家\n\n### 引言：MoE架构的挑战\n\n混合专家（MoE）架构是当前最先进（SOTA）大型语言模型（LLM）的核心组成部分，它通过稀疏参数激活实现了模型的可扩展性。然而，传统的MoE架构存在局限性：它使用同质的、大小统一的专家，无论输入复杂性如何，都会激活固定数量的参数，这限制了计算效率。\n\n### Grove MoE架构创新\n\n为了克服传统MoE的局限性，本文引入了 **Grove MoE**，这是一种新颖的架构，其灵感来源于异构CPU架构，并融入了大小各异的专家。其核心创新点在于：\n\n*   **异构专家**：采用不同大小的专家，以更灵活地适应不同的计算需求。\n*   **新型伴随专家（Adjugate Experts）**：引入了具有动态激活机制的伴随专家，这使得模型容量得以扩展，同时将计算开销控制在可管理的范围内。\n\n### GroveMoE模型的实现与性能\n\n基于Grove MoE架构，研究人员开发了两个330亿参数的LLM模型：**GroveMoE-Base** 和 **GroveMoE-Inst**。这些模型通过对Qwen3-30B-A3B-Base模型进行中期训练和后期训练的“升级再利用”（upcycling）策略而开发。\n\n*   **动态参数激活**：GroveMoE模型能够根据令牌的复杂性动态激活3.14亿至3.28亿参数，这意味着它只激活处理当前输入所需的参数量，显著提高了效率。\n*   **卓越性能**：尽管动态激活的参数量相对较少，GroveMoE模型仍能实现与同等规模甚至更大规模的SOTA开源模型相媲美的性能。\n\n### 总结\n\nGrove MoE通过引入异构专家和动态激活的伴随专家，解决了传统MoE架构在计算效率上的瓶颈。其实现的GroveMoE-Base和GroveMoE-Inst模型展示了在保持高性能的同时，显著优化参数激活和计算开销的潜力，为未来高效LLM的发展提供了新的方向。",
      "shortSummary": "Grove MoE 引入了一种新型MoE架构，通过采用大小各异的“伴随专家”和动态激活机制，解决了传统MoE模型中专家同质化和参数固定激活的效率问题。基于此，330亿参数的GroveMoE-Base和GroveMoE-Inst模型能够根据输入复杂性动态激活3.14-3.28亿参数，实现了与同等或更大规模SOTA模型相当的性能，显著提升了LLM的计算效率和可扩展性。",
      "translated_title": "Grove MoE：迈向高效卓越的MoE LLM，采用伴随专家",
      "images": [],
      "contentSource": "完整文章",
      "content": "The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size."
    },
    {
      "title": "GLiClass：序列分类任务的通用轻量级模型 (原标题: GLiClass: Generalist Lightweight Model for Sequence Classification Tasks)",
      "link": "https://arxiv.org/abs/2508.07662",
      "pubDate": "Mon, 11 Aug 2025 02:22:25 GMT",
      "isoDate": "2025-08-11T02:22:25.000Z",
      "creator": "Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko",
      "summary": "## GLiClass：序列分类任务的通用轻量级模型\n\n### 摘要\n\n分类是人工智能应用中最广泛的任务之一，常作为数据过滤、排序和分类的第一步。现代AI系统需要处理大量输入数据，且早期管道阶段的错误可能向下游传播，因此实现高效率和高准确性至关重要。此外，分类要求可能根据用户需求动态变化，这需要模型具备强大的零样本（zero-shot）能力。\n\n### 现有方法的局限性\n\n文章指出了当前主流分类方法面临的挑战：\n\n*   **生成式大型语言模型 (LLMs)**：\n    *   尽管因其多功能性在零样本分类中变得主流，但存在指令遵循不一致的问题。\n    *   计算效率低下。\n*   **交叉编码器 (Cross-encoders)**：\n    *   常在RAG（检索增强生成）管道中用作重排序器。\n    *   面临瓶颈：必须顺序处理文本-标签对，在标签集较大时会显著降低效率。\n*   **基于嵌入的方法 (Embedding-based approaches)**：\n    *   效率较高。\n    *   但在涉及逻辑和语义约束的复杂场景中表现不佳。\n\n### GLiClass 提案\n\n文章提出了一种名为 **GLiClass** 的新颖方法，旨在克服上述局限性：\n\n*   **核心架构**：GLiClass 适配了 GLiNER 架构，用于序列分类任务。\n*   **性能优势**：\n    *   实现了与基于嵌入方法相当的强大准确性和效率。\n    *   保持了零样本（zero-shot）和少样本（few-shot）学习场景所需的灵活性。\n*   **多标签分类的创新**：\n    *   额外适配了近端策略优化（PPO）算法，用于多标签文本分类。\n    *   这一适配使得分类器能够在数据稀疏或基于人类反馈的条件下进行训练。\n\n### 结论\n\nGLiClass 提供了一个通用且轻量级的解决方案，旨在提高序列分类任务的效率和准确性，同时支持零样本和少样本学习，并通过PPO增强了在数据受限环境下的训练能力。",
      "shortSummary": "GLiClass是一种新型通用轻量级模型，专为序列分类任务设计。它旨在解决现有AI分类方法（如LLMs、交叉编码器和基于嵌入的方法）在效率、准确性及零样本能力上的不足。GLiClass通过适配GLiNER架构，实现了与基于嵌入方法相当的高准确性和效率，并支持零样本和少样本学习。该模型还引入了PPO，以支持在数据稀疏或人类反馈条件下进行多标签文本分类训练。",
      "translated_title": "GLiClass：序列分类任务的通用轻量级模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback."
    },
    {
      "title": "Klear-Reasoner：通过梯度保留裁剪策略优化提升推理能力 (原标题: Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization)",
      "link": "https://arxiv.org/abs/2508.07629",
      "pubDate": "Mon, 11 Aug 2025 01:17:51 GMT",
      "isoDate": "2025-08-11T01:17:51.000Z",
      "creator": "Zhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Guorui Zhou",
      "summary": "### Klear-Reasoner：通过梯度保留裁剪策略优化提升推理能力\n\n本文介绍了Klear-Reasoner模型，该模型在解决问题时展现出卓越的长期推理能力和细致的思考过程，并在多个基准测试中取得了出色的表现。\n\n**背景与挑战**\n\n尽管当前社区中存在许多优秀的推理模型，但由于训练细节披露不完整，重现高性能推理模型仍然面临诸多挑战。本报告旨在深入分析推理模型，涵盖从数据准备、长链式思考监督微调（long CoT SFT）到强化学习（RL）的整个训练后工作流程，并对每个实验组件进行了详细的消融研究。\n\n**关键发现与贡献**\n\n*   **监督微调（SFT）数据策略**\n    *   实验表明，少量高质量的数据源比大量多样化的数据源更有效。\n    *   对于SFT数据，即使不进行准确性过滤，困难样本也能取得更好的结果。\n\n*   **强化学习（RL）中的裁剪机制问题**\n    *   当前RL中的裁剪机制存在两个关键问题：\n        1.  裁剪抑制了关键的探索信号。\n        2.  裁剪机制忽略了次优轨迹。\n\n*   **梯度保留裁剪策略优化（GPPO）**\n    *   为解决上述挑战，本文提出了梯度保留裁剪策略优化（GPPO）。\n    *   GPPO通过温和地反向传播来自被裁剪令牌的梯度来工作。\n    *   GPPO的优势在于：\n        *   增强了模型的探索能力。\n        *   提高了模型从负样本中学习的效率。\n\n**性能表现**\n\nKlear-Reasoner在数学和编程领域展现出卓越的推理能力，具体表现如下：\n\n*   AIME 2024：90.5%\n*   AIME 2025：83.2%\n*   LiveCodeBench V5：66.0%\n*   LiveCodeBench V6：58.1%\n\n**总结**\n\nKlear-Reasoner通过优化训练流程，特别是引入GPPO来改进强化学习中的裁剪机制，显著提升了模型的推理能力和学习效率，在多个复杂任务上取得了领先的性能。",
      "shortSummary": "Klear-Reasoner是一个通过梯度保留裁剪策略优化（GPPO）提升推理能力的模型。它解决了现有强化学习裁剪机制抑制探索和忽略次优轨迹的问题，通过温和反向传播梯度，增强了模型探索和从负样本学习的效率。研究表明，少量高质量数据在监督微调中更有效。Klear-Reasoner在数学和编程基准测试中表现出色，例如AIME 2024达到90.5%。",
      "translated_title": "Klear-Reasoner：通过梯度保留裁剪策略优化提升推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\% on AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6."
    },
    {
      "title": "VisR-Bench：多语言长文档理解中视觉检索增强生成经验研究 (原标题: VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding)",
      "link": "https://arxiv.org/abs/2508.07493",
      "pubDate": "Sun, 10 Aug 2025 17:44:43 GMT",
      "isoDate": "2025-08-10T17:44:43.000Z",
      "creator": "Jian Chen, Ming Li, Jihyung Kil, Chenguang Wang, Tong Yu, Ryan Rossi, Tianyi Zhou, Changyou Chen, Ruiyi Zhang",
      "summary": "VisR-Bench：多语言长文档理解的视觉检索增强生成经验研究\n\n本文介绍了 **VisR-Bench**，这是一个旨在弥补现有文档检索基准不足的新型多语言基准。\n\n**现有挑战**\n*   大多数组织数据以文档形式存储，视觉检索对于从这些文档中获取集体智能至关重要。\n*   现有基准主要关注英语文档检索，或仅考虑单页图像上的多语言问答。\n\n**VisR-Bench 的贡献与特点**\n*   **目的**：专为长文档中的问题驱动型多模态检索而设计。\n*   **规模与质量**：包含超过 3.5 万个高质量问答对，涵盖 1200 篇文档。\n*   **评估能力**：支持对多模态检索进行细粒度评估。\n*   **语言覆盖**：涵盖十六种语言，提供多样化的语言覆盖。\n*   **问题类型**：包含三种问题类型——图表、文本和表格，确保问题覆盖的多样性。\n*   **独特设计**：与现有数据集不同，VisR-Bench 包含没有明确答案的查询，以防止模型依赖肤浅的关键词匹配。\n\n**模型评估与发现**\n*   **评估模型**：研究评估了多种检索模型，包括基于文本的方法、多模态编码器和多模态大型语言模型（MLLMs）。\n*   **性能对比**：结果显示，MLLMs 显著优于基于文本和多模态编码器模型。\n*   **面临挑战**：尽管 MLLMs 表现出色，但它们在处理结构化表格和低资源语言方面仍面临挑战。\n*   **研究意义**：这些发现突出了多语言视觉检索领域的关键挑战和未来研究方向。",
      "shortSummary": "VisR-Bench是一个新的多语言基准，用于长文档中的问题驱动型多模态检索。它旨在解决现有基准仅限于英语或单页多语言问答的局限性。VisR-Bench包含超过3.5万个问答对，涵盖16种语言和图表、文本、表格三种问题类型。评估结果显示，多模态大型语言模型（MLLMs）表现优异，但在处理结构化表格和低资源语言时仍面临挑战，揭示了多语言视觉检索的关键难题。",
      "translated_title": "VisR-Bench：多语言长文档理解中视觉检索增强生成经验研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval."
    },
    {
      "title": "少即是多：用于高效推理的免训练全局局部稀疏注意力 (原标题: Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning)",
      "link": "https://arxiv.org/abs/2508.07101",
      "pubDate": "Sat, 09 Aug 2025 17:10:33 GMT",
      "isoDate": "2025-08-09T17:10:33.000Z",
      "creator": "Lijie Yang, Zhihao Zhang, Arti Jain, Shijie Cao, Baihong Yuan, Yiwei Chen, Zhihao Jia, Ravi Netravali",
      "summary": "### LessIsMore：用于高效推理的免训练全局局部稀疏注意力\n\n**1. 问题背景与挑战**\n\n*   **计算开销巨大：** 大型推理模型通过测试时扩展实现了强大的性能，但带来了巨大的计算开销，尤其是在处理短输入提示时，会产生过多的token。\n*   **现有稀疏注意力方法的局限性：**\n    *   虽然稀疏注意力机制可以减少延迟和内存使用，但在长生成推理过程中，由于累积误差，会导致显著的准确性下降。\n    *   这些方法通常需要高token保留率或昂贵的再训练。\n\n**2. LessIsMore 机制介绍**\n\n*   **核心创新：** 本文引入了 LessIsMore，一种专为推理任务设计的免训练稀疏注意力机制。\n*   **全局注意力模式：** LessIsMore 利用全局注意力模式，而非依赖传统的、特定于头的局部优化。\n*   **统一的跨头token排序：**\n    *   它将来自局部注意力头的token选择与最近的上下文信息进行聚合。\n    *   这种聚合使得能够为未来的解码层进行统一的跨头token排序。\n*   **效率与泛化：** 这种统一的选择通过避免为每个头维护单独的token子集，从而提高了泛化能力和效率。\n\n**3. 评估结果与性能**\n\n*   **准确性保持与提升：** 在各种推理任务和基准测试中的评估表明，LessIsMore 在保持（在某些情况下甚至提高）准确性的同时，实现了显著的性能提升。\n*   **解码速度提升：** 相对于全注意力，LessIsMore 平均实现了 1.1 倍的解码速度提升。\n*   **token关注效率：** 在不损失准确性的情况下，LessIsMore 关注的token数量减少了 2 倍。\n*   **端到端速度提升：** 与现有稀疏注意力方法相比，LessIsMore 实现了 1.13 倍的端到端速度提升。",
      "shortSummary": "LessIsMore 是一种免训练的稀疏注意力机制，旨在解决大型推理模型的高计算开销和现有稀疏方法准确性下降的问题。它通过聚合全局注意力模式和上下文信息，实现统一的跨头token选择。LessIsMore 在保持甚至提高准确性的同时，实现了相对于全注意力 1.1 倍的解码速度提升，并比现有稀疏方法快 1.13 倍，同时关注的token数量减少一半。",
      "translated_title": "少即是多：用于高效推理的免训练全局局部稀疏注意力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods."
    },
    {
      "title": "ReasonRank：赋予段落排序强大的推理能力 (原标题: ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability)",
      "link": "https://arxiv.org/abs/2508.07050",
      "pubDate": "Sat, 09 Aug 2025 13:26:18 GMT",
      "isoDate": "2025-08-09T13:26:18.000Z",
      "creator": "Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou",
      "summary": "## ReasonRank：赋予段落排序强大的推理能力\n\n### 摘要\n\n该研究提出了一种名为 ReasonRank 的新型重排序器，旨在解决现有基于大型语言模型（LLM）的段落重排序器在复杂场景下推理能力不足的问题。尽管LLM在列表式排序任务中表现出色，且测试时逐步推理有助于提升性能，但由于缺乏推理密集型训练数据，现有重排序器在许多复杂排序场景中表现不佳，其推理能力仍未得到充分开发。\n\n### 核心贡献与方法\n\nReasonRank 的核心在于其创新的训练数据合成框架和两阶段后训练方法：\n\n1.  **自动化推理密集型训练数据合成框架：**\n    *   **数据来源：** 从多样化领域获取训练查询和段落。\n    *   **标签生成：** 应用 DeepSeek-R1 模型生成高质量的训练标签。\n    *   **质量控制：** 设计了一种自洽性数据过滤机制，以确保生成数据的质量。\n\n2.  **两阶段后训练方法：**\n    *   **第一阶段：冷启动监督微调（SFT）：** 旨在学习推理模式，为模型打下基础。\n    *   **第二阶段：强化学习（RL）：** 进一步增强模型的排序能力。\n        *   **奖励设计：** 基于列表式排序的特性，设计了一种多视图排序奖励机制，该机制比基于单一排序指标的奖励更为有效。\n\n### 实验结果与性能\n\n广泛的实验证明，ReasonRank 在多个方面取得了显著的优势：\n\n*   **性能超越：** ReasonRank 显著优于现有基线模型。\n*   **低延迟：** 相比于逐点重排序器 Rank1，ReasonRank 实现了更低的延迟。\n*   **最先进表现（SOTA）：** ReasonRank 在 BRIGHT 排行榜上取得了 40.6 的最先进性能。\n\n### 资源可用性\n\n相关代码已公开提供。",
      "shortSummary": "ReasonRank 旨在解决现有LLM重排序器在复杂场景下推理能力不足的问题。该研究提出了一种自动化推理密集型训练数据合成框架，并采用两阶段后训练方法（包括监督微调和基于多视图奖励的强化学习）。实验结果表明，ReasonRank 显著优于现有基线，具有更低的延迟，并在BRIGHT排行榜上取得了最先进（SOTA）性能，有效提升了段落排序的推理能力。",
      "translated_title": "ReasonRank：赋予段落排序强大的推理能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank."
    },
    {
      "title": "深度无知：通过过滤预训练数据为开源大语言模型构建防篡改安全保障 (原标题: Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs)",
      "link": "https://arxiv.org/abs/2508.06601",
      "pubDate": "Fri, 08 Aug 2025 13:59:47 GMT",
      "isoDate": "2025-08-08T13:59:47.000Z",
      "creator": "Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman",
      "summary": "## 深度无知：通过过滤预训练数据为开源大语言模型构建防篡改安全保障\n\n### 研究背景与问题\n\n*   **开源AI系统的脆弱性**：开源AI系统虽然具有透明度、促进开放研究和去中心化访问等独特优势，但它们极易受到篡改攻击。这些攻击可以通过修改模型权重或激活来高效地诱导有害行为。\n*   **现有方法的局限性**：目前，开源模型风险管理尚未形成一套稳健的科学体系。现有的安全微调方法和其他训练后技术难以使大型语言模型（LLMs）抵抗超过几十步的对抗性微调攻击。\n\n### 研究目的与方法\n\n*   **核心问题**：本研究旨在探讨从训练数据中过滤关于双重用途（dual-use）主题的文本，是否能有效预防不必要的能力，并作为一种更具防篡改性的安全保障。\n*   **创新方法**：研究引入了一个多阶段、可扩展的数据过滤流程。该流程被证明是一种可行且有效的方法，能够最大限度地减少LLMs中与生物威胁相关的代理知识。\n\n### 实验与主要发现\n\n*   **模型预训练**：研究从头开始预训练了多个6.9亿参数的模型。\n*   **显著的防篡改能力**：实验结果显示，这些模型对生物威胁相关文本的对抗性微调攻击表现出显著的抵抗力，能够抵御高达10,000步和3亿个token的攻击。这比现有训练后基线方法的性能高出一个数量级以上。\n*   **无负面影响**：在增强防篡改能力的同时，研究未观察到模型在无关能力方面的性能退化。\n*   **局限性与深度防御需求**：尽管过滤后的模型缺乏内化的危险知识，但研究发现，当这些信息在上下文中提供时（例如通过搜索工具增强），模型仍然能够利用这些信息。这表明需要采取“深度防御”（defense-in-depth）的方法。\n\n### 结论与启示\n\n*   **数据筛选的重要性**：这些发现有助于确立预训练数据筛选作为开源AI系统一个有前景的防御层。\n*   **未来方向**：强调了在构建安全、稳健的开源AI系统时，结合多种防御策略的必要性。",
      "shortSummary": "本研究探讨通过过滤预训练数据来增强开源大语言模型（LLMs）的防篡改能力。结果显示，过滤双重用途主题数据能显著提高模型对对抗性微调的抵抗力，性能优于现有方法一个数量级，且不影响其他能力。然而，模型仍可利用上下文中提供的危险信息。这表明预训练数据筛选是有效的防御层，但仍需结合深度防御策略。",
      "translated_title": "深度无知：通过过滤预训练数据为开源大语言模型构建防篡改安全保障",
      "images": [],
      "contentSource": "完整文章",
      "content": "Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems."
    },
    {
      "title": "BrowseComp-Plus：一个更公平透明的深度研究智能体评估基准 (原标题: BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent)",
      "link": "https://arxiv.org/abs/2508.06600",
      "pubDate": "Fri, 08 Aug 2025 13:55:11 GMT",
      "isoDate": "2025-08-08T13:55:11.000Z",
      "creator": "Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, Jimmy Lin",
      "summary": "# BrowseComp-Plus: 一个更公平透明的深度研究智能体评估基准\n\n## 摘要\n\n本文介绍了BrowseComp-Plus，一个旨在解决当前深度研究智能体评估基准（如BrowseComp）所面临的公平性和透明度挑战的新基准。深度研究智能体结合了大型语言模型（LLMs）和搜索工具，在处理需要迭代搜索规划和对搜索结果进行推理的复杂查询方面表现出色。\n\n## 当前评估的挑战\n\n现有的评估基准，例如BrowseComp，依赖于黑盒式的实时网络搜索API，存在显著的局限性：\n\n*   **公平性问题**：动态且不透明的网络API阻碍了深度研究方法之间的公平比较和结果的复现性。每次运行可能因为API的变化而得到不同的结果。\n*   **透明度问题**：由于无法控制文档语料库，很难单独评估检索器对整体性能的贡献。这意味着难以隔离和理解底层深度研究LLM的能力。\n\n简而言之，当前的评估可能在特定时间点比较一个完整的深度研究系统，但它们无法提供良好控制的实验，从而深入了解底层深度研究LLMs的能力。\n\n## 引入 BrowseComp-Plus\n\n为了应对这些挑战，研究人员推出了BrowseComp-Plus。这是一个从BrowseComp派生而来的基准，但采用了经过精心策划的固定语料库。\n\n### 核心特点：\n\n*   **固定且精选的语料库**：确保了评估的一致性和可复现性。\n*   **人工验证的支持文档**：BrowseComp-Plus中的每个查询都包含经过人工验证的支持文档。\n*   **挖掘的挑战性负例**：基准还包含了经过挖掘的具有挑战性的负面示例，这有助于更全面地测试系统的鲁棒性。\n\n这些特点使得BrowseComp-Plus能够进行受控的实验，从而更精确地评估深度研究智能体的性能。\n\n## 评估效果与案例\n\nBrowseComp-Plus已被证明在区分不同深度研究系统的性能方面非常有效。例如：\n\n*   开源模型**Search-R1**与**BM25检索器**结合时，准确率达到3.86%。\n*   **GPT-5**的准确率达到55.9%。\n*   将**GPT-5**与**Qwen3-Embedding-8B检索器**集成后，其准确率进一步提升至70.1%，同时搜索调用次数更少。\n\n## 结论与展望\n\nBrowseComp-Plus允许对深度研究智能体和检索方法进行全面的评估和解耦分析。它有助于深入了解：\n\n*   检索效率\n*   引用准确性\n*   深度研究系统中的上下文工程\n\n该基准为研究人员提供了一个更公平、更透明的平台，以推动深度研究领域的发展。",
      "shortSummary": "BrowseComp-Plus是一个新的深度研究智能体评估基准，旨在解决现有基准在公平性和透明度方面的不足。现有评估因依赖动态网络API而难以进行公平比较和复现。BrowseComp-Plus采用固定、精选的语料库，包含人工验证的支持文档和挑战性负例，从而实现受控实验。它能有效区分不同系统性能，例如GPT-5结合Qwen3-Embedding-8B检索器可达70.1%准确率。该基准促进了对深度研究智能体和检索方法的全面解耦分析。",
      "translated_title": "BrowseComp-Plus：一个更公平透明的深度研究智能体评估基准",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system."
    },
    {
      "title": "GLM-4.5：智能体、推理与编码（ARC）基础模型 (原标题: GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models)",
      "link": "https://arxiv.org/abs/2508.06471",
      "pubDate": "Fri, 08 Aug 2025 13:21:06 GMT",
      "isoDate": "2025-08-08T13:21:06.000Z",
      "creator": "GLM-4. 5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang",
      "summary": "## GLM-4.5：智能体、推理与编码（ARC）基础模型\n\n### 引言\n\n本文介绍了GLM-4.5，一个开源的混合专家（MoE）大型语言模型，旨在推动智能体（Agentic）、推理（Reasoning）和编码（Coding）（简称ARC）任务领域的研究进展。\n\n### 模型概述\n\n*   **模型类型**：GLM-4.5是一个开源的混合专家（MoE）大型语言模型。\n*   **参数规模**：\n    *   总参数量：3550亿（355B）。\n    *   激活参数量：320亿（32B）。\n*   **推理方法**：采用混合推理方法，支持“思考”（thinking）模式和“直接响应”（direct response）模式。\n*   **训练过程**：\n    *   经过多阶段训练，使用了23万亿（23T）个tokens。\n    *   通过专家模型迭代和强化学习进行全面的后期训练。\n\n### 性能表现\n\nGLM-4.5在智能体、推理和编码（ARC）任务中展现出强大的性能：\n\n*   **TAU-Bench**：得分70.1%。\n*   **AIME 24**：得分91.0%。\n*   **SWE-bench Verified**：得分64.2%。\n\n尽管参数量远少于一些竞争对手，GLM-4.5在所有评估模型中总体排名第三，在智能体基准测试中排名第二。\n\n### 发布版本\n\n为了促进推理和智能体AI系统的研究，GLM-4.5团队发布了两个版本：\n\n*   **GLM-4.5**：包含3550亿参数的完整版本。\n*   **GLM-4.5-Air**：一个更紧凑的版本，包含1060亿（106B）参数。\n\n### 可用性\n\n模型的代码、模型文件及更多详细信息已在线提供。",
      "shortSummary": "GLM-4.5是一个开源的混合专家（MoE）大型语言模型，总参数3550亿，激活参数320亿。它采用混合推理方法，在智能体、推理和编码（ARC）任务中表现出色，尽管参数量较少，仍位居评估模型前列。团队发布了GLM-4.5及其紧凑版GLM-4.5-Air，旨在推动AI研究。代码和模型已在线提供。",
      "translated_title": "GLM-4.5：智能体、推理与编码（ARC）基础模型",
      "images": [
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png",
          "alt": "BibSonomy logo",
          "title": "",
          "position": 1
        },
        {
          "url": "https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png",
          "alt": "Reddit logo",
          "title": "",
          "position": 2
        }
      ],
      "contentSource": "完整文章",
      "content": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5."
    },
    {
      "title": "Memp：探索智能体的程序记忆 (原标题: Memp: Exploring Agent Procedural Memory)",
      "link": "https://arxiv.org/abs/2508.06433",
      "pubDate": "Fri, 08 Aug 2025 12:20:56 GMT",
      "isoDate": "2025-08-08T12:20:56.000Z",
      "creator": "Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang",
      "summary": "Memp：探索智能体的程序记忆\n\n**1. 背景与问题**\n\n*   大型语言模型（LLM）驱动的智能体在执行各种任务方面表现出色。\n*   然而，它们面临一个关键挑战：程序记忆（procedural memory）脆弱，通常是手动设计或固化在静态参数中，难以学习和更新。\n\n**2. 核心贡献：Memp 框架**\n\n*   本文提出 Memp，旨在为智能体提供一种可学习、可更新且终身持续的程序记忆机制。\n*   Memp 的核心思想是将智能体过去的轨迹（trajectories）提炼成两种形式的知识：\n    *   **细粒度、分步的指令**：详细描述每一步的操作。\n    *   **高层次、脚本式的抽象**：概括任务的整体流程和策略。\n\n**3. 记忆的构建、检索与更新策略**\n\n*   研究探讨了构建（Build）、检索（Retrieval）和更新（Update）程序记忆的不同策略。\n*   Memp 结合了一个动态机制，持续地更新、纠正和废弃其内容，确保记忆库随着新经验的获取而不断演进。\n\n**4. 实验评估与成果**\n\n*   **评估任务**：在 TravelPlanner 和 ALFWorld 任务上进行了实证评估。\n*   **主要发现**：\n    *   随着记忆库的不断完善，智能体在类似任务上的成功率稳步提高，效率也显著提升。\n    *   由更强模型构建的程序记忆具有持久价值：将这些程序记忆迁移到较弱的模型上，能够带来显著的性能提升。\n\n**5. 项目状态**\n\n*   该研究目前处于进行中（Work in progress）的状态。",
      "shortSummary": "Memp 提出了一种为大型语言模型（LLM）智能体构建可学习、可更新程序记忆的方法。它通过提炼智能体过去的经验，生成细粒度指令和高层次脚本，并采用动态机制持续更新记忆。在 TravelPlanner 和 ALFWorld 任务上的评估显示，Memp 显著提高了智能体的成功率和效率。此外，由强模型构建的记忆迁移到弱模型上也能带来显著性能提升。",
      "translated_title": "Memp：探索智能体的程序记忆",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains."
    },
    {
      "title": "通用机器人策略中的捷径学习：数据集多样性和碎片化的作用 (原标题: Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation)",
      "link": "https://arxiv.org/abs/2508.06426",
      "pubDate": "Fri, 08 Aug 2025 12:14:01 GMT",
      "isoDate": "2025-08-08T12:14:01.000Z",
      "creator": "Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song",
      "summary": "## 通用机器人策略中的捷径学习：数据集多样性和碎片化的作用\n\n### 核心问题\n\n*   **泛化能力受限：** 尽管通用机器人策略（如在Open X-Embodiment (OXE)等大规模数据集上训练的策略）在广泛任务中表现出色，但它们在泛化到训练数据分布之外时常常遇到困难。\n\n### 根本原因\n\n*   **捷径学习：** 研究将这种有限泛化能力的根本原因确定为“捷径学习”，即策略依赖于与任务无关的特征。\n\n### 导致捷径学习的主要因素\n\n通过全面的理论和实证分析，研究揭示了导致捷径学习的两个主要因素：\n\n1.  **个体子数据集内部多样性有限：** 单个子数据集内部缺乏足够的多样性。\n2.  **子数据集之间显著的分布差异（数据集碎片化）：** 不同子数据集之间存在显著的分布不一致，导致数据集碎片化。\n\n### 问题根源\n\n*   这些问题源于OXE等大规模数据集的固有结构。这类数据集通常由多个子数据集组成，这些子数据集是在不同环境和不同机器人实体上独立收集的。\n\n### 研究发现与解决方案\n\n*   **数据集收集策略：** 研究结果为数据集收集策略提供了关键见解，这些策略可以减少捷径学习并增强通用机器人策略的泛化能力。\n*   **数据增强：** 在难以获取新的大规模数据的情况下，研究表明，精心选择的机器人数据增强策略可以有效减少现有离线数据集中的捷径学习，从而提高通用机器人策略（例如$\\pi_0$）在模拟和真实世界环境中的泛化能力。",
      "shortSummary": "通用机器人策略在泛化能力上存在局限，主要归因于“捷径学习”，即依赖任务无关特征。研究发现，这源于大规模数据集中子数据集内部多样性不足以及子数据集间显著的分布差异（碎片化）。论文提供了减少捷径学习的数据集收集策略，并指出数据增强能有效提升现有离线数据集的泛化能力，适用于模拟和真实环境。",
      "translated_title": "通用机器人策略中的捷径学习：数据集多样性和碎片化的作用",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., pi_0, in both simulation and real-world environments. More information at https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/."
    },
    {
      "title": "时序自我奖励语言模型：通过过去-未来解耦选中-拒绝样本 (原标题: Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future)",
      "link": "https://arxiv.org/abs/2508.06026",
      "pubDate": "Fri, 08 Aug 2025 01:25:54 GMT",
      "isoDate": "2025-08-08T01:25:54.000Z",
      "creator": "Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang",
      "summary": "## 时序自我奖励语言模型：通过过去-未来解耦选中-拒绝样本\n\n### 摘要\n\n本文提出了一种名为“时序自我奖励语言模型”（Temporal Self-Rewarding Language Models, TSRLM）的新架构，旨在解决现有自我奖励语言模型（Self-Rewarding Language Models, SR-LLMs）在迭代优化过程中遇到的关键限制。SR-LLMs通过LLM作为评判者（LLM-as-a-Judge）提示词来生成并评估自身输出，并通过迭代直接偏好优化（Direct Preference Optimization, DPO）动态提升生成能力。\n\n### 现有自我奖励范式的局限性\n\n作者分析指出，现有自我奖励范式存在一个关键限制：选中（chosen）和拒绝（rejected）响应的同步改进会逐渐缩小对比样本之间的表征差异，从而削弱有效的偏好学习。\n\n### 提出的解决方案：时序自我奖励语言模型（TSRLM）\n\n为了克服这一限制，TSRLM 策略性地协调过去、现在和未来的模型生成，以维持学习信号。其双阶段框架包括：\n\n1.  **锚定拒绝（Anchored Rejection）**：\n    *   通过使用过去初始模型的输出来固定被拒绝的响应。\n    *   这有助于保持被拒绝样本的“旧”特征，确保与“新”选中样本之间存在足够的差异。\n\n2.  **未来引导选中（Future-Guided Chosen）**：\n    *   通过使用下一代模型的预测来动态地策划选中样本。\n    *   这使得选中样本能够反映模型未来的潜在改进方向，进一步拉开与固定拒绝样本的距离。\n\n### 实验结果\n\n研究人员在三个模型家族（Llama、Qwen、Mistral）和不同模型尺寸（Llama3B/8B/70B）上进行了广泛实验，结果表明，与使用相同计算资源的自我奖励基线相比，TSRLM 训练的模型表现出显著改进。\n\n*   **生成能力提升**：\n    *   例如，使用 TSRLM 训练的 Llama3.1-8B 在 AlpacaEval 2.0 上的胜率达到 29.44%，比自我奖励基线（19.69%）高出 9.75%。\n\n*   **域外泛化能力**：\n    *   值得注意的是，尽管没有专门收集相关训练数据，TSRLM 在数学推理（GSM8K）、基于知识的问答（ARC、TruthfulQA）和代码生成（HumanEval）任务中也表现出卓越的域外泛化能力。\n\n### 结论\n\nTSRLM 通过巧妙地解耦选中和拒绝样本的改进过程，有效解决了现有自我奖励范式中偏好学习信号衰减的问题，显著提升了大型语言模型的生成性能和泛化能力。",
      "shortSummary": "现有自我奖励语言模型因选中与拒绝样本同步改进导致偏好学习效率下降。本文提出时序自我奖励语言模型（TSRLM），通过“锚定拒绝”（使用过去模型输出）和“未来引导选中”（使用下一代模型预测）的双阶段框架，有效解耦样本差异，维持学习信号。实验证明，TSRLM在多种模型和任务上显著优于现有方法，提升了生成能力和域外泛化性，例如Llama3.1-8B在AlpacaEval 2.0上胜率提升9.75%。",
      "translated_title": "时序自我奖励语言模型：通过过去-未来解耦选中-拒绝样本",
      "images": [],
      "contentSource": "完整文章",
      "content": "Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data."
    },
    {
      "title": "修剪不意外：通过首词惊奇度实现高效代码推理 (原标题: Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal)",
      "link": "https://arxiv.org/abs/2508.05988",
      "pubDate": "Thu, 07 Aug 2025 23:46:21 GMT",
      "isoDate": "2025-08-07T23:46:21.000Z",
      "creator": "Wenhao Zeng, Yaoning Wang, Chao Hu, Yuling Shi, Chengcheng Wan, Hongyu Zhang, Xiaodong Gu",
      "summary": "# ASAP：一种高效代码推理的CoT压缩框架\n\n## 引言：大型推理模型（LRMs）与思维链（CoT）的挑战\n\n近年来，大型推理模型（LRMs）通过扩展思维链（CoT）的长度，在代码推理方面展现出卓越的能力。然而，过长的推理轨迹带来了巨大的挑战，包括高昂的训练成本、推理延迟以及部署可行性。为了解决这一问题，尽管出现了多种CoT压缩方法，但它们面临固有的权衡：\n\n*   **Token级方法**：通常会破坏语法和逻辑连贯性。\n*   **步级方法**：基于困惑度（perplexity）的方法无法可靠地捕捉到逻辑上关键的推理步骤。\n\n## ASAP框架：锚点引导与基于惊奇度的剪枝\n\n本文提出了一种名为ASAP（Anchor-guided, Surprisal-based Pruning）的新型粗粒度到细粒度CoT压缩框架。ASAP旨在克服现有方法的局限性，实现高效的代码推理。\n\nASAP框架的核心步骤包括：\n\n1.  **锚点引导剪枝（Anchor-guided Pruning）**\n    *   ASAP首先执行锚点引导剪枝，以保留核心推理结构。\n    *   这一步骤能够高效地减少后续处理的搜索空间。\n\n2.  **基于首词惊奇度的逻辑感知剪枝（Logic-aware Pruning via First-Token Surprisal）**\n    *   在锚点引导剪枝的基础上，ASAP通过选择逻辑上必要的推理步骤来实现逻辑感知剪枝。\n    *   这一选择是基于一种新颖的“首词惊奇度”（first-token surprisal）指标。\n\n3.  **推理时自主生成与利用（Autonomous Generation and Leveraging at Inference Time）**\n    *   最后，ASAP训练模型在推理时自主生成并利用这些简洁的CoT。\n    *   这使得在编码任务中能够实现高效的推理。\n\n## 实验结果与性能\n\n实验结果表明，ASAP在多个代码生成基准测试中实现了最先进的准确性，同时显著降低了训练和推理成本。\n\n*   **整体表现**：ASAP在代码生成任务中表现出色，实现了高准确性和成本效益。\n*   **LiveCodeBench v4_v5基准测试**：\n    *   在具有挑战性的LiveCodeBench v4_v5基准测试中，ASAP相较于最强的基线方法，将token生成量减少了23.5%。\n    *   推理延迟降低了43.5%。\n    *   同时，在Pass@1指标上取得了36.19%的竞争性准确率。\n\n## 结论\n\nASAP的成果突显了构建强大且高效的LRMs的一个有前景的方向。该方法通过智能地压缩CoT，解决了LRMs在实际应用中面临的效率瓶颈，为未来的代码推理模型发展提供了新的思路。",
      "shortSummary": "大型推理模型（LRMs）的过长思维链（CoT）导致高成本和延迟。本文提出ASAP框架，通过锚点引导和基于首词惊奇度的逻辑感知剪枝来压缩CoT。ASAP使模型在推理时自主生成简洁CoT，实现了高效代码推理。实验表明，ASAP在代码生成任务中达到最先进的准确率，并显著降低训练和推理成本。在LiveCodeBench上，ASAP减少23.5%的token生成和43.5%的推理延迟，同时保持高准确率。",
      "translated_title": "修剪不意外：通过首词惊奇度实现高效代码推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs."
    },
    {
      "title": "Bifrost-1：使用补丁级CLIP潜在变量连接多模态大型语言模型和扩散模型 (原标题: Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents)",
      "link": "https://arxiv.org/abs/2508.05954",
      "pubDate": "Thu, 07 Aug 2025 22:38:47 GMT",
      "isoDate": "2025-08-07T22:38:47.000Z",
      "creator": "Han Lin, Jaemin Cho, Amir Zadeh, Chuan Li, Mohit Bansal",
      "summary": "## Bifrost-1：连接多模态LLM与扩散模型的新框架\n\n### 核心问题\n\n当前研究面临的挑战是如何将高保真视觉合成能力整合到大型语言模型（LLM）中，同时不损害其强大的推理能力。现有方法，无论是直接训练LLM还是桥接LLM与扩散模型，通常面临高昂的训练成本，因为LLM骨干网络在预训练阶段并未接触过图像表示。\n\n### Bifrost-1 解决方案\n\nBifrost-1 提出了一种统一的框架，旨在解决上述问题。它通过以下方式连接预训练的多模态LLM（MLLM）和扩散模型：\n\n*   **关键连接点**：使用**补丁级CLIP图像嵌入**作为潜在变量。这些嵌入与MLLM的CLIP视觉编码器天然对齐。\n*   **扩散模型集成**：将这些补丁级图像嵌入通过对扩散模型进行轻量级的ControlNet适应性修改，整合到扩散模型中。\n*   **保留MLLM推理能力**：为了保持MLLM原有的多模态推理能力，Bifrost-1 为MLLM配备了一个视觉生成分支。该分支在预测补丁级图像嵌入时，从原始MLLM参数初始化。\n\n### 优势与成果\n\n通过无缝整合预训练的MLLM和扩散模型，并利用补丁级CLIP潜在变量，Bifrost-1 框架实现了：\n\n*   **高保真可控图像生成**：能够生成高质量且可控的图像。\n*   **显著的训练效率**：在训练过程中计算成本大幅降低。\n\n实验结果表明，Bifrost-1 在视觉保真度和多模态理解方面，达到了与现有方法相当或更优的性能，同时训练所需的计算资源显著减少。此外，全面的消融研究也验证了其设计选择的有效性。",
      "shortSummary": "Bifrost-1是一个统一框架，旨在通过补丁级CLIP潜在变量连接预训练的多模态LLM（MLLM）和扩散模型。该框架实现了高保真、可控的图像生成，同时显著提高了训练效率。实验证明，Bifrost-1在视觉保真度和多模态理解方面表现优异，且训练计算成本远低于现有方法，同时保留了MLLM的推理能力。",
      "translated_title": "Bifrost-1：使用补丁级CLIP潜在变量连接多模态大型语言模型和扩散模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices."
    }
  ],
  "lastUpdated": "2025-08-12T09:36:27.390Z"
}