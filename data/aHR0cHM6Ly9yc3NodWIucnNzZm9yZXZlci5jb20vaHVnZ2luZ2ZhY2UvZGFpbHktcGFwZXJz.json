{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "可追溯证据增强的视觉基础推理：评估与方法 (原标题: Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology)",
      "link": "https://arxiv.org/abs/2507.07999",
      "pubDate": "Thu, 10 Jul 2025 13:59:58 GMT",
      "isoDate": "2025-07-10T13:59:58.000Z",
      "creator": "Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang",
      "summary": "## 可追溯证据增强的视觉基础推理：评估与方法\n\n### 引言\n\n当前，像OpenAI-o3这样的模型通过动态引用视觉区域，在视觉基础推理方面取得了突破，这类似于人类的“图像思考”能力。然而，目前缺乏一个能够全面评估这些能力的基准。\n\n### TreeBench：诊断性基准\n\n为弥补这一空白，研究人员提出了 **TreeBench (Traceable Evidence Evaluation Benchmark)**，这是一个诊断性基准，其构建基于以下三个核心原则：\n\n1.  **聚焦于复杂场景中细微目标的视觉感知**：强调对难以察觉目标的识别能力。\n2.  **通过边界框评估实现可追溯证据**：确保推理过程中的视觉证据是可定位和可验证的。\n3.  **二阶推理**：测试模型超越简单目标定位，理解对象间交互和空间层次的能力。\n\n**构建过程与挑战：**\n\n*   TreeBench优先选择包含密集对象的图像，最初从SA-1B数据集中采样了1000张高质量图像。\n*   八位大型多模态模型（LMM）专家手动标注了每张图像的问题、候选选项和答案。\n*   经过三阶段的严格质量控制，TreeBench最终包含405个极具挑战性的视觉问答对。\n*   **模型表现**：即使是最先进的模型也难以应对TreeBench，没有一个模型的准确率能达到60%，例如OpenAI-o3的得分仅为54.87%。\n\n### TreeVGR：可追溯证据增强的视觉基础推理训练范式\n\n除了基准，研究还引入了 **TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning)**，这是一种新的训练范式：\n\n*   **方法**：它通过强化学习共同监督定位和推理过程，旨在实现精确的定位和可解释的推理路径。\n*   **初始化与效果**：TreeVGR以Qwen2.5-VL-7B为基础进行初始化，并在多个基准测试中展现出显著提升：\n    *   V* Bench：提升16.8个百分点\n    *   MME-RealWorld：提升12.6个百分点\n    *   TreeBench：提升13.4个百分点\n*   **核心发现**：这些结果证明，可追溯性是推动视觉基础推理进步的关键。\n\n### 结论与代码可用性\n\n该研究通过提出TreeBench基准和TreeVGR训练范式，为视觉基础推理的评估和发展提供了新的方向。代码已公开。",
      "shortSummary": "本研究针对视觉基础推理领域缺乏全面评估基准的问题，提出了TreeBench，一个包含405个挑战性视觉问答对的诊断性基准，强调细微感知、可追溯证据和二阶推理。现有先进模型在该基准上表现不佳。此外，研究还引入了TreeVGR训练范式，通过强化学习联合监督定位和推理，显著提升了模型在多个基准上的性能。研究表明，可追溯性是推动视觉基础推理进步的关键。",
      "translated_title": "可追溯证据增强的视觉基础推理：评估与方法",
      "images": [],
      "contentSource": "完整文章",
      "content": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human \"thinking with images\". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR."
    },
    {
      "title": "PyVision：具有动态工具的智能体视觉 (原标题: PyVision: Agentic Vision with Dynamic Tooling)",
      "link": "https://arxiv.org/abs/2507.07998",
      "pubDate": "Thu, 10 Jul 2025 13:59:55 GMT",
      "isoDate": "2025-07-10T13:59:55.000Z",
      "creator": "Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, Chen Wei",
      "summary": "## PyVision：具有动态工具的智能体视觉\n\n### 引言\n\n大型语言模型（LLMs）正日益被部署为智能体，这些系统能够进行规划、推理并动态调用外部工具。然而，在视觉推理领域，现有方法大多受限于预定义的工作流程和静态工具集，这限制了其灵活性和适应性。\n\n### PyVision 框架概述\n\nPyVision 是一个创新性的交互式、多轮次框架，旨在解决传统视觉推理方法的局限性。其核心功能在于：\n\n*   **自主工具生成与执行**：PyVision 使得多模态大型语言模型（MLLMs）能够自主地生成、执行和完善基于 Python 的工具。\n*   **任务定制化**：这些生成的工具是根据当前任务量身定制的，从而实现了高度灵活且可解释的问题解决能力。\n\n### 研究方法与分析\n\n为了全面评估 PyVision 的能力，研究团队进行了以下工作：\n\n*   **工具分类体系**：开发了一套 PyVision 所创建工具的分类体系，有助于理解其内部机制和应用范围。\n*   **基准测试分析**：分析了这些工具在各种多样化基准测试中的使用情况，以验证其有效性和泛化能力。\n\n### 量化性能提升\n\n实验结果表明，PyVision 带来了显著且持续的性能提升：\n\n*   **GPT-4.1 提升**：在 V* 基准测试中，PyVision 使 GPT-4.1 的性能提升了 **+7.8%**。\n*   **Claude-4.0-Sonnet 提升**：在 VLMsAreBlind-mini 基准测试中，PyVision 使 Claude-4.0-Sonnet 的性能大幅提升了 **+31.1%**。\n\n### 深远意义\n\n这些量化结果指向了一个更广泛、更重要的转变：\n\n*   **从“使用”到“发明”**：动态工具不仅允许模型使用现有工具，更重要的是，它赋予了模型“发明”新工具的能力。\n*   **推动智能体视觉推理**：这一能力上的飞跃，标志着智能体视觉推理领域迈向了更高级、更自主的阶段。\n\n### 其他信息\n\n*   本报告共26页，并包含10张图表（注：文章内容未提供具体的图片链接）。\n*   研究主题涵盖：计算与语言（cs.CL）、人工智能（cs.AI）和计算机视觉与模式识别（cs.CV）。\n*   引用信息：arXiv:2507.07998。",
      "shortSummary": "PyVision 是一个创新框架，使多模态大型语言模型（MLLMs）能够自主生成、执行和优化定制化的 Python 工具，以进行灵活且可解释的视觉推理。它解决了传统方法中静态工具集的局限性。实验证明，PyVision 显著提升了 GPT-4.1 和 Claude-4.0-Sonnet 在视觉基准测试上的表现。这项工作表明，动态工具允许模型“发明”而非仅仅使用工具，从而推动了智能体视觉推理的发展。",
      "translated_title": "PyVision：具有动态工具的智能体视觉",
      "images": [],
      "contentSource": "完整文章",
      "content": "LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning."
    },
    {
      "title": "跳过一层还是循环它？预训练大型语言模型在测试时期的深度自适应 (原标题: Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs)",
      "link": "https://arxiv.org/abs/2507.07996",
      "pubDate": "Thu, 10 Jul 2025 13:59:53 GMT",
      "isoDate": "2025-07-10T13:59:53.000Z",
      "creator": "Ziyue Li, Yang Li, Tianyi Zhou",
      "summary": "## 预训练大型语言模型在测试时期的深度自适应：跳过层或循环层\n\n### 核心问题\n\n文章探讨了预训练神经网络在不进行微调的情况下，能否根据不同输入自适应其架构。具体而言，它质疑了对于简单任务是否需要所有层，以及对于复杂任务现有层是否足够。\n\n### 提出的解决方案：层链 (CoLa)\n\n研究发现，预训练大型语言模型（LLM）的层可以被视为独立的模块进行操作，从而为每个测试样本构建一个更优甚至更浅的模型。这种操作包括：\n\n*   **跳过/剪枝层：** 对于某些输入，可以跳过不必要的层。\n*   **重复层（循环神经网络式）：** 同一层可以被重复多次，类似于循环神经网络（RNN）的行为。\n\n这些操作允许层以任意顺序堆叠，为每个样本形成一个独特的“层链”（Chain-of-Layers, CoLa）。这种组合空间极大地扩展了现有关于循环/递归预训练模块、层剪枝或提前退出网络的工作范围。\n\n### 探索与优化方法\n\n研究开发了一种**蒙特卡洛树搜索（MCTS）协议**，用于探索并识别数学和常识推理基准中每个样本的最佳CoLa配置。\n\n### CoLa的优势\n\n与固定深度的静态模型相比，CoLa提供了更灵活、动态的架构，以适应不同的输入：\n\n*   **快捷路径（快速思考）：** 允许跳过层，实现更快的推理。\n*   **相同层的重复（慢速思考）：** 允许重复使用层，进行更深入的计算。\n*   **结合两者：** 能够根据任务需求灵活地结合快速和慢速思考模式。\n\n### 关键发现\n\n对MCTS优化的CoLa进行了广泛分析，得出了两个关键发现：\n\n1.  **推理效率提升：** 对于原始LLM预测正确的样本中，超过75%的样本可以找到更短的CoLa。这表明在提高推理效率方面存在巨大潜力。\n2.  **性能增强：** 对于原始LLM预测错误的样本中，超过60%的样本通过CoLa实现了正确的预测。这表明在提升模型性能方面存在巨大潜力。\n\n### 结论与展望\n\n研究结果强调了使用固定架构的预训练LLM在处理不同样本时的局限性，并为解锁测试时深度自适应的泛化能力铺平了道路。",
      "shortSummary": "本文提出“层链”（CoLa）方法，允许预训练LLM在测试时动态调整架构，通过跳过或重复层来适应不同输入。利用蒙特卡洛树搜索（MCTS）优化CoLa，研究发现：对于原始LLM预测正确的样本，CoLa能显著提高推理效率；对于预测错误的样本，CoLa能提升性能。这表明固定架构LLM存在局限性，测试时深度自适应有望解锁其更强的泛化能力。",
      "translated_title": "跳过一层还是循环它？预训练大型语言模型在测试时期的深度自适应",
      "images": [],
      "contentSource": "完整文章",
      "content": "Can a pretrained neural network adapt its architecture to different inputs without any finetuning? Do we need all layers for simple tasks, and are they adequate for challenging tasks? We found that the layers of a pretrained large language model (LLM) can be manipulated as separate modules to build a better and even shallower model customized for each test sample. In particular, each layer from the pretrained model can be skipped/pruned or repeated multiple times as recurrent neural networks (RNN), and stacked with others in arbitrary orders, yielding a chain-of-layers (CoLa) per sample. This compositional space greatly expands the scope of existing works on looped/recurrent pretrained modules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree Search (MCTS) protocol to explore and identify the optimal CoLa for each sample from math and commonsense reasoning benchmarks. Compared to a static model of a fixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same layer(s) (slow thinking), and combining both, offering more flexible, dynamic architectures for different inputs. We conduct an extensive analysis of the MCTS-optimized CoLa, which leads to two key findings: (1) For &gt;75% of samples with correct predictions by the original LLM, we can find shorter CoLa, suggesting a large space for improving inference efficiency; (2) For &gt;60% of samples with originally incorrect predictions, we can identify CoLa achieving correct predictions, suggesting a large space of performance enhancement. Our results highlight the shortcomings of using a fixed architecture of pre-trained LLMs for inference on different samples and pave the way to unlock the generalization power of test-time depth adaptation."
    },
    {
      "title": "用于视频LLM免训练加速的多粒度时空令牌合并 (原标题: Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs)",
      "link": "https://arxiv.org/abs/2507.07990",
      "pubDate": "Thu, 10 Jul 2025 13:59:02 GMT",
      "isoDate": "2025-07-10T13:59:02.000Z",
      "creator": "Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim",
      "summary": "# 用于视频LLM免训练加速的多粒度时空令牌合并 (STTM)\n\n## 概述\n视频大语言模型（Video LLMs）通过利用大量的时空令牌实现了强大的视频理解能力，但其计算成本随令牌数量呈二次方增长。为了解决这一问题，本文提出了一种名为STTM（Spatio-Temporal Token Merging）的免训练时空令牌合并方法。\n\n## 核心洞察\nSTTM的关键在于利用视频数据中被先前工作忽视的局部空间和时间冗余。\n\n## 方法论\nSTTM采用分解式合并方法，其步骤如下：\n\n1.  **多粒度空间令牌转换：** 首先，STTM通过在四叉树结构上进行从粗到精的搜索，将每一帧转换为多粒度空间令牌。\n2.  **定向成对时间合并：** 接着，STTM在时间维度上执行定向的成对合并。\n\n这种分解式合并方法能够有效地减少令牌数量。\n\n## 性能与优势\n\n*   **卓越性能：** STTM在六个视频问答（Video QA）基准测试中，表现优于现有的令牌缩减方法。\n*   **显著加速：**\n    *   在50%的令牌预算下，STTM实现了2倍的加速，而准确率仅下降0.5%。\n    *   在30%的令牌预算下，STTM实现了3倍的加速，而准确率仅下降2%。\n*   **查询无关性：** STTM是查询无关的，这意味着对于同一视频，可以重复使用KV缓存来处理不同的问题，进一步提高了效率。\n\n## 结论\nSTTM提供了一种高效且免训练的视频LLM加速方案，通过智能地利用视频数据的时空冗余，在保持高准确率的同时显著降低了计算成本。该研究已获得ICCV2025接受。",
      "shortSummary": "STTM是一种免训练的多粒度时空令牌合并方法，旨在加速视频大语言模型（Video LLMs）。它通过利用视频数据的局部空间和时间冗余，将每帧转换为多粒度空间令牌，并进行时间维度上的合并。STTM在视频问答基准测试中表现优异，能在50%令牌预算下实现2倍加速且准确率仅下降0.5%，或在30%预算下实现3倍加速且准确率仅下降2%。它还支持KV缓存复用，提高效率。",
      "translated_title": "用于视频LLM免训练加速的多粒度时空令牌合并",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video large language models (LLMs) achieve strong video understanding by leveraging a large number of spatio-temporal tokens, but suffer from quadratic computational scaling with token count. To address this, we propose a training-free spatio-temporal token merging method, named STTM. Our key insight is to exploit local spatial and temporal redundancy in video data which has been overlooked in prior work. STTM first transforms each frame into multi-granular spatial tokens using a coarse-to-fine search over a quadtree structure, then performs directed pairwise merging across the temporal dimension. This decomposed merging approach outperforms existing token reduction methods across six video QA benchmarks. Notably, STTM achieves a 2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and a 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is query-agnostic, allowing KV cache reuse across different questions for the same video. The project page is available at https://www.jshyun.me/projects/sttm."
    },
    {
      "title": "OST-Bench：评估多模态大语言模型在线时空场景理解能力 (原标题: OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding)",
      "link": "https://arxiv.org/abs/2507.07984",
      "pubDate": "Thu, 10 Jul 2025 13:56:07 GMT",
      "isoDate": "2025-07-10T13:56:07.000Z",
      "creator": "JingLi Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai Wang, Jiangmiao Pang",
      "summary": "# OST-Bench：评估多模态大语言模型在线时空场景理解能力\n\n## 引言\n近期多模态大语言模型（MLLMs）在整合视觉与语言进行复杂推理方面展现出卓越能力。然而，大多数现有基准测试是在离线环境中，使用固定预录制输入集来评估模型。为了更好地反映真实世界具身感知的挑战，本文引入了**OST-Bench**，一个旨在从主动探索场景的智能体视角评估在线时空理解能力的基准。\n\n## OST-Bench 的核心特点\nOST-Bench 的设计强调了两个关键方面：\n*   **在线（Online）**：要求模型处理并推理增量获取的观察结果。这意味着模型需要实时地、逐步地处理信息。\n*   **时空（Spatio-Temporal）**：需要将当前视觉输入与历史记忆相结合，以支持动态的空间推理。这模拟了智能体在环境中移动时，需要记住之前看到的内容并将其与当前视图关联起来的能力。\n\n这种设计更好地反映了真实世界中具身智能体（如机器人）在探索未知环境时所面临的挑战。\n\n## 数据集构建\nOST-Bench 建立在一个高效的数据收集管道之上，包含：\n*   **场景数量**：1.4k 个场景。\n*   **问答对数量**：10k 对问答。\n*   **数据来源**：数据从 ScanNet、Matterport3D 和 ARKitScenes 等多样化来源收集。\n\n## 评估结果与挑战\n研究人员使用 OST-Bench 评估了多个领先的 MLLMs，并观察到：\n*   **性能不足**：这些模型在需要复杂时空推理的任务上表现不佳。\n*   **在线设置下的性能下降**：在在线设置中，随着探索范围的扩大和记忆的增长，模型的准确性会下降。这表明模型在处理长期、累积的信息时面临困难。\n\n通过进一步的实验分析，研究人员识别出模型普遍存在的错误模式，并发现两个独立轴上的核心挑战显著降低了模型性能：\n1.  **复杂线索驱动的空间推理需求**：模型难以根据复杂线索进行精确的空间判断。\n2.  **长期记忆检索要求**：模型在需要检索和利用长期历史记忆时表现不佳。\n\n这些发现突出了在改进在线具身推理方面必须解决的核心挑战。\n\n## 资源可用性\n为了促进该领域的进一步研究和发展，OST-Bench 的代码、数据集和基准测试均已公开。项目页面为：`this https URL`。",
      "shortSummary": "OST-Bench是一个新基准，旨在评估多模态大语言模型（MLLMs）的在线时空场景理解能力。它模拟智能体主动探索场景，要求模型处理增量观察并整合历史记忆进行动态空间推理。研究发现，现有MLLMs在复杂时空推理任务上表现不足，尤其是在探索范围扩大和记忆增长时准确性下降。主要挑战在于复杂的空间推理和长期记忆检索。该基准及其资源已公开，以促进相关研究。",
      "translated_title": "OST-Bench：评估多模态大语言模型在线时空场景理解能力",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in multimodal large language models (MLLMs) have shown remarkable capabilities in integrating vision and language for complex reasoning. While most existing benchmarks evaluate models under offline settings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a benchmark designed to evaluate Online Spatio-Temporal understanding from the perspective of an agent actively exploring a scene. The Online aspect emphasizes the need to process and reason over incrementally acquired observations, while the Spatio-Temporal component requires integrating current visual inputs with historical memory to support dynamic spatial reasoning. OST-Bench better reflects the challenges of real-world embodied perception. Built on an efficient data collection pipeline, OST-Bench consists of 1.4k scenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and ARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that they fall short on tasks requiring complex spatio-temporal reasoning. Under the online setting, their accuracy declines as the exploration horizon extends and the memory grows. Through further experimental analysis, we identify common error patterns across models and find that both complex clue-based spatial reasoning demands and long-term memory retrieval requirements significantly drop model performance along two separate axes, highlighting the core challenges that must be addressed to improve online embodied reasoning. To foster further research and development in the field, our codes, dataset, and benchmark are available. Our project page is: https://rbler1234.github.io/OSTBench.github.io/"
    },
    {
      "title": "几何强制：结合视频扩散与3D表示以实现一致的世界建模 (原标题: Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling)",
      "link": "https://arxiv.org/abs/2507.07982",
      "pubDate": "Thu, 10 Jul 2025 13:55:08 GMT",
      "isoDate": "2025-07-10T13:55:08.000Z",
      "creator": "Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, Jiang Bian",
      "summary": "## 几何强制：结合视频扩散与3D表示以实现一致的世界建模\n\n### 核心问题\n\n文章指出，视频本质上是动态3D世界的2D投影。然而，仅通过原始视频数据训练的视频扩散模型，在学习到的表示中往往未能捕捉到有意义的几何感知结构。\n\n### 解决方案：几何强制（Geometry Forcing）\n\n为了弥合视频扩散模型与物理世界潜在3D性质之间的差距，研究人员提出了“几何强制”（Geometry Forcing）方法。这是一种简单而有效的方法，旨在鼓励视频扩散模型内化潜在的3D表示。\n\n### 关键洞察\n\n“几何强制”的核心洞察在于，通过将模型的中间表示与预训练的几何基础模型中的特征对齐，从而引导这些中间表示朝向几何感知的结构。\n\n### 互补的对齐目标\n\n为此，该方法引入了两个互补的对齐目标：\n\n1.  **角度对齐（Angular Alignment）**：\n    *   通过余弦相似度强制执行方向一致性。\n\n2.  **尺度对齐（Scale Alignment）**：\n    *   通过从归一化的扩散表示中回归未归一化的几何特征，从而保留与尺度相关的信息。\n\n### 实验评估\n\n研究人员在以下两种视频生成任务上评估了“几何强制”方法：\n\n*   相机视角条件视频生成（camera view-conditioned video generation）\n*   动作条件视频生成（action-conditioned video generation）\n\n### 实验结果\n\n实验结果表明，与基线方法相比，该方法显著提高了视觉质量和3D一致性。",
      "shortSummary": "文章提出了“几何强制”（Geometry Forcing）方法，旨在解决视频扩散模型在捕捉3D几何结构方面的不足。该方法通过将模型的中间表示与预训练的几何基础模型特征对齐，并引入角度对齐和尺度对齐两个目标，来鼓励模型内化潜在的3D表示。实验结果表明，该方法显著提高了视频生成任务中的视觉质量和3D一致性。",
      "translated_title": "几何强制：结合视频扩散与3D表示以实现一致的世界建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Videos inherently represent 2D projections of a dynamic 3D world. However, our analysis suggests that video diffusion models trained solely on raw video data often fail to capture meaningful geometric-aware structure in their learned representations. To bridge this gap between video diffusion models and the underlying 3D nature of the physical world, we propose Geometry Forcing, a simple yet effective method that encourages video diffusion models to internalize latent 3D representations. Our key insight is to guide the model's intermediate representations toward geometry-aware structure by aligning them with features from a pretrained geometric foundation model. To this end, we introduce two complementary alignment objectives: Angular Alignment, which enforces directional consistency via cosine similarity, and Scale Alignment, which preserves scale-related information by regressing unnormalized geometric features from normalized diffusion representation. We evaluate Geometry Forcing on both camera view-conditioned and action-conditioned video generation tasks. Experimental results demonstrate that our method substantially improves visual quality and 3D consistency over the baseline methods. Project page: https://GeometryForcing.github.io."
    },
    {
      "title": "将强化学习扩展到长视频 (原标题: Scaling RL to Long Videos)",
      "link": "https://arxiv.org/abs/2507.07966",
      "pubDate": "Thu, 10 Jul 2025 13:47:40 GMT",
      "isoDate": "2025-07-10T13:47:40.000Z",
      "creator": "Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han",
      "summary": "# 将强化学习扩展到长视频：LongVILA-R1 框架\n\n本文介绍了一个名为 LongVILA-R1 的全栈框架，旨在利用强化学习（RL）将视觉-语言模型（VLMs）中的推理能力扩展到长视频。该框架通过整合三个关键组件，解决了长视频推理的独特挑战。\n\n## 核心组件\n\n1.  **大规模数据集：LongVideo-Reason**\n    *   包含 52,000 对长视频问答（QA）对。\n    *   具有高质量的推理标注。\n    *   涵盖体育、游戏和视频博客等多种领域。\n\n2.  **两阶段训练流程**\n    *   通过链式思考监督微调（CoT-SFT）和强化学习（RL）来扩展 VLM。\n    *   旨在逐步提升模型在长视频上的推理能力。\n\n3.  **长视频 RL 训练基础设施：多模态强化序列并行（MR-SP）**\n    *   集成了序列并行技术。\n    *   采用基于 vLLM 的引擎，专为长视频优化。\n    *   利用缓存的视频嵌入进行高效的 rollout 和预填充，显著提高训练效率。\n\n## 实验结果与性能\n\n*   **模型表现：** LongVILA-R1-7B 在 VideoMME 等长视频问答基准测试中取得了强大的性能。\n*   **与现有模型对比：** 在 LongVideo-Reason-eval 基准测试中，LongVILA-R1-7B 优于 Video-R1-7B，甚至在时间推理、目标和目的推理、空间推理以及情节推理方面与 Gemini-1.5-Pro 相当。\n*   **训练效率提升：** MR-SP 系统在长视频 RL 训练中实现了高达 2.1 倍的加速。\n*   **可扩展性：** 随着输入视频帧数量的增加，LongVILA-R1 表现出持续的性能提升。\n\n## 贡献与未来展望\n\nLongVILA-R1 标志着 VLM 在长视频推理方面迈出了坚实的一步。此外，作者团队还发布了其训练系统，供公众使用。该系统支持在各种模态（视频、文本和音频）、各种模型（VILA 和 Qwen 系列），甚至图像和视频生成模型上进行 RL 训练。在一个 A100 节点（8 个 GPU）上，该系统支持对长达一小时的视频（例如 3,600 帧/约 256k tokens）进行 RL 训练。",
      "shortSummary": "本文提出了一个名为 LongVILA-R1 的全栈框架，旨在通过强化学习将视觉-语言模型（VLMs）的推理能力扩展到长视频。该框架整合了大规模数据集 LongVideo-Reason、两阶段训练流程（CoT-SFT + RL）和高效的训练基础设施 MR-SP。实验表明，LongVILA-R1-7B 在长视频问答任务上表现出色，性能可与 Gemini-1.5-Pro 媲美，且 MR-SP 系统能将训练速度提升高达 2.1 倍。该工作为 VLM 的长视频推理迈出了坚实一步，并发布了可支持多模态 RL 训练的系统。",
      "translated_title": "将强化学习扩展到长视频",
      "images": [],
      "contentSource": "完整文章",
      "content": "We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens)."
    },
    {
      "title": "用于端到端分层序列建模的动态分块 (原标题: Dynamic Chunking for End-to-End Hierarchical Sequence Modeling)",
      "link": "https://arxiv.org/abs/2507.07955",
      "pubDate": "Thu, 10 Jul 2025 13:39:37 GMT",
      "isoDate": "2025-07-10T13:39:37.000Z",
      "creator": "Sukjun Hwang, Brandon Wang, Albert Gu",
      "summary": "## 动态分块与端到端分层序列建模\n\n### 现有挑战\n\n尽管近年来语言模型（LMs）取得了显著进展，主要得益于从特定任务模型转向基于强大架构（如Transformer）的通用模型，但分词等预处理步骤仍然是实现真正端到端基础模型的障碍。\n\n### 提出的解决方案：动态分块与H-Net\n\n本文引入了一系列新技术，实现了一种**动态分块机制**。该机制能够自动学习内容和上下文相关的分段策略，并与模型的其余部分联合学习。将此机制整合到一个**显式分层网络（H-Net）**中，可以取代传统的（隐式分层的）“分词-语言模型-反分词”管道，实现一个完全端到端学习的单一模型。\n\n### 关键发现与优势\n\n*   **性能超越传统模型**：在计算资源和数据量匹配的情况下，一个在字节级别操作的单层H-Net，其性能优于强大的基于BPE（字节对编码）标记的Transformer语言模型。\n*   **多层级提升**：将层级迭代到多个阶段，可以进一步提高H-Net的性能，因为它能够建模多个抽象层次。这展示了其在数据扩展方面的显著优势，并能匹配两倍大小的基于标记的Transformer模型。\n*   **字符级鲁棒性**：在英语上预训练的H-Net显著提高了字符级鲁棒性。\n*   **自学习分块策略**：H-Net能够定性地学习到有意义的、依赖于数据的分块策略，而无需任何启发式规则或显式监督。\n*   **在弱分词模态中的优势**：H-Net相对于传统分词管道的改进，在分词启发式较弱的语言和模态中表现更为突出，例如中文、代码或DNA序列（在DNA序列上，数据效率比基线模型提高了近4倍）。这充分展示了真正端到端模型从原始数据中学习和扩展的巨大潜力。",
      "shortSummary": "本文提出动态分块机制和分层网络（H-Net），旨在实现真正的端到端语言模型。H-Net通过联合学习内容和上下文相关的分段策略，取代传统分词流程。研究表明，H-Net在性能、数据扩展性和字符级鲁棒性方面超越了基于标记的Transformer模型。尤其在中文、代码和DNA序列等分词启发式较弱的语言和模态中，H-Net的优势更为显著，展现了从原始数据中高效学习的潜力。",
      "translated_title": "用于端到端分层序列建模的动态分块",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite incredible progress in language models (LMs) in recent years, largely resulting from moving away from specialized models designed for specific tasks to general models based on powerful architectures (e.g. the Transformer) that learn everything from raw data, pre-processing steps such as tokenization remain a barrier to true end-to-end foundation models. We introduce a collection of new techniques that enable a dynamic chunking mechanism which automatically learns content -- and context -- dependent segmentation strategies learned jointly with the rest of the model. Incorporating this into an explicit hierarchical network (H-Net) allows replacing the (implicitly hierarchical) tokenization-LM-detokenization pipeline with a single model learned fully end-to-end. When compute- and data- matched, an H-Net with one stage of hierarchy operating at the byte level outperforms a strong Transformer language model operating over BPE tokens. Iterating the hierarchy to multiple stages further increases its performance by modeling multiple levels of abstraction, demonstrating significantly better scaling with data and matching a token-based Transformer of twice its size. H-Nets pretrained on English show significantly increased character-level robustness, and qualitatively learn meaningful data-dependent chunking strategies without any heuristics or explicit supervision. Finally, the H-Net's improvement over tokenized pipelines is further increased in languages and modalities with weaker tokenization heuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement in data efficiency over baselines), showing the potential of true end-to-end models that learn and scale better from unprocessed data."
    },
    {
      "title": "Re-Bottleneck：神经音频自编码器的潜在重构 (原标题: Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders)",
      "link": "https://arxiv.org/abs/2507.07867",
      "pubDate": "Thu, 10 Jul 2025 11:47:43 GMT",
      "isoDate": "2025-07-10T11:47:43.000Z",
      "creator": "Dimitrios Bralios, Jonah Casebeer, Paris Smaragdis",
      "summary": "## Re-Bottleneck：神经音频自编码器的潜在重构\n\n### 核心问题\n\n*   神经音频编解码器和自编码器在音频压缩、传输、特征提取和潜在空间生成方面表现出多功能性。\n*   然而，一个主要限制是大多数模型都旨在最大化重建保真度，却常常忽视了为在各种下游应用中实现最佳性能所需的特定潜在结构。\n\n### 提出的解决方案：Re-Bottleneck 框架\n\n*   本文提出了一种简单、事后（post-hoc）的框架来解决上述问题，通过修改预训练自编码器的瓶颈层。\n*   该方法引入了一个“Re-Bottleneck”，这是一个内部瓶颈层，它专门通过潜在空间损失进行训练，以灌输用户定义的结构。\n\n### 框架的有效性演示\n\n作者通过三个实验证明了 Re-Bottleneck 框架的有效性：\n\n1.  **潜在通道排序：** 在不牺牲重建质量的前提下，强制对潜在通道进行排序。\n2.  **语义对齐：** 将潜在表示与语义嵌入对齐，并分析其对下游扩散模型的影响。\n3.  **引入等变性：** 确保输入波形上的滤波操作直接对应于潜在空间中的特定变换。\n\n### 结论与优势\n\n*   Re-Bottleneck 框架提供了一种灵活且高效的方式来定制神经音频模型的表示。\n*   它使这些模型能够以最少的额外训练无缝地满足不同应用的多样化需求。\n\n### 其他信息\n\n*   该论文已被 IEEE MLSP 2025 接受。",
      "shortSummary": "本文提出了“Re-Bottleneck”框架，通过修改预训练神经音频自编码器的瓶颈层，解决其潜在结构不足以支持下游应用的问题。该框架引入一个内部瓶颈，通过潜在空间损失训练以注入用户定义结构。实验证明，它能在不牺牲重建质量的情况下实现潜在通道排序、语义对齐和等变性。Re-Bottleneck 提供了一种灵活高效的方式，以最少的额外训练定制音频表示，满足各种应用需求。",
      "translated_title": "Re-Bottleneck：神经音频自编码器的潜在重构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Neural audio codecs and autoencoders have emerged as versatile models for audio compression, transmission, feature-extraction, and latent-space generation. However, a key limitation is that most are trained to maximize reconstruction fidelity, often neglecting the specific latent structure necessary for optimal performance in diverse downstream applications. We propose a simple, post-hoc framework to address this by modifying the bottleneck of a pre-trained autoencoder. Our method introduces a \"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space losses to instill user-defined structure. We demonstrate the framework's effectiveness in three experiments. First, we enforce an ordering on latent channels without sacrificing reconstruction quality. Second, we align latents with semantic embeddings, analyzing the impact on downstream diffusion modeling. Third, we introduce equivariance, ensuring that a filtering operation on the input waveform directly corresponds to a specific transformation in the latent space. Ultimately, our Re-Bottleneck framework offers a flexible and efficient way to tailor representations of neural audio models, enabling them to seamlessly meet the varied demands of different applications with minimal additional training."
    },
    {
      "title": "超越线性可分性上限 (原标题: Beyond the Linear Separability Ceiling)",
      "link": "https://arxiv.org/abs/2507.07574",
      "pubDate": "Thu, 10 Jul 2025 05:23:32 GMT",
      "isoDate": "2025-07-10T05:23:32.000Z",
      "creator": "Enrico Vompa, Tanel Tammet, Mohit Vaishnav",
      "summary": "## 超越线性可分性上限：视觉-语言模型（VLM）的推理瓶颈分析\n\n### 核心问题：线性推理瓶颈\n\n*   **现象：** 大多数最先进的视觉-语言模型（VLMs）在抽象推理任务上似乎受到其视觉嵌入的线性可分性限制。\n*   **定义：** 本研究将此限制称为“线性推理瓶颈”。\n\n### 引入线性可分性上限（LSC）\n\n*   **测量方法：** 为了调查这一瓶颈，研究引入了线性可分性上限（LSC），即一个简单线性分类器在VLM视觉嵌入上的表现。\n\n### 瓶颈的根源与性质\n\n*   **普遍性：** 研究发现，这一瓶颈普遍存在。\n*   **原因分析：** 瓶颈并非源于模型糟糕的感知能力，而是由于语言模型推理路径的失败。\n\n### 解决方案：可解决的对齐问题\n\n*   **本质：** 研究表明，这是一个可解决的对齐问题。\n*   **干预策略：** 所需的干预措施是任务依赖的：\n    *   对于语义概念，激活现有路径就足够了。\n    *   对于复杂的关联推理，则需要调整核心模型权重。\n\n### 潜在的强大推理能力\n\n*   **方法控制：** 通过使用后缀微调（postfix tuning）作为方法学控制，研究发现了VLM内部存在强大、休眠的推理路径的有力证据。\n\n### 挑战与结论\n\n*   **复杂任务的挑战：** 然而，对于需要更深层适应的复杂关联任务，即使显式地提高表示质量，也可能导致模型在新的提示格式上失败，尽管其嵌入仍然保持良好的可分性。\n*   **研究视角：** 最终，这项工作为VLM分析提供了一个新视角，表明鲁棒的推理是目标对齐的问题，而不仅仅是改进表示学习的问题。",
      "shortSummary": "本研究探讨了视觉-语言模型（VLMs）在抽象推理任务中面临的“线性推理瓶颈”，发现其并非源于感知不足，而是语言模型推理路径的失败。通过引入线性可分性上限（LSC），研究表明这是一个可解决的对齐问题，干预措施需根据任务而异。尽管VLM内部存在休眠的强大推理路径，但对于复杂任务，提升表示质量可能导致模型在新的提示格式上失效。研究强调，鲁棒推理在于目标对齐，而非简单地改进表示学习。",
      "translated_title": "超越线性可分性上限",
      "images": [],
      "contentSource": "完整文章",
      "content": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by the linear separabilty of their visual embeddings on abstract reasoning tasks. This work investigates this \"linear reasoning bottleneck\" by introducing the Linear Separability Ceiling (LSC), the performance of a simple linear classifier on a VLM's visual embeddings. We find this bottleneck is widespread and stems not from poor perception, but from failures in the language model's reasoning pathways. We demonstrate this is a solvable alignment issue. The required intervention, however, is task-dependent: activating existing pathways suffices for semantic concepts, while complex relational reasoning requires adapting core model weights. Using postfix tuning as a methodological control, we find strong evidence for powerful, dormant reasoning pathways within VLMs. However, for complex relational tasks requiring deeper adaptation, explicitly improving representation quality causes the model to fail on new prompt formats despite its embeddings remaining well separated. Ultimately, this work provides a new lens for VLM analysis, showing that robust reasoning is a matter of targeted alignment, not simply improved representation learning."
    },
    {
      "title": "机器胡言乱语：表征大型语言模型中新兴的对真相的漠视 (原标题: Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models)",
      "link": "https://arxiv.org/abs/2507.07484",
      "pubDate": "Thu, 10 Jul 2025 03:11:57 GMT",
      "isoDate": "2025-07-10T03:11:57.000Z",
      "creator": "Kaiqu Liang, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas L. Griffiths, Jaime Fernández Fisac",
      "summary": "### 机器胡言乱语：表征大型语言模型中新兴的对真相的漠视\n\n本文引入了“机器胡言乱语”（Machine Bullshit）这一概念，旨在表征大型语言模型（LLMs）中出现的对真相的漠视现象。\n\n#### 概念框架\n\n*   **定义：** 借鉴哲学家哈里·法兰克福对“胡言乱语”的定义，即不顾及真相的陈述，本文将“机器胡言乱语”定义为LLMs在生成内容时对真相价值的漠视。\n*   **区别与拓展：** 这一框架超越了以往对LLM“幻觉”（hallucination）和“谄媚”（sycophancy）的研究，提供了一个更全面的视角来理解LLMs中普遍存在的真相丧失现象及其潜在机制。\n\n#### 量化与分类\n\n*   **胡言乱语指数（Bullshit Index）：** 提出了一种新颖的量化指标，用于衡量LLMs对真相的漠视程度。\n*   **四种定性形式的分类法：** 补充性地分析了四种不同形式的机器胡言乱语：\n    *   **空洞修辞（empty rhetoric）：** 内容空泛，缺乏实质性信息。\n    *   **含糊其辞（paltering）：** 故意使用模糊或误导性语言，部分真实但整体误导。\n    *   **闪烁其词（weasel words）：** 使用模棱两可的词语，为后续推翻或否认留下余地。\n    *   **未经证实的主张（unverified claims）：** 提出未经证实或无法验证的说法。\n\n#### 实证评估与主要发现\n\n研究团队在多个数据集上进行了实证评估，包括Marketplace数据集、Political Neutrality数据集，以及专门为评估机器胡言乱语而设计的新基准——**BullshitEval**（包含2,400个场景，涵盖100个AI助手）。\n\n*   **RLHF（人类反馈强化学习）的影响：** 结果表明，通过RLHF进行模型微调会显著加剧机器胡言乱语现象。\n*   **CoT（思维链）提示的影响：** 推理时采用思维链（Chain-of-Thought）提示会显著放大特定形式的胡言乱语，尤其是空洞修辞和含糊其辞。\n*   **政治语境中的表现：** 在政治语境中，机器胡言乱语普遍存在，其中“闪烁其词”是主导策略。\n\n#### 研究意义\n\n这些发现凸显了AI对齐（AI alignment）面临的系统性挑战，并为实现更真实的LLM行为提供了新的见解。\n\n#### 补充信息\n\n*   项目页面、代码和数据可在提供的链接中获取。",
      "shortSummary": "这项研究引入了“机器胡言乱语”概念，将其定义为大型语言模型（LLMs）对真相的漠视。研究提出了“胡言乱语指数”和四种定性分类（空洞修辞、含糊其辞、闪烁其词、未经证实的主张）。通过实证评估，发现人类反馈强化学习（RLHF）会显著加剧胡言乱语，而思维链（CoT）提示会放大特定形式。尤其在政治语境中，闪烁其词现象普遍。研究强调了AI对齐的挑战，并为提升LLM的真实性提供了新视角。",
      "translated_title": "机器胡言乱语：表征大型语言模型中新兴的对真相的漠视",
      "images": [],
      "contentSource": "完整文章",
      "content": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior."
    },
    {
      "title": "长视频故事生成综述：架构、一致性与电影级质量 (原标题: A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality)",
      "link": "https://arxiv.org/abs/2507.07202",
      "pubDate": "Wed, 09 Jul 2025 14:20:33 GMT",
      "isoDate": "2025-07-09T14:20:33.000Z",
      "creator": "Mohamed Elmoghany, Ryan Rossi, Seunghyun Yoon, Subhojyoti Mukherjee, Eslam Bakr, Puneet Mathur, Gang Wu, Viet Dac Lai, Nedim Lipka, Ruiyi Zhang, Varun Manjunatha, Chien Nguyen, Daksh Dangi, Abel Salinas, Mohammad Taesiri, Hongjie Chen, Xiaolei Huang, Joe Barrow, Nesreen Ahmed, Hoda Eldardiry, Namyong Park, Yu Wang, Jaemin Cho, Anh Totti Nguyen, Zhengzhong Tu, Thien Nguyen, Dinesh Manocha, Mohamed Elhoseiny, Franck Dernoncourt",
      "summary": "# 长视频故事生成综述\n\n本综述深入探讨了长视频故事生成领域的现状、挑战及解决方案。\n\n## 当前挑战\n*   **时长限制：** 尽管视频生成模型取得了显著进展，但现有最先进的方法通常只能生成5-16秒的视频，这些视频常被标记为“长视频”。\n*   **一致性问题：** 视频时长一旦超过16秒，就难以在整个叙事过程中保持角色外观和场景布局的一致性。\n*   **多主体复杂性：** 特别是多主体长视频，在保持角色一致性和运动连贯性方面仍未能达到理想效果。\n*   **冗余与多样性：** 尽管有些方法可以生成长达150秒的视频，但它们往往存在帧冗余和时间多样性低的问题。\n\n## 研究目的与方法\n*   近期研究已尝试生成包含多个角色、叙事连贯且细节高保真的长视频。\n*   本综述全面研究了32篇关于视频生成的论文，旨在识别能够持续产生这些高质量长视频的关键架构组件和训练策略。\n\n## 主要贡献\n*   构建了一个全面的新型现有方法分类体系。\n*   提供了比较表格，根据架构设计和性能特征对论文进行分类。",
      "shortSummary": "当前视频生成模型在生成长视频时面临时长短、角色和场景一致性差、帧冗余等挑战。本综述全面研究了32篇相关论文，旨在识别生成具有多角色、叙事连贯性和高保真细节长视频的关键架构与训练策略。研究构建了新的分类体系，并提供了按架构和性能分类的比较表格，为克服长视频生成难题提供了见解。",
      "translated_title": "长视频故事生成综述：架构、一致性与电影级质量",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite the significant progress that has been made in video generative models, existing state-of-the-art methods can only produce videos lasting 5-16 seconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16 seconds struggle to maintain consistent character appearances and scene layouts throughout the narrative. In particular, multi-subject long videos still fail to preserve character consistency and motion coherence. While some methods can generate videos up to 150 seconds long, they often suffer from frame redundancy and low temporal diversity. Recent work has attempted to produce long-form videos featuring multiple characters, narrative coherence, and high-fidelity detail. We comprehensively studied 32 papers on video generation to identify key architectural components and training strategies that consistently yield these qualities. We also construct a comprehensive novel taxonomy of existing methods and present comparative tables that categorize papers by their architectural designs and performance characteristics."
    },
    {
      "title": "4KAgent：代理式任意图像4K超分辨率 (原标题: 4KAgent: Agentic Any Image to 4K Super-Resolution)",
      "link": "https://arxiv.org/abs/2507.07105",
      "pubDate": "Wed, 09 Jul 2025 13:59:19 GMT",
      "isoDate": "2025-07-09T13:59:19.000Z",
      "creator": "Yushen Zuo, Qi Zheng, Mingyang Wu, Xinrui Jiang, Renjie Li, Jian Wang, Yide Zhang, Gengchen Mai, Lihong V. Wang, James Zou, Xiaoyu Wang, Ming-Hsuan Yang, Zhengzhong Tu",
      "summary": "## 4KAgent：代理式任意图像4K超分辨率系统\n\n4KAgent是一个统一的、代理式的超分辨率通用系统，旨在将任意图像普遍提升至4K分辨率，甚至通过迭代应用可达到更高分辨率。该系统能够将来自极低分辨率且严重退化的输入（例如256x256的高度扭曲图像）转换为晶莹剔透、照片般逼真的4K输出。\n\n### 核心组件\n4KAgent由三个核心组件构成，协同工作以实现高质量的图像超分辨率：\n\n1.  **Profiling（分析模块）**：\n    *   根据特定的用例需求，定制4KAgent的管道流程，确保系统能够适应不同的应用场景。\n\n2.  **Perception Agent（感知代理）**：\n    *   利用视觉-语言模型（VLM）和图像质量评估专家，对输入图像进行深入分析。\n    *   根据分析结果，制定量身定制的图像修复计划，为后续的修复工作提供指导。\n\n3.  **Restoration Agent（修复代理）**：\n    *   执行由感知代理制定的修复计划。\n    *   遵循递归式的执行-反思范式，即在每一步执行后进行评估和调整。\n    *   由质量驱动的专家混合策略（mixture-of-expert policy）指导，以在每一步中选择最优的输出结果。\n\n### 特色功能\n*   **专业面部修复管道**：4KAgent内置了一个专门的面部修复管道，显著增强了肖像和自拍照片中的面部细节，提升了人像图像的质量。\n\n### 性能评估\n*   **广泛的评估范围**：研究人员对4KAgent进行了严格的评估，涵盖了11个不同的任务类别，总计26个多样化的基准测试。\n*   **领先的性能**：在广泛的成像领域（包括自然图像、肖像照片、AI生成内容、卫星图像、荧光显微镜以及眼底镜、超声波和X射线等医学影像）中，4KAgent均达到了新的最先进水平（state-of-the-art）。\n*   **综合指标表现**：在感知指标（如NIQE、MUSIQ）和保真度指标（如PSNR）方面均表现出卓越的性能。\n\n### 意义与展望\n通过为低级视觉任务建立一种新颖的代理范式，4KAgent旨在激发不同研究社区对以视觉为中心的自主代理的更广泛兴趣和创新。",
      "shortSummary": "4KAgent是一个统一的代理式超分辨率系统，旨在将任意图像（包括低分辨率和严重退化的图像）提升至4K分辨率。它由Profiling、感知代理和修复代理三个核心组件构成，并内置了专门的面部修复功能。该系统在11个任务类别、26个基准测试中表现出色，在自然图像、医学影像等多个领域达到了最先进水平，为低级视觉任务开创了新的代理范式。",
      "translated_title": "4KAgent：代理式任意图像4K超分辨率",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present 4KAgent, a unified agentic super-resolution generalist system designed to universally upscale any image to 4K resolution (and even higher, if applied iteratively). Our system can transform images from extremely low resolutions with severe degradations, for example, highly distorted inputs at 256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three core components: (1) Profiling, a module that customizes the 4KAgent pipeline based on bespoke use cases; (2) A Perception Agent, which leverages vision-language models alongside image quality assessment experts to analyze the input image and make a tailored restoration plan; and (3) A Restoration Agent, which executes the plan, following a recursive execution-reflection paradigm, guided by a quality-driven mixture-of-expert policy to select the optimal output for each step. Additionally, 4KAgent embeds a specialized face restoration pipeline, significantly enhancing facial details in portrait and selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task categories encompassing a total of 26 diverse benchmarks, setting new state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover natural images, portrait photos, AI-generated content, satellite imagery, fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and X-ray, demonstrating superior performance in terms of both perceptual (e.g., NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic paradigm for low-level vision tasks, we aim to catalyze broader interest and innovation within vision-centric autonomous agents across diverse research communities. We will release all the code, models, and results at: https://4kagent.github.io."
    },
    {
      "title": "走向零：利用百万级数据实现零样本运动生成 (原标题: Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data)",
      "link": "https://arxiv.org/abs/2507.07095",
      "pubDate": "Wed, 09 Jul 2025 13:52:04 GMT",
      "isoDate": "2025-07-09T13:52:04.000Z",
      "creator": "Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, Jingbo Wang",
      "summary": "### 走向零：利用百万级数据实现零样本运动生成\n\n#### 引言与背景\n文本描述生成多样化、自然的人体运动序列是计算机视觉、图形学和机器人学领域一个基础且极具挑战性的研究方向。尽管该领域取得了显著进展，但现有方法在零样本泛化能力方面仍面临挑战，这主要归因于训练数据集规模的限制。此外，缺乏全面的评估框架也阻碍了该任务的进一步发展，未能有效指明改进方向。\n\n#### 研究目标\n本研究旨在将文本到运动生成领域推向一个新时代，即实现零样本泛化能力。\n\n#### 主要贡献与方法\n为了实现这一目标，本研究提出了以下关键贡献：\n\n*   **高效标注流程的开发**\n    研究团队开发了一种高效的标注流程，为大规模数据收集奠定了基础。\n\n*   **MotionMillion数据集的引入**\n    本研究引入了迄今为止最大的人体运动数据集——**MotionMillion**。该数据集包含超过2000小时和200万条高质量的运动序列，极大地扩充了可用于训练的数据规模，从而有望提升模型的泛化能力。\n\n*   **MotionMillion-Eval评估基准的提出**\n    为了解决评估框架不足的问题，研究团队提出了**MotionMillion-Eval**，这是目前最全面的零样本运动生成评估基准。该基准旨在更准确地衡量模型在未知或复杂场景下的泛化表现。\n\n*   **可扩展架构与大规模模型训练**\n    研究利用一种可扩展的架构，将模型参数规模扩展至70亿（7B）。这种大规模的模型训练旨在充分利用MotionMillion数据集的丰富信息，进一步提升模型的性能和泛化能力。\n\n#### 实验与结果\n研究团队在MotionMillion-Eval基准上验证了所提出模型的性能。结果表明，该模型在处理域外（out-of-domain）和复杂组合运动（complex compositional motions）方面展现出强大的泛化能力。这标志着在实现零样本人体运动生成方面迈出了重要一步。\n\n#### 结论\n本研究通过构建大规模数据集、提出全面的评估基准以及训练大规模模型，显著推动了文本到运动生成领域向零样本泛化能力迈进。",
      "shortSummary": "该研究旨在解决文本到运动生成中零样本泛化能力不足的问题。通过构建迄今最大的MotionMillion数据集（包含200万运动序列）和最全面的MotionMillion-Eval评估基准，并利用可扩展架构将模型扩展至70亿参数，研究团队显著提升了模型对域外和复杂组合运动的泛化能力。这标志着零样本人体运动生成领域取得了重要进展。",
      "translated_title": "走向零：利用百万级数据实现零样本运动生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Generating diverse and natural human motion sequences based on textual descriptions constitutes a fundamental and challenging research area within the domains of computer vision, graphics, and robotics. Despite significant advancements in this field, current methodologies often face challenges regarding zero-shot generalization capabilities, largely attributable to the limited size of training datasets. Moreover, the lack of a comprehensive evaluation framework impedes the advancement of this task by failing to identify directions for improvement. In this work, we aim to push text-to-motion into a new era, that is, to achieve the generalization ability of zero-shot. To this end, firstly, we develop an efficient annotation pipeline and introduce MotionMillion-the largest human motion dataset to date, featuring over 2,000 hours and 2 million high-quality motion sequences. Additionally, we propose MotionMillion-Eval, the most comprehensive benchmark for evaluating zero-shot motion generation. Leveraging a scalable architecture, we scale our model to 7B parameters and validate its performance on MotionMillion-Eval. Our results demonstrate strong generalization to out-of-domain and complex compositional motions, marking a significant step toward zero-shot human motion generation. The code is available at https://github.com/VankouF/MotionMillion-Codes."
    },
    {
      "title": "首次返回，熵启发式探索 (原标题: First Return, Entropy-Eliciting Explore)",
      "link": "https://arxiv.org/abs/2507.07017",
      "pubDate": "Wed, 09 Jul 2025 12:45:48 GMT",
      "isoDate": "2025-07-09T12:45:48.000Z",
      "creator": "Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma",
      "summary": "## FR3E：一种用于LLM推理的结构化探索框架\n\n### 摘要\n\n本文介绍了FR3E（First Return, Entropy-Eliciting Explore），一个旨在解决大型语言模型（LLMs）在强化学习从可验证奖励（RLVR）中探索不稳定问题的结构化探索框架。\n\n### 背景与问题\n\n*   **强化学习从可验证奖励（RLVR）**：该方法能够有效提升大型语言模型（LLMs）的推理能力。\n*   **主要挑战**：RLVR在实际应用中面临探索不稳定的问题，这限制了其在复杂推理任务中的表现。\n\n### FR3E 方法介绍\n\n*   **核心理念**：FR3E，即“首次返回，熵启发式探索”，提出了一种新颖的结构化探索框架，旨在提供更稳定和有针对性的探索。\n*   **工作原理**：\n    *   **识别高不确定性决策点**：该框架能够识别推理轨迹中那些具有高不确定性的决策点。\n    *   **执行有针对性的Rollouts**：针对这些不确定点，FR3E会执行有目标的“rollouts”（展开），以收集更多信息。\n    *   **构建语义接地中间反馈**：通过这些rollouts，系统能够构建出语义上接地（semantically grounded）的中间反馈。\n    *   **无需密集监督**：FR3E的独特之处在于，它能够在不依赖密集监督的情况下提供有针对性的指导，从而降低了对大量标注数据的需求。\n\n### 实验结果\n\n*   **基准测试**：FR3E在数学推理基准（AIME24）上进行了实证评估。\n*   **主要发现**：\n    *   **训练稳定性**：FR3E显著促进了更稳定的模型训练。\n    *   **响应质量**：模型能够生成更长、更连贯的响应。\n    *   **正确轨迹比例**：完全正确推理轨迹的比例有所增加。\n\n### 结论\n\n这些实证结果充分证明了FR3E框架的有效性。通过提供更鲁棒和结构化的探索机制，FR3E成功提升了大型语言模型的推理能力。\n\n### 作者与引用信息\n\n*   **作者**：Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma\n*   **研究领域**：人工智能 (cs.AI)\n*   **引用**：arXiv:2507.07017 [cs.AI]\n*   **提交历史**：2025年7月9日星期三 16:45:48 UTC",
      "shortSummary": "FR3E (First Return, Entropy-Eliciting Explore) 旨在解决大型语言模型（LLMs）在强化学习中探索不稳定的问题。它通过识别高不确定性决策点并进行有针对性的rollouts，构建语义接地中间反馈，提供指导。在数学推理基准（AIME24）上的实验表明，FR3E促进了更稳定的训练，生成了更长、更连贯的响应，并提高了完全正确轨迹的比例，有效提升了LLM的推理能力。",
      "translated_title": "首次返回，熵启发式探索",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration."
    },
    {
      "title": "重新思考LLM代码生成的验证：从生成到测试 (原标题: Rethinking Verification for LLM Code Generation: From Generation to Testing)",
      "link": "https://arxiv.org/abs/2507.06920",
      "pubDate": "Wed, 09 Jul 2025 10:58:47 GMT",
      "isoDate": "2025-07-09T10:58:47.000Z",
      "creator": "Zihan Ma, Taolin Zhang, Maosong Cao, Wenwei Zhang, Minnan Luo, Songyang Zhang, Kai Chen",
      "summary": "## 重新思考LLM代码生成的验证\n\n### 引言与问题背景\n\n大型语言模型（LLM）在HumanEval和LiveCodeBench等代码生成基准测试中取得了显著成功。然而，深入研究发现，这些评估套件通常只包含数量有限且同质的测试用例，导致细微的错误未能被检测出来。这不仅人为地夸大了测量的性能，也损害了在利用可验证奖励的强化学习框架（RLVR）中准确估计奖励的能力。\n\n### 研究目标与方法\n\n为解决这些关键缺陷，本研究系统地调查了测试用例生成（TCG）任务，并提出了以下创新：\n\n*   **多维度指标：** 设计了多维度指标，旨在严格量化测试套件的彻底性。\n*   **SAGA方法：** 引入了一种人机协作方法（SAGA），该方法结合了人类编程专业知识与LLM的推理能力，旨在显著增强生成测试用例的覆盖率和质量。\n*   **TCGBench基准：** 开发了一个名为TCGBench的基准，以促进对TCG任务的研究。\n\n### 实验结果\n\n实验结果证明了所提出方法的有效性：\n\n*   SAGA在TCGBench上实现了90.62%的检测率。\n*   SAGA在TCGBench上实现了32.58%的验证器准确率。\n*   SAGA合成的代码生成评估基准的验证器准确率（Verifier Acc）比LiveCodeBench-v6高10.78%。\n\n### 结论与贡献\n\n这些结果表明了我们所提出方法的有效性。本研究期望能为构建可靠的LLM代码评估提供一个可扩展的基础，进一步推动代码生成领域的RLVR发展，并为自动化对抗性测试合成和自适应基准集成铺平道路。",
      "shortSummary": "本文重新思考LLM代码生成验证，指出现有基准测试用例不足，导致错误未被发现和性能虚报。为解决此问题，研究提出多维度指标量化测试彻底性，并引入人机协作方法SAGA，结合人类专业知识与LLM推理能力，以提升测试用例质量。同时开发了TCGBench。实验表明SAGA在检测率和验证器准确率上表现出色，显著优于现有基准，为可靠的LLM代码评估奠定基础。",
      "translated_title": "重新思考LLM代码生成的验证：从生成到测试",
      "images": [],
      "contentSource": "完整文章",
      "content": "Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration."
    },
    {
      "title": "DiffSpectra：使用扩散模型从光谱中解析分子结构 (原标题: DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models)",
      "link": "https://arxiv.org/abs/2507.06853",
      "pubDate": "Wed, 09 Jul 2025 09:57:20 GMT",
      "isoDate": "2025-07-09T09:57:20.000Z",
      "creator": "Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang",
      "summary": "# DiffSpectra：基于扩散模型的光谱分子结构解析\n\n本文介绍了 **DiffSpectra**，一个创新的生成式框架，旨在通过扩散模型直接从多模态光谱数据中推断分子的二维（2D）和三维（3D）结构。这项工作解决了化学领域中分子结构解析这一基础性问题所面临的挑战。\n\n## 背景与挑战\n\n*   **传统方法局限性：** 传统的分子结构解析方法高度依赖专家解读，难以扩展到大规模应用。\n*   **现有机器学习方法不足：**\n    *   **基于检索的策略：** 依赖有限的分子库，限制了对新型分子的泛化能力。\n    *   **生成模型（SMILES-based）：** 大多数采用自回归SMILES架构，忽略了分子的3D几何信息，并且难以有效整合多样化的光谱模态。\n\n## DiffSpectra 框架概述\n\nDiffSpectra 将分子结构解析公式化为一个条件生成过程，其核心在于利用扩散模型进行去噪。\n\n### 核心组件\n\n1.  **去噪网络：Diffusion Molecule Transformer**\n    *   该网络参数化为 **Diffusion Molecule Transformer**，这是一种 **SE(3) 等变架构**。\n    *   它能够有效地整合分子的拓扑（2D）和几何（3D）信息，确保在三维空间中的变换不变性。\n\n2.  **条件编码器：SpecFormer**\n    *   条件信息由 **SpecFormer** 提供，这是一个基于 Transformer 的光谱编码器。\n    *   SpecFormer 能够从多模态光谱数据中捕获光谱内部（intra-spectral）和光谱之间（inter-spectral）的依赖关系，为结构生成提供丰富的上下文信息。\n\n## 实验结果与优势\n\n广泛的实验证明，DiffSpectra 在结构解析方面取得了高精度：\n\n*   **准确率：** 通过采样，实现了 **16.01% 的 Top-1 准确率** 和 **96.86% 的 Top-20 准确率**。\n*   **关键贡献因素：**\n    *   **3D 几何建模：** 模型从对3D几何信息的建模中显著受益。\n    *   **SpecFormer 预训练：** SpecFormer 的预训练对于提升模型性能至关重要。\n    *   **多模态条件化：** 整合多种光谱模态作为条件信息，进一步增强了模型的解析能力。\n\n## 创新性与意义\n\n*   **开创性统一：** 据作者所知，DiffSpectra 是首个将多模态光谱推理与联合2D/3D生成建模相结合，用于 *de novo*（从头开始）分子结构解析的框架。\n*   **有效性验证：** 这些结果突显了光谱条件扩散模型在解决分子结构解析挑战方面的有效性。\n*   **深远影响：** 这项技术对化合物识别、合成以及药物开发等领域具有深远意义。",
      "shortSummary": "DiffSpectra是一种创新的生成式框架，利用扩散模型直接从多模态光谱数据中解析2D和3D分子结构。它通过SE(3)等变的Diffusion Molecule Transformer进行去噪，并使用SpecFormer编码多模态光谱作为条件。该模型在结构解析中表现出高准确率（Top-1 16.01%，Top-20 96.86%），并受益于3D几何建模和多模态条件化。DiffSpectra是首个统一多模态光谱推理与2D/3D生成建模的框架，对化学和药物开发具有重要意义。",
      "translated_title": "DiffSpectra：使用扩散模型从光谱中解析分子结构",
      "images": [],
      "contentSource": "完整文章",
      "content": "Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation."
    },
    {
      "title": "令牌瓶颈：一个令牌记忆动态 (原标题: Token Bottleneck: One Token to Remember Dynamics)",
      "link": "https://arxiv.org/abs/2507.06543",
      "pubDate": "Wed, 09 Jul 2025 00:57:29 GMT",
      "isoDate": "2025-07-09T00:57:29.000Z",
      "creator": "Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun",
      "summary": "## 令牌瓶颈（ToBo）：一种自监督学习方法，用于捕捉动态场景中的时间依赖性\n\n### 核心问题\n\n在视觉跟踪和机器人操作等顺序场景理解任务中，从动态场景中提取紧凑且具有时间感知能力的视觉表示至关重要。\n\n### 解决方案：令牌瓶颈（ToBo）\n\n本文提出了一种名为“令牌瓶颈”（Token Bottleneck, ToBo）的简单直观的自监督学习流程。ToBo旨在通过将场景压缩成一个“瓶颈令牌”并利用少量补丁作为提示来预测后续场景，从而学习顺序场景表示。\n\n### ToBo 的工作原理\n\n1.  **压缩（Squeeze）步骤**：\n    *   将参考场景保守地编码为一个紧凑的瓶颈令牌。\n    *   这一步确保了表示的紧凑性。\n\n2.  **扩展（Expansion）步骤**：\n    *   模型利用瓶颈令牌以及少量目标场景的补丁作为提示，来预测目标场景。\n    *   这种设计鼓励视觉骨干网络嵌入时间依赖性，从而实现对场景间动态转换的理解。\n\n### 实验验证与成果\n\n*   **任务多样性**：ToBo 在多种顺序任务中进行了广泛实验，包括视频标签传播和模拟环境中的机器人操作。\n*   **性能优越性**：实验结果表明，ToBo 在这些任务中表现出优于基线方法的性能。\n*   **实际部署**：将预训练的ToBo模型部署到物理机器人上，证实了其在真实世界环境中的鲁棒性和有效性。\n*   **可扩展性**：研究还验证了ToBo在不同模型规模下的可扩展性。",
      "shortSummary": "令牌瓶颈（ToBo）是一种新颖的自监督学习方法，旨在从动态场景中提取紧凑且具有时间感知的视觉表示。ToBo通过将参考场景压缩成一个“瓶颈令牌”，并利用该令牌和少量提示预测后续场景，从而学习时间依赖性。在视频标签传播和机器人操作等顺序任务中，ToBo表现出优于基线的性能，并在物理机器人上验证了其在真实世界中的鲁棒性和有效性，同时展示了良好的可扩展性。",
      "translated_title": "令牌瓶颈：一个令牌记忆动态",
      "images": [],
      "contentSource": "完整文章",
      "content": "Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales."
    },
    {
      "title": "LangSplatV2：高维3D语言高斯泼溅，帧率超过450 FPS (原标题: LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS)",
      "link": "https://arxiv.org/abs/2507.07136",
      "pubDate": "Tue, 08 Jul 2025 20:19:58 GMT",
      "isoDate": "2025-07-08T20:19:58.000Z",
      "creator": "Wanhua Li, Yujie Zhao, Minghan Qin, Yang Liu, Yuanhao Cai, Chuang Gan, Hanspeter Pfister",
      "summary": "## LangSplatV2：高维3D语言高斯泼溅\n\n### 概述\nLangSplatV2 是一种创新的方法，旨在显著提升3D语言场处理的实时性能和查询准确性。它解决了现有技术 LangSplat 在推理速度上的主要瓶颈，使其能够应用于更广泛的复杂场景中的语言交互。\n\n### LangSplat 的局限性\nLangSplat 利用高斯泼溅技术将2D CLIP语言特征嵌入到3D空间中，从而学习精确的3D语言场并结合 SAM 语义。尽管它在速度和3D语言场学习方面有所增强，但其推理性能（即使在先进的 A100 GPU 上也仅为 8.2 FPS）远未达到实时要求，这严重限制了其广泛应用。通过详细的时间分析，研究人员发现 LangSplat 的主要速度瓶颈在于其“重量级解码器”。\n\n### LangSplatV2 的创新解决方案\nLangSplatV2 提出了一种全新的架构，通过以下方式克服了 LangSplat 的速度限制：\n\n*   **稀疏编码假设**：LangSplatV2 假设每个高斯都充当全局字典中的稀疏代码。\n*   **消除解码器**：通过学习一个3D稀疏系数场，LangSplatV2 完全消除了对重量级解码器的需求，这是其实现高速的关键。\n*   **高效稀疏系数泼溅**：该方法进一步提出了一种高效的稀疏系数泼溅技术，并进行了 CUDA 优化。这使得系统能够以仅泼溅超低维特征的时间成本，渲染高质量的高维特征图。\n\n### 性能提升\n实验结果表明，LangSplatV2 在性能上取得了显著飞跃：\n\n*   **高维特征泼溅**：达到 476.2 FPS，比 LangSplat 提速 42 倍。\n*   **3D 开放词汇文本查询**：达到 384.6 FPS，比 LangSplat 提升 47 倍。\n*   **查询准确性**：在保持或超越竞争对手查询准确性的同时，实现了大幅度的速度提升。\n\n### 应用前景\n3D 语言场的进步对于需要在复杂场景中进行语言交互的应用至关重要。LangSplatV2 的实时性能使其在这些领域具有巨大的潜力。\n\n### 资源\n项目的代码和演示可在项目页面获取。",
      "shortSummary": "LangSplatV2 显著提升了3D语言高斯泼溅的性能，实现了高维特征泼溅和3D开放词汇文本查询的实时帧率（分别达到 476.2 FPS 和 384.6 FPS）。通过假设高斯为稀疏代码并学习3D稀疏系数场，LangSplatV2 成功消除了 LangSplat 中导致速度瓶颈的重量级解码器。这一创新带来了高达 42 倍的速度提升和更高的查询准确性，对需要复杂场景中语言交互的应用至关重要。",
      "translated_title": "LangSplatV2：高维3D语言高斯泼溅，帧率超过450 FPS",
      "images": [],
      "contentSource": "完整文章",
      "content": "In this paper, we introduce LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images, providing a 42 times speedup and a 47 times boost over LangSplat respectively, along with improved query accuracy. LangSplat employs Gaussian Splatting to embed 2D CLIP language features into 3D, significantly enhancing speed and learning a precise 3D language field with SAM semantics. Such advancements in 3D language fields are crucial for applications that require language interaction within complex scenes. However, LangSplat does not yet achieve real-time inference performance (8.2 FPS), even with advanced A100 GPUs, severely limiting its broader application. In this paper, we first conduct a detailed time analysis of LangSplat, identifying the heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2 assumes that each Gaussian acts as a sparse code within a global dictionary, leading to the learning of a 3D sparse coefficient field that entirely eliminates the need for a heavyweight decoder. By leveraging this sparsity, we further propose an efficient sparse coefficient splatting method with CUDA optimization, rendering high-dimensional feature maps at high quality while incurring only the time cost of splatting an ultra-low-dimensional feature. Our experimental results demonstrate that LangSplatV2 not only achieves better or competitive query accuracy but is also significantly faster. Codes and demos are available at our project page: https://langsplat-v2.github.io."
    },
    {
      "title": "混合线性注意力机制的系统分析 (原标题: A Systematic Analysis of Hybrid Linear Attention)",
      "link": "https://arxiv.org/abs/2507.06457",
      "pubDate": "Tue, 08 Jul 2025 19:54:11 GMT",
      "isoDate": "2025-07-08T19:54:11.000Z",
      "creator": "Dustin Wang, Rui-Jie Zhu, Steven Abreu, Yong Shan, Taylor Kergan, Yuqi Pan, Yuhong Chou, Zheng Li, Ge Zhang, Wenhao Huang, Jason Eshraghian",
      "summary": "## 混合线性注意力机制的系统分析\n\n### 引言\nTransformer模型在处理长序列时面临二次复杂度与内存消耗问题。为解决此问题，研究人员引入了使用固定大小隐藏状态的线性注意力机制。然而，线性模型通常在召回性能上表现有限，这促使了结合线性注意力层和全注意力层的混合架构的出现。尽管对混合架构进行了广泛研究，但其中线性注意力组件的选择尚未得到深入探索。\n\n### 研究目的\n本研究旨在系统地评估不同代际的线性注意力模型（从向量递归到高级门控机制），无论是独立使用还是与全注意力层混合使用，以填补现有研究空白。\n\n### 研究方法\n为了实现这一全面的分析，研究团队进行了以下工作：\n\n1.  **模型训练与开源：** 训练并开源了72个模型。\n    *   其中36个模型参数量为3.4亿，在200亿个token上进行训练。\n    *   另外36个模型参数量为13亿，在1000亿个token上进行训练。\n2.  **变量控制：** 研究涵盖了六种线性注意力变体和五种不同的混合比例（线性注意力层与全注意力层的比例）。\n3.  **基准测试：** 在标准语言建模和召回任务上对所有模型进行了性能评估。\n\n### 主要发现\n本研究的基准测试揭示了以下关键发现：\n\n*   **独立性能与混合性能：** 独立表现优异的线性模型在混合架构中不一定能保持其优势。\n*   **语言建模稳定性：** 语言建模性能在不同的线性-全注意力比例下保持稳定。\n*   **召回性能提升：** 召回性能随全注意力层比例的增加而显著提升，尤其是在线性-全注意力比例低于3:1时。\n*   **关键因素：** 研究强调，选择性门控（selective gating）、分层递归（hierarchical recurrence）和受控遗忘（controlled forgetting）是构建有效混合模型的关键。\n\n### 研究建议\n基于上述发现，研究团队推荐以下架构和配置，以高效实现Transformer级别的召回性能：\n\n*   **推荐架构：** HGRN-2或GatedDeltaNet。\n*   **推荐比例：** 线性注意力层与全注意力层的比例在3:1到6:1之间。\n\n### 数据可用性\n本研究中训练的所有模型均已开源。",
      "shortSummary": "本研究系统分析了混合线性注意力机制，旨在解决Transformer在长序列处理中的复杂性问题。通过训练并开源72个模型，涵盖多种线性注意力变体和混合比例，研究发现独立表现优异的线性模型在混合架构中不一定出色。语言建模性能稳定，但召回性能随全注意力层增加而显著提升，尤其在特定比例下。选择性门控、分层递归和受控遗忘对有效混合模型至关重要。研究建议采用HGRN-2或GatedDeltaNet，线性-全注意力比例为3:1至6:1，以高效实现Transformer级召回。",
      "translated_title": "混合线性注意力机制的系统分析",
      "images": [],
      "contentSource": "完整文章",
      "content": "Transformers face quadratic complexity and memory issues with long sequences, prompting the adoption of linear attention mechanisms using fixed-size hidden states. However, linear models often suffer from limited recall performance, leading to hybrid architectures that combine linear and full attention layers. Despite extensive hybrid architecture research, the choice of linear attention component has not been deeply explored. We systematically evaluate various linear attention models across generations - vector recurrences to advanced gating mechanisms - both standalone and hybridized. To enable this comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six linear attention variants across five hybridization ratios. Benchmarking on standard language modeling and recall tasks reveals that superior standalone linear models do not necessarily excel in hybrids. While language modeling remains stable across linear-to-full attention ratios, recall significantly improves with increased full attention layers, particularly below a 3:1 ratio. Our study highlights selective gating, hierarchical recurrence, and controlled forgetting as critical for effective hybrid models. We recommend architectures such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1 to achieve Transformer-level recall efficiently. Our models are open-sourced at https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e."
    }
  ],
  "lastUpdated": "2025-07-12T09:34:51.005Z"
}