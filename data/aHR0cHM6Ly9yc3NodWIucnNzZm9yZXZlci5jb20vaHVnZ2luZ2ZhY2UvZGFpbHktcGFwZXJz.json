{
  "sourceUrl": "https://rsshub.rssforever.com/huggingface/daily-papers",
  "title": "Huggingface Daily Papers",
  "description": "Huggingface Daily Papers - Powered by RSSHub",
  "link": "https://huggingface.co/papers",
  "items": [
    {
      "title": "Ultra3D：基于部分注意力的高效高保真3D生成 (原标题: Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention)",
      "link": "https://arxiv.org/abs/2507.17745",
      "pubDate": "Wed, 23 Jul 2025 13:57:16 GMT",
      "isoDate": "2025-07-23T13:57:16.000Z",
      "creator": "Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, Guosheng Lin",
      "summary": "## Ultra3D：基于部分注意力的高效高保真3D生成\n\n### 摘要\n\n本文介绍了Ultra3D，一个旨在解决当前3D内容生成中计算效率问题的框架，特别是在使用稀疏体素表示时。现有方法虽然提升了3D模型的质量和精细度，但其两阶段扩散管道中注意力机制的二次复杂度导致了严重的计算效率低下。\n\n### 核心创新与方法\n\nUltra3D通过以下关键创新显著加速了稀疏体素建模，同时不损害生成质量：\n\n1.  **第一阶段：高效的粗略对象布局生成**\n    *   Ultra3D利用紧凑的**VecSet表示**来高效地生成对象的粗略布局。\n    *   这种方法有效减少了token数量，从而加速了体素坐标的预测过程。\n\n2.  **第二阶段：基于部分注意力的潜在特征精炼**\n    *   为了精炼每个体素的潜在特征，Ultra3D引入了**部分注意力（Part Attention）**机制。\n    *   **部分注意力特性：**\n        *   这是一种几何感知的局部注意力机制。\n        *   它将注意力计算限制在语义一致的部分区域内，而非全局范围。\n        *   这种设计既保留了结构连续性，又避免了不必要的全局注意力计算，从而大幅提升效率。\n        *   在潜在特征生成方面，部分注意力实现了高达6.7倍的速度提升。\n\n3.  **支持机制：可扩展的部分标注管道**\n    *   为了支持部分注意力机制的有效运作，Ultra3D构建了一个可扩展的部分标注管道。\n    *   该管道能够将原始网格数据转换为带有部分标签的稀疏体素表示。\n\n### 实验结果\n\n广泛的实验证明，Ultra3D能够支持1024分辨率的高分辨率3D生成。在视觉保真度和用户偏好方面，Ultra3D均达到了最先进的性能。",
      "shortSummary": "Ultra3D是一个高效高保真3D生成框架，旨在解决现有稀疏体素方法中注意力机制导致的计算效率低下问题。它通过两阶段方法实现：首先利用VecSet高效生成粗略布局，减少token数量；其次引入“部分注意力”机制，将注意力计算限制在语义一致的局部区域，显著加速潜在特征生成（高达6.7倍）。Ultra3D支持1024分辨率的3D生成，并在视觉保真度和用户偏好方面达到最先进水平。",
      "translated_title": "Ultra3D：基于部分注意力的高效高保真3D生成",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recent advances in sparse voxel representations have significantly improved the quality of 3D content generation, enabling high-resolution modeling with fine-grained geometry. However, existing frameworks suffer from severe computational inefficiencies due to the quadratic complexity of attention mechanisms in their two-stage diffusion pipelines. In this work, we propose Ultra3D, an efficient 3D generation framework that significantly accelerates sparse voxel modeling without compromising quality. Our method leverages the compact VecSet representation to efficiently generate a coarse object layout in the first stage, reducing token count and accelerating voxel coordinate prediction. To refine per-voxel latent features in the second stage, we introduce Part Attention, a geometry-aware localized attention mechanism that restricts attention computation within semantically consistent part regions. This design preserves structural continuity while avoiding unnecessary global attention, achieving up to 6.7x speed-up in latent generation. To support this mechanism, we construct a scalable part annotation pipeline that converts raw meshes into part-labeled sparse voxels. Extensive experiments demonstrate that Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves state-of-the-art performance in both visual fidelity and user preference."
    },
    {
      "title": "Yume：一个交互式世界生成模型 (原标题: Yume: An Interactive World Generation Model)",
      "link": "https://arxiv.org/abs/2507.17744",
      "pubDate": "Wed, 23 Jul 2025 13:57:09 GMT",
      "isoDate": "2025-07-23T13:57:09.000Z",
      "creator": "Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang",
      "summary": "### Yume: 一个交互式世界生成模型\n\n**项目概述**\nYume 是一个旨在利用图像、文本或视频创建交互式、逼真且动态世界的模型。其目标是允许用户使用外围设备或神经信号进行探索和控制。当前发布的预览版本能够从输入的图像生成一个动态世界，并支持通过键盘操作进行探索。\n\n**核心框架组件**\n为了实现高保真和交互式的视频世界生成，Yume 引入了一个精心设计的框架，该框架包含四个主要组成部分：\n\n1.  **相机运动量化**\n    *   为了实现稳定的训练和用户友好的键盘输入交互，模型对相机运动进行了量化处理。\n\n2.  **视频生成架构**\n    *   引入了带有记忆模块的 Masked Video Diffusion Transformer (MVDT)。\n    *   该架构能够以自回归方式实现无限视频生成。\n\n3.  **高级采样器**\n    *   集成了两种训练无关的机制以提升视觉质量和控制精度：\n        *   **无训练抗伪影机制 (AAM)**：用于改善视觉质量。\n        *   **基于随机微分方程的时间旅行采样 (TTS-SDE)**：用于实现更精确的控制。\n\n4.  **模型加速**\n    *   通过对抗蒸馏和缓存机制的协同优化，实现了模型的加速。\n\n**训练与成果**\nYume 模型使用高质量的世界探索数据集 \\sekai 进行训练。该模型在多样化的场景和应用中取得了显著成果。\n\n**资源可用性与未来展望**\n*   所有数据、代码库和模型权重均已公开提供。\n*   Yume 项目将每月更新，以逐步实现其最初设定的宏伟目标。\n*   项目页面提供了更多详细信息。",
      "shortSummary": "Yume 是一个交互式世界生成模型，旨在利用图像、文本或视频创建逼真、动态且可探索的世界。其预览版能从图像生成动态世界并支持键盘探索。核心框架包含相机运动量化、基于MVDT的无限视频生成架构、用于提升质量和控制的先进采样器（AAM和TTS-SDE），以及模型加速技术。该模型使用\\sekai数据集训练，表现出色，所有资源均已公开，并将每月更新。",
      "translated_title": "Yume：一个交互式世界生成模型",
      "images": [],
      "contentSource": "完整文章",
      "content": "Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \\method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \\sekai to train \\method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/."
    },
    {
      "title": "一个领域能否助力其他领域？一项基于数据驱动的强化学习多领域推理研究 (原标题: Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning)",
      "link": "https://arxiv.org/abs/2507.17512",
      "pubDate": "Wed, 23 Jul 2025 09:51:04 GMT",
      "isoDate": "2025-07-23T09:51:04.000Z",
      "creator": "Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, Lijun Wu",
      "summary": "## 强化学习多领域推理研究\n\n### 摘要\n\n现有研究主要集中于孤立的推理领域（如数学问题解决、编码任务或逻辑推理），以增强大型语言模型（LLMs）的推理能力。然而，现实世界的推理场景本质上需要多种认知技能的综合应用。尽管如此，在强化学习（RL）框架下，这些推理技能之间的相互作用仍未被充分理解。\n\n### 研究目的\n\n为了弥补这一空白，本研究系统性地调查了可验证奖励强化学习（RLVR）框架下的多领域推理，明确关注三个主要领域：数学推理、代码生成和逻辑谜题解决。\n\n### 研究方法与组成\n\n本研究包含四个关键组成部分：\n\n1.  **单领域训练评估**：\n    *   利用GRPO算法和Qwen-2.5-7B模型家族，本研究彻底评估了模型在单领域数据集上训练时的领域内改进和跨领域泛化能力。\n\n2.  **跨领域联合训练交互**：\n    *   此外，本研究考察了在联合跨领域训练过程中出现的复杂交互，包括相互增强和冲突。\n\n3.  **SFT对RL的影响分析**：\n    *   为了进一步理解指令微调（SFT）对强化学习（RL）的影响，本研究还分析并比较了在相同RL配置下，基础模型（base models）和指令微调模型（instruct models）之间的性能差异。\n\n4.  **RL训练细节探索**：\n    *   此外，本研究深入探讨了关键的RL训练细节，系统性地探索了课程学习策略、奖励设计变体以及特定语言因素的影响。\n\n### 主要发现与贡献\n\n通过大量实验，本研究结果为领域交互的动态提供了重要见解，揭示了影响专业化和泛化推理性能的关键因素。这些发现为优化RL方法以培养LLMs全面的多领域推理能力提供了宝贵指导。",
      "shortSummary": "本研究系统性地探讨了在强化学习（RL）框架下大型语言模型（LLMs）的多领域推理能力。针对数学、代码和逻辑谜题三个核心领域，研究评估了单领域训练的泛化性、跨领域联合训练的交互（增强与冲突）、指令微调（SFT）对RL的影响，并深入分析了RL训练细节（如课程学习和奖励设计）。研究结果揭示了影响LLMs专业化和泛化推理性能的关键因素，为优化RL方法以提升LLMs的综合多领域推理能力提供了重要指导。",
      "translated_title": "一个领域能否助力其他领域？一项基于数据驱动的强化学习多领域推理研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing research has predominantly concentrated on isolated reasoning domains such as mathematical problem-solving, coding tasks, or logical reasoning. However, real world reasoning scenarios inherently demand an integrated application of multiple cognitive skills. Despite this, the interplay among these reasoning skills under reinforcement learning remains poorly understood. To bridge this gap, we present a systematic investigation of multi-domain reasoning within the RLVR framework, explicitly focusing on three primary domains: mathematical reasoning, code generation, and logical puzzle solving. We conduct a comprehensive study comprising four key components: (1) Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the models' in-domain improvements and cross-domain generalization capabilities when trained on single-domain datasets. (2) Additionally, we examine the intricate interactions including mutual enhancements and conflicts that emerge during combined cross-domain training. (3) To further understand the influence of SFT on RL, we also analyze and compare performance differences between base and instruct models under identical RL configurations. (4) Furthermore, we delve into critical RL training details, systematically exploring the impacts of curriculum learning strategies, variations in reward design, and language-specific factors. Through extensive experiments, our results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance. These findings provide valuable guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs."
    },
    {
      "title": "DesignLab：通过迭代检测和纠正设计幻灯片 (原标题: DesignLab: Designing Slides Through Iterative Detection and Correction)",
      "link": "https://arxiv.org/abs/2507.17202",
      "pubDate": "Wed, 23 Jul 2025 00:49:48 GMT",
      "isoDate": "2025-07-23T00:49:48.000Z",
      "creator": "Jooyeol Yun, Heng Wang, Yotaro Shimose, Jaegul Choo, Shingo Takamatsu",
      "summary": "# DesignLab：通过迭代检测和纠正设计幻灯片\n\n## 摘要\n\n本文提出了 DesignLab，一个旨在解决非专业人士在设计高质量演示文稿幻灯片时所面临挑战的系统。尽管现有的自动化工具可以提供布局和配色方案建议，但它们通常缺乏自我完善输出的能力，而这在实际工作流程中至关重要。\n\n## DesignLab 方法论\n\nDesignLab 的核心思想是将幻灯片设计过程分解为两个明确的角色：\n\n*   **设计审阅者（Design Reviewer）**：负责识别设计中存在的问题和缺陷。\n*   **设计贡献者（Design Contributor）**：负责纠正由审阅者发现的设计问题。\n\n这种分解使得一个迭代循环成为可能：审阅者持续检测问题，贡献者随即进行纠正。通过这种方式，幻灯片草稿可以在每次迭代中得到进一步的打磨和完善，最终达到传统方法难以企及的专业品质。\n\n## 技术实现\n\n为了实现这两个角色，DesignLab 对大型语言模型（LLMs）进行了微调。为了训练这些模型，研究人员通过引入受控的扰动（即故意制造设计错误）来模拟中间草稿。这使得设计审阅者能够学习如何识别各种设计错误，而设计贡献者则能够学习如何有效地修复这些错误。\n\n## 实验结果与优势\n\n实验结果表明，DesignLab 在幻灯片设计生成方面优于现有的方法，甚至包括一款商业工具。DesignLab 的卓越表现归因于其采纳了设计的迭代性质，这使得它能够生成高度精美和专业的幻灯片。\n\n**重要提示：** 文章内容中未包含有效的实际图片链接，因此本摘要中不包含任何图片。",
      "shortSummary": "DesignLab 提出了一种通过迭代检测和纠正来设计高质量幻灯片的新方法。针对非专业人士设计难题及现有工具无法自我完善的痛点，DesignLab 将设计过程分解为“设计审阅者”和“设计贡献者”两个角色，通过微调大型语言模型实现迭代优化。审阅者识别问题，贡献者负责纠正，从而持续提升幻灯片质量。实验证明，DesignLab 优于现有设计工具，能生成更专业、精美的幻灯片。",
      "translated_title": "DesignLab：通过迭代检测和纠正设计幻灯片",
      "images": [],
      "contentSource": "完整文章",
      "content": "Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides."
    },
    {
      "title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理 (原标题: ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning)",
      "link": "https://arxiv.org/abs/2507.16815",
      "pubDate": "Tue, 22 Jul 2025 13:59:46 GMT",
      "isoDate": "2025-07-22T13:59:46.000Z",
      "creator": "Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang",
      "summary": "本文提出了一种名为 ThinkAct 的双系统框架，旨在解决视觉-语言-动作（VLA）推理任务中的挑战。\n\n**背景与问题**\n*   VLA 推理任务要求智能体解释多模态指令、执行长周期规划并在动态环境中自适应行动。\n*   现有方法通常采用端到端训练，直接将输入映射到动作，缺乏显式推理过程。\n*   这限制了它们在多步骤规划和适应复杂任务变化方面的能力。\n\n**ThinkAct 框架**\n*   **核心理念：** ThinkAct 通过强化视觉潜在规划，连接高层推理与低层动作执行。\n*   **工作机制：**\n    1.  **多模态大型语言模型（LLM）训练：** 训练一个多模态 LLM 来生成具身推理计划。\n    2.  **奖励引导：** 这些推理计划通过强化与动作对齐的视觉奖励进行引导，奖励基于目标完成度和轨迹一致性。\n    3.  **视觉计划潜在表示：** 生成的推理计划被压缩成一个视觉计划潜在表示。\n    4.  **下游动作模型：** 这个视觉计划潜在表示作为条件，输入到一个下游动作模型中，以在目标环境中实现鲁棒的动作执行。\n\n**实验结果与优势**\n*   在具身推理和机器人操作基准上的广泛实验表明，ThinkAct 展现出：\n    *   **少样本适应能力：** 能够以少量样本进行适应。\n    *   **长周期规划能力：** 能够进行跨越多步骤的规划。\n    *   **自我纠正行为：** 在复杂的具身 AI 任务中表现出自我纠正能力。",
      "shortSummary": "ThinkAct 是一种双系统框架，通过强化视觉潜在规划，解决了视觉-语言-动作（VLA）推理任务中现有端到端方法缺乏显式推理的问题。它训练一个多模态LLM生成具身推理计划，并将其压缩为视觉潜在表示，以指导下游动作模型执行。实验证明，ThinkAct 在复杂具身AI任务中实现了少样本适应、长周期规划和自我纠正能力。",
      "translated_title": "ThinkAct：通过强化视觉潜在规划实现视觉-语言-动作推理",
      "images": [],
      "contentSource": "完整文章",
      "content": "Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks."
    },
    {
      "title": "用于视觉-语言慢思考推理的半离策略强化学习 (原标题: Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning)",
      "link": "https://arxiv.org/abs/2507.16814",
      "pubDate": "Tue, 22 Jul 2025 13:59:34 GMT",
      "isoDate": "2025-07-22T13:59:34.000Z",
      "creator": "Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, Kai Chen",
      "summary": "## 用于视觉-语言慢思考推理的半离策略强化学习 (SOPHIA)\n\n### 引言\n\n提升大型视觉-语言模型（LVLMs）的视觉慢思考推理能力对于解决复杂的多模态任务至关重要。然而，现有方法面临挑战：\n\n*   **在策略强化学习（On-policy RL）**：由于LVLMs主要通过视觉-语言对齐进行训练，其初始能力限制了策略探索空间，难以有效发展慢思考能力。\n*   **离策略强化学习（Off-policy RL）**：直接从外部模型蒸馏轨迹可能导致视觉幻觉，因为不同模型间的视觉感知能力存在不匹配。\n\n### SOPHIA 方法\n\n为解决上述问题，本文提出了一种名为 **SOPHIA** (Semi-Off-Policy RL for vision-language slow-tHInking reAsoning) 的简单且可扩展的半离策略强化学习方法。\n\nSOPHIA 的核心机制包括：\n\n1.  **构建半离策略行为模型**：SOPHIA 结合了可训练LVLM的“在策略视觉理解”能力与语言模型的“离策略慢思考推理”能力。\n2.  **奖励分配与传播**：\n    *   对推理过程分配基于结果的奖励。\n    *   将视觉奖励反向传播。\n3.  **模型学习**：LVLM 通过离策略RL算法，利用获得的推理轨迹和传播的奖励来学习慢思考推理能力。\n\n### 实验与结果\n\n研究团队使用 InternVL2.5 和 InternVL3.0（包括 8B 和 38B 参数规模）进行了广泛实验，结果表明 SOPHIA 具有显著的有效性：\n\n*   **性能提升**：SOPHIA 使 InternVL3.0-38B 的平均性能提升了 8.50%。\n*   **最先进表现**：在多个多模态推理基准测试中，SOPHIA 在开源LVLM中达到了最先进（state-of-the-art, SOTA）的性能。\n*   **超越闭源模型**：在挑战性的 MathVision 和 OlympiadBench 基准测试中，SOPHIA 甚至超越了一些闭源模型（例如 GPT-4.1），分别取得了 49.08% 和 49.95% 的 pass@1 准确率。\n\n### 分析\n\n分析结果显示，SOPHIA 的表现优于传统的监督微调（supervised fine-tuning）和直接的在策略RL方法。这表明 SOPHIA 为后续的在策略训练提供了更好的策略初始化，进一步提升了模型的推理能力。",
      "shortSummary": "SOPHIA是一种半离策略强化学习方法，旨在增强大型视觉-语言模型（LVLMs）的慢思考推理能力。它通过结合LVLM的视觉理解与语言模型推理，并利用奖励传播机制进行训练。实验表明，SOPHIA显著提升了InternVL3.0-38B的性能，在多模态推理基准测试中达到开源LVLM的最先进水平，并在MathVision和OlympiadBench等挑战性任务上超越了部分闭源模型。",
      "translated_title": "用于视觉-语言慢思考推理的半离策略强化学习",
      "images": [],
      "contentSource": "完整文章",
      "content": "Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training."
    },
    {
      "title": "HOComp：交互感知的人体-物体合成 (原标题: HOComp: Interaction-Aware Human-Object Composition)",
      "link": "https://arxiv.org/abs/2507.16813",
      "pubDate": "Tue, 22 Jul 2025 13:59:21 GMT",
      "isoDate": "2025-07-22T13:59:21.000Z",
      "creator": "Dong Liang, Jinyuan Jia, Yuhao Liu, Rynson W. H. Lau",
      "summary": "### HOComp：交互感知的人体-物体合成\n\n**问题背景**\n\n现有的图像引导合成方法在将前景对象插入背景图像的指定区域时，通常难以在涉及人体-物体交互的任务中实现无缝且自然的合成。这些方法在处理人与物体之间的复杂互动时，往往无法确保合成结果的和谐与一致性。\n\n**HOComp 方法概述**\n\n本文提出了一种名为 HOComp 的新颖方法，旨在解决人体-物体合成中的挑战。HOComp 专注于将前景对象合成到以人为中心的背景图像上，同时确保前景对象与背景人物之间存在和谐的交互，并保持两者外观的一致性。\n\n**HOComp 的两大核心设计**\n\n1.  **MLLMs 驱动的基于区域的姿态引导 (MRPG)**：\n    *   **交互区域与类型识别**：利用多模态大型语言模型 (MLLMs) 来识别图像中的交互区域以及具体的交互类型（例如，握持、举起）。\n    *   **粗粒度到细粒度姿态约束**：为生成的交互姿态提供从粗粒度到细粒度的约束。这意味着它首先确定大致的交互姿态，然后进行精细调整。\n    *   **人体姿态关键点整合**：结合人体姿态关键点来跟踪动作变化，并施加更精细的姿态约束，确保交互的准确性。\n\n2.  **细节一致的外观保持 (DCAP)**：\n    *   **统一机制**：DCAP 机制整合了多种技术，以确保合成结果的视觉质量。\n    *   **形状感知注意力调制**：通过形状感知的注意力调制机制，确保前景对象的形状和纹理与背景环境保持一致。\n    *   **多视图外观损失**：引入多视图外观损失，进一步提升合成对象外观的真实感和一致性。\n    *   **背景一致性损失**：通过背景一致性损失，确保背景中的人物能够被忠实地再现，且与合成对象无缝融合。\n\n**新数据集：交互感知的人体-物体合成 (IHOC)**\n\n为了支持和评估这项任务，本文首次提出了一个名为“交互感知的人体-物体合成 (IHOC)”的数据集。该数据集专门用于研究和开发人体-物体交互合成方法。\n\n**实验结果**\n\n在 IHOC 数据集上的实验结果表明，HOComp 能够有效地生成具有和谐人体-物体交互和一致外观的图像。无论是从定性还是定量的角度来看，HOComp 都优于现有的相关方法，证明了其在解决该任务上的卓越性能。",
      "shortSummary": "HOComp 是一种新颖的图像合成方法，旨在解决人体-物体交互合成中现有方法的不足。它通过两大核心设计实现和谐自然的合成：一是“MLLMs 驱动的基于区域的姿态引导 (MRPG)”，利用 MLLMs 识别交互区域和类型，并提供精细的姿态约束；二是“细节一致的外观保持 (DCAP)”，通过形状感知注意力、多视图外观损失和背景一致性损失确保外观真实性。论文还提出了首个“交互感知的人体-物体合成 (IHOC)”数据集。实验证明 HOComp 在生成高质量人体-物体交互方面表现优异。",
      "translated_title": "HOComp：交互感知的人体-物体合成",
      "images": [],
      "contentSource": "完整文章",
      "content": "While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively."
    },
    {
      "title": "MegaScience：推动科学推理后训练数据集的前沿 (原标题: MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning)",
      "link": "https://arxiv.org/abs/2507.16812",
      "pubDate": "Tue, 22 Jul 2025 13:59:03 GMT",
      "isoDate": "2025-07-22T13:59:03.000Z",
      "creator": "Run-Ze Fan, Zengzhi Wang, Pengfei Liu",
      "summary": "### MegaScience：推动科学推理后训练数据集的前沿\n\n**引言与背景**\n科学推理能力对于开发AI科学家和支持人类研究人员在自然科学发现领域取得进展至关重要。然而，开源社区主要关注数学和编码领域，却忽视了科学领域，这在很大程度上是由于缺乏开放、大规模、高质量、可验证的科学推理数据集。\n\n**核心贡献**\n为了弥补这一空白，研究团队提出了两项主要贡献：\n\n1.  **TextbookReasoning 数据集**\n    *   这是一个开放数据集，旨在提供真实、高质量的科学推理数据。\n    *   其参考答案从12,000本大学级别的科学教科书中提取。\n    *   包含65万个推理问题，涵盖7个不同的科学学科。\n\n2.  **MegaScience 数据集**\n    *   这是一个大规模、高质量的开源数据集混合体，总计包含125万个实例。\n    *   该数据集是通过系统的消融研究开发的，旨在评估各种数据选择方法，从而为每个公开可用的科学数据集识别出最优子集。\n\n3.  **综合评估系统**\n    *   研究团队构建了一个全面的评估系统，以确保评估的准确性。\n    *   该系统涵盖了15个基准测试中的多样化主题和问题类型。\n    *   整合了全面的答案提取策略，以确保评估指标的准确性。\n\n**实验结果与性能**\n\n*   **卓越的性能与训练效率**：实验结果表明，与现有开源科学数据集相比，TextbookReasoning 和 MegaScience 数据集在性能和训练效率方面表现出显著优势，并且能够生成更简洁的响应。\n*   **模型性能提升**：研究团队在 MegaScience 数据集上训练了Llama3.1、Qwen2.5和Qwen3系列的基础模型。这些模型在平均性能上显著优于相应的官方指令模型。\n*   **规模效益**：MegaScience 数据集对更大、更强的模型表现出更大的有效性，这表明在科学调优方面存在规模效益。\n\n**社区贡献**\n为了推动科学推理研究的进展，研究团队向社区发布了以下资源：\n*   数据整理管道\n*   评估系统\n*   所有数据集（包括TextbookReasoning和MegaScience）\n*   七个训练好的模型\n\n**图片说明**\n文章内容中未包含有效的实际图片链接，因此摘要中不包含图片。",
      "shortSummary": "该研究旨在解决科学推理领域高质量开放数据集的缺乏。为此，团队推出了TextbookReasoning，一个包含65万个大学级别科学推理问题的开放数据集；并引入了MegaScience，一个包含125万个实例的大规模高质量数据集。通过综合评估系统，实验证明这些数据集显著提升了AI模型在科学推理任务上的性能和训练效率，尤其对大型模型效果更佳。所有数据、系统和模型均已开源，以促进科学推理研究。",
      "translated_title": "MegaScience：推动科学推理后训练数据集的前沿",
      "images": [],
      "contentSource": "完整文章",
      "content": "Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research."
    },
    {
      "title": "通过概念消融微调引导大模型域外泛化 (原标题: Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning)",
      "link": "https://arxiv.org/abs/2507.16795",
      "pubDate": "Tue, 22 Jul 2025 13:45:04 GMT",
      "isoDate": "2025-07-22T13:45:04.000Z",
      "creator": "Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda",
      "summary": "### 概念消融微调（CAFT）：引导大模型域外泛化\n\n**1. 问题背景**\n\n*   大语言模型（LLM）在微调后可能出现意想不到的“域外泛化”（out-of-distribution generalization）问题。\n*   这可能导致模型在通用问题上产生“涌现错位”（emergent misalignment）响应，即给出严重不一致或不恰当的回答。\n\n**2. 传统解决方案及其局限性**\n\n*   标准方法通常依赖于修改训练数据，例如添加更多数据以更明确地指定预期的泛化行为。\n*   然而，这种方法在实践中并不总是可行或方便。\n\n**3. 引入新方法：概念消融微调（CAFT）**\n\n*   本文提出了一种名为“概念消融微调”（Concept Ablation Fine-Tuning, CAFT）的新技术。\n*   CAFT旨在控制LLM从微调中泛化的方式，而无需修改训练数据或使用来自目标分布的数据。\n\n**4. CAFT的工作原理**\n\n*   CAFT利用可解释性工具来识别LLM潜在空间中对应于“不期望概念”的一组方向。\n*   在微调过程中，CAFT通过线性投影来“消融”（ablating）这些不期望的概念。\n*   这种消融操作能够引导模型，使其偏离不期望的泛化行为。\n\n**5. 应用与实验结果**\n\n*   CAFT已成功应用于三个不同的微调任务，其中包括解决“涌现错位”现象。\n*   在不改变任何微调数据的情况下，CAFT将错位响应的发生率降低了10倍。\n*   同时，CAFT并未降低模型在训练分布上的性能。\n\n**6. 结论与意义**\n\n*   CAFT代表了一种新颖的方法，可以在不修改训练数据的前提下，有效引导和控制LLM的泛化行为。\n*   这为解决LLM微调后出现的意外域外泛化问题提供了一条有前景的途径。",
      "shortSummary": "概念消融微调（CAFT）是一种无需修改训练数据即可控制大语言模型（LLM）域外泛化的新方法。它利用可解释性工具识别并消除LLM潜在空间中不期望的概念方向，从而引导模型避免意外泛化。CAFT成功将模型错位响应减少10倍，同时不影响训练性能，为解决LLM泛化问题提供了有效途径。",
      "translated_title": "通过概念消融微调引导大模型域外泛化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data."
    },
    {
      "title": "超越上下文限制：用于长程推理的潜意识线程 (原标题: Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning)",
      "link": "https://arxiv.org/abs/2507.16784",
      "pubDate": "Tue, 22 Jul 2025 13:30:04 GMT",
      "isoDate": "2025-07-22T13:30:04.000Z",
      "creator": "Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass",
      "summary": "### 超越上下文限制：用于长程推理的潜意识线程\n\n本文介绍了一种名为**线程推理模型（TIM）**的新型大型语言模型（LLM）家族，以及配套的推理运行时**TIMRUN**。该系统旨在突破当前LLM在推理准确性和效率方面受到的上下文限制，从而实现更长程、更复杂的推理任务。\n\n**核心问题与解决方案：**\n\n*   **问题：** 现有LLM受限于其上下文窗口大小，这限制了其处理复杂、长程推理任务的能力，并影响了推理的准确性和效率。\n*   **解决方案：** TIM和TIMRUN协同工作，提供了一种递归和分解式的问题解决范式，旨在克服这些限制。\n\n**TIM和TIMRUN的协同优势：**\n\n*   **无限工作记忆：** TIM在TIMRUN上运行时，能够支持几乎无限的工作记忆，从而克服了传统LLM面临的输出限制、位置嵌入约束和GPU内存瓶颈。\n*   **多跳工具调用：** 在单次LLM推理过程中，系统能够实现多跳工具调用，增强了模型的实用性和解决复杂问题的能力。\n*   **性能提升：** 通过将自然语言建模为推理树（同时衡量其长度和深度）而非传统的线性序列，系统显著提升了推理性能。\n\n**工作原理：**\n\n*   **推理树结构：** 推理树由一系列任务组成，每个任务包含思想、递归子任务和基于Schroeder et al, 2025年提出的概念的结论。\n*   **高效工作记忆维护：** 在生成过程中，系统维护一个动态的工作记忆，该记忆仅保留最相关上下文令牌的关键-值（KV）状态。\n*   **子任务剪枝机制：** 通过基于规则的子任务剪枝机制，系统能够智能地选择并保留最相关的令牌，从而在整个推理过程中实现位置嵌入和GPU内存页的复用，极大地提高了内存效率。\n\n**实验结果：**\n\n*   **高推理吞吐量：** 实验结果表明，即使在GPU内存中操作高达90%的关键-值（KV）缓存，该系统也能保持高推理吞吐量。\n*   **准确的数学推理：** 在数学任务上，系统展现出准确的推理能力。\n*   **信息检索挑战：** 能够有效处理需要长程推理和多跳工具使用的复杂信息检索挑战。\n\n**研究状态：**\n\n*   本文是一项研究预览，属于计算与语言（cs.CL）领域。",
      "shortSummary": "本文提出线程推理模型（TIM）及其运行时TIMRUN，旨在突破大型语言模型（LLM）的上下文限制。TIM通过将自然语言建模为推理树，并利用高效的工作记忆管理，实现了几乎无限的工作记忆和多跳工具调用。实验表明，该系统在数学推理和信息检索等长程推理任务中表现出高吞吐量和准确性，有效解决了LLM的上下文瓶颈。",
      "translated_title": "超越上下文限制：用于长程推理的潜意识线程",
      "images": [],
      "contentSource": "完整文章",
      "content": "To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use."
    },
    {
      "title": "Zebra-CoT：一个用于交错视觉语言推理的数据集 (原标题: Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning)",
      "link": "https://arxiv.org/abs/2507.16746",
      "pubDate": "Tue, 22 Jul 2025 12:35:36 GMT",
      "isoDate": "2025-07-22T12:35:36.000Z",
      "creator": "Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum",
      "summary": "## Zebra-CoT：一个用于交错视觉语言推理的数据集\n\n### 引言与背景\n人类在解决复杂问题时，通常会利用视觉辅助，例如图表或草图。然而，训练多模态模型以实现类似的视觉思维链（Visual CoT）能力面临两大挑战：\n\n1.  **现有Visual CoT性能不佳：** 现成的Visual CoT模型表现不理想，这阻碍了强化学习等训练方法的应用。\n2.  **高质量训练数据匮乏：** 缺乏用于Visual CoT训练的高质量数据集。\n\n### Zebra-CoT数据集介绍\n为了解决上述挑战，研究人员引入了 **Zebra-CoT**，这是一个多样化的大规模数据集，旨在促进交错视觉语言推理能力的开发。\n\n*   **规模与构成：** Zebra-CoT包含182,384个样本，每个样本都包含逻辑连贯的交错文本-图像推理轨迹。\n*   **任务类别：** 数据集重点关注四类特别适合草图或视觉推理的任务，包括：\n    *   **科学问题：** 如几何、物理和算法等领域的推理。\n    *   **2D视觉推理任务：** 例如视觉搜索和拼图。\n    *   **3D推理任务：** 包括3D多跳推理、具身（embodied）和机器人规划。\n    *   **视觉逻辑问题与策略游戏：** 例如国际象棋。\n\n### 实验结果与有效性\nZebra-CoT数据集在提升多模态模型性能方面展现出显著效果：\n\n*   **Anole-7B模型：** 在Zebra-CoT训练语料库上对Anole-7B模型进行微调后，其在测试集上的准确率提高了12%，并在标准VLM（视觉语言模型）基准评估中获得了高达13%的性能提升。\n*   **Bagel-7B模型：** 微调Bagel-7B模型后，该模型能够生成高质量的交错视觉推理链。\n\n这些结果充分证明了Zebra-CoT在开发多模态推理能力方面的有效性。\n\n### 开放资源\n为了支持Visual CoT的进一步开发和评估，Zebra-CoT数据集和相关模型均已开源。\n\n### 相关领域\n该研究主要涉及计算机视觉与模式识别（cs.CV）、计算与语言（cs.CL）以及机器学习（cs.LG）等领域。",
      "shortSummary": "Zebra-CoT是一个大型数据集，旨在解决多模态模型中视觉思维链（Visual CoT）训练数据不足的问题。该数据集包含182,384个样本，涵盖科学、2D/3D推理及视觉逻辑等多种任务的交错文本-图像推理轨迹。通过在Zebra-CoT上微调，Anole-7B模型在测试集准确率上提升12%，并在标准VLM基准上获得显著性能增益。Zebra-CoT有效促进了多模态推理能力的发展，其数据集和模型均已开源。",
      "translated_title": "Zebra-CoT：一个用于交错视觉语言推理的数据集",
      "images": [],
      "contentSource": "完整文章",
      "content": "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT."
    },
    {
      "title": "寻找多莉：文本到图像扩散模型中的记忆化比假设的更不局部化 (原标题: Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed)",
      "link": "https://arxiv.org/abs/2507.16880",
      "pubDate": "Tue, 22 Jul 2025 11:02:38 GMT",
      "isoDate": "2025-07-22T11:02:38.000Z",
      "creator": "Antoni Kowalczuk, Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, Franziska Boenisch",
      "summary": "## 文本到图像扩散模型中的记忆化问题与现有缓解策略的局限性\n\n### 摘要\n\n本文探讨了文本到图像扩散模型（DMs）中存在的训练数据记忆化问题，并评估了当前旨在缓解此问题的策略的有效性。研究发现，现有基于剪枝的方法不足以解决记忆化问题，并提出了一种新的对抗性微调方法。\n\n### 背景与问题\n\n*   **扩散模型的成功与挑战**：文本到图像扩散模型在图像生成方面取得了显著成功，但其潜在的训练数据记忆化和复制能力引发了数据隐私和知识产权方面的担忧。\n*   **现有缓解措施**：目前的缓解努力主要集中于识别和剪枝负责触发数据复制的模型权重，其核心假设是记忆化可以被“局部化”到模型的特定部分。\n\n### 研究发现与挑战\n\n*   **剪枝方法的脆弱性**：研究表明，即使在进行剪枝操作后，只需对输入提示的文本嵌入进行微小调整，就足以重新触发数据复制。这突显了现有防御措施的脆弱性。\n*   **记忆化局部性假设的挑战**：本文对记忆化是局部化的这一基本假设提出了质疑。研究发现，数据复制可以从文本嵌入空间中不同的位置被触发，并且在模型内部遵循不同的路径，这表明记忆化并非如先前假设的那样局限于特定区域。\n\n### 结论与未来方向\n\n*   **现有策略的不足**：研究结果明确指出，当前的记忆化缓解策略是不足够的。\n*   **需求与目标**：迫切需要开发能够真正移除记忆化内容的方法，而非仅仅试图抑制其检索。\n*   **提出的解决方案**：作为迈向这一目标的第一步，研究引入了一种新颖的对抗性微调方法。该方法迭代地搜索复制触发器，并更新模型以提高其鲁棒性。\n\n### 贡献\n\n通过这项研究，作者为理解文本到图像扩散模型中记忆化的本质提供了新的见解，并为构建更值得信赖和符合规范的生成式AI奠定了基础。",
      "shortSummary": "文本到图像扩散模型存在训练数据记忆化问题。现有通过剪枝权重来缓解记忆化的方法被证明是脆弱且无效的，因为记忆化并非局部化，即使微调文本嵌入也能重新触发复制。研究挑战了记忆化局部性的假设，并提出需要真正移除记忆化内容的方法。为此，论文引入了一种新的对抗性微调方法，旨在提高模型鲁棒性，为构建更可信赖的生成式AI奠定基础。",
      "translated_title": "寻找多莉：文本到图像扩散模型中的记忆化比假设的更不局部化",
      "images": [],
      "contentSource": "完整文章",
      "content": "Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI."
    },
    {
      "title": "Step-Audio 2 技术报告 (原标题: Step-Audio 2 Technical Report)",
      "link": "https://arxiv.org/abs/2507.16632",
      "pubDate": "Tue, 22 Jul 2025 10:23:55 GMT",
      "isoDate": "2025-07-22T10:23:55.000Z",
      "creator": "Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu",
      "summary": "## Step-Audio 2 技术报告\n\n本文介绍了 Step-Audio 2，一个专为工业级音频理解和语音对话设计的端到端多模态大型语言模型。\n\n### 核心技术与创新\n\n*   **多模态集成**：Step-Audio 2 通过整合潜在音频编码器和以推理为中心的强化学习（RL），在自动语音识别（ASR）和音频理解方面取得了显著性能。\n*   **增强语音对话**：为了实现真正的端到端语音对话，Step-Audio 2 将离散音频令牌的生成融入语言建模中，显著增强了其对语调信息（如说话风格和情感）的响应能力。\n*   **知识利用与工具调用**：\n    *   模型集成了检索增强生成（RAG）技术，以有效利用真实世界数据中丰富的文本和声学知识。\n    *   Step-Audio 2 能够调用外部工具，例如网络搜索以减少幻觉，以及音频搜索以切换音色。\n\n### 训练与性能\n\n*   **大规模训练**：Step-Audio 2 在数百万小时的语音和音频数据上进行训练，使其能够在各种对话场景中展现出智能和表现力。\n*   **领先的性能**：评估结果表明，与现有的开源和商业解决方案相比，Step-Audio 2 在各种音频理解和对话基准测试中均达到了最先进的性能。",
      "shortSummary": "Step-Audio 2 是一个端到端的多模态大型语言模型，专为工业级音频理解和语音对话设计。它通过整合潜在音频编码器、推理强化学习和离散音频令牌生成，增强了对语调信息的响应。模型还集成了检索增强生成（RAG）并能调用外部工具。Step-Audio 2 在数百万小时数据上训练，并在音频理解和对话基准测试中取得了最先进的性能。",
      "translated_title": "Step-Audio 2 技术报告",
      "images": [],
      "contentSource": "完整文章",
      "content": "This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information."
    },
    {
      "title": "Re:Form -- 在LLM中利用强化学习减少人类先验知识以实现可扩展的正式软件验证：一项基于Dafny的初步研究 (原标题: Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny)",
      "link": "https://arxiv.org/abs/2507.16331",
      "pubDate": "Tue, 22 Jul 2025 04:13:01 GMT",
      "isoDate": "2025-07-22T04:13:01.000Z",
      "creator": "Chuanhao Yan, Fengdi Che, Xuhan Huang, Xu Xu, Xin Li, Yizhi Li, Xingwei Qu, Jingzhe Shi, Zhuangzhuang He, Chenghua Lin, Yaodong Yang, Binhang Yuan, Hang Zhao, Yu Qiao, Bowen Zhou, Jie Fu",
      "summary": "## Re:Form：利用强化学习在LLM中减少人类先验知识以实现可扩展的正式软件验证\n\n### 研究背景与挑战\n\n*   **现有LLM的局限性**：当前基于非正式语言（如人类语言）的大型语言模型（LLMs），尽管通过强化学习（RL）进行训练，但在软件验证过程中面临显著挑战。它们的验证过程既不可靠也难以扩展，导致主流的专有模型难以生成可验证的程序。\n*   **人类先验知识的成本**：为了诱导LLMs的推理和编码能力，通常需要使用人类标注的思维链（chain-of-thought）和其他人类先验知识。然而，对于复杂的编程任务，提供这些先验知识的成本高昂且耗时，变得难以接受。\n\n### 提出的解决方案\n\n*   **基于形式语言的推理**：本研究提出了一种有前景的替代方案——将LLMs根植于严格的形式系统。通过让生成模型在形式语言空间（如Dafny）中操作，可以实现对其推理过程和结果的自动、数学可证明的验证。这一能力对于实现大规模、可靠的正式软件验证至关重要。\n*   **减少人类先验知识的方法**：本工作系统地探索了如何利用形式语言Dafny作为主要环境来减少人类先验知识。其核心流程包括：\n    *   引入一个自动化且可扩展的数据整理（data curation）流程。\n    *   精心设计的强化学习（RL）方案，并整合了来自形式语言验证器的反馈。\n\n### DafnyComp基准测试\n\n*   **新基准的引入**：研究引入了DafnyComp，这是一个由组合式形式程序组成的基准测试集，其规范（specifications）经过了自动形式化处理，用于评估规范推理能力。\n\n### 实验结果与性能\n\n*   **监督微调（SFT）阶段**：即使是小型模型（例如0.5B参数），经过监督微调后，也能够生成语法有效且可验证的Dafny代码，其性能甚至超越了专有模型。\n*   **强化学习（RL）的提升**：结合正则化的强化学习进一步提升了模型的性能，使其在域外任务（out-of-domain tasks）上表现出更强的泛化能力。\n*   **基准测试表现**：该方法在具有挑战性的DafnyComp基准测试上，超越了所有强大的基线模型。\n\n### 结论\n\n这项初步研究展示了通过将LLMs与形式语言系统结合，并利用自动化数据整理和强化学习，可以有效减少对昂贵人类先验知识的依赖，从而为实现可扩展、可靠的正式软件验证开辟了新的途径。",
      "shortSummary": "现有LLM在软件验证中面临可靠性和可扩展性挑战，且过度依赖昂贵的人类先验知识。本研究提出“Re:Form”方法，通过将LLM根植于Dafny等形式语言，并结合自动化数据整理和强化学习，减少人类先验。研究引入DafnyComp基准，结果显示即使小型模型也能生成可验证的Dafny代码，且RL进一步提升了泛化能力，在DafnyComp上超越了现有基线，为可扩展的正式软件验证提供了新方向。",
      "translated_title": "Re:Form -- 在LLM中利用强化学习减少人类先验知识以实现可扩展的正式软件验证：一项基于Dafny的初步研究",
      "images": [],
      "contentSource": "完整文章",
      "content": "Existing informal language-based (e.g., human language) Large Language Models (LLMs) trained with Reinforcement Learning (RL) face a significant challenge: their verification processes, which provide crucial training signals, are neither reliable nor scalable. In fact, the prevalent large proprietary models could hardly generate verifiable programs. A promising yet largely uncharted alternative is formal language-based reasoning. Grounding LLMs in rigorous formal systems where generative models operate in formal language spaces (e.g., Dafny) enables the automatic and mathematically provable verification of their reasoning processes and outcomes. This capability is pivotal for achieving large-scale, reliable formal software verification. It is a common practice to employ human-annotated chain-of-thought and other human priors to induce the reasoning and coding capabilities of LLMs. Unfortunately, it becomes unacceptably all-consuming to provide such priors for supervising complex programming tasks. In this work, we systematically explore ways to reduce human priors with the formal language, Dafny, as the main environment for our pilot study. Our pipeline mainly relies on introducing an automatic and scalable data curation pipeline, and careful RL designs integrated with feedback from the formal language verifier. We introduce DafnyComp, a benchmark of compositional formal programs with auto-formalized specifications for specification reasoning. Our supervised fine-tuning (SFT) stage enables even small models (e.g., 0.5B) to generate syntactically valid and verifiable Dafny code, surpassing proprietary models. RL with regularization further improves performance, achieving stronger generalization to out-of-domain tasks and outperforming all strong baselines on the challenging DafnyComp benchmark."
    },
    {
      "title": "像素、模式，但缺乏诗意：像人类一样感知世界 (原标题: Pixels, Patterns, but No Poetry: To See The World like Humans)",
      "link": "https://arxiv.org/abs/2507.16863",
      "pubDate": "Mon, 21 Jul 2025 17:50:16 GMT",
      "isoDate": "2025-07-21T17:50:16.000Z",
      "creator": "Hongcheng Gao, Zihao Huang, Lin Xu, Jingyi Tang, Xinhao Li, Yue Liu, Haoyang Li, Taihang Hu, Minhua Lin, Xinlong Yang, Ge Wu, Balong Bi, Hongyu Chen, Wentao Zhang",
      "summary": "## 像素、模式，但缺乏诗意：像人类一样感知世界\n\n### 引言与挑战\n\n在人工智能领域，多模态大型语言模型（MLLM）实现类人感知和推理能力仍然是一个核心挑战。尽管近期研究主要集中于增强MLLM的推理能力，但一个根本性问题依然存在：多模态大型语言模型能否真正像人类一样感知世界？本文将研究焦点从推理转向感知。\n\n### 图灵眼测试（TET）基准\n\n为了评估MLLM的感知能力，研究团队引入了**图灵眼测试（TET）**。这是一个以感知为导向的挑战性基准，旨在评估MLLM在人类能够直观处理的合成图像上的表现。TET包含四个诊断任务，专门用于揭示模型在视觉感知方面的局限性。\n\n### 主要发现\n\n研究结果揭示了当前最先进的MLLM在TET的感知任务上存在**灾难性失败**，而这些任务对人类来说是微不足道的。具体发现包括：\n\n*   **上下文学习（In-context learning）**：这种在先前基准测试中对语言骨干有效的学习方法，未能改善模型在TET任务上的表现。\n*   **基于语言骨干的训练**：同样，针对语言骨干的训练方法也未能提升感知性能。\n*   **视觉塔微调**：与上述方法相反，对视觉塔（vision tower）进行微调能够使模型快速适应TET任务，显著改善其表现。\n\n### 核心结论\n\n这些发现强烈表明，TET基准对**视觉塔的泛化能力**提出了挑战，而非语言骨干的知识和推理能力。这揭示了当前MLLM与人类感知之间的一个关键差距：模型在处理直观视觉信息时，其视觉组件的泛化能力是主要瓶颈。\n\n### 未来工作\n\n本版本发布了TET任务的代表性子集。未来，研究团队将引入更多样化的任务和方法，以进一步增强MLLM的视觉泛化能力。",
      "shortSummary": "本文关注多模态大型语言模型（MLLM）的类人感知能力。研究引入了“图灵眼测试”（TET）基准，旨在评估MLLM在人类直观处理的合成图像上的感知表现。结果显示，当前最先进的MLLM在这些感知任务上表现出灾难性失败。研究发现，问题主要在于视觉塔的泛化能力，而非语言骨干的推理能力。这揭示了当前MLLM与人类感知之间在视觉泛化方面的关键差距。",
      "translated_title": "像素、模式，但缺乏诗意：像人类一样感知世界",
      "images": [],
      "contentSource": "完整文章",
      "content": "Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work."
    },
    {
      "title": "更多的推理时间计算真的有助于鲁棒性吗？ (原标题: Does More Inference-Time Compute Really Help Robustness?)",
      "link": "https://arxiv.org/abs/2507.15974",
      "pubDate": "Mon, 21 Jul 2025 14:08:38 GMT",
      "isoDate": "2025-07-21T14:08:38.000Z",
      "creator": "Tong Wu, Chong Xiang, Jiachen T. Wang, Weichen Yu, Chawin Sitawarin, Vikash Sehwag, Prateek Mittal",
      "summary": "本文探讨了增加推理时间计算对模型鲁棒性的影响，并揭示了其复杂性，尤其是在对抗性环境中。\n\n*   **背景与初步发现**\n    *   近期研究（Zaremba et al.）表明，增加大型专有推理LLM的推理时间计算可以提高其鲁棒性。\n    *   本文首先证实，小规模的开源模型（如DeepSeek R1、Qwen3、Phi-reasoning）也能通过简单的预算强制策略从推理时间扩展中受益。\n\n*   **关键假设的揭示与安全风险**\n    *   本文揭示并批判性地审视了先前工作中的一个隐含假设：即中间推理步骤对攻击者是隐藏的。\n    *   通过放宽这一假设，研究发现了一个重要的安全风险：如果中间推理步骤变得明确可访问，增加推理时间计算反而会持续降低模型的鲁棒性。这一现象被经验性地验证为一种“逆向扩展定律”。\n\n*   **实际应用中的脆弱性**\n    *   即使推理链是隐藏的，模型在某些实际场景中仍然容易受到攻击，例如：\n        *   集成工具的推理模型。\n        *   高级推理提取攻击。\n\n*   **结论与建议**\n    *   研究结果共同表明，推理时间扩展带来的鲁棒性益处，在很大程度上取决于对抗环境和部署上下文。\n    *   作者敦促从业者在将推理时间扩展应用于安全敏感的实际应用之前，仔细权衡这些微妙的权衡。",
      "shortSummary": "本文研究了增加推理时间计算对模型鲁棒性的影响。研究发现，小规模开源模型也能从中受益。然而，如果中间推理步骤对攻击者可见，增加推理时间计算反而会降低模型鲁棒性，这构成了一个重要的安全风险。研究强调，推理时间扩展的鲁棒性益处高度依赖于对抗环境和部署上下文，从业者在安全敏感应用中应谨慎权衡。",
      "translated_title": "更多的推理时间计算真的有助于鲁棒性吗？",
      "images": [],
      "contentSource": "完整文章",
      "content": "Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications."
    },
    {
      "title": "潜在去噪造就优秀的视觉分词器 (原标题: Latent Denoising Makes Good Visual Tokenizers)",
      "link": "https://arxiv.org/abs/2507.15856",
      "pubDate": "Mon, 21 Jul 2025 13:59:56 GMT",
      "isoDate": "2025-07-21T13:59:56.000Z",
      "creator": "Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, Yue Wang",
      "summary": "### 视觉分词器与潜在去噪\n\n**1. 背景与问题**\n\n*   尽管视觉分词器在生成模型中扮演着基础性角色，但目前尚不清楚哪些特性能够使其在生成建模中更有效。\n\n**2. 核心观察与洞察**\n\n*   研究人员观察到，现代生成模型共享一个概念上相似的训练目标：从高斯噪声或掩码等损坏输入中重建干净信号。这一过程被研究人员称为“去噪”（denoising）。\n*   受此启发，研究人员提出将分词器嵌入（tokenizer embeddings）直接与下游的去噪目标对齐。这种对齐旨在鼓励潜在嵌入（latent embeddings）即使在严重损坏的情况下也能更容易地被重建。\n\n**3. 提出的方法：潜在去噪分词器 (l-DeTok)**\n\n*   为了实现上述目标，文章引入了一种简单而有效的视觉分词器——潜在去噪分词器（Latent Denoising Tokenizer, l-DeTok）。\n*   **工作原理：** l-DeTok 的训练目标是从被插值噪声（interpolative noise）和随机掩码（random masking）损坏的潜在嵌入中重建干净图像。\n\n**4. 实验结果与性能**\n\n*   研究人员在 ImageNet 256x256 数据集上进行了广泛的实验。\n*   实验结果表明，l-DeTok 在六种代表性的生成模型中，始终优于标准的分词器。\n\n**5. 结论与未来展望**\n\n*   研究发现强调了“去噪”作为分词器开发的一个基本设计原则。\n*   作者希望这一发现能够为未来的分词器设计提供新的视角和启发。",
      "shortSummary": "该研究提出“潜在去噪”是提升视觉分词器效果的关键原则。文章引入了潜在去噪分词器（l-DeTok），它通过从损坏的潜在嵌入中重建图像进行训练。在ImageNet 256x256上的广泛实验表明，l-DeTok在多种生成模型中表现优于标准分词器。这强调了去噪作为分词器设计的基本原则，为未来发展提供了新视角。",
      "translated_title": "潜在去噪造就优秀的视觉分词器",
      "images": [],
      "contentSource": "完整文章",
      "content": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design."
    },
    {
      "title": "SeC：通过渐进式概念构建推进复杂视频对象分割 (原标题: SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction)",
      "link": "https://arxiv.org/abs/2507.15852",
      "pubDate": "Mon, 21 Jul 2025 13:59:02 GMT",
      "isoDate": "2025-07-21T13:59:02.000Z",
      "creator": "Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang",
      "summary": "## SeC：通过渐进式概念构建推进复杂视频对象分割\n\n### 引言\n\n视频对象分割（VOS）是计算机视觉领域的核心任务，要求模型在视频帧中跟踪和分割目标对象。尽管现有技术取得了显著进展，但在处理剧烈视觉变化、遮挡和复杂场景变化方面，当前技术仍远不及人类能力。这种局限性源于它们过度依赖外观匹配，而忽略了人类在跨时间动态中实现鲁棒识别所依赖的、类似人类的物体概念理解。\n\n### SeC框架：概念驱动的分割\n\n为弥补这一差距，本文提出了 **Segment Concept (SeC)**，一个概念驱动的分割框架。SeC 的核心思想是从传统的特征匹配转向高层、以对象为中心的表示的渐进式构建和利用。\n\n*   **构建鲁棒概念先验：** SeC 利用大型视觉-语言模型（LVLMs）整合来自不同帧的视觉线索，从而构建出鲁棒的概念先验。\n*   **推理阶段的鲁棒分割：** 在推理过程中，SeC 基于已处理的帧形成目标的全面语义表示，从而实现后续帧的鲁棒分割。\n*   **自适应平衡：** SeC 能够自适应地平衡基于 LVLM 的语义推理与增强的特征匹配，根据场景复杂性动态调整计算工作量。\n\n### SeCVOS 基准：评估复杂场景下的概念理解\n\n为了严格评估 VOS 方法在需要高层概念推理和鲁棒语义理解的场景中的表现，本文引入了 **语义复杂场景视频对象分割（SeCVOS）** 基准。\n\n*   **构成：** SeCVOS 包含 160 个手动标注的多场景视频，这些视频旨在通过显著的外观变化和动态场景转换来挑战模型。\n\n### 实验结果\n\nSeC 在 SeCVOS 基准上比 SAM 2.1 取得了 11.8 点的显著提升，这确立了 SeC 在概念感知视频对象分割领域的新技术水平（State-of-the-Art, SOTA）。",
      "shortSummary": "SeC（Segment Concept）是一种新的概念驱动视频对象分割（VOS）框架，旨在解决现有方法在处理复杂场景时缺乏人类概念理解的局限。SeC利用大型视觉-语言模型（LVLMs）构建高层、以对象为中心的语义表示，并自适应地平衡语义推理与特征匹配。为评估概念感知VOS，研究者引入了SeCVOS基准。SeC在SeCVOS上比SAM 2.1提升11.8点，达到了新的技术水平。",
      "translated_title": "SeC：通过渐进式概念构建推进复杂视频对象分割",
      "images": [],
      "contentSource": "完整文章",
      "content": "Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation."
    },
    {
      "title": "GUI-G^2: 用于GUI定位的高斯奖励建模 (原标题: GUI-G^2: Gaussian Reward Modeling for GUI Grounding)",
      "link": "https://arxiv.org/abs/2507.15846",
      "pubDate": "Mon, 21 Jul 2025 13:53:42 GMT",
      "isoDate": "2025-07-21T13:53:42.000Z",
      "creator": "Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang",
      "summary": "# GUI-G^2: 用于GUI定位的高斯奖励建模\n\n## 引言：GUI定位及其现有挑战\n图形用户界面（GUI）定位是一项关键任务，旨在将自然语言指令映射到精确的界面位置，以实现自主交互。当前基于强化学习的方法通常采用二元奖励机制，将元素视为“命中或未命中”的目标。这种方法产生稀疏的信号，未能充分捕捉空间交互的连续性，从而限制了模型的学习效率和性能。\n\n## GUI-G^2：高斯奖励框架的提出\n受人类点击行为自然形成以目标元素为中心的高斯分布的启发，本文引入了GUI高斯定位奖励（GUI-G^2）框架。这是一个原则性的奖励框架，将GUI元素建模为界面平面上的连续高斯分布。\n\n### GUI-G^2 的协同机制\nGUI-G^2 框架融合了两种协同机制，以提供更丰富、更密集的奖励信号：\n1.  **高斯点奖励（Gaussian Point Rewards）**：\n    *   通过以元素质心为中心的指数衰减分布来建模精确的定位。\n    *   这种机制鼓励模型精确地指向目标元素的中心区域。\n2.  **覆盖奖励（Coverage Rewards）**：\n    *   通过测量预测高斯分布与目标区域之间的重叠程度来评估空间对齐。\n    *   这确保了模型不仅关注中心点，还能考虑到目标元素的整体形状和大小。\n\n### 自适应方差机制\n为了有效处理不同尺寸的GUI元素，GUI-G^2 开发了一种自适应方差机制。该机制根据元素的尺寸校准奖励分布的方差，确保奖励信号能够适应各种大小的交互目标，从而提高模型的泛化能力和精度。\n\n## 优势与影响\nGUI-G^2 框架将GUI定位任务从稀疏的二元分类问题转化为密集的连续优化问题。高斯分布能够生成丰富的梯度信号，有效引导模型学习最佳的交互位置。\n\n## 实验验证与结果\n研究人员在ScreenSpot、ScreenSpot-v2和ScreenSpot-Pro等基准测试上进行了广泛的实验。结果表明，GUI-G^2 显著优于当前最先进的方法UI-TARS-72B，其中在ScreenSpot-Pro上的性能提升最为显著，达到了24.7%。\n\n## 结论\n分析表明，连续建模为界面变化提供了卓越的鲁棒性，并增强了对未见布局的泛化能力。GUI-G^2 为GUI交互任务中的空间推理建立了一个新的范式。",
      "shortSummary": "GUI-G^2 提出了一种用于GUI定位的新型高斯奖励模型，旨在解决现有强化学习方法中稀疏二元奖励的问题。该模型将GUI元素建模为连续高斯分布，结合高斯点奖励和覆盖奖励，并采用自适应方差机制。这使得GUI定位从稀疏分类转变为密集连续优化，生成丰富的梯度信号。实验证明，GUI-G^2 在ScreenSpot基准测试中显著优于现有技术，尤其在ScreenSpot-Pro上性能提升24.7%，增强了模型对界面变化的鲁棒性和泛化能力。",
      "translated_title": "GUI-G^2: 用于GUI定位的高斯奖励建模",
      "images": [],
      "contentSource": "完整文章",
      "content": "Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks."
    },
    {
      "title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计 (原标题: LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra)",
      "link": "https://arxiv.org/abs/2507.15815",
      "pubDate": "Mon, 21 Jul 2025 13:21:14 GMT",
      "isoDate": "2025-07-21T13:21:14.000Z",
      "creator": "Seth Karten, Wenzhe Li, Zihan Ding, Samuel Kleiner, Yu Bai, Chi Jin",
      "summary": "# LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计\n\n本文介绍了“LLM经济学家”（LLM Economist），这是一个新颖的框架，利用基于智能体的建模方法，在具有层级决策的策略环境中设计和评估经济政策。\n\n## 框架结构与智能体角色\n\n该框架采用双层结构，模拟经济系统中的决策过程：\n\n*   **下层：有限理性工人智能体**\n    *   这些智能体被实例化为基于“人物画像”的提示，其数据来源于经过美国人口普查校准的收入和人口统计数据。\n    *   它们通过最大化在上下文中学习到的基于文本的效用函数来选择劳动供给。\n*   **上层：规划者智能体**\n    *   该智能体利用上下文强化学习（in-context reinforcement learning）来提出分段线性边际税率表。\n    *   这些税率表以当前美国联邦税级为锚点。\n\n## 经济模拟器的关键能力\n\n这种构建方式赋予了经济模拟器进行可靠财政实验所需的三项关键能力：\n\n1.  **异质性效用优化：** 能够优化不同智能体的异质性效用函数。\n2.  **大规模人口生成：** 能够原则性地生成大规模、符合人口统计学现实的智能体群体。\n3.  **自然语言机制设计：** 机制设计（即终极的“助推”问题）完全以自然语言表达。\n\n## 实验结果与政策含义\n\n研究人员对多达一百个交互智能体的群体进行了实验，结果显示：\n\n*   规划者智能体收敛到接近斯塔克尔伯格均衡（Stackelberg equilibria）的状态。\n*   相对于Saez解决方案，这种均衡显著改善了总社会福利。\n*   在去中心化治理下，周期性、基于人物画像的投票程序进一步提升了这些收益。\n\n## 结论与重要性\n\n这些结果表明，基于大型语言模型的智能体能够联合建模、模拟和治理复杂的经济系统。这为社会层面的政策评估提供了一个可操作的测试平台，有助于构建更美好的文明。",
      "shortSummary": "“LLM经济学家”是一个创新框架，利用基于大型语言模型的智能体进行经济政策设计与评估。它通过模拟具有层级决策的多智能体系统实现此目标：下层工人智能体优化劳动供给，上层规划者智能体设计税收政策。该框架能生成大规模、真实的人口模型，并以自然语言进行机制设计。实验表明，其设计的政策能改善总社会福利。这为社会规模的政策评估提供了一个可行的测试平台。",
      "translated_title": "LLM经济学家：多智能体生成模拟中的大规模人口模型与机制设计",
      "images": [],
      "contentSource": "完整文章",
      "content": "We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations."
    }
  ],
  "lastUpdated": "2025-07-24T09:36:06.683Z"
}